Should We Always Separate?: Switching Between Enhanced and Observed Signals for Overlapping Speech Recognition
Hiroshi Sato, Tsubasa Ochiai, Marc Delcroix, Keisuke Kinoshita, Takafumi Moriya, Naoyuki Kamo
NTT Corporation, Japan
hiroshi.satou.bh@hco.ntt.co.jp

arXiv:2106.00949v1 [eess.AS] 2 Jun 2021

Abstract
Although recent advances in deep learning technology improved automatic speech recognition (ASR), it remains difficult to recognize speech when it overlaps other people's voices. Speech separation or extraction is often used as a front-end to ASR to handle such overlapping speech. However, deep neural network-based speech enhancement can generate `processing artifacts' as a side effect of the enhancement, which degrades ASR performance. For example, it is well known that single-channel noise reduction for non-speech noise (nonoverlapping speech) often does not improve ASR. Likewise, the processing artifacts may also be detrimental to ASR in some conditions when processing overlapping speech with a separation/extraction method, although it is usually believed that separation/extraction improves ASR. In order to answer the question `Do we always have to separate/extract speech from mixtures?', we analyze ASR performance on observed and enhanced speech at various noise and interference conditions, and show that speech enhancement degrades ASR under some conditions even for overlapping speech. Based on these findings, we propose a simple switching algorithm between observed and enhanced speech based on the estimated signal-to-interference ratio and signal-to-noise ratio. We demonstrated experimentally that such a simple switching mechanism can improve recognition performance when processing artifacts are detrimental to ASR. Index Terms: input switching, speech extraction, speech separation, noise robust speech recognition
1. Introduction
Recent advances in deep learning technology greatly improved the ability of speech recognition to recognize speech [1, 2]. However, it remains challenging to recognize speech under severe conditions such as speech overlapping with interfering speech (speech noise) [3] and/or background noise (nonspeech noise) [4]. To mitigate the adverse effect of interfering speech and noise, speech-enhancement technologies, including speech separation, target speech extraction, and noise reduction have widely been introduced and investigated as a front-end of ASR [5­7].
Speech separation and speech extraction are proposed to deal with an overlapping speech of multiple speakers. Speech separation [8, 9] enables speech recognition of each speaker's voice by separating all speakers in the observed speech mixture. Target speech extraction [10] enables the recognition of the target speaker's voice by extracting only his/her voice from the observed mixture. Various approaches have been proposed for speech separation and extraction so far, and even in single-channel setup, they realized drastic improvement on overlapping speech recognition by dealing with interfering speech [5, 6, 11­13].

Noise reduction is another technology to deal mainly with background noise. For multichannel setup, beamformer has been shown to be very effective to improve ASR performance because it introduces only few `processing artifacts' [4, 14, 15]. On the other hand, for single-channel setup, it is well-known that noise reduction technologies tend to degrade ASR performance due to the processing artifacts introduced by non-linear transformations in neural network [16­18]. Especially recent ASR systems trained on data covering various noise conditions tend to be less affected by non-speech noise than by the artifacts.
Although many have reported performance improvement using speech separation or extraction in overlapping conditions, we wonder if in some conditions the artifacts induced by these enhancement methods may be detrimental to ASR. In other words, should we always separate/extract speech from mixtures? In this work, we focus on the single-channel speech enhancement, especially single-channel `speech extraction' problem. To investigate the above question, we evaluate the ASR performance on the `observed mixture' and `enhanced speech' at various noise and interference conditions. As a result, we show that depending on the balance of signal-to-interference ratio (SIR) and signal-to-noise ratio (SNR), ASR performance on observed mixtures outperforms that on enhanced speech. Besides, we also demonstrate that we can improve ASR performance by simply switching between the observed mixture and enhanced speech based on estimated SIR and SNR.
Indeed speech separation/extraction is a strong way to deal with overlapping speech, but the results suggest that we should consider where to apply it. The results obtained in this work showed a new strategy for further improving ASR performance for overlapping speech and present a new direction of speech separation/extraction research as the ASR front-end.
2. Related Works
In this work, we propose a switching method between observed mixture and enhanced speech for overlapping speech. Similarly, a preceding work called Voice Filter Light [19] switched observed mixture and enhanced speech to improve ASR results.
In the preceding work, they improved the ASR performance on the non-overlapping region by decreasing the enhancement strength for the absence of interfering speakers, according to the assumption that speech extraction hurt ASR performance for non-overlapping regions.
Indeed both preceding work and our work share the common idea of switching between enhanced and observed speech, they are different in that the preceding work used the observed mixture for non-overlapping region, while our work use it even for overlapping region. Therefore, both approaches would be complementary.
Although we conducted experiments on the almost fully

overlapping dataset to validate the concept, we plan to investigate the application for the partially overlapping dataset by combining our proposal with switching for non-overlapping region [19] in future works.
Other approaches to mitigate the effect of processing artifacts include re-training of the ASR module on enhanced speech or performing joint-training of the speech enhancement and ASR. Unlike our proposed method, these approaches often imply modifying the ASR back-end, which may not be possible in many practical applications. Besides, to the best of our knowledge, there has not been extensive investigation comparing the performance of the enhancement with the observed mixture for various conditions for joint-training of speech separation/extraction and ASR, and thus the proposed switching approach may still be useful in some conditions. Investigation of the switching mechanism in joint-training conditions including optimization of the switching parameters with ASR criterion will be part of our future works.

3. Proposed Method
In this section, we introduce the ASR pipeline that we use for our experiments and describe the proposed switching approach.

3.1. Overview of target speech recognition pipeline

In this paper, we assume that a speech signal is captured by a
single microphone. The single-channel observed mixture Y  RT is given by:

Y = S + I + N,

(1)

where S denotes the target signal, I denotes a signal that contains interfering speakers, and N denotes the background noise. T is the number of samples of the observed waveform.
The target speech extraction framework aims at extracting the signal S of the target speaker from the observed mixture Y [5,6]. In target speech extraction, it is assumed that an `auxiliary information' about the target speaker is available to inform the network of which speaker to extract. Hereafter the auxiliary information is referred to as `target speaker clues'. The target speaker clue is typically given as a waveform of a pre-recorded enrollment utterance of the target speaker, which is denoted as CS  R where  is the number of samples of the enrollment waveform. Given the observed mixture and the target speaker clue, the target signal is estimated as follows:

S^ = SE(Y, CS)

(2)

where S^ denotes the estimated signal of the target speaker and SE(·) denotes the speech extraction module.
The target speech extraction can be utilized as a front-end for ASR to recognize the target speech in the mixture. In ASR pipeline for the target speaker, the extracted signal S^ is used as an input to the ASR module as follows:

W^ = ASR(S^),

(3)

where W^ denotes the estimated transcription of the target speaker and ASR(·) denotes the ASR module, which transforms the waveform into its corresponding transcription.

3.2. SNR and SIR-based input signal switching
Target speech extraction benefits ASR by removing interfering speech that hinders the ASR performance even for ASR models trained robust for noise. However, it is possible that the

Figure 1: The overview of ASR pipeline with proposed switching.

target speech extraction is detrimental to ASR in some conditions, because of 1) processing artifacts produced by non-linear transformations in target speech extraction and 2) the capability of ASR to recognize the dominant speaker. It is empirically known that ASR can recognize only the dominant speaker even under the presence of the interfering speaker under relatively moderate SIR conditions.
In our experiment, we actually observed that sometimes ASR can perform well on observed mixture Y than on enhanced speech S^ according to the balance between SIR and SNR (see experiments in Section 4.3.1). This finding suggests that we can improve ASR performance if we can correctly switch between the observed mixture and enhanced speech for the input of ASR, instead of always using enhanced speech for overlapping input. In this paper, as an initial investigation for the switching between enhanced and observed signals, we propose a rule-based switching algorithm based on estimated SIR and SNR.
We define the SIR and SNR as:

||S||2

||S||2

SIR := 10 log10 ||I||2 , SNR := 10 log10 ||N||2 (4)

where || · || denotes the L2 norm. Based on SIR and SNR, the ASR input S~ is switched according to the following rule:

S~ = SE(Y, CS) (f (SIR, SNR) < )

(5)

Y

(otherwise)

where f (·) denotes the conditional function and  denotes a threshold to control the input switching. In this paper's experiment, we simply define the conditional function as follows:
||N||2 f (SIR, SNR) := SIR - SNR = 10 log10 ||I||2 . (6)
This rule reflects the observation that the enhanced speech tends to benefit ASR as the interfering speech I becomes more dominant, and that the observed mixture tends to benefit ASR as the noise N becomes more dominant. Even if the interfering speaker is large to some extent, this rule suggests choosing observed mixtures as the input of ASR if noise is also large. The way to estimate f (SIR, SNR) score is described in Section 3.3.
Although we investigated a simple rule-based switching to show the possibility of selective use of observed mixtures instead of enhanced speech, further improvement could be achieved by using more advanced rules or introducing learningbased switching (refer to Section 4.3.2 for more detailed explanations for prospects).

3.3. Speech extraction-based SNR and SIR estimation
In order to implement switching mechanism in Eq. (5), the score f (SIR, SNR) defined in Eq. (6) has to be estimated.
In this work, we assume that the speaker clue of the `interfering speaker' CI is also available to estimate the interfering speech ^I by applying target speech extraction expressed in

Eq. (2). For noise, we assume that the noise signal power is almost stable during a speech segment. We used the region of mixture where both target and interfering speakers are inactive for noise L2 norm calculation. We could eliminate the use of the interfering speaker clue by introducing noise prediction head to the SpeakerBeam structure, which is a part of our future works.
Since the duration of the region used for the calculation of L2 norm are different between the interference and noise, we substituted L2 norm in Eq. (6) with the average signal power over time || · ||2/T where T denotes the length of the regions used for L2 norm calculation. The active region of target and interference were estimated by multi-class voice activity detection explained in Section 4.1.
4. Experiments
The experimental results will be presented in two parts: the analysis part and the evaluation of the switching part. In the analysis part, we show the result of ASR performance comparison on observed mixture and enhanced speech at various SIR, SNR, and noise type combinations. In the evaluation of the switching part, the ASR performance with the proposed switching method is presented and compared with those on observed mixture and enhanced speech. The threshold for the switching in Eq. (5) was experimentally determined as  = 10 according to the character error rate (CER) for observed mixture and enhanced speech on the development set. For both parts, we evaluated CER as a speech recognition performance metric. All experiments were performed on speech recordings simulated based on Corpus of Spontaneous Japanese (CSJ) [20] and CHiME-4 [21] corpora, sampled at 16 kHz.
4.1. Target Speech Extraction
Dataset: For training of target speech extraction model and evaluation of target speech recognition pipeline, we created two-speaker and noise mixtures generated by mixing utterances and noise. For training, two-speaker mixtures are created by randomly selecting SIR values between -5 to 5 dB, in a similar way as widely used WSJ0-2mix corpus [22]. To generate noisy mixtures, we randomly selected a noise signal out of four noise environments of CHiME-4 noise: street, cafe, bus, and pedestrian, and added it to every mixture at an SNR value randomly selected between 0 and 20 dB. For the target speaker clue, we randomly chose an utterance of the target speaker different from that in the mixture. The training set consisted of 50,000 mixtures from 937 speakers. The development set consisted of 5,000 mixtures from 49 speakers. For evaluation, we created mixtures at various SIR between 0 and 20 dB and SNR between 0 and 20 dB for each type of noise environment. Every evaluation dataset has 1,000 mixtures from 30 speakers, and each dataset has the same combination of speakers and utterances with only differences in SNR, SIR, and noise types. The evaluation dataset consists of almost fully overlapping conditions with an overlap ratio of about 94 % on average.
Network configuration: In this experiment, we adopted a time-domain speakerBeam structure [23,24] as a speech extraction module in Eq. (2). The time-domain SpeakerBeam conducts time-domain speech extraction, which has been shown to outperform time-frequency mask-based approaches. Thus, the input feature is a waveform. In this work, we adopted multitask model of speech enhancement and multi-class voice activity detection (VAD) to further raise the performance of the speech extraction model [25]. VAD head estimates the presence of target and interfering speaker respectively for each time frame from the output of the last dilated convolution block in the

time-domain SpeakerBeam structure. The target was 4 classes of possible combinations of the presence of target and interfering speakers. Teacher labels for VAD were created by applying WebRTC VAD [26] for original clean speeches before mixing.
Training setup: Training objective was scale-dependent source to distortion ratio (sd-SDR) loss for enhancement head and cross-entropy loss averaged over frames for VAD head. We set hyperparameters of the speakerBeam as follows: N = 256, L = 20, B = 256, R = 3, X = 8, H = 512 and P = 3 following the notation in the original paper [24]. VAD head is composed of 2 layer BLSTM network with 128 cells followed by a linear layer. For the optimization, we adopted the Adam algorithm [27]. The initial learning rate was set as 5e-4 and we trained the models for 50 epochs and chose the best performing model based on a development set. Multitask weight was set as 1 for the speech enhancement loss and 10 for the VAD loss to make the range of each loss term roughly the same scale.
4.2. ASR backend
Dataset: In order to train the noise-robust speech recognition model, we created noisy CSJ dataset by adding CHiME-4 noise to the original CSJ corpus. The data creation procedure of the clean speech is based on Kaldi CSJ recipe [28]. For noise-robust training, we omitted cafe noise from CHiME-4 noise and randomly selected a noise signal out of the remaining three noise environments. This setting allows testing with unseen noise conditions. Since the adopted three noise environments do not contain much background speech compared with cafe noise, the ASR model trained here is not particularly robust to interference speakers, and thus speech extraction would play an important role for this setup. The SNR was randomly selected between 0 and 20 dB. Besides, we convolved randomly generated simulated room impulse responses (RIRs) generated by image method [29] for 90% of the training and validation dataset to make the ASR model closer to those used in practical situations. T60 was randomly selected between 150 and 600 ms and the distance between a microphone and a speaker was randomly selected between 10 and 300 cm.
Network configuration and training setup: We used ESPnet to build the ASR backend model [30], which is an opensource toolkit for end-to-end speech processing. We adopted transformer-based encoder-decoder ASR model with connectionist temporal classification objective [31]. The training was conducted according to the default setup of ESPnet CSJ recipe [32] with speed perturbation [33] and SpecAugment [34], except that we did not use language model shallow fusion.
4.3. Experimental results
4.3.1. Analysis: Observed mixture vs Enhanced speech for different SIR and SNR ratios
Table 1 shows the relative CER reduction of speech extraction from the mixture. For low SIR conditions as 0 dB, the speech enhancement technique benefits ASR performance drastically by reducing CER 80% compared with the observed mixture on average. On the other hand, for high SIR conditions as 20 dB, speech enhancement degraded speech recognition performance. This result supports the empirical knowledge that ASR has an ability to recognize dominant speech under the presence of interfering speech. In addition, as non-speech noise becomes more dominant, the enhanced speech becomes more detrimental for ASR. It should be noted that under severe noise condition as SNR 0 dB, speech extraction did not improve ASR performance even if the interfering speech is present at a moderate level of

Table 1: Relative CER reduction of target speech extraction from the observed mixture. Positive values indicate that the speech extraction improved ASR performance and negative values indicate that speech extraction degreaded ASR performance. Under severe noise condition like SNR 0 dB, speech extraction sometimes did not improve ASR performance even if the interfering speech present at a moderate level of SIR 10 dB.

SIR [dB]

20

CAF 10

0

Noise Type and SNR [dB]

PED

STR

20 10

0

20 10

0

BUS

20 10

0

0 89% 86% 57% 90% 85% 49% 90% 87% 67% 90% 89% 80%

5 83% 74% 20% 83% 72% 7% 84% 77% 36% 84% 81% 62%

10 64% 37% -25% 62% 31% -41% 66% 46% -22% 68% 57% 18%

15 28% -11% -47% 27% -21% -53% 33% 7% -61% 39% 20% -23%

20 -27% -52% -50% -29% -68% -57% -15% -41% -75% -6% -23% -43%

Table 2: CER for (a) enhanced speech, (b) observed mixture and (c) proposed switching method between (a) enhanced and (b) mixture. (d) shows the relative improvement of (c) proposed switching from (a) enhanced speech. The switching rule is supposed to choose observed mixture for shaded conditions in (c) and (d).

(a) enhanced speech

SIR

SNR [dB]

[dB] 20 10 0

0 8.4 10.9 31.2

5 6.7 9.0 28.2

10 5.8 7.8 24.9

15 5.4 7.6 23.6

20 6.3 8.9 23.2

(b) observed mixture

SIR

SNR [dB]

[dB] 20 10 0

0 80.7 80.6 83.9

5 40.9 37.5 40.1

10 16.5 13.8 20.4

15 7.9 7.5 15.9

20 5.3 6.0 14.8

(c) proposed

switching between

enhanced and mixture

SIR

SNR [dB]

[dB] 20 10 0

0 8.4 10.9 31.2

5 6.7 9.0 28.4

10 5.8 7.8 21.1

15 5.4 7.3 17.3

20 6.1 7.3 17.0

(d) relative improvement

of (c) switching

from (a) enhanced speech

SIR

SNR [dB]

[dB] 20 10 0

0 0% 0% 0%

5 0% 0% -1%

10 0% 0% 15%

15 0% 4% 27%

20 2% 18% 27%

SIR 10 dB in some conditions. The result also shows that whether the enhancement tech-
nique benefits ASR or not also depends on noise types. For example, the enhancement benefit ASR for street and bus noise but degrade ASR for cafe and pedestrian noise, when SIR is 15 dB and SNR is 10 dB.
The results shown here indicate that we should not apply speech extraction for all the overlapping speech to get better ASR results.
4.3.2. Evaluation: Estimation-based input signal switching
We evaluated out proposed switching method of the ASR input between the observed mixture and enhanced speech. Table 2 shows the CER of (a) enhanced speech, (b) observed mixture, (c) the proposed switching method, and (d) the relative improvement of CER derived by the switching method from enhanced speech. The bold numbers in (a) and (b) indicate which one of enhanced speech or observed mixture is best for ASR. The results shown here are average values over noise types. Based on the threshold  = 10, the adopted switching rule is supposed to choose `observed mixture' for ASR input when SIR - SNR  10 dB, which corresponds to shaded conditions

in (c) and (d). In (d), results displayed in boldface are conditions where the switching brought improvement, and underlined results are conditions where the switching degraded ASR performance.
According to the results shown in (c), the proposed switching mechanism significantly reduced CER for SIR - SNR  10 dB conditions from that for (a) enhanced speech. The average CER reduction for SIR - SNR  10 dB conditions were 22 %. Although the switching rule is supposed to choose enhanced speech for conditions like SIR 20 dB and SNR 20 dB based on the rule, the estimation errors of SIR and SNR make the switching method select observed mixtures, which unintentionally improved the ASR performance. (Note that the switching rule is defined based on the experimental observations for the development set and not necessarily optimal for the evaluation set.) At the same time of gaining improvement, the ASR result degradation were also very limited for SIR - SNR < 10 dB conditions. These results suggested that the switching mechanism worked well even with the estimated SIR and SNR scores.
In case the switching worked perfectly, the achieved CER would be the minimum value of (a) enhanced speech and (b) observed mixture. Since the achieved CER still has a gap from such oracle values, it seems that there remains room for further improvement of speech recognition performance for overlapping speech. As prospects, we can improve SIR and SNR prediction methods as well as the switching rules into more advanced ones, including learning-based SNR and SIR prediction, rules considering noise types, frame-level switching, etc. It is also possible to adopt joint training frameworks to train neural network-based switching modules on ASR loss, in order to make the switching module acquire selection criteria that maximize ASR performance.
5. Conclusion
In this paper, we investigated in which conditions speech separation/extraction were beneficial for ASR. We observed that depending on the SIR and SNR conditions, better ASR performance could be obtained by recognizing the observed mixture signal instead of the enhanced speech. As an initial investigation for the switching between enhanced and observed signals, we proposed a rule-based switching mechanism based on estimated SIR and SNR. It is experimentally confirmed that even a simple switching rule could already improve ASR performance by up to 27 % relative CER reduction. We expect that more advanced switching mechanisms could further improve the performance.

6. References
[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-R. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups," IEEE Signal processing magazine, vol. 29, no. 6, pp. 82­97, 2012.
[2] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, X. Cui, B. Ramabhadran, M. Picheny, L. L. Lim et al., "English conversational telephone speech recognition by humans and machines," in Proc. Interspeech, 2017, pp. 132­136.
[3] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, "The fifth'chime'speech separation and recognition challenge: dataset, task and baselines," arXiv preprint arXiv:1803.10609, 2018.
[4] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, "The third `chime'speech separation and recognition challenge: Dataset, task and baselines," in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 504­511.
[5] Y. Qian, X. Chang, and D. Yu, "Single-channel multi-talker speech recognition with permutation invariant training," Speech Communication, vol. 104, pp. 1­11, 2018.
[6] M. Delcroix, K. Z mol´ikova´, K. Kinoshita, A. Ogawa, and T. Nakatani, "Single channel target speaker extraction and recognition with speaker beam," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5554­5558.
[7] J. Li, L. Deng, Y. Gong, and R. Haeb-Umbach, "An overview of noise-robust automatic speech recognition," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745­777, 2014.
[8] P. Comon and C. Jutten, Handbook of Blind Source Separation: Independent component analysis and applications. Academic press. Elsevier, 2010.
[9] D. Wang and J. Chen, "Supervised speech separation based on deep learning: An overview," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1702­ 1726, 2018.
[10] K. Z mol´ikova´, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, L. Burget, and J. C ernocky`, "Speakerbeam: Speaker aware neural network for target speaker extraction in speech mixtures," IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 4, pp. 800­814, 2019.
[11] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu, and S. Watanabe, "Auxiliary interference speaker loss for targetspeaker speech recognition," in Proc. Interspeech, 2019, pp. 236­ 240.
[12] P. Denisov and N. T. Vu, "End-to-end multi-speaker speech recognition using speaker embeddings and transfer learning," in Proc. Interspeech, 2019, pp. 4425­4429.
[13] M. Delcroix, S. Watanabe, T. Ochiai, K. Kinoshita, S. Karita, A. Ogawa, and T. Nakatani, "End-to-end speakerbeam for single channel target speech recognition," in Proc. Interspeech, 2019, pp. 451­455.
[14] J. Heymann, L. Drude, and R. Haeb-Umbach, "Neural network based spectral mask estimation for acoustic beamforming," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 196­200.
[15] T. N. Sainath, R. J. Weiss, K. W. Wilson, B. Li, A. Narayanan, E. Variani, M. Bacchiani, I. Shafran, A. Senior, K. Chin et al., "Multichannel signal processing with deep neural networks for automatic speech recognition," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 5, pp. 965­ 979, 2017.
[16] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, C. Yu, W. J. Fabian, M. Espi, T. Higuchi et al., "The ntt chime-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices," in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 436­443.

[17] S.-J. Chen, A. S. Subramanian, H. Xu, and S. Watanabe, "Building state-of-the-art distant speech recognition using the chime-4 challenge with a setup of speech enhancement baseline," in Proc. Interspeech, 2018, pp. 1571­1575.

[18] M. Fujimoto and H. Kawai, "One-pass single-channel noisy speech recognition using a combination of noisy and enhanced features," in Proc. Interspeech, 2019, pp. 486­490.

[19] Q. Wang, I. L. Moreno, M. Saglam, K. Wilson, A. Chiao, R. Liu, Y. He, W. Li, J. Pelecanos, M. Nika et al., "Voicefilter-lite: Streaming targeted voice separation for on-device speech recognition," in Proc. Interspeech, 2020, pp. 2677­2681.

[20] K. Maekawa, "Corpus of spontaneous japanese: Its design and evaluation," in ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2003.

[21] "Chime4

challenge,"

http://spandh.dcs.shef.ac.uk/

chime challenge/CHiME4, Cited April 2 2021.

[22] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, "Deep clustering: Discriminative embeddings for segmentation and separation," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 31­35.

[23] Y. Luo and N. Mesgarani, "Conv-tasnet: Surpassing ideal time­ frequency magnitude masking for speech separation," IEEE/ACM transactions on audio, speech, and language processing, vol. 27, no. 8, pp. 1256­1266, 2019.

[24] M. Delcroix, T. Ochiai, K. Zmolikova, K. Kinoshita, N. Tawara, T. Nakatani, and S. Araki, "Improving speaker discrimination of target speech extraction with time-domain speakerbeam," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 691­695.

[25] Y. Zhuang, S. Tong, M. Yin, Y. Qian, and K. Yu, "Multi-task jointlearning for robust voice activity detection," in 10th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2016, pp. 1­5.

[26] "Webrtc voice activity detector," https://github.com/wiseman/ py-webrtcvad, Cited April 2 2021.

[27] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in International Conference on Learning Representations (ICLR), 2015.

[28] D. Povey, A. Ghoshal, and G. Boulianne, "The kaldi speech recognition toolkit," in IEEE Workshop on Automatic Speech Recognition and Understanding, 2011.

[29] J. B. Allen and D. A. Berkley, "Image method for efficiently simulating small-room acoustics," The Journal of the Acoustical Society of America, vol. 65, no. 4, pp. 943­950, 1979.

[30] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen et al., "Espnet: End-to-end speech processing toolkit," in Proc. Interspeech, 2018, pp. 2207­2211.

[31] S. Karita, N. E. Y. Soplin, S. Watanabe, M. Delcroix, A. Ogawa, and T. Nakatani, "Improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration," in Proc. Interspeech, 2019, pp. 1408­1412.

[32] https://github.com/espnet/espnet/blob/master/egs2/csj/asr1/conf/ tuning/train asr transformer.yaml, Cited April 2 2021.

[33] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, "Audio augmentation for speech recognition," in Proc. Interspeech, 2015, pp. 3586­3589.

[34] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, "SpecAugment: A simple data augmentation method for automatic speech recognition," Proc. Interspeech, pp. 2613­2617, 2019.

