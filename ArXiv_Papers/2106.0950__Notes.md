
# A Multi-Level Attention Model for Evidence-Based Fact Checking

[arXiv](https://arxiv.org/abs/2106.0950), [PDF](https://arxiv.org/pdf/2106.0950.pdf)

## Authors

- Canasai Kruengkrai
- Junichi Yamagishi
- Xin Wang

## Abstract

Evidence-based fact checking aims to verify the truthfulness of a claim against evidence extracted from textual sources. Learning a representation that effectively captures relations between a claim and evidence can be challenging. Recent state-of-the-art approaches have developed increasingly sophisticated models based on graph structures. We present a simple model that can be trained on sequence structures. Our model enables inter-sentence attentions at different levels and can benefit from joint training. Results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that our model outperforms the graph-based approaches and yields 1.09% and 1.42% improvements in label accuracy and FEVER score, respectively, over the best published model.

## Comments

Findings of ACL 2021

## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{kruengkrai2021multilevel,
      title={A Multi-Level Attention Model for Evidence-Based Fact Checking}, 
      author={Canasai Kruengkrai and Junichi Yamagishi and Xin Wang},
      year={2021},
      eprint={2106.00950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

