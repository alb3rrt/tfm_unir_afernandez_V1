IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse

arXiv:2106.00563v1 [cs.LG] 1 Jun 2021

Liangliang Shi Shanghai Jiao Tong University shiliangliang@sjtu.edu.cn

Yang Li Shanghai Jiao Tong University
yanglily@sjtu.edu.cn

Junchi Yan Shanghai Jiao Tong University
yanjunchi@sjtu.edu.cn

Abstract
Despite its success, generative adversarial networks (GANs) still suffer from mode collapse, namely the generator can only map latent variables to a partial set of modes of the target distribution. In this paper, we analyze and try to regularize this issue with an independent and identically distributed (IID) sampling perspective and emphasize that holding the IID property for generation in target space (i.e. real data) can naturally avoid mode collapse. This is based on the basic IID assumption for real data in machine learning. However, though the source samples z obey IID, the target generation G(z) may not necessarily be IID. Based on this observation, we provide a new loss to encourage the closeness between the inverse source from generation, and a standard Gaussian distribution in the latent space, as a way of regularizing the generation to be IID. The logic is that the inverse samples back from target data should also be IID for source distribution. Experiments on both synthetic and real-world data show the superiority and robustness of our model.
1 Introduction
In fields of generative models, the training data (in target space) is often assumed to be IID sampled from an unknown implicit distribution. Deep generative models often try to construct a mapping from a known distribution in source space (e.g. Gaussian distribution) to the implicit target. Popular generative schemes include Variational Auto-Encoders (VAEs) [16], Generative Flow models [33], Generative Adversarial Networks (GANs) [9]. Among them, GANs have been a popular tool for data generation, especially for generating images with a high resolution. Mode collapse is one of the standing issues in GAN, a phenomenon that the generator tends to get stuck in a subset of modes while excludes other parts of the target distribution [21, 38], leading to a poor diversity of generation.
Efforts have been made to address mode collapse, which can be broadly categorized into three branches: 1) get a better convergence between the generated distribution and the target (real data) distribution [10, 26]; 2) encourage the diversity or penalize the similarity of the generated images [7, 27, 24]; 3) multiple generators or discriminators to learn diverse modes [20, 30]. However, for the first kind of methods, the generated and target distributions are both hard to represent, which may lead to a local convergence. While methods of the latter two categories do not directly address the mode collapse, instead they pay attention to diversity by other means like penalizing the similarity, which may not directly guarantee that all the modes are covered by generation.
*Correspondence author
Submitted to 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Do not distribute.

In this paper, we first make a basic yet under-exploited (in GAN literature) observation that datasets for generation in the study are assumed sampled to be independent and identically distributed (IID) from an unknown target distribution. For the task of data generation, IID sampling from the real data distribution means a perfect recovery of the real data distribution without mode collapse.
We revisit vanilla GAN under this IID sampling perspective. For a generated data point, the discriminator D distinguishes whether it is sampled from an identical target distribution. However, it cannot tell whether the sampling is independent in each separate judgment. For the generator G, from the generative mapping view, the IID property for generated points may be broken during the mapping of IID sampled source data points (from a certain distribution e.g. Gaussian) to the target ones, which brings the risk of mode collapse. Based on the above observation, we departure from the popular line of research dwelling on the relation between the generation's distribution and the real one, but paying orthogonal attention to the mapping between the source distribution and the target to satisfy a strong condition for mode completeness, i.e., the generated data also obey the IID property as the real data.
However, without knowing the target distribution, it is difficult to directly ensure that the generations are IID. In this paper we seek a weaker condition (can be regarded as a regularization) based on an inverse mapping perspective: if the real data are IID samples from target distribution, then their inverses back to the source space are also IID. In another word, the IID property of real data's inverses is a necessary condition for the non-collapse generation. More rigorous formulation and proof are presented later in Proposition 1. To achieve this IID property, one simple and general idea is to prompt the distribution of the overall inverse samples close to a standard Gaussian by certain measures*. This idea also obviously differs from existing inverse-based methods [37, 5, 35] which learn the relations between the latent data z and real data x with discriminator.
Our new perspective can also enrich and extend the understanding of mode collapse, whose concept itself has not been well established. Mode collapse in existing literature mostly refers to the mode dropping, that is, a few modes are not covered by the generation ­ see example in Fig. 1(b). Our IID view naturally allows for a soft description of mode collapse and the resulting new model is expected to handle both hard mode dropping and soft mode deficiency i.e., some modes are not hit with enough generated samples as the real distribution. We will show that our new method can be effective in both cases, as more appropriately measured by respective metrics. The contributions are:
1) We take an IID perspective to address the mode collapse issue in GAN. Specifically, we show in Proposition 1 that the IID property of the inverse samples from the target generation can serve as a regularizer for avoiding mode collapse in the target space. We further show that such a property can be generally achieved by enforcing the inverse samples to be close to a standard Gaussian distribution, which is termed as Gaussian consistency loss in this paper (see Section 4.3).
2) Under our IID framework, the concept of mode collapse (though not well defined in literature) can be generalized from the hard mode dropping to soft mode deficiency, which can be unified by mode completeness as defined in Definition 1 and for the overall distribution closeness.
3) On synthetic data with multiple modes, we show that IID-GAN outperforms by different metrics: number of covered modes, quality, reverse KL divergence. Our simple technique also performs competitively on natural images. Unsupervised disentanglement feature learning and conditional GAN are also studied. Our new loss can be seen as an orthogonal plug-in to existing GAN models.
2 Related Work
Since its debut [9], subsequent works of Generative Adversarial Networks (GANs) have been developed to improve the stability and quality of generation. However, GANs still suffer unstable training, and mode collapse has been one of the most common issues, as studied in literature.
Improving training behavior. Unrolled GAN [26] presents a surrogate objective to train the generator along with the unrolled optimization of the discriminator, which shows improvements in terms of training stability and reduction of mode collapse. As an improvement of Wasserstein GANs [1], WGAN-GP [10] devises a gradient penalty whose effectiveness has shown in the realistic generation system [14]. [25] gives the ODE view, proving that zero gradient penalty can improve the
*More precisely, in batch-based GAN training, it means for each batch shall be close to standard Gaussian.
2

convergence for generation. However, these methods focus more on convergence than mode collapse, which may consider less for diversity among real and generated data.

Enforcing to capture diverse modes. To generate data with the same diversity as the real data is, many methods are proposed to solve mode collapse for the GAN model. [7] uses the theory of determinantal point processes and it gives a penalty in the layer of the discriminator to enforce the generated data having similar covariance of real data. The approach in [27] uses Bure metric instead and it has discussed more training details. However, due to the randomness of generated samples and real samples, these methods may lack theoretical guarantees and stability of diversity generations.

Multiple generators and discriminators. One direct way to reduce mode collapse is involving more than one generators to achieve wider coverage for the real distribution. In [20], two coupled generator networks are trained with parameter sharing to jointly learn the real distribution. The multi-agent system MAD-GAN [8] involves multiple generators along with one discriminator. The system implicitly encourages each generator to learn its own mode. On the other hand, multiple discriminators are used in [6] as an ensemble. Similarly, two additional discriminators are trained to improve the diversity [30]. These methods rely more on network design and parameter adjustment.

Mapping back to learn the representations. Similar to BiGAN [5] and VEEGAN [37] design an inverse of the generator and encourages the discriminator to distinguish the joint distribution for real and generated samples. VAEbased models [16] learn an encoding network for representation. Our work mainly follows these works while takes one step further to justify the role of IID informed inverse mapping for solving the mode collapse issue.

These works are orthogonal to ours and most of them can be fulfilled in conjunction with ours to further improve the training stability, which we leave for future work.

(a) two mode collapse cases

3 Preliminaries and Motivation

We define the mode completeness to induce a mapping between two probability measures:

Definition 1 (Mode Completeness) The probability measures  and  are defined in the source space A and the target space B, respectively. Given an arbitrary set S  B, the generative mapping G : A  B is defined as mode completeness from  to  if G satisfies:

(S) = (z  A : G(z)  S})

(1)

which can be exactly written as the push-forward operator [31]  = G# in optimal transportation problems.

(b) mode completeness via inverse
Figure 1: Mapping view on mode collapse: map from source and target distribution. (a) hard mode drop (left) and soft mode deficiency (right). Soft mode deficiency can be treated as a special case of the mode dropping when the green part is sparse to the limit. (b) Assume the existence of the inverse of mapping G(·). Given any set S, the probability measures of the set G-1(S) and S are equal. See Proposition 1.

We aim to design the mapping G that satisfies that  = G# and the operator G# means that G pushes forward the mass of  to  [31]. Based on Definition 1, we can find that the mode missing will not happen because of the same value of the probability measures given the corresponding sets based on G (i.e. ({zi}) = ({T (zi)})). Then when zi are IID sampled from , the generated samples {G(zi)} can also be viewed as IID from  due to the equal probability.

The IID generation, i.e. mode completeness  = G# is hard to achieve for the generative mapping in GAN because  is unknown and can be complex. The only information that we will make full use
of, is the IID basic assumptions of the real training data. Then we assume the existence of the inverse of the mapping G i.e. G-1 : B  A, which satisfies z = G-1(G(z)) and x = G(G-1(x)) for any
z  A and x  B. We can get a necessary condition of mode completeness to address the mode collapse if there exists the inverse of G-1. For any sample {x} from , its inverse sample {G-1(x)} can be enforced for training G and the inverse mapping G-1. Such a necessary condition for IID
generation is formally specified by the following proposition (see Appendix A for proof):

3

Figure 2: An example for 2-D source space (z) to 2-D target space (x) generation and inverse ­ Ring dataset with 8 modes as the real training set to illustrate the `overlapping' (resp. `scattering') issues of VAE (resp. BiGAN). Given the trained VAE/BiGAN models that consist of both generator and inverse mapper, the first column shows the inverse of the real target data (shown in the third column), the second column shows the sampled data from Gaussian in source space. Points in the source space are colored with nine colors (8 `modes' + 1 `bad') according to their generation's mode in the target space. Note in the second column, there are many bad generations in white. The pie charts show the corresponding ratio of valid generation points in different modes of the second column. The validness of generation is defined according to [27] ­ see more details in the footnote in the experiment part.
Proposition 1 (IID Property for Inverse Targets) If the generative mapping G satisfies mode completeness from the source probability measure  to the target probability measure  and its inverse G-1 exists, then given IID samples {x(i)}ni=1 from the target distribution, their inverse {G-1(x(i))}ni=1 can be viewed as IID samples from the source distribution.
Remarks: Since it is usually assumed that the real data {x(i)}ni=1 are independently sampled from an unknown distribution , we can find that their inverse {G-1(x(i))} can be viewed as IID samples from a known distribution . So to meet the mode completeness, we need to train the inverse mapping G-1 to make the inverse samples of real data more like IID samples from . Besides, we can not get the strict inverse mapping G-1. A function F which maps back to the source space, will be designed to approximate G-1 by penalizing z = F (G(z)) and x = G(F (x)).
Our motivation and techniques are essentially different from [16, 37], which mainly design an inverse or encoder to learn the representations or relations of z and x, which may suffer two issues:
1) The overlap of z may cause bad generations. In many VAE-based works [16], they mainly learn the latent representation of z with its distribution q(z|x) and encourage it to be closer to standard Gaussian. Then we can get the representation z with the sampling of q(z|x). However, as shown in Fig. 2 A, the inverse samples in source space from different modes in the target space can be overlapped to each other. This may, in turn, cause the bad generation (white points in Fig. 2 B).
2) Inverse samples disobey original source distribution (standard Gaussian). Some studies like BiGAN [5] train the discriminator by learning the relations of z and x. However, the discriminator cannot directly impact the IID behavior of the bi-directional mapping i.e. the generator and inverse. As shown in C of Fig. 2, the inverse samples do not obey the (expected) Gaussian distribution (as here the source is sampled from a stand Gaussian) and many samples of some classes are scattered far from the Gaussian origin point, which causes mode collapse.
To address these problems, we design an inverse mapping and learn the latent as samples instead of conditional distribution q(z|x). Besides, to make the set of inverse samples closer to IID Gaussian samples, we learn the inverse mapping by considering the overall samples in each batch, which can be used to meet the necessary condition for IID generation as given in Proposition 1.
4 The Proposed Approach
Our new loss for GAN (and conditional GAN [28]) are comprised of three parts as shown in Fig. 3 .
4

(a) GAN loss to generate the identical sample

(b) Cycle-Consistency for inverse mapping

(c) GAN loss to generate the independent samples

Figure 3: IID-GAN: generator G maps random samples from original standard M -D Gaussian to target ones and F inverts the target sample back to a source sample obeying M -D Gaussian. Note
CycleGAN addresses image-to-image generation while we only involve latent space to target space. 4.1 The Adversarial Learning Term: Vanilla GAN Loss

The vanilla GAN model [9] consists of a discriminator D : Rd  R and a generator G : RM  Rd, which are typically embodied by deep neural networks. Given the empirical distribution p(x), D(x) is used to distinguish whether the generated samples from real data, while G(z) is the mapping from Gaussian sample z to a point in the target space Rd. The objective V (G, D) is optimized for the discriminator and generator by solving the mini-max problem in an alternating fashion:

Exp(x) [log(D(x))] + Ezp(z) [log(1 - D(G(z)))]

(2)

The first term gives the expectation of probability that x comes from real data distribution p(x) and
the second involves an input distribution p(z), which is embodied by a standard multi-dimensional (M -D) Gaussian distribution N (z; 0, I) in this paper. I  RM×M is the identity matrix.

4.2 The Reconstruction Term: Cycle-Consistency Loss for Single Sample

Proposition 1 requires an inverse mapping G-1 of the generation. Note our method involves inverse
mapping as embodied by neural network F , a popular and effective technique [37, 5] is to adopt a cycle-consistency loss to prompt F = G-1, where  = d/M is the ratio of the dimensions of x to z.

Lre(G, F ) = Ez z - F (G(z)) 1 + Ex x - G(F (x)) 1

(3)

to achieve inverse mapping

to avoid mode dropping

The first term promotes F to be the inverse of G, which uses the reconstruction loss as an expectation of the cost of auto-encoding noise vectors [37]. The second term promotes that F (x)  RM , which
makes z~ = F (x) possible to be sampled from Gaussian distribution. Then for each x, we can find the corresponding z~ in RM which satisfies G(z~) = x. Note that it cannot avoid mode imbalance as
the imbalance refers to the overall distribution rather than a single data point studied here. Then we
introduce our source space distribution closeness loss.

4.3 The Distribution Regularizer Term: Gaussian Consistency Loss for Inverse Distribution

Recall the necessary condition for IID generation as proposed in Proposition 1, suppose the given real data {x(i)} are IID sampled from p(x), then the inverse samples for the real data should be independent and satisfy the same distribution p(z) (i.e. IID samples for source distribution). So given a batch of real data, the mapping F should make {F (x(i))} closer to independent samples from standard Gaussian p(z). For simplicity, we choose the Gaussian as the distribution for IID sampling. We leave other forms of distribution and their impact for future work.

M -D Gaussian consistency loss. Suppose the inverse samples {z~(i)}Ni=1  RM of real data follow a Gaussian distribution N (z; µ, ) in the latent space, then maximum likelihood estimation is:

µ~ = 1

N
z~(i),

~ = 1 N

z~(i) - µ~

z~(i) - µ~

(4)

N

N

i=1

i=1

5

where µ~  RM , and ~  RM×M is the estimated covariance. Our goal is to make the Gaussian q(z) = N (z; µ~, ~ ) closer to the standard Gaussian p(z) = N (z; 0, I), then we can say that {z~(i)}Ni=1 are IID samples from standard Gaussian p(z). A direct way is to adopt a distribution closeness loss
LGau. For instance, it can be specified as square of Wasserstein distance between two Gaussians:

LGau = µ~ 2 + trace(~ + I - 2~ 1/2)

(5)

The two Gaussian distributions q(z) and p(z) can also be evaluated with static divergence or by designing a discriminator to distinguish directly such as p-norm, KL-divergence and z~-discriminator. Note that the method with z~-discriminator which train another discriminator to distinguish the Gaussian samples[23]. More details are in appendix B about these methods.

Remarks from the Disentanglement View. Many previous studies [12, 15] learn the unsupervised

disentanglement representation with the assumption of independent factors, i.e. q(z) =

M j=1

q(zj

).

However, the recent work [22] opposes this view and argues it is impossible for unsupervised learning

of disentangled representations without inductive biases on both models and data. Our work gives a

new sight with M -D Gaussian guarantee. When q(z) approximates to M -D standard Gaussian, it

is obvious that z is independent for different dimensions. However, varying zi can not disentangle

the different modes as shown in the first column of Fig. 4. We find that using polar coordinates to

represent the data and varying the polar angle and diameter may be a good way for unsupervised

learning of disentangled representations. Experiments will show some results of MNIST.

Decoupling M -D Gaussian into M 1-D Gaussians. For large M and small batch size, the training
can suffer from the curse of dimensionality for estimating . So we decouple the M -D Gaussian loss
to the sum of M number of 1-D Gaussian loss. Concretely, we add an assumption on the covariance that non-diagonal values are all zeros. Equivalently, the inverse samples {z~(i)}Ni=1 follow M 1-D Gaussian distributions N (z~; µ, ) and then we can get the estimation µ~ and ~ for the Gaussians.

Here our goal is to make the 1-D Gaussian distribution q(zj) more approximate to the standard 1-D Gaussian distribution for each dimension j. Similar to the M -D case, we can design the Gaussian loss with Wasserstein distance between two Gaussian distributions, and sum them up over M dimensions:

M

LGau =

µ~2m + (~m - 1)2

(6)

m=1

In the rest of this paper, we often directly call the Gaussian loss by omitting the term consistency.

By inputting as many samples sampled from p(z) as possible, together with the corresponding inverse samples {z~} from the real data, we optimize Dz and F by adversarial learning to push {z~} closer to the sampling results of p(z). Then we can get our final loss as:

min max V (G, D) + reLre(G, F ) + GauLGau(F )

(7)

G,F D

where LGau(F ) can be specified based on different distances or divergence and re, Gau are loss weights which will be discussed in experiment in detail.

When the label c i.e. the category of the real data is known, IID-GAN can be extended to the conditioned case [28]. As for conditioned IID-GAN, (z, c) are the inputs of the generator G to generate the data points and the real x is the input of F to perform classification and the Gaussian consistency loss is to keep the independent condition. The Gaussian loss is mainly used to learn the diversity of hidden features (e.g. the thickness, inclination for MNIST). See details in Appendix F.

4.4 Further Comparisons with Peer Methods
Comparison to VAE-based methods. Most VAE-based methods [35, 12, 15] contain an encoder and learn the latent representation with the Gaussian distributions for each real data. Differently, our methods consider the overall mini-batch data to make the inverse samples of the real data view as IID standard Gaussian samples. Then our method can generate more accurate and diverse data.
Comparison to GAN methods with inverse mapping. Many GAN based methods [5, 13] also map back to learn the representation. However, compared to IID-GAN, these methods do not consider the data from the perspective of the overall samples and do not consider the IID property. CycleGAN [39]

6

Figure 4: Illustration for Ring dataset (with eight modes) in source (left two columns) and target space (the third column), by comparing VEEGAN and IID-GAN under different Gaussian consistency losses as detailed in Sec. 4.3 after training 24K batches. Both 1-D and M -D Wasserstein distance are used. See also Fig. 2 for the interpretation of each column. Note for such a 2-D source space case, the red boxes in the third row suggest that the 1-D Gaussian loss causes some modes to be partially collapsed (see also the pie-chart where the yellow part is tiny while our method's pie-chart is more even), i.e. all the modes are covered but not balanced. The white points denote bad generation.
proposes a cycle consistency loss to obtain the transition between two different styles of images. However, different from our methods, CycleGAN is designated for image translation and focus less on the diversity of the generated images. Their method has nothing to do with IID.
Comparison of M -D and 1-D Gaussian loss. Our method claims the Gaussian necessary conditions for the inverse samples {~z} of real data. {~z} is a multi-dimensional vector in latent space and we hope the inverse samples obey the standard Gaussian N (z; 0, I), which is the goal of M -D Gaussian loss. However, when training the real-world data, {~z} may have a high dimension and cause the dimension curse with small batch size. We consider estimating M single-dimensional standard Gaussian along each dimension in the latent space to address the problem, as shown by the delegate loss in Eq. 6.

5 Experiments

To highlight our techniques, we adopt a simpler network architecture to directly evaluate the contributions of our techniques.Experiments are performed on a desktop with a GeForce RTX 2080Ti.

5.1 Experiments on Synthetic Datasets

Mode collapse can be directly measured on synthetic data with known distribution. In line with [26], we simulate two synthetic datasets. The batch size is set to 128.

Ring dataset. A mixture of 8
2-D Gaussians p(z) with mean {(2 cos (i/4), 2 cos (i/4))}8i=1 and Figure 5: Generations of grid data given poor initialstandard deviation 0.001. 12.5K samples ization. Compared with 1-D Gaussian consistency loss, are simulated from each Gaussian distri- the M -D loss outperforms after enough batch iterations bution i.e. 100K samples in total. 50K (i.e. training steps). Similar results are shown on Ring samples from p(z) are used to generate x in Appendix F. for test.

Grid dataset. A mixture of 25 2-D isotropic Gaussians i.e. p(z) with mean {(2i, 2j)}2i,j=-2 and standard deviation 0.0025. 4K samples are simulated from each Gaussian (i.e. 100K samples in total).
100K samples from p(z) are used to generate target samples {x~} for test.

Metrics. Following [26, 7], we use the numbers of covered modes, generation quality and reverse

KL divergence. Since in the experiment, each mode shares the same number of real samples, one can

calculate the reverse KL divergence between the generated distribution and the real one [30] . The

reverse KL divergence is not strictly defined as

m i=1

pi

<

1

(i.e.

there

exist

poor

generated

points),

and allows being negative.

We follow the protocol in [27]: if the generated data point is within 3 times the std of the Gaussian mean, consider it a good (or valid) generation (otherwise bad) and the resulting ratio is used as the generation quality.

7

Table 1: Comparison on the synthetic data: Ring and Grid.

Models
GAN [9] BiGAN [5] Unrolled GAN [26] VEEGAN [37] IID-GAN (1-D) IID-GAN (M -D)

Mode# 3.6 ± 0.5 6.8 ± 1.0 6.4 ± 2.2 5.4 ± 1.2 8.0 ± 0.0 8.0 ± 0.0

2D-Ring Quality%  98.8 ± 0.6 38.6 ± 9.5 98.6 ± 0.5 38.8 ± 16.7 97.3 ± 0.6 99.0 ± 0.2

RKL 0.92 ± 0.11 0.43 ± 0.18 0.42 ± 0.53 0.40 ± 0.10 0.18 ± 0.06 0.17 ± 0.06

Mode# 18.4 ± 1.6 24.2 ± 1.2 8.2 ± 1.7 20.0 ± 2.6 25.0 ± 0.0 25.0 ± 0.0

2D-Grid Quality%  98.0 ± 0.4 83.4 ± 2.9 98.7 ± 0.6 85.0 ± 5.9 97.8 ± 0.49 98.0 ± 0.4

RKL 0.75 ± 0.25 0.26 ± 0.20 1.27 ± 0.17 0.41 ± 0.10 0.32 ± 0.09 0.26 ± 0.12

Network architecture. Instead of using tanh as the activation function as adopted in [26, 7] for more stable training, to more directly verify our technique, we resort to ReLU with four linear layers as the network architecture (see details in appendix F). We discuss the results as follows.
M -D Gaussian consistency loss comparison. As shown in Fig. 4, M -D Gaussian consistency loss performs well on synthetic data. Compared with VEEGAN and our method w/o using Gaussian loss, which view data in isolation, IID-GAN considers the entire batch as a whole. IID-GAN with Gaussian loss outperforms other methods that each mode is recovered and the inverse of real samples are close to the center of standard Gaussian.
Compared with 1-D Gaussian, M -D Gaussian loss makes the inverse of real data closer to real Gaussian samples. As shown in the third row in Fig. 4, 1-D Gaussian loss can easily make it imbalance and M -D Gaussian loss overcomes this limitation. Besides, it is well known that the training of GANs is sometimes sensitive to bad initialization which leads to mode dropping as shown in Fig. 5. However, M -D Gaussian loss performs more robustly.
Overall comparison. IID-GAN is compared with vanilla GAN [9], BiGAN [5], Unrolled GAN [26] and VEEGAN [37] on Ring and Grid datasets. We conduct experiments on the choice of different M -D Gaussian losses, and p-norm loss is chosen. Table 1 shows that IID-GAN outperforms.

5.2 Experiments on Real-world Dataset

The image datasets include MNIST, stackedMNIST, CIFAR-10, CIFAR-100, and STL-10. For all experiments, the model is trained for 300 epochs with a batch size of 256 and 0.0002 learning rate. We adopt different architectures to test our model which mainly follows the previous studies [32, 27, 4] and more details about the architectures of different datasets are given in appendix G.

All compared models are trained by

100K steps and the results are cal-

culated based on 10K generated im-

ages for CIFAR-10 and CIFAR-100

and STL-10. The detailed information

for network architectures for the real-

world images is given appendix G.

Here we use the p-norm distance as

M -D Gaussian loss. Most notably,

(a) Generation Quality

(b) Inverse KL divergence

our IID-GAN rarely encounters train- Figure 6: Generating quality and KL divergence from the in-

ing failure e.g. gradient explosion verse source to the standard Gaussian on MNIST. IID-GANs

which all the other compared methods outperforms VAE in terms of mode collapse and generation

struggled during the training.

quality.

Metrics. For image generation, we Table 2: Evaluation on real-world datasets: CIFAR-10,

adopt the popular Inception Score CIFAR-100. Architecture follows [27].

(IS) [36] and Fréchet Inception Distance (FID) [11] as quantitative met-

Models

CIFAR-10 IS FID

CIFAR-100 IS FID

rics. Mode Score (MS) [2] is adopted

GAN [9] 4.84 78.4

4.79 85.6

to make full use of information from real datasets for reasonable evaluation. Following [34], JSD is used to measure the similarity between the generated distribution and the real distribu-

Unrolled GAN [26] 4.65 76.2 VEEGAN [37] 3.56 161.1 GDPP [7] 4.43 80.4 IID-GAN(1-D) 4.89 65.4
IID-GAN(M -D) 5.00 76.9

4.96 83.1 4.34 88.6 4.87 82.8 5.05 84.5 5.27 82.1

tion. For datasets like MNIST which are easy to distinguish, we further use the number of covered

modes and reverse KL for evaluation.

8

Figure 7: Comparison of conditional GAN on CIFAR-10 by training epochs: IID-GAN outperforms. Results on MNIST and Stacked MNIST. MNIST and Stacked MNIST are both simple datasets that the classifier can achieve high accuracy. Thus we use the the classifier proposed by [4] to do the classification to calculate the number of modes and KL divergence. Fig. 6 shows the result that our 1-D and M -D IID-GAN lead to less mode imbalance and higher quality than VAE, while VEEGAN is unstable. And as shown in Table 3, IID GAN performs the best compare with existing methods.

Results on CIFAR-10 and CIFAR-100. We evaluate on CIFAR-10 and CIFAR-100 [17]. In Table 4, Inception Score and Fréchet Inception Distance (FID) are used to evaluate the image quality and

mode completeness. The best performance is observed for IID-GAN w.r.t. image quality, measured

by FID and Inception score and mode completeness, measured by FID. Table 2 shows that IID-GAN

outperforms UnrolledGAN, VEEGAN and GDPP. Besides, the IID regularization is also test in the

framework of WGANGP and SNGAN, which has a little improvements with the same random seed.

Table 3: Evaluation results on Stacked MNIST and

the architecture is in line with [32].

Models

Stacked MNIST Mode KL FID

GAN [9] 392.0 8.012 97.788

VEEGAN [37] 761.8 2.173 86.689

PACGAN [19] 992.0 0.277 117.128

IID-GAN(1-D) 996.4 0.152 86.911

IID-GAN(M -D) 999.7 0.101 69.675

Table 4: Evaluation results on STL-10.

Models

IS

STL-10 FID MS

GAN [9] 2.28 245.21 2.29

BiGAN [5] 1.22 251.21 1.22

Unrolled GAN [26] 4.78 142.16 4.62

VEEGAN [37] 1.45 298.95 1.46

IID-GAN(M -D) 5.16 139.10 5.12

Results on STL-10. STL-10 [3] contains higher resolution images of size 96×96 and we resize them into 64×64.

Table

5:

Evaluation on real-world

Vanilla GAN [9], BiGAN [5], Unrolled GAN [26] and datasets: CIFAR-10, STL-10, conduct-

VEEGAN [37] are compared with our method as shown ing IID strategy based on WGAN-

in Table 4.

GP, SNGAN. Metrics are calculated at

100,000 steps of training.

Results on conditional generation. Fig. 7 shows the results and the conditional IID-GANs (including its 1-D version) are most stable and robust without suffering from mode collapse, on CIFAR10 given the different categories for generation. MSGAN [24] is a popular method to penalize the similarity of generated images for conditional

Models
WGAN-GP [10] IID-GAN(WGAN-GP)
SNGAN [29] IID-GAN(SNGAN)

CIFAR-10 IS JSD 7.343 0.00339 7.443 0.00326
7.255 0.0327 7.350 0.0219

GAN. More results are given in appendix G.

Results on different network configurations. To fit with state-of-the-art generative model achievements, we try to introduce IID strategy on advanced architectures e.g. WGANGP [10] and SNGAN [29], to examine the performance of IID strategy on more architectures. We conducted experiments on CIFAR-10 and STL-10. For neural network complexity, we choose two networks(i.e. DCGAN network for CIFAR-10 and ResNet for STL-10). The results are shown in Table 5, which improves with our regularization.

6 Conclusion

We have provided an IID sampling perspective to address the mode collapse of GAN. Our devised inverse mapping technique and the new loss show their effectiveness to address mode collapse, on both synthetic and real-world datasets. The source code will be made publicly available.
Limitation: This work adopt Gaussian prior to learn the generation with IID perspective. However, the Gaussian distribution may be complex enough to represent complex datasets such as Imagenet. Using a more complex distribution or hidden distribution can be the direction of future work.
Negative social impact: the model can be used in Deepfake, which may raise concerns for criminal application .

9

References
[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML, pages 214­223, 2017.
[2] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode regularized generative adversarial networks. In ICLR, 2017.
[3] A. Coates, A. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215­223. JMLR Workshop and Conference Proceedings, 2011.
[4] A. B. Dieng, F. J. Ruiz, D. M. Blei, and M. K. Titsias. Prescribed generative adversarial networks. arXiv preprint arXiv:1910.04302, 2019.
[5] J. Donahue, P. Krähenbühl, and T. Darrell. Adversarial feature learning. In ICLR, 2017.
[6] I. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-adversarial networks. In ICLR, 2017.
[7] M. Elfeki, C. Couprie, M. Riviére, and M. Elhoseiny. Gdpp: Learning diverse generations using determinantal point processes. In ICML, 2019.
[8] A. Ghosh, V. Kulharia, V. Namboodiri, P. H. Torr, and P. K. Dokania. Multi-agent diverse generative adversarial networks. In CVPR, 2018.
[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.
[10] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein gans. In NIPS, 2017.
[11] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017.
[12] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, M. Shakir, and A. Lerchner. Learning basic visual concepts with a constrained variational framework. ICLR, 2017.
[13] D. Jeff and K. Simonyan. Large scale adversarial representation learning. arXiv preprint, 2019.
[14] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018.
[15] H. Kim and A. Mnih. Disentangling by factorising. ICML, 2018.
[16] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014.
[17] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Tech Report, 2009.
[18] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
[19] Z. Lin, A. Khetan, G. Fanti, and S. Oh. Pacgan: The power of two samples in generative adversarial networks. NIPS, 2018.
[20] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In NIPS, 2016.
[21] S. Liu, T. Wang, D. Bau, J.-Y. Zhu, and A. Torralba. Diverse image generation via selfconditioned gans. In CVPR, 2020.
[22] F. Locatello, M. Bauer, Stefan andLucic, G. Rätsch, S. Gelly, B. Schölkopf, and O. Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. ICLM, 2019.
[23] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders. ICML, 2015.
[24] Q. Mao, H.-Y. Lee, H.-Y. Tseng, S. Ma, and M.-H. Yang. Mode seeking generative adversarial networks for diverse image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.
[25] L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for gans do actually converge? In ICML, 2018.
10

[26] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. In ICLR, 2017.
[27] H. Meulemeester, J. Schreurs, M. Fanuel, B. Moor, and J. Suykens. The bures metric for taming mode collapse in generative adversarial networks. In CVPR, 2020.
[28] M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
[29] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
[30] T. Nguyen, T. Le, H. Vu, and D. Phung. Dual discriminator generative adversarial nets. In NIPS, 2017.
[31] G. Peyré and M. Cuturi. Computational optimal transport. 2018. [32] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolu-
tional generative adversarial networks. ICLR, 2016. [33] D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. In ICML, 2015. [34] E. Richardson and Y. Weiss. On gans and gmms. arXiv preprint arXiv:1805.12462, 2018. [35] M. Rosca, B. SLakshminarayanan, D. Warde-Farley, and S. Mohamed. Variational approaches
for auto-encoding generative adversarial networks. arXiv preprint, 2017. [36] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
techniques for training gans. In NIPS, 2017. [37] A. Srivastava, L. Valkov, C. Russell, M. Gutmann, and C. Sutton. Veegan: Reducing mode
collapse in gans using implicit variational learning. In NIPS, 2017. [38] D. Yang, S. Hong, Y. Jang, T. Zhao, and H. Lee. Diversity-sensitive conditional generative
adversarial networks. In ICLR, 2019. [39] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223­2232, 2017.
11

Appendix

A Details in Section 3

A.1 A general case of Proposition 1

Proposition 2 Assume existence of the mapping G which satisfies that G# =  and its inverse G-1. Let  is the joint probability measure of n independent  and  is the joint probability measure of n  where T~# =  and T~ = [T, T, . . . , T ] with concat of n mapping T . then we can get that  is
the joint probability measure with n independent .

Proof 1 Given n measurable set Si  B and S = S1 × S2 × · · · × Sn, then we can get that

(S) = (S1)(S2) . . . (Sn)

(8)

For the reason that T~# =  and T# = , we know that (S) = (T~-1(S)) and (Si) = (T -1(Si)). And then we can get that

(T~-1(S)) = (T -1(S1)) · (T -1(S2)) . . . (T -1(Sn))

(9)

which means that  is the joint probability measure with n independent .

A.2 The proof of Proposition 1

Proposition 1 is a special case of Proposition 2. So the proof of Proposition 1 can easily get from the proof of Proposition 2.

Proof 2 Set n measurable sets as S1 = {x1}, S2 = {x2}, . . . , Sn = {xn} where x1, x2, . . . , xn are n IID samples from target distribution, then according to Proposition 2, we can get that the following
equation with Eq. 9:

(T -1(x1)) · (T -1(x2)) . . . (T -1(xn)) = (T~-1({x1, . . . , xn}))

(10)

which means that {T -1(x(i))}ni=1 can be viewed as n independent samples from source distribution.

B More M-D Gaussian loss with different divergence

In this paper, four methods are used to optimize as Gaussian Consistency loss to reduce the mode collapse:

1) p-norm for the difference of mean and variance. To evaluate the divergence of two Gaus-

sian distributionsN (z; 0, I) and N (z; µ~, ~ ), we first calculate the difference of the parameters of

Gaussian with p-norm:

LGau = µ~ p + ~ - I p

(11)

2) Wasserstein distance. The Wasserstein distance has been widely used to evaluate the distance

between two distributions. Given two M -D Gaussains p(z) and q(z), the 2-Wasserstein distance is:

We can see that W2(p(z), q(z)) = 0 if and only if µ~ = 0 and ~ = I.

3) KL divergence. It is an important divergence to measure the difference between two distributions. Given M -D Gaussians p(z) and q(z), the KL divergence KL(p(z), q(z)) can be specified as:

1 LGau = 2

log(det(~ )) - M + tr(~ -1) + µ~ ~ -1µ~

(12)

4) The z~-discriminator. Real discriminators distinguish whether the generated image is a sample of real distribution p(x). Similarly, we can also use a discriminator to distinguish the difference between real Gaussian samples and generated ones as is done in [23]. We introduce a discriminator to distinguish whether it is from standard Gaussian distribution. We can get the final loss as

min max V (G, D) + Lcons(G, F ) + LGau(F, Dz)

(13)

G,F D,Dz

where LGau(F, Dz) can be defined as

EzPz [log(Dz(z))] + Exp(x) [log(1 - Dz(F (x)))]

(14)

Through alternating training, we can get the optimal G, F and D, Dz.

12

C Conditioned IID-GAN

When the conditioned label c is known, the objective function V (G, D) is optimized for the discriminator and generator by solving the minimax problem in an alternating fashion as:

Exp(x) [log(D(x))] + Ez,cp(z) [log(1 - D(G(z, c)))]

(15)

The first term gives the expectation of probability that x comes from real data distribution p(x) and the

second involves an input distribution p(z, c), which is embodied by a standard multi-dimensional(M -

D) Gaussian distribution N (z; 0, I) and discrete uniform distribution in this paper. To get the

approximate inverse mapping, We adopt the neural network F as the inverse of the generator G. The

reconstruction loss is specified as:

Lre(G, F ) = Ez,c

z c

- F (G(z, c)) 2 + Ex x - G(F (x)) 2

(16)

where z, c are the inputs of the generator to generate the data points x and given x, the inverse mapping F will reconstruct source sample z and label c. Besides, Gaussian loss for conditional IID-GAN are similar to unconditioned case.

D Synthetic Data

D.1 Network Architectures for Synthetic Data

Instead of using tanh as the activation function as adopted in [26, 7] for more stable training, to more directly verify our technique, we resort to ReLU with four linear layers as the network architecture.

Table 6: Network Architecture of Inverse F for

Synthetic Ring-Grid Data.

Layer Output size Activation

Linear

100

ReLu

Linear

200

ReLu

Linear

100

ReLu

Linear

2

-

Table 7: Network Architecture of Discriminator

D for Synthetic Ring-Grid Data.

Layer Output size Activation

Linear

100

ReLu

Linear

200

ReLu

Linear

100

ReLu

Linear

2

-

Table 8: Network Architecture of Discriminator D for Synthetic Ring-Grid Data.

Layer Linear Linear Linear Linear

Output size 100 200 100 1

Activation ReLu ReLu ReLu -

E Training Details
MNIST MNIST contains 70,000 images of handwritten digits [18]. KL divergence is used for evaluation. We set the weights (re, Gau) = (0.5, 0.1). Following [4], a classifier is trained to distinguish the category of generation images. We use the 10-category classifier to divide the generated images into 11 categories. If the highest probability of the generated picture's prediction is smaller than 0.75, it means that the generation quality is poor and classified as bad, otherwise, its label is determined according to the highest probability.
StackedMNIST. Our StackedMNIST covers 1,000 known modes, as constructed by stacking three randomly sampled MNIST images along the RGB channels in line with the practice in [37]. We also follow [37] to evaluate the number of covered modes and divergence between the real and generation distributions. The weights are set (re, Gau) = (3, 3). Results on CIFAR-10 and CIFAR-100. All models are trained for 100K steps i.e. mini-batches. We set (re, Gau) = (3, 3). In Table 4, Inception Score and Fréchet Inception Distance (FID) are used to evaluate the image quality and mode completeness. The best performance is observed for IID-GAN w.r.t. image quality, measured by FID and Inception score and mode completeness, measured by FID. Table 2 shows that IID-GAN outperforms UnrolledGAN, VEEGAN and GDPP.

13

F Synthetic Results
The generation results for Ring and Grid data compared with other methods are given in Figure 9(a) and Figure 9(b). Results of M-D different Gaussion loss are presented in Figure 8.

(a) Generation quality

(b) Inverse KL divergence

Figure 8: Comparison of Gaussian consistency loss on Ring data.

(a) Results of Ring data

(b) Results of Grid data

Figure 9: Comparison among different methods for Ring and Grid.

14

G Real Data

G.1 Network Architectures fo Real Data Table 9: Network Architecture of Inverse F for CIFAR-10 and CIFAR-100.

Layer Output size Activation BN

Conv2d 16, 16, 3

ReLu Yes

Conv2d 8, 8, 64

ReLu Yes

Conv2d 8, 8, 128

ReLu Yes

Flatten

-

-

-

Linear

100

-

-

Table 10: Network Architecture of Generator G for CIFAR-10 and CIFAR-100.

Layer Output size Activation BN

Linear

16384

Relu Yes

Conv'2d 8, 8, 128

ReLu Yes

Conv'2d 16, 16, 64 ReLu Yes

Table 11: Network ACrochnvit'e2cdture o3f2D, i3s2c,ri3minatorTDanfhor CIFAYeRs-10 and CIFAR-100.

Layer Output size Activation BN

Conv2d 16, 16, 64 LeakyReLu No

Conv2d 8, 8, 128 LeakyReLu Yes

Conv2d 4, 4, 256 LeakyReLu Yes

Flatten

-

-

-

Linear

1

-

-

Table 12: Network Architectures of Generator G

for STL-10.

Layer Output size

Kernel

Linear

8192

3 × 3, 256

ResnetBlock 4 × 4

3 × 3, 256

1 × 1, 256

Upsample

8 × 8 scale factor = 2.0

3 × 3, 128

ResnetBlock 8 × 8

3 × 3, 128

1 × 1, 128

Upsample 16 × 16 scale factor = 2.0

3 × 3, 64

ResnetBlock 16 × 16

3 × 3, 64

1 × 1, 64

Upsample 32 × 32 scale factor = 2.0

ResnetBlock 32 × 32

3 × 3, 64 3 × 3, 64

Conv2d

32 × 32

Table 13: Network Architecture of Inverse F for

STL-10.

Layer

Output size

Kernel

Conv2d

32 × 32

3 × 3, 64

ResnetBlock 32 × 32

3 × 3, 64 3 × 3, 64

AvgPool2d 16 × 16 3 × 3, stride 2

3 × 3, 64

ResnetBlock 16 × 16 3 × 3, 128

1 × 1, 128

AvgPool2d

8 × 8 3 × 3, stride 2

3 × 3, 128

ResnetBlock 8 × 8

3 × 3, 256

1 × 1, 256

AvgPool2d

4 × 4 3 × 3, stride 2

3 × 3, 256

ResnetBlock 4 × 4

3 × 3, 512

1 × 1, 512

Linear

20

G.2 Results for Real Data

Results on disentanglement. We perform unsupervised disentanglement learning with M -D IIDGAN as shown in Fig. 11. We study it with polar coordinates system and found that the disentanglement near the Gaussian origin is poor, and it is better if sampling is far from the origin point (i.e. the area with a larger polar radius). By varying polar radius and polar angle, we obtain a good disentanglement result. As shown in Figure 10, we can see the disentanglement results with different coordinate systems. By varying polar angle (y-axis), we can get unsupervised disentanglement results with a large polar radius (x-axis).
Condtional GAN Results. in this experiment, we chose relatively low latent dimensions in order to make the generation more challenging and to make the mode collapse contrast more obvious. Here we use DCGAN network for generation. The generation results for CIFAR-10 with the latent dimension equal to 5 and 10 are shown in Figure 13 and Figure 14 and the generation results for MNIST with the latent dimension equal to 2 are shown in Figure 12.

15

Table 14: Network Architecture of Discriminator D for STL-10.

Layer Conv2d ResnetBlock AvgPool2d
ResnetBlock
AvgPool2d
ResnetBlock
Linear Linear

Output size 32 × 32 32 × 32 16 × 16
16 × 16
8×8
8×8
10 1

Kernel 3 × 3, 64 3 × 3, 64 3 × 3, 64 3 × 3, stride 2 3 × 3, 64 3 × 3, 128 1 × 1, 128 3 × 3, stride 2 3 × 3, 128 3 × 3, 256 1 × 1, 256

Figure 10: Uniformly sampling in different Coordinate systems for MNIST with IID-GAN and VAE model.

Figure 11: Unsupervised disenchantment with uniform sampling from the polar coordinate system by IID-GAN. By varying polar angle (yaxis), we can get unsupervised disentanglement results with a large polar radius (x-axis). See more details in appendix.

On the MNIST dataset, due to the low complexity of the images, we are able to generate recognizable images with 2-dimensional latent input z, which allows us to correspond the image to the twodimensional z-plane. To observe the distribution and diversity characteristics of the generated images, we averaged 20 points between -2 and 2 for each dimension of z. The images were generated and arranged according to the distribution of z as shown in Figure 15. To further explore the role of our framework for generating diversity on CIFAR10 dataset, we change a dimension of the input latent code z by increasing or decreasing the value by gradient and use this z to generate, producing a gradual series of images, and we can observe that compared to the similarity of the images generated by the original CGAN during the change of the input latent code z, our model is able to capture more patterns in a single dimension, improving the generative diversity and presenting a decoupling effect to some extent. We select three image labels for each model under each number of z-dimension for display, and the image results can be found in Figure 16 and Figure 17.
Unconditional Results. As shown in Figure 18, we can see that our IID-GAN can cover almost all modes in StackedMNIST datasets. Figure 19 and Figure 20 are the generation results for two IID-GAN models in CIFAR10 and CIFAR100.

16

(a) Conditional GAN

(b) IID-GAN(1-D)

(c) IID-GAN(M -D)

Figure 12: Results of conditional IID-GAN for MNIST dataset with latent dimension equal to 2. The configuration is the same as Figure 13. Although the images express same Arabic numbers, our model generates these numbers with significantly more patterns, reflecting the promotion of diversity.

(a) Conditional GAN

(b) MSGAN

(c) IID-GAN(M -D)

Figure 13: Results of conditional IID-GAN for CIFAR-10 dataset. Different columns represent different labels. We use the configuration with a batchsize of 256 and a learning rate of 0.0002 here.

(a) Conditional GAN

(b) IID-GAN(1-D)

(c) IID-GAN(M-D)

Figure 14: Results of conditional IID-GAN for CIFAR-10 dataset with random sampling on labels and z.

17

(a) Conditional GAN

(b) IID-GAN(1-D)

(c) IID-GAN(M -D)

Figure 15: The generated distribution of images in the two-dimensional z-plane, with both dimensions of z taking values from -2 to 2. The configuration is the same as Figure 12. It can be seen that IID-GAN have a better diversity performance compared to CGAN.

(a) Conditioned GAN

(b) Conditioned IID-GAN(1-D) (c) Conditioned IID-GAN(M -D)

Figure 16: Comparison on latent dimension of 10 on CIFAR10 dataset with one dimension value of the input latent code z increasing by gradient. The configuration is the same as Figure 13.

(a) Conditional GAN

(b) IID-GAN(M -D)

Figure 17: Comparison on latent dimension of 5 on CIFAR10 dataset with one dimension value of the input latent code z increasing by gradient. The configuration is the same as Figure 13.

18

(a) Ground truth

(b) IID-GAN

Figure 18: Generation results on StackedMNIST for unconditional IID-GAN.

(a) IID-GAN(1-D)

(b) IID-GAN(M -D)

Figure 19: Generation results on CIFAR-10 for unconditional IID-GAN.

(a) IID-GAN(1-D)

(b) IID-GAN(M -D)

Figure 20: Generation results on CIFAR-100 for unconditional IID-GAN.

19

