On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study
Divyansh Kaushik, Douwe Kiela, Zachary C. Lipton, Wen-tau Yih  Carnegie Mellon University;  Facebook AI Research
{dkaushik,zlipton}@cmu.edu, {dkiela,scottyih}@fb.com

arXiv:2106.00872v1 [cs.CL] 2 Jun 2021

Abstract
In adversarial data collection (ADC), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions. Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle. However, despite ADC's intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. In this paper, we conduct a large-scale controlled study focused on question answering, assigning workers at random to compose questions either (i) adversarially (with a model in the loop); or (ii) in the standard fashion (without a model). Across a variety of models and datasets, we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research.1
1 Introduction
Across such diverse natural language processing (NLP) tasks as natural language inference (NLI; Poliak et al., 2018; Gururangan et al., 2018), question answering (QA; Kaushik and Lipton, 2018), and sentiment analysis (Kaushik et al., 2020), researchers have discovered that models can succeed on popular benchmarks by exploiting spurious associations that characterize a particular dataset but do not hold more widely. Despite performing well on independent and identically distributed (i.i.d.) data, these models are liable under plausible domain shifts. With the goal of providing more challenging benchmarks that require this stronger form of generalization, an emerging line of research has
1Data collected during this study is publicly available at https://github.com/facebookresearch/aqa-study.

investigated adversarial data collection (ADC), a scheme in which a worker interacts with a model (in real time), attempting to produce examples that elicit incorrect predictions (e.g., Dua et al., 2019; Nie et al., 2020). The hope is that by identifying parts of the input domain where the model fails one might make the model more robust. Researchers have shown that models trained on ADC perform better on such adversarially collected data and that with successive rounds of ADC, crowdworkers are less able to fool the models (Dinan et al., 2019).
While adversarial data may indeed provide more challenging benchmarks, the process and its actual benefits vis-a-vis tasks of interest remain poorly understood, raising several key questions: (i) do the resulting models typically generalize better out of distribution compared to standard data collection (SDC)?; (ii) how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strategy?
In this paper, we conduct a large-scale randomized controlled study to address these questions. Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures--BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020)-- each fine-tuned on 23.1k examples. To eliminate confounding factors when assessing the impact of ADC, we randomly assign the crowdworkers tasked with generating questions to one of three groups: (i) with an incentive to fool the BERT model; (ii) with an incentive to fool the ELECTRA model; and (iii) a standard, non-adversarial setting (no model in the loop). The pool of contexts is the same for each group and each worker is asked to

Figure 1: Platform shown to workers generating questions in the ADC setting.

generate five questions for each context that they see. Workers are shown similar instructions (with minimal changes), and paid the same base amount.
We fine-tune three models (BERT, RoBERTa, and ELECTRA) on resulting datasets and evaluate them on held-out test sets, adversarial test sets from prior work (Bartolo et al., 2020), and 12 MRQA (Fisch et al., 2019) datasets. For all models, we find that while fine-tuning on adversarial data usually leads to better performance on (previously collected) adversarial data, it typically leads to worse performance on a large, diverse collection of out-of-domain datasets (compared to fine-tuning on standard data). We observe a similar pattern when augmenting the existing dataset with the adversarial data. Results on an extensive collection of out-of-domain evaluation sets suggest that ADC training data does not offer clear benefits vis-a`-vis robustness under distribution shift.
To study the differences between adversarial and standard data, we perform a qualitative analysis, categorizing questions based on a taxonomy (Hovy et al., 2000). We notice that more questions in the ADC dataset require numerical reasoning compared to the SDC sample. These qualitative insights may offer additional guidance to future researchers.
2 Related Work
In an early example of model-in-the-loop data collection, Zweig and Burges (2012) use n-gram lan-

guage models to suggest candidate incorrect answers for a fill-in-the-blank task. Richardson et al. (2013) suggested ADC for QA as proposed future work, speculating that it might challenge state-ofthe-art models. In the Build It Break It, The Language Edition shared task (Ettinger et al., 2017), teams worked as builders (training models) and breakers (creating challenging examples for subsequent training) for sentiment analysis and QA-SRL.
Research on ADC has picked up recently, with Chen et al. (2019) tasking crowdworkers to construct multiple-choice questions to fool a BERT model and Wallace et al. (2019) employing Quizbowl community members to write Jeopardystyle questions to compete against QA models. Zhang et al. (2018) automatically generated questions from news articles, keeping only those questions that were incorrectly answered by a QA model. Dua et al. (2019) and Dasigi et al. (2019) required crowdworkers to submit only questions that QA models answered incorrectly. To construct FEVER 2.0 (Thorne et al., 2019), crowdworkers were required to fool a fact-verification system trained on the FEVER (Thorne et al., 2018) dataset. Some works explore ADC over multiple rounds, with adversarial data from one round used to train models in the subsequent round. Yang et al. (2018b) ask workers to generate challenging datasets working first as adversaries and later as collaborators. Dinan et al. (2019) build on their work, employing ADC to address offensive lan-

guage identification. They find that over successive rounds of training, models trained on ADC data are harder for humans to fool than those trained on standard data. Nie et al. (2020) applied ADC for an NLI task over three rounds, finding that training for more rounds improves model performance on adversarial data, and observing improvements on the original evaluations set when training on a mixture of original and adversarial training data. Williams et al. (2020) conducted an error analysis of model predictions on the datasets collected by Nie et al. (2020). Bartolo et al. (2020) studied the empirical efficacy of ADC for SQuAD (Rajpurkar et al., 2016), observing improved performance on adversarial test sets but noting that trends vary depending on the models used to collect data and to train. Previously, Lowell et al. (2019) observed similar issues in active learning, when the models used to acquire data and for subsequent training differ. Yang et al. (2018a); Zellers et al. (2018, 2019) first collect datasets and then filter examples based on predictions from a model. Paperno et al. (2016) apply a similar procedure to generate a language modeling dataset (LAMBADA). Kaushik et al. (2020, 2021) collect counterfactually augmented data (CAD) by asking crowdworkers to edit existing documents to make counterfactual labels applicable, showing that models trained on CAD generalize better out-of-domain.
Absent further assumptions, learning classifiers robust to distribution shift is impossible (BenDavid et al., 2010). While few NLP papers on the matter make their assumptions explicit, they typically proceed under the implicit assumptions that the labeling function is deterministic (there is one right answer), and that covariate shift (Shimodaira, 2000) applies (the labeling function p(y|x) is invariant across domains). Note that neither condition is generally true of prediction problems. For example, faced with label shift (Scho¨lkopf et al., 2012; Lipton et al., 2018) p(y|x) can change across distributions, requiring one to adapt the predictor to each environment.
3 Study Design
In our study of ADC for QA, each crowdworker is shown a short passage and asked to create 5 questions and highlight answers (spans in the passage, see Fig. 1). We provide all workers with the same base pay and for those assigned to ADC, pay out an additional bonus for each question that fools

the QA model. Finally, we field a different set of workers to validate the generated examples.
Context passages For context passages, we use the first 100 words of Wikipedia articles. Truncating the articles keeps the task of generating questions from growing unwieldy. These segments typically contain an overview, providing ample material for factoid questions. We restrict the pool of candidate contexts by leveraging a variant of the Natural Questions dataset (Kwiatkowski et al., 2019; Lee et al., 2019). We first keep only a subset of 23.1k question/answer pairs for which the context passages are the first 100 words of Wikipedia articles2. From these passages, we sample 10k at random for our study.
Models in the loop We use BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020) models as our adversarial models in the loop, using the implementations provided by Wolf et al. (2020). We fine-tune these models for span-based question-answering, using the 23.1k training examples (subsampled previously) for 20 epochs, with early-stopping based on word-overlap F13 over the validation set. Our BERT model achieves an EM score of 73.1 and an F1 score of 80.5 on an i.i.d. validation set. The ELECTRA model performs slightly better, obtaining an 74.2 EM and 81.2 F1 on the same set.
Crowdsourcing protocol We build our crowdsourcing platform on the Dynabench interface (Kiela et al., 2021) and use Amazon's Mechanical Turk to recruit workers to write questions. To ensure high quality, we restricted the pool to U.S. residents who had already completed at least 1000 HITs and had over 98% HIT approval rate. For each task, we conducted several pilot studies to gather feedback from crowdworkers on the task and interface. We identified median time taken by workers to complete the task in our pilot studies and used that to design the incentive structure for the main task. We also conducted multiple studies with different variants of instructions to observe trends in the quality of questions and refined our instructions based on feedback from crowdworkers. Feedback from the pilots also guided improvements to
2We used the data prepared by Karpukhin et al. (2020), available at https://www.github.com/facebookresearch/DPR.
3Word-overlap F1 and Exact Match (EM) metrics introduced in Rajpurkar et al. (2016) are commonly used to evaluate performance of passage-based QA systems, where the correct answer is a span in the given passage.

Resource Num. Passages

Num. QA Pairs

Train Val Test Train Val Test

BERT 3,412 992 1,056 11,330 1,130 1,130 ELECTRA 3,925 1,352 1,352 14,556 1,456 1,456

Table 1: Number of unique passages and questionanswer pairs for each data resource.

our crowdsourcing interface. In total, 984 workers took part in the study, with 741 creating questions. In our final study, we randomly assigned workers to generate questions in the following ways: (i) to fool the BERT baseline; (ii) to fool the ELECTRA baseline; or (iii) without a model in the loop. Before beginning the task, each worker completes an onboarding process to familiarize them with the platform. We present the same set of passages to workers regardless of which group they are assigned to, tasking them with generating 5 questions for each passage.

Incentive structure During our pilot studies, we found that workers spend  2­3 minutes to generate 5 questions. We provide workers with the same base pay--$0.75 per HIT--(to ensure compensation at a $15/hour rate). For tasks involving a model in the loop, we define a model prediction to be incorrect if its F1 score is less than 40%, following the threshold set by Bartolo et al. (2020). Workers tasked with fooling the model receive bonus pay of $0.15 for every question that leads to an incorrect model prediction. This way, a worker can double their pay if all 5 of their generated questions induce incorrect model predictions.

4 Experiments and Results
Our study allows us to answer three questions: (i) how well do models fine-tuned on ADC data generalize to unseen distributions compared to finetuning on SDC? (ii) Among the differences between ADC and SDC, how many are due to workers trying to fool the model regardless of whether they are successful? and (iii) what is the impact of training on adversarial data only versus using it as a data augmentation strategy?
Datasets For both BERT and ELECTRA, we first identify contexts for which at least one question elicited an incorrect model prediction. Note that this set of contexts is different for BERT and ELECTRA. For each such context c, we identify the number of questions kc (out of 5) that successfully fooled the model. We then create 3 datasets per model by, for each context, (i) choosing precisely those kc questions that fooled the model (BERTfooled and ELECTRAfooled); (ii) randomly choosing kc questions (out of 5) from ADC data without replacement (BERTrandom and ELECTRArandom)--regardless of whether they fooled the model; and (iii) randomly choosing kc questions (out of 5) from the SDC data without replacement. Thus, we create 6 datasets, where all 3 BERT datasets have the same number of questions per context (and 11.3k total training examples), while all 3 ELECTRA datasets likewise share the same number of questions per context (and 14.7k total training examples). See Table 1 for details on the number of passages and question-answer pairs used in the different splits.

Quality control Upon completion of each batch of our data collection process, we presented  20% of the collected questions to a fourth group of crowdworkers who were tasked with validating whether the questions were answerable and the answers were correctly labeled. In addition, we manually verified a small fraction of the collected question-answer pairs. If validations of at least 20% of the examples generated by a particular worker were incorrect, their work was discarded in its entirety. The entire process, including the pilot studies cost  $50k and spanned a period of seven months. Through this process, we collected over 150k question-answer pairs corresponding to the 10k contexts (50k from each group) but the final datasets are much smaller, as we explain below.

Models For our empirical analysis, we fine-tune BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ELECTRA (Clark et al., 2020) models on all six datasets generated as part of our study (four datasets via ADC: BERTfooled, BERTrandom, ELECTRAfooled, ELECTRArandom, and the two datasets via SDC). We also fine-tune these models after augmenting the original data to collected datasets. We report the means and standard deviations (in subscript) of EM and F1 scores following 10 runs of each experiment. Models fine-tuned on all ADC datasets typically perform better on their held-out test sets than those trained on SDC data and vice-versa (Table 2 and Appendix Table 5). RoBERTa fine-tuned on the BERTfooled training set obtains EM and F1 scores of 49.2 and 71.2, respectively, on the BERTfooled test set, outperforming

Evaluation set  Training set 
Original (O; 23.1k) Original (11.3k)
BERTfooled (F; 11.3k) BERTrandom (R; 11.3k) SDC (11.3k)
O + F (34.4k) O + R (34.4k) O + SDC (34.4k)
Original (O; 23.1k) Original (11.3k)
BERTfooled (F; 11.3k) BERTrandom (R; 11.3k) SDC (11.3k)
O + F (34.4k) O + R (34.4k) O + SDC (34.4k)
Original (O; 23.1k) Original (11.3k)
BERTfooled (F; 11.3k) BERTrandom (R; 11.3k) SDC (11.3k)
O + F (34.4k) O + R (34.4k) O + SDC (34.4k)

BERTfooled

EM

F1

BERTrandom

EM

F1

SDC

EM

F1

0.0 8.40.9
34.45.1 37.72.7 33.60.3
39.90.8 38.10.5 33.40.4

17.1 18.70.6
57.05.7 58.92.5 54.40.4
61.70.5 58.80.6 54.50.6

Finetuned model: BERTlarge

29.6 28.80.5

45.2 42.70.9

32.5 33.10.7

44.08.8 57.04.5 57.60.6

61.78.2 73.93.5 74.50.4

47.510.0 62.44.5 68.60.5

50.60.9 57.91.0 60.64.4

68.50.9 74.80.5 77.23.6

52.61.4 62.60.5 69.00.3

49.1 48.61.1
66.88.6 79.73.1 84.20.3
71.81.1 80.20.3 84.30.3

7.3 4.50.4
49.20.5 48.00.4 42.90.9
49.50.5 47.60.7 41.50.4

Finetuned model: RoBERTalarge

16.7 10.81.1

28.6 17.50.9

44.5 26.72.0

32.7 19.52.1

71.20.7 69.80.4 65.30.8

64.91.3 70.30.7 67.00.6

81.31.1 85.30.4 83.60.5

67.91.5 72.50.4 74.40.5

71.10.6 69.50.5 64.20.4

61.60.8 69.20.5 67.30.6

79.50.6 84.60.5 84.30.4

58.32.0 71.10.7 75.00.6

50.1 30.03.2
84.81.0 87.80.1 88.90.3
78.51.2 86.80.3 88.90.2

7.5 8.40.9
40.24.6 42.12.7 39.20.3
40.93.4 41.55.6 38.00.6

Finetuned model: ELECTRAlarge

17.1 18.70.6

29.6 28.80.5

45.2 42.70.9

32.5 33.10.7

63.43.2 63.52.1 40.30.4

50.74.7 58.82.2 59.60.7

68.54.8 76.01.5 76.10.6

56.14.4 65.81.9 69.30.7

63.72.3 61.95.7 58.70.6

52.62.5 58.64.6 59.40.6

70.82.1 75.04.4 76.10.4

55.44.5 64.44.1 70.90.4

49.1 48.61.1
75.63.0 81.71.3 84.20.5
74.44.1 80.43.3 85.10.3

Original Dev.

EM

F1

73.3 66.10.3
34.52.6 46.43.1 48.61.6
72.20.4 72.50.5 72.10.2

80.5 74.20.4
47.93.3 60.63.8 62.31.9
79.80.6 80.20.3 79.80.2

73.5 70.60.3
41.41.0 50.60.8 51.00.5
72.60.4 72.80.6 73.00.2

80.5 78.50.4
55.11.1 64.91.0 62.80.6
80.00.4 80.30.5 80.40.1

74.2 71.80.1
41.04.8 52.61.9 55.70.7
72.71.2 72.62.0 73.60.7

81.2 79.60.1
56.64.2 67.51.4 69.50.5
80.51.0 80.32.1 81.20.4

Table 2: EM and F1 scores of various models evaluated on adversarial and non-adversarial datasets. Adversarial results in bold are statistically significant compared to SDC setting and vice versa with p < 0.05.

RoBERTa models fine-tuned on BERTrandom (EM: 48.0, F1: 69.8) and SDC (EM: 42.0, F1: 65.3). Performance on the original dev set (Karpukhin et al., 2020) is generally comparable across all models.
Out-of-domain generalization to adversarial data We evaluate these models on adversarial test sets constructed with BiDAF (DBiDAF), BERT (DBERT) and RoBERTa (DRoBERTa) in the loop (Bartolo et al., 2020). Prior work suggests that training on ADC data leads to models that perform better on similarly constructed adversarial evaluation sets. Both BERT and RoBERTa models fine-tuned on adversarial data generally outperform models fine-tuned on SDC data (or when either datasets are augmented to the original data) on all three evaluation sets (Table 3 and Appendix Table 6). A RoBERTa model fine-tuned on BERTfooled outperforms a RoBERTa model fine-tuned on SDC by 9.1, 9.3, and 6.2 EM points on DRoBERTa, DBERT, and DBiDAF, respectively. We observe similar trends on ELECTRA models fine-tuned on ADC data versus SDC data, but these gains disappear when the same models are finetuned on augmented data. For instance, while ELECTRA fine-tuned on BERTrandom obtains an EM score of 14.8 on DRoBERTa, outperforming an ELECTRA fine-tuned on SDC data by  3 pts, the difference is no longer significant when respective models are fine-tuned

after original data is augmented to these datasets. ELECTRA models fine-tuned on ADC data with ELECTRA in the loop perform no better than those trained on SDC. Fine-tuning ELECTRA on SDC augmented to original data leads to an  1 pt improvement on both metrics compared to augmenting ADC. Overall, we find that models fine-tuned on ADC data typically generalize better to out-ofdomain adversarial test sets than models fine-tuned on SDC data, confirming the findings by Dinan et al. (2019).
Out-of-domain generalization to MRQA We further evaluate these models on 12 out-of-domain datasets used in the 2019 MRQA shared task4 (Table 4 and Appendix Table 7).5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on
4The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al., 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017).
5Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa's better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020).

Evaluation set  Training set 
Original (23.1k) Original (11.3k)
BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)
Orig + BERTfooled (34.4k) Orig + BERTrandom (34.4k) Orig + SDC (34.4k)
Original (23.1k) Original (11.3k)
BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)
Orig + BERTfooled (34.4k) Orig + BERTrandom (34.4k) Orig + SDC (34.4k)
Original (23.1k) Original (11.3k)
BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)
Orig + BERTfooled (34.4k) Orig + BERTrandom (34.4k) Orig + SDC (34.4k)

DRoBERTa

EM

F1

DBERT

EM

F1

Finetuned model: BERTlarge

6.0 5.40.3

13.5 12.20.1

8.1 7.00.6

11.02.6 12.41.6 9.10.7

21.03.0 22.12.2 20.40.7

14.63.7 16.43.0 14.01.0

15.20.8 16.90.5 9.40.6

25.10.6 23.90.5 20.20.5

20.40.4 20.50.6 15.31.0

14.2 13.60.8
24.74.0 26.22.7 24.60.7
31.00.4 31.20.9 25.81.1

Finetuned model: RoBERTalarge

15.7 14.60.3

25.0 23.80.5

26.5 22.51.2

37.0 32.61.5

21.91.6 21.31.3 12.81.2

32.21.6 31.61.5 23.41.3

30.21.6 31.32.2 20.01.8

42.51.6 43.62.3 32.12.2

25.20.9 24.61.5 16.10.8

36.41.0 35.21.5 27.61.1

35.90.9 35.71.0 26.60.8

48.50.8 48.01.2 39.70.6

Finetuned model: ELECTRAlarge

8.2 8.50.4

17.4 16.70.5

15.7 14.31.0

24.2 23.00.9

13.83.7 14.81.8 11.60.6

24.35.6 25.91.1 22.70.7

18.86.0 22.32.9 17.81.2

31.18.1 34.62.5 30.41.3

16.53.8 18.44.2 15.61.1

28.03.8 28.95.0 27.01.1

23.13.9 25.95.9 22.70.6

35.64.2 37.26.9 36.00.8

DBiDAF

EM

F1

12.6 11.00.9
25.16.5 29.63.7 30.11.2
32.40.6 34.10.4 32.71.2

21.4 19.40.7
39.16.9 43.74.0 43.81.2
47.00.6 47.80.7 47.21.0

37.9 36.01.1
46.31.6 48.01.4 40.02.0
49.60.7 50.61.5 43.40.4

50.4 48.91.2
61.91.5 63.41.3 55.01.8
65.11.1 65.81.2 59.40.3

22.4 20.71.4
29.19.0 34.83.4 32.51.8
34.85.1 37.27.5 34.50.9

34.3 32.01.3
44.311.0 50.52.7 49.31.6
50.25.7 51.19.1 49.51.2

Table 3: EM and F1 scores of various models evaluated on dev datasets of Bartolo et al. (2020). Adversarial results in bold are statistically significant compared to SDC setting and vice versa with p < 0.05.

ADC data collected with BERT) on 9 out of 12 MRQA datasets, with gains of more than 10 EM pts on 6 of them. On BioASQ, BERT fine-tuned on BERTfooled obtains EM and F1 scores of 23.5 and 30.3, respectively. By comparison, fine-tuning on SDC data yields markedly higher EM and F1 scores of 35.1 and 55.7, respectively. Similar trends hold across models and datasets. Interestingly, ADC fine-tuning often improves performance on DROP compared to SDC. For instance, RoBERTa finetuned on ELECTRArandom outperforms RoBERTa fine-tuned on SDC by  7 pts. Note that DROP itself was adversarially constructed. On Natural Questions, models fine-tuned on ADC data generally perform comparably to those fine-tuned on SDC data. RoBERTa fine-tuned on BERTrandom obtains EM and F1 scores of 48.1 and 62.6, respectively, whereas RoBERTa fine-tuned on SDC data obtains scores of 47.9 and 61.7, respectively. It is worth noting that passages sourced to construct both ADC and SDC datasets come from the Natural Questions dataset, which could be one reason why models fine-tuned on ADC datasets perform similar to those fine-tuned on SDC datasets when evaluated on Natural Questions.
On the the adversarial process versus adversarial success We notice that models fine-tuned on

BERTrandom and ELECTRArandom typically outperform models fine-tuned on BERTfooled and ELECTRAfooled, respectively, on adversarial test data collected in prior work (Bartolo et al., 2020), as well as on MRQA. Similar observation can be made when the ADC data is augmented with the original training data. These trends suggest that the ADC process (regardless of the outcome) explains our results more than successfully fooling a model. Furthermore, models fine-tuned only on SDC data tend to outperform ADC-only fine-tuned models; however, following augmentation, ADC fine-tuning achieves comparable performance on more datasets than before, showcasing generalization following augmentation. Notice that augmenting ADC data to original data may not always help. BERT fine-tuned on original 23.1k examples achieves an EM 11.3 on SearchQA. When fine-tuned on BERTfooled augmented to the original data, this drops to 8.7, and when fine-tuned on BERTrandom augmented to the original data, it drops to 11.2. Fine-tuning on SDC augmented to the original data, however, results in EM of 13.6.
5 Qualitative Analysis
Finally, we perform a qualitative analysis over the collected data, revealing profound differences with models in (versus out of) the loop. Recall that be-

Evaluation set  Training set 

BioASQ

EM

F1

DROP

EM

F1

Finetuned model: BERTlarge

DuoRC

EM

F1

Relation Extraction

EM

F1

RACE

EM

F1

TextbookQA

EM

F1

Original (23.1k) Original (11.3k)

19.4

32.5

20.81.7 36.03.4

7.8 6.21.4

16.2

14.5

22.8

12.71.8 13.11.1 19.81.6

BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)

23.56.0 30.33.5 35.12.1

30.33.5 46.82.8 55.71.1

11.53.2 14.42.0 14.60.4

22.23.4 25.12.5 24.70.6

20.34.5 26.73.3 31.70.7

28.25.0 35.33.0 41.20.7

Orig + Fooled (34.4k) Orig + Random (34.4k) Orig + SDC (34.4k)

31.71.2 34.91.2 38.81.5

48.21.2 51.80.9 56.01.3

19.90.9 21.40.6 19.40.9

31.00.8 33.10.4 31.11.0

24.40.9 27.11.2 31.90.4

33.11.4 36.11.2 41.60.6

32.0 42.40.4
51.58.2 61.35.8 63.21.2
55.01.7 62.30.9 62.40.7

47.1 55.90.1
68.96.6 75.94.5 77.70.7
71.51.2 77.10.7 77.80.2

11.4 10.30.6
15.13.1 18.41.8 19.70.6
19.21.3 21.01.4 20.71.4

18.8 18.30.4
26.14.3 29.92.0 31.00.6
31.01.1 33.01.3 32.71.2

25.0

33.4

20.00.9 27.90.7

16.73.8 21.93.1 26.04.3

24.74.6 30.93.8 35.54.7

22.24.7 27.73.9 29.02.4

30.95.4 37.14.0 38.83.1

HotpotQA

EM

F1

Natural Questions

EM

F1

NewsQA

EM

F1

SearchQA

EM

F1

SQuAD

EM

F1

TriviaQA

EM

F1

Original (23.1k) Original (11.3k)

19.4 20.10.3

BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)

27.26.4 37.53.1 41.20.9

Orig + Fooled (34.4k) 34.41.0

Orig + Random (34.4k) 41.00.7

Orig + SDC (34.4k)

43.30.2

33.9 32.60.6
43.27.5 54.43.1 57.91.0
51.10.8 57.30.7 60.00.3

36.3 38.40.5
28.05.7 36.73.9 39.31.2
39.91.3 44.50.4 45.60.9

48.7 50.60.6
42.86.5 51.23.5 53.61.1
54.10.8 58.20.2 58.71.1

16.2

25.6

11.3

19.3

32.5

46.0

16.8

25.3

15.01.0 24.91.7 11.10.7 18.61.2 29.60.4 43.00.7 15.31.0 23.91.4

22.74.7 29.61.9 32.00.8

37.56.4 44.91.9 48.01.1

6.11.7 8.61.4 10.61.4

11.82.2 14.61.8 18.01.3

42.67.6 51.92.6 56.40.4

60.67.9 69.32.1 72.50.4

16.14.6 24.72.8 28.60.8

24.35.4 34.43.0 39.90.9

26.30.9 30.00.5 32.00.8

42.81.1 45.90.6 48.61.1

8.71.5 11.20.7 13.60.4

14.51.7 17.70.9 22.20.5

47.60.5 53.40.4 57.00.3

66.30.5 70.80.4 73.20.3

21.90.7 28.61.3 30.91.0

30.90.8 38.61.4 42.40.9

Evaluation set  Training set 

BioASQ

EM

F1

Finetuned model: RoBERTalarge

DROP

EM

F1

DuoRC

EM

F1

Relation Extraction

EM

F1

RACE

EM

F1

TextbookQA

EM

F1

Original (23.1k) Original (11.3k)

47.7

63.5

37.2

48.1

38.6

49.1

74.4

85.9

33.7

44.9

36.4

46

46.30.1 62.71.0 34.70.3 46.50.8 36.61.8 46.92.1 72.30.8 84.50.3 30.70.2 42.20.3 34.90.4 44.40.2

BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)

35.61.3 40.41.2 41.31.0

51.01.2 57.41.2 59.71.0

34.12.5 38.12.2 24.42.2

46.82.4 51.22.0 38.92.9

31.42.5 36.71.6 41.10.8

39.73.0 45.51.7 51.80.5

67.01.0 71.00.5 72.60.6

81.90.5 84.40.3 84.60.3

28.21.3 31.61.3 29.51.1

41.41.1 45.31.1 43.31.2

25.42.4 29.81.4 35.61.8

35.12.4 39.31.6 46.11.7

Orig + Fooled (34.4k) Orig + Random (34.4k) Orig + SDC (34.4k)

41.21.2 45.71.0 43.10.8

56.70.9 62.20.8 60.90.4

43.31.4 46.51.4 40.21.4

54.71.6 58.01.2 53.80.8

32.00.7 38.90.9 40.01.4

41.51.0 48.90.8 51.91.5

61.32.3 67.61.2 70.90.4

78.31.2 82.60.9 83.30.4

31.70.6 33.61.1 32.90.8

45.71.0 47.10.7 45.70.7

37.62.5 40.01.6 40.91.1

48.02.6 50.31.7 51.91.3

HotpotQA

EM

F1

Natural Questions

EM

F1

NewsQA

EM

F1

SearchQA

EM

F1

SQuAD

EM

F1

TriviaQA

EM

F1

Original (23.1k) Original (11.3k)

48.1 46.60.3

BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)

46.50.8 50.70.6 52.01.3

Orig + Fooled (34.4k) Orig + Random (34.4k) Orig + SDC (34.4k)

47.21.1 53.20.5 53.90.9

63.5 63.20.3
63.30.8 67.70.7 68.71.4
64.71.1 70.10.5 70.70.9

55.3 54.60.4
41.61.2 48.10.9 47.91.2
53.20.7 54.80.4 55.90.4

67.6 66.90.4
56.61.1 62.60.8 61.71.3
66.80.6 68.20.3 68.70.5

38.6 36.31.0
33.81.2 39.50.8 44.00.9
33.90.7 41.60.6 44.20.3

54.4 51.61.2
50.71.6 56.11.1 61.90.7
52.00.7 58.90.7 62.50.4

39.7 33.80.8
15.31.9 17.01.7 24.92.0
28.22.1 30.61.9 36.01.3

49.3 43.00.6
21.51.9 23.61.8 33.02.0
35.32.5 38.32.0 45.21.6

61.9

76.7

60.10.4 75.30.3

60.00.6 65.40.4 66.40.6

77.60.5 81.40.3 82.20.5

58.20.8 65.30.5 66.60.4

76.90.6 81.80.3 82.70.2

47.5 44.90.6
37.01.7 43.31.1 47.00.6
38.80.9 46.71.0 48.00.8

59.6 57.20.7
45.92.1 52.51.2 58.30.7
48.61.0 57.10.9 59.80.7

Evaluation set  Training set 

BioASQ

EM

F1

Finetuned model: ELECTRAlarge

DROP

EM

F1

DuoRC

EM

F1

Relation Extraction

EM

F1

RACE

EM

F1

TextbookQA

EM

F1

Original (23.1k) Original (11.3k)

29.1

42.8

17.6

26.9

18.9

27.1

33.11.4 49.42.5 15.51.8 26.51.1 21.20.8 29.40.6

BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)

32.44.6 37.12.9 40.61.7

50.23.6 55.12.1 59.21.4

19.94.3 21.11.9 17.50.9

33.43.5 35.01.6 30.71.1

25.24.2 30.52.1 33.32.1

35.13.7 40.31.6 43.61.9

Orig + Fooled (34.4k) Orig + Random (34.4k) Orig + SDC (34.4k)

31.71.3 37.85.2 40.00.9

48.21.3 54.45.4 57.60.9

19.90.9 27.66.8 19.40.9

31.00.8 39.48.1 31.11.0

24.50.9 28.45.1 31.90.4

33.11.4 38.25.7 41.60.6

53.4 54.90.9
57.04.9 64.32.9 65.91.4
55.01.7 62.96.8 62.40.7

67.4 69.41.1
74.63.1 78.71.3 79.60.8
71.51.2 77.25.2 76.80.2

19.6

28.5

18.00.8 28.40.7

20.62.5 23.31.5 23.41.1

34.02.5 36.51.5 35.51.0

19.21.3 24.34.6 19.51.4

31.01.1 37.45.3 31.71.2

32.5 29.20.5
19.53.3 25.73.3 27.42.7
22.24.7 34.06.1 29.02.4

41.8 37.80.3
28.54.0 35.13.5 36.82.9
30.95.4 43.56.2 38.83.1

HotpotQA

EM

F1

Natural Questions

EM

F1

NewsQA

EM

F1

SearchQA

EM

F1

SQuAD

EM

F1

TriviaQA

EM

F1

Original (23.1k) Original (11.3k)

29.6

43

40.9

55.3

20.4

32.2

26.80.2 39.70.2 38.70.9 54.20.9 21.01.0 33.21.1

BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k)

36.74.0 41.42.4 43.01.4

54.22.9 58.41.6 59.61.1

35.13.8 43.21.7 46.11.0

51.73.1 58.51.3 60.40.8

28.52.4 33.31.6 35.31.1

45.12.4 49.81.6 51.91.1

Orig + Fooled (34.4k) 34.41.0

Orig + Random (34.4k) 41.44.7

Orig + SDC (34.4k)

43.90.5

51.10.8 57.44.5 60.40.3

45.42.9 46.23.8 49.40.5

59.92.6 60.03.5 63.00.7

26.30.9 31.74.2 32.40.7

42.81.1 47.55.2 49.00.8

21.5 17.21.5
7.01.3 9.21.5 10.51.4
8.71.5 14.92.2 13.60.4

30.3

39.9

54.8

21

31.2

24.81.6 40.51.2 55.91.2 23.91.8 33.51.8

13.91.7 16.82.1 19.01.6

48.34.2 55.42.3 58.61.4

67.53.4 72.91.7 74.91.0

23.82.9 28.91.4 29.01.6

34.52.3 39.91.0 60.71.3

14.51.7 23.12.2 22.20.5

47.60.5 55.24.6 57.61.0

66.30.5 72.14.6 74.01.0

21.90.7 29.85.2 31.70.8

30.90.8 40.25.2 43.40.6

Table 4: EM and F1 scores of various models evaluated on MRQA dev and test sets. Adversarial results in bold are statistically significant compared to SDC setting and vice versa with p < 0.05.

cause these datasets were constructed in a randomized study, any observed differences are attributable to the model-in-the loop collection scheme.
To begin, we analyze 100 questions from each

dataset and categorize them using the taxonomy introduced by Hovy et al. (2000).6 We also look at
6This taxonomy can be accessed at https://www.isi.edu/nat ural-language/projects/webclopedia/Taxonomy/taxonomy

600 550 500 450 400 350 300 250 200 150 100 50
0
who what when where which how
(a) BERTfooled

500 450 400 350 300 250 200 150 100 50
0
who what when where which how
(b) BERTrandom

400 350 300 250 200 150 100 50
0
who what when where which how
(c) SDC-BERT

650 600 550 500 450 400 350 300 250 200 150 100 50
0
who what when where which how

600 550 500 450 400 350 300 250 200 150 100 50
0
who what when where which how

550 500 450 400 350 300 250 200 150 100 50
0
who what when where which how

(d) ELECTRAfooled (e) ELECTRArandom (f) SDC-ELECTRA

Figure 2: Frequency of wh-questions generated.

15

15

15

10

10

10

5

5

5

0

0

0

NUMERICAL-QUANTITY OCCUPATION-PERSON
BDOEFDIYN-IPTIAORNT NAME
WHY-FAMOUS PRTIOTPLEERD--PWLAOCREK
PERSON METHOD-MEANS GROUP-OF-PEDOAPLTEE CIRCUMSTANCE-MEANS
ANIMAL TEEXMPPROERSASIL-OQN-UAONRITIGITNY
SUBSTANCE PROPER-PERSON SPATIAL-AQDJUEACNTTIIVTEY PROPER-ORGANIZATION
INSTLROUCAMTTEEONXTTR DATE-RANGE A INTNEFLCUEEDNECNET NUMERICAL-QUANTITY
PERSON DATE
GROUP-OF-PEOPLE DATE-RANGE WHY-FAMOUS NAME
PROPER-PLACE PROPER-ORGANIZATION
TEMPORAL-QUANTITY PROPER-PERSON SUBSTANCE DEFINITION INFLUENCE ANTECEDENT TITLED-WORK
EXPRESSION-ORIGIN BODY-PART
METHOD-MEANS ABSTRACT ANIMAL COLOR DEFINITION
PROPER-PERSON PROPER-PLACE DATE
MONETARY-QUANTITY PROPER-ORGANIZATION
DATE-RANGE TITLED-WORK WHY-FAMOUS
BODY-PART NUMERICAL-QUANTITY
NAME DISEASE SUBSTANCE ANIMAL TEMPORAL-QUANTITY DEFINITION EXPRESSION-ORIGIN
TEXT GROUP-OF-PEOPLE
INSTRUMENT ADJECTIVE

(a) BERTfooled
20 15 10 5 0

(b) BERTrandom
20 15 10 5 0

(c) SDC-BERT
25 20 15 10 5 0

NUMERICAL-QUANTITY INFLUENCE
PROPER-PERSON DEFINITION
TITLED-WORK PROPER-PLACE
RATING METHOD-MEANS
DATE-RANGE PROPER-ORGANIZATION
DATE LOCATOR MASS-QUANTITY WHY-FAMOUS
NAME GROUP-OF-PEOPLE
PRO-CON CONTRAST DEFINITION PROPER-PERSON PROPER-PLACE PROPER-ORGANIZATION DATE-RANGE WHY-FAMOUS
DATE NAME TITLED-WORK BODY-PART NUMERICAL-QUANTITY YES:NO MASS-QUANTITY INFLUENCE METHOD-MEANS COUNSEL-ADVICE CIRCUMSTANCE-MEANS GROUP-OF-PEOPLE NUMERICAL-QUANTITY PROPER-ORGANIZATION TITLED-WORK DATE BODY-PART PROPER-PERSON WHY-FAMOUS DATE-RANGE PROPER-PLACE TEMPORAL-QUANTITY NAME SUBSTANCE INSTRUMENT SPATIAL-QUANTITY ANTECEDENT LOCATOR EXPRESSION-ORIGIN ANIMAL DEFINITION

(d) ELECTRAfooled

(e) ELECTRArandom

(f) SDC-ELECTRA

Figure 3: Frequency of question types based on the taxonomy introduced by Hovy et al. (2000).

the first word of the wh-type questions in each dev set (Fig. 3) and observe key qualitative differences between data via ADC and SDC for both models.
In case of ADC with BERT (and associated SDC), while we observe that most questions in the dev sets start with what, ADC has a higher proportion compared to SDC (587 in BERTfooled and 492 in BERTrandom versus 416 in SDC). Furthermore, we notice that compared to BERTfooled dev set, SDC has more when- (148) and who-type (220) questions, the answers to which typically refer to dates, places and people (or organizations), respectively. This is also reflected in the taxonomy categorization. Interestingly, the BERTrandom dev set has more when- and who-type questions than BERTfooled (103 and 182 versus 50 and 159, respectively). This indicates that the BERT model could have been better at answering questions related to dates and people (or organizations), which could have further incentivized workers not to generate
toplevel.html

such questions upon observing these patterns. Similarly, in the 100-question samples, we find that a larger proportion of questions in ADC are categorized as requiring numerical reasoning (11 and 18 in BERTfooled and BERTrandom, respectively) compared to SDC (7). It is possible that the model's performance on numerical reasoning (as also demonstrated by its lower performance on DROP compared to fine-tuning on ADC or SDC) would have incentivized workers to generate more questions requiring numerical reasoning and as a result, skewed the distribution towards such questions.
Similarly, with ELECTRA, we observe that what-type questions constitute most of the questions in the development sets for both ADC and SDC, although data collected via ADC has a higher proportion of these (641 in ELECTRAfooled and 619 in ELECTRArandom versus 542 in SDC). We also notice more how-type questions in ADC (126 in ELECTRArandom) vs 101 in SDC, and that the SDC sample has more questions that relate

to dates (223) but the number is lower in the ADC samples (157 and 86 in ELECTRArandom and ELECTRAfooled, respectively). As with BERT, the ELECTRA model was likely better at identifying answers about dates or years which could have further incentivized workers to generate less questions of such types. However, unlike with BERT, we observe that the ELECTRA ADC and SDC 100-question samples contain similar numbers of questions involving numerical answers (8, 9 and 10 in ELECTRAfooled, ELECTRArandom and SDC respectively).
Lastly, despite explicit instructions not to generate questions about passage structure (Fig. 1), a small number of workers nevertheless created such questions. For instance, one worker wrote, "What is the number in the passage that is one digit less than the largest number in the passage?" While most such questions were discarded during validation, some of these are present in the final data. Overall, we notice considerable differences between ADC and SDC data, particularly vis-avis what kind of questions workers generate. Our qualitative analysis offers additional insights that suggest that ADC would skew the distribution of questions workers create, as the incentives align with quickly creating more questions that can fool the model. This is reflected in all our ADC datasets. One remedy could be to provide workers with initial questions, asking them to edit those questions to elicit incorrect model predictions. Similar strategies were employed in (Ettinger et al., 2017), where breakers minimally edited original data to elicit incorrect predictions from the models built by builders, as well as in recently introduced adversarial benchmarks for sentiment analysis (Potts et al., 2020).
6 Conclusion
In this paper, we demonstrated that across a variety of models and datasets, training on adversarial data leads to better performance on evaluation sets created in a similar fashion, but tends to yield worse performance on out-of-domain evaluation sets not created adversarially. Additionally, our results suggest that the ADC process (regardless of the outcome) might matter more than successfully fooling a model. We also identify key qualitative differences between data generated via ADC and SDC, particularly the kinds of questions created.
Overall, our work investigates ADC in a con-

trolled setting, offering insights that can guide future research in this direction. These findings are particularly important given that ADC is more timeconsuming and expensive than SDC, with workers requiring additional financial incentives. We believe that a remedy to these issues could be to ask workers to edit questions rather than to generate them. In the future, we would like to extend this study and investigate the efficacy of various constraints on question creation, and the role of other factors such as domain complexity, passage length, and incentive structure, among others.
Acknowledgements
The authors thank Max Bartolo, Robin Jia, Tanya Marwah, Sanket Vaibhav Mehta, Sina Fazelpour, Kundan Krishna, Shantanu Gupta, Simran Kaur, and Aishwarya Kamath for their valuable feedback on the crowdsourcing platform and the paper.
Ethical Considerations
The passages in our datasets are sourced from the datasets released by Karpukhin et al. (2020) under a Creative Commons License. As described in main text, we designed our incentive structure to ensure that crowdworkers were paid $15/hour, which is twice the US federal minimum wage. Our datasets focus on the English language, and are not collected for the purpose of designing NLP applications but to conduct a human study. We share our dataset to allow the community to replicate our findings and do not foresee any risks associated with the use of this data.
References
Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. 2019. Cloze-driven pretraining of self-attention networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, November 3-7, 2019, pages 5359­5368. Association for Computational Linguistics.
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the AI: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662­678.
Shai Ben-David, Tyler Lu, Teresa Luu, and Da´vid Pa´l. 2010. Impossibility theorems for domain adaptation. In Artificial Intelligence and Statistics (AISTATS).

Michael Chen, Mike D'Arcy, Alisa Liu, Jared Fernandez, and Doug Downey. 2019. CODAH: An adversarially-authored question answering dataset for common sense. In Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 63­69, Minneapolis, USA. Association for Computational Linguistics.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
Pradeep Dasigi, Nelson F. Liu, Ana Marasovic´, Noah A. Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5925­5932, Hong Kong. Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171­4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. 2019. Build it break it fix it for dialogue safety: Robustness from adversarial human attack. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4537­4546, Hong Kong. Association for Computational Linguistics.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368­2378, Minneapolis, Minnesota. Association for Computational Linguistics.
Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. SearchQA: A new Q&A dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179.
Allyson Ettinger, Sudha Rao, Hal Daume´ III, and Emily M. Bender. 2017. Towards linguistically generalizable NLP systems: A workshop and shared

task. In Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 1­10, Copenhagen, Denmark. Association for Computational Linguistics.
Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 1­13, Hong Kong. Association for Computational Linguistics.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107­112, New Orleans, Louisiana. Association for Computational Linguistics.
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained transformers improve out-of-distribution robustness. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2744­2751, Online. Association for Computational Linguistics.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael Junk, and Chin-Yew Lin. 2000. Question answering in webclopedia. In TREC, volume 52.
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601­1611, Vancouver, Canada. Association for Computational Linguistics.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769­ 6781, Online. Association for Computational Linguistics.
Divyansh Kaushik, Eduard H. Hovy, and Zachary Chase Lipton. 2020. Learning the difference that makes A difference with counterfactuallyaugmented data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
Divyansh Kaushik and Zachary C. Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages

5010­5015, Brussels, Belgium. Association for Computational Linguistics.
Divyansh Kaushik, Amrith Setlur, Eduard Hovy, and Zachary C Lipton. 2021. Explaining the efficacy of counterfactually-augmented data. International Conference on Learning Representations (ICLR).
Aniruddha Kembhavi, Min Joon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5376­5384. IEEE Computer Society.
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in NLP. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4110­4124.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452­466.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785­794, Copenhagen, Denmark. Association for Computational Linguistics.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086­6096, Florence, Italy. Association for Computational Linguistics.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333­342.
Zachary C. Lipton, Yu-Xiang Wang, and Alexander J. Smola. 2018. Detecting and correcting for label shift with black box predictors. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of

Proceedings of Machine Learning Research, pages 3128­3136. PMLR.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
David Lowell, Zachary C. Lipton, and Byron C. Wallace. 2019. Practical obstacles to deploying active learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 21­30.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885­4901, Online. Association for Computational Linguistics.
Denis Paperno, Germa´n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Ferna´ndez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525­1534, Berlin, Germany. Association for Computational Linguistics.
Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180­191, New Orleans, Louisiana. Association for Computational Linguistics.
Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. 2020. DynaSent: A Dynamic Benchmark for Sentiment Analysis. arXiv preprint arXiv:2012.15349.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383­2392, Austin, Texas. Association for Computational Linguistics.
Matthew Richardson, Christopher J.C. Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193­203, Seattle, Washington, USA. Association for Computational Linguistics.
Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. 2018. DuoRC: Towards

complex language understanding with paraphrased reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1683­1693, Melbourne, Australia. Association for Computational Linguistics.
Bernhard Scho¨lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris M. Mooij. 2012. On causal and anticausal learning. In Proceedings of the 29th International Conference on Machine Learning, ICML.
Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227­244.
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809­819, New Orleans, Louisiana. Association for Computational Linguistics.
James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. 2019. The FEVER2.0 shared task. In Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 1­6, Hong Kong. Association for Computational Linguistics.
George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. 2015. An overview of the bioasq largescale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16(1).
Lifu Tu, Garima Lalwani, Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models. Transactions of the Association for Computational Linguistics, 8:621­633.
Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber. 2019. Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering. Transactions of the Association for Computational Linguistics, 7:387­401.

Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38­45, Online. Association for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018a. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369­2380, Brussels, Belgium. Association for Computational Linguistics.
Zhilin Yang, Saizheng Zhang, Jack Urbanek, Will Feng, Alexander H. Miller, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018b. Mastering the dungeon: Grounded language learning by mechanical turker descent. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93­ 104, Brussels, Belgium. Association for Computational Linguistics.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791­4800.
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885.
Geoffrey Zweig and Chris J.C. Burges. 2012. A challenge set for advancing language modeling. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 29­36, Montre´al, Canada. Association for Computational Linguistics.
A Appendix

Adina Williams, Tristan Thrush, and Douwe Kiela. 2020. Anlizing the adversarial natural language inference dataset. arXiv preprint arXiv:2010.12729.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,

Evaluation set  Training set 

ELECTRAfooled

EM

F1

ELECTRArandom

EM

F1

SDC

EM

F1

Original Dev.

EM

F1

Finetuned model: BERTlarge

Original (O; 23.1k) Original (14.6k)

23.3

31.9

56.7

72.6

63.8

78.5

73.3

80.5

36.70.4 50.70.3 48.20.4 64.40.2 55.70.1 70.50.3 67.10.2 75.20.1

ELECTRAfooled (F; 14.6k) 25.11.0

ELECTRArandom (R; 14.6k) 25.41.1

SDC (14.6k)

23.11.0

42.41.0 42.01.0 40.81.3

35.41.5 38.40.9 36.31.3

54.31.1 56.80.8 56.31.3

39.12.4 42.01.4 45.21.8

59.31.7 61.71.3 65.41.5

31.97.9 46.43.1 48.61.6

45.09.2 60.63.8 62.31.9

O + F (37.7k) O + R (37.7k) O + SDC (37.7k)

26.71.7 26.00.8 24.50.7

43.10.9 42.90.6 41.70.7

40.11.3 41.70.5 41.40.9

58.71.5 60.30.6 60.70.4

44.60.9 47.11.4 50.91.0

64.21.2 66.51.3 69.70.3

72.10.5 73.00.5 72.00.1

79.70.7 80.50.2 79.70.1

Finetuned model: RoBERTalarge

Original (O; 23.1k) Original (14.6k)

49.2

64.4

59.1

75.8

64.5

79.8

73.5

80.5

48.30.9 63.31.4 58.70.9 74.91.0 62.70.4 79.00.7 71.50.5 79.30.6

ELECTRAfooled (F; 14.6k) ELECTRArandom (R; 14.6k) SDC (14.6k)

65.30.5 64.60.5 61.00.2

79.90.5 79.40.4 77.10.3

69.40.6 70.40.5 67.90.4

84.60.5 85.40.3 84.10.4

75.80.6 76.50.5 77.30.5

89.00.3 89.40.3 89.90.3

55.91.2 59.81.2 55.71.0

67.51.0 70.60.9 68.80.8

O + F (37.7k) O + R (37.7k) O + SDC (37.7k)

65.00.3 64.30.3 61.50.5

79.90.3 78.80.3 77.20.3

70.10.5 70.70.2 69.00.4

85.20.4 85.80.2 84.70.4

76.20.3 76.50.6 77.60.4

89.70.2 89.70.3 90.50.2

73.30.3 73.40.5 73.60.5

80.70.2 80.80.3 80.90.4

Finetuned model: ELECTRAlarge

Original (O; 23.1k) Original (14.6k)

0

10.8

40.2

57.8

44.8

60.9

74.2

81.2

25.90.2 40.90.4 37.30.6 63.90.7 53.61.3 74.71.1 71.90.3 79.50.3

ELECTRAfooled (F; 14.6k) ELECTRArandom (R; 14.6k) SDC (14.6k)

26.41.5 23.44.9 24.52.4

44.01.6 40.55.6 43.73.5

41.21.5 42.36.9 40.63.5

60.81.3 62.37.0 61.53.8

42.74.0 42.18.0 46.95.4

63.53.2 62.97.5 68.24.7

57.50.9 57.60.8 54.91.8

68.80.7 69.31.0 68.31.2

O + F (37.7k) O + R (37.7k) O + SDC (37.7k)

25.31.9 21.71.1 24.51.8

43.72.0 40.11.1 43.41.6

40.21.9 42.22.3 42.81.5

60.61.9 64.81.9 63.51.0

41.73.9 38.03.6 49.61.9

63.43.6 60.82.9 70.31.5

73.60.5 74.40.3 74.20.2

81.10.4 81.70.1 81.50.1

Table 5: EM and F1 scores of various models evaluated on adversarial datasets collected with an ELECTRAlarge model and non-adversarial datasets. Adversarial results in bold are statistically significant compared to SDC setting and vice versa with p < 0.05.

Evaluation set  Training set 

DRoBERTa

EM

F1

DBERT

EM

F1

DBiDAF

EM

F1

Original (23.1k) Original (14.6k)
ELECTRAfooled 14.6k) ELECTRArandom 14.6k) SDC (14.6k)
Orig + ELECTRAfooled (37.7k) Orig + ELECTRArandom (37.7k) Orig + SDC (37.7k)

Finetuned model: BERTlarge

6.0 5.30.2

13.5 11.40.2

8.1 6.80.8

3.80.5 4.30.5 3.90.4

13.30.7 13.70.7 13.20.4

6.20.7 6.40.4 5.40.4

6.40.5 6.60.6 5.80.2

16.10.3 16.10.3 15.60.4

7.80.8 8.50.6 8.70.5

14.2

12.6

21.4

13.90.5 12.10.4 20.60.2

16.40.5 16.40.8 15.10.5

12.61.2 13.60.8 10.80.7

26.21.0 27.11.2 23.80.8

18.00.6 18.40.5 18.70.6

17.00.2 16.90.3 17.40.7

31.00.6 30.80.4 30.00.8

Finetuned model: RoBERTalarge

Original (23.1k) Original (14.6k)

15.7

25.0

26.5

37.0

37.9

50.4

14.30.2 23.70.3 25.10.3 35.40.7 37.40.7 50.20.5

ELECTRAfooled 14.6k) ELECTRArandom 14.6k) SDC (14.6k)

16.40.9 15.81.4 12.11.0

27.71.2 27.21.4 23.91.3

27.41.3 28.11.6 22.71.1

40.81.5 41.51.8 35.41.5

46.81.1 48.00.9 40.51.3

62.41.1 63.00.6 56.81.3

Orig + ELECTRAfooled (37.7k) Orig + ELECTRArandom (37.7k) Orig + SDC (37.7k)

18.90.8 18.00.4 18.21.0

30.40.9 29.60.3 29.70.9

33.20.8 32.30.6 28.20.3

46.40.6 45.11.2 41.40.5

49.20.9 48.20.8 45.00.9

65.10.8 63.50.6 60.90.6

Finetuned model: ELECTRAlarge

Original (23.1k) Original (14.6k)

8.2 9.50.2

17.4

15.7

24.2

22.4

34.3

18.00.5 15.40.5 24.20.6 21.70.2 33.10.1

ELECTRAfooled 14.6k) ELECTRArandom 14.6k) SDC (14.6k)
Orig + ELECTRAfooled (37.7k) Orig + ELECTRArandom (37.7k) Orig + SDC (37.7k)

10.20.3 10.40.5 10.30.8
10.20.3 10.40.5 10.30.8

21.70.5 21.30.5 21.60.7
21.70.5 21.30.5 21.60.7

17.00.7 16.50.2 15.81.1
17.00.7 16.50.2 15.81.1

29.70.6 28.60.8 28.51.2
29.70.6 28.60.8 28.51.2

21.71.7 19.95.0 19.34.8

36.61.1 34.45.9 33.37.8

24.00.7 23.50.5 24.50.6

39.20.7 38.40.4 39.90.6

Table 6: EM and F1 scores of various models evaluated on dev datasets of Bartolo et al. (2020). Adversarial results in bold are statistically significant compared to SDC setting and vice versa with p < 0.05.

Evaluation set  Training set 

BioASQ

EM

F1

DROP

EM

F1

Finetuned model: BERTlarge

DuoRC

EM

F1

Relation Extraction

EM

F1

RACE

EM

F1

TextbookQA

EM

F1

Original (23.1k) Original (14.6k)

19.4

32.5

20.40.3 35.90.7

ELECTRAfooled (14.6k) ELECTRArandom (14.6k) SDC (14.6k)

13.60.9 15.90.8 17.10.7

29.11.1 32.01.7 34.51.0

Orig + Fooled (37.7k) Orig + Random (37.7k) Orig + SDC (37.7k)

17.81.0 20.01.1 21.80.6

33.52.0 36.41.6 39.21.1

7.8 5.10.3
3.20.4 3.10.4 2.60.3
6.11.1 6.80.9 6.10.5

16.2 12.40.3
11.90.7 10.50.9 10.10.9
16.11.7 17.11.0 16.10.7

14.5 11.60.4
11.00.9 12.10.9 11.90.8
14.21.4 14.61.0 16.70.9

22.8

32.0

17.80.6 33.00.9

19.30.6 20.41.4 21.21.2

33.62.2 35.73.1 34.23.4

22.91.9 23.51.5 25.91.0

42.02.2 44.01.3 43.40.7

47.1 44.22.0
52.52.3 55.63.7 53.74.1
59.62.5 61.81.3 61.01.1

11.4 10.40.6
7.90.7 9.50.7 9.21.0
12.00.9 12.00.9 11.90.7

18.8 17.70.9
17.70.8 19.10.8 19.00.7
22.20.9 22.00.9 22.50.7

25.0

33.4

19.50.6 27.30.7

12.21.7 14.61.8 17.51.1

21.21.8 23.91.8 27.41.3

24.61.0 23.90.8 25.40.5

33.71.2 33.51.0 35.50.6

HotpotQA

EM

F1

Natural Questions

EM

F1

NewsQA

EM

F1

SearchQA

EM

F1

SQuAD

EM

F1

TriviaQA

EM

F1

Original (23.1k) Original (14.6k)

19.4

33.9

17.40.9 28.71.2

ELECTRAfooled (14.6k) ELECTRArandom (14.6k) SDC (14.6k)

19.10.7 21.21.0 23.51.2

33.40.8 35.51.3 37.81.3

Orig + Fooled (37.7k) Orig + Random (37.7k) Orig + SDC (37.7k)

25.51.4 26.71.2 29.01.0

40.81.5 41.91.2 42.60.8

36.3 35.00.7
28.01.4 29.02.3 28.41.7
38.51.1 38.61.0 38.70.3

48.7 47.70.7
43.11.4 43.82.3 43.51.4
52.21.1 52.60.7 52.40.1

16.2

25.6

12.80.2 22.60.1

11.3 9.00.1

19.3

32.5

46.0

16.8

25.3

13.80.4 26.00.3 39.20.7 11.80.5 18.20.7

12.90.8 13.80.8 15.60.8

25.90.8 27.11.3 30.31.0

4.00.3 4.20.4 5.00.5

9.10.5 9.10.6 9.90.7

26.91.4 29.21.6 31.50.7

46.41.4 48.32.2 50.50.8

9.20.8 10.00.7 10.00.9

16.31.1 17.31.2 19.11.3

17.00.7 17.00.4 18.70.6

30.91.2 30.70.7 33.90.5

9.90.4 9.20.9 11.10.7

15.80.8 14.61.2 16.60.9

32.71.5 34.30.6 36.10.7

51.71.5 53.30.8 54.90.5

14.21.6 14.10.7 15.10.3

22.61.8 22.71.1 24.20.2

Evaluation set  Training set 

BioASQ

EM

F1

Finetuned model: RoBERTalarge

DROP

EM

F1

DuoRC

EM

F1

Relation Extraction

EM

F1

RACE

EM

F1

TextbookQA

EM

F1

Original (23.1k) Original (14.6k)

47.7 45.41.7

ELECTRAfooled (14.6k) ELECTRArandom (14.6k) SDC (14.6k)

41.21.4 43.31.4 43.71.0

Orig + Fooled (37.7k) Orig + Random (37.7k) Orig + SDC (37.7k)

45.01.2 46.31.0 47.50.5

63.5 61.81.0
57.21.1 60.01.5 62.50.7
61.21.0 62.60.8 64.00.5

37.2

48.1

37.51.7 48.72.0

30.31.7 34.12.4 27.52.6

44.91.8 48.82.0 43.42.9

45.91.6 45.51.2 42.71.1

58.11.3 57.80.8 55.51.0

38.6 37.80.7
37.92.1 39.21.5 42.30.9
36.81.4 39.11.3 42.11.3

49.1 48.70.8
47.22.3 48.81.6 53.51.1
47.21.7 49.31.3 53.71.1

74.4 75.00.6
74.10.8 75.50.5 74.90.8
73.90.4 74.70.5 74.70.9

85.9 86.00.2
86.00.4 85.90.2 85.30.7
86.30.3 86.60.2 86.90.5

33.7 32.40.7
31.71.3 32.60.7 31.50.9
33.70.9 34.10.2 33.91.2

44.9 43.40.9
45.41.0 46.30.5 46.01.0
47.30.9 47.20.4 47.31.0

36.4

46

36.81.1 46.21.3

30.81.7 32.21.2 36.32.0

40.51.8 42.21.4 47.22.0

38.50.9 39.91.5 41.90.4

48.31.2 49.91.9 52.50.3

HotpotQA

EM

F1

Natural Questions

EM

F1

NewsQA

EM

F1

SearchQA

EM

F1

SQuAD

EM

F1

TriviaQA

EM

F1

Original (23.1k) Original (14.6k)

19.4

33.9

47.00.3 62.70.3

ELECTRAfooled (14.6k) 51.90.9

ELECTRArandom (14.6k) 54.50.8

SDC (14.6k)

55.80.8

67.91.0 71.00.8 71.80.8

Orig + Fooled (37.7k) Orig + Random (37.7k) Orig + SDC (37.7k)

55.60.8 56.00.2 57.50.7

71.70.9 71.90.3 72.80.6

36.3 55.60.4
49.60.6 51.60.6 51.70.5
57.10.3 56.50.2 56.90.3

48.7 67.50.5
64.10.7 65.90.6 65.80.5
69.60.3 69.10.3 69.40.3

16.2 38.20.2
37.80.9 40.21.1 43.90.8
40.61.5 42.30.3 44.30.7

25.6 53.60.3
54.91.0 57.71.2 62.11.0
57.71.8 59.30.7 62.70.7

11.3 34.50.8
24.02.0 24.32.6 24.42.4
38.32.4 39.41.6 39.31.0

19.3 43.80.6
31.32.2 32.92.6 32.92.4
47.32.7 48.51.7 48.61.1

32.5

46.0

60.50.4 75.60.5

66.20.4 66.90.2 68.40.5

82.00.3 82.60.2 84.30.3

67.00.5 68.00.2 69.90.4

82.70.4 83.30.2 84.30.2

16.8 46.50.5
45.11.1 45.80.8 47.30.7
46.71.0 47.80.3 48.60.5

25.3 58.50.7
55.21.1 56.21.0 59.10.7
57.51.0 58.80.3 60.10.5

Finetuned model: ELECTRAlarge

Evaluation set  Training set 

BioASQ

EM

F1

DROP

EM

F1

DuoRC

EM

F1

Relation Extraction

EM

F1

RACE

EM

F1

TextbookQA

EM

F1

Original (23.1k) Original (14.6k)

29.1

42.8

17.6

26.9

18.9

27.1

53.4

67.4

19.6

28.5

32.5

41.8

35.40.4 51.00.8 16.20.5 26.60.8 18.80.4 26.70.8 46.21.3 61.11.7 17.30.9 27.90.6 29.60.6 37.80.7

ELECTRAfooled (14.6k) ELECTRArandom (14.6k) SDC (14.6k)

25.31.1 25.54.9 25.07.5

41.01.6 41.65.5 41.01.7

7.60.9 7.82.6 5.92.1

18.91.4 19.25.3 17.94.4

12.31.5 12.12.3 13.23.0

20.52.0 19.72.9 22.54.9

42.12.0 40.37.7 42.76.6

61.42.3 57.79.4 61.97.5

13.50.6 13.02.7 13.42.7

25.11.0 24.03.7 24.74.0

20.82.5 20.33.5 20.83.8

29.52.9 28.83.4 29.53.4

Orig + Fooled (37.7k) Orig + Random (37.7k) Orig + SDC (37.7k)

28.42.0 28.61.6 29.71.9

45.22.6 44.92.0 47.02.2

15.60.8 16.30.6 15.60.8

28.61.0 29.01.2 29.11.3

13.31.0 12.81.0 16.40.7

21.21.7 20.91.6 27.10.8

41.52.8 39.43.3 48.01.8

60.53.3 58.83.6 67.01.5

17.60.7 16.61.3 19.00.6

29.60.9 29.01.1 32.10.8

32.20.9 32.40.4 33.70.4

41.61.1 42.20.5 43.80.9

HotpotQA

EM

F1

Natural Questions

EM

F1

NewsQA

EM

F1

SearchQA

EM

F1

SQuAD

EM

F1

TriviaQA

EM

F1

Original (23.1k) Original (14.6k)

19.4

33.9

36.3

48.7

16.2

25.6

11.3

19.3

32.5

46.0

16.8

25.3

23.21.0 40.21.1 33.40.8 49.80.5 17.90.5 31.10.9 16.00.5 22.31.1 31.10.4 50.10.5 21.00.9 29.81.3

ELECTRAfooled (14.6k) ELECTRArandom (14.6k) SDC (14.6k)

26.20.9 24.75.5 24.43.3

42.20.9 40.96.9 41.75.2

31.51.4 27.96.8 28.86.2

49.71.1 45.77.6 46.78.3

18.71.2 17.23.1 19.23.6

32.11.6 30.83.8 35.53.2

6.50.7 6.41.6 8.30.9

10.41.0 10.32.1 12.81.6

34.51.3 34.15.8 34.74.2

53.71.5 53.16.2 54.15.1

13.21.0 12.43.4 13.42.0

21.51.3 20.14.5 22.73.5

Orig + Fooled (37.7k) Orig + Random (37.7k) Orig + SDC (37.7k)

28.50.9 28.11.5 30.51.1

45.81.3 45.91.3 47.80.8

35.00.8 34.11.1 35.81.1

52.51.0 51.71.1 53.40.8

20.30.7 19.21.1 23.00.7

34.91.0 34.11.8 40.20.7

14.31.0 14.30.8 16.50.6

19.81.4 20.11.3 22.81.1

36.71.3 35.61.7 40.60.6

56.51.5 55.31.4 60.70.4

15.31.6 15.01.4 18.80.8

24.32.0 24.52.0 30.00.8

Table 7: EM and F1 scores of various models evaluated on MRQA dev and test sets. Adversarial results in bold are statistically significant compared to SDC setting and vice versa with p < 0.05.

Resource Examples

BERTfooled

Lothal [SEP] Lothal ( ) is one of the southernmost cities of the ancient Indus Valley Civilization , located in the Bha¯l region ( Ahammedabad District , Dholka Taluk)of the modern state of Gujara¯t and first inhabited 3700 BCE . The meaning of the word Lothal is " the mount of the dead " exactly same as that of Mohenjodaro another famous site of Indus Valley civilization . Discovered in 1954 , Lothal was excavated from 13 February 1955 to 19 May 1960 by the Archaeological Survey of India ( ASI ) , the official Indian government agency for the preservation of ancient monuments . According to the ASI , Lothal had the world 's earliest What is Lothal and its ancient location? One Way or Another [SEP] " One Way or Another " is a song by American new wave band Blondie from the album " Parallel Lines " . The song was released as the fourth single in the US and Canada as the follow - up to the no . 1 hit " Heart of Glass " . " One Way or Another " reached No . 24 on the " Billboard " Hot 100 and No . 7 on the " RPM " 100 Singles . Written by Debbie Harry and Nigel Harrison for the band 's third studio album , " Parallel Lines " ( 1978 ) , the song was inspired by one of Harry 's ex - boyfriends who stalked her after their breakup . The song was Not only did One Way or Another chart on Billboard Hot 100 but it also climbed what other chart? India International Exchange [SEP] The India International Exchange ( INX ) is India 's first international stock exchange , opened in 2017 . It is located at the International Financial Services Centre ( IFSC ) , GIFT City in Gujarat . It is a wholly owned subsidiary of the Bombay Stock Exchange ( BSE ) . The INX will be initially headed by V. Balasubramanian with other staff from the BSE . It was inaugurated on 9 January 2017 by Indian prime minister Narendra Modi , the trading operations were scheduled to begin on 16 January 2017 . It was claimed to be the world 's most advanced technological platform with a turn - around time of 4 micro Where will the workers of the INX come from?

True Detective ( season 2 ) [SEP] The second season of " True Detective " , an American anthology crime drama television series created by Nic Pizzolatto , began airing on June 21 , 2015 , on the premium cable network HBO . With a principal cast of Colin Farrell , Rachel McAdams , Taylor Kitsch , Kelly Reilly , and Vince Vaughn , the season comprises eight episodes and concluded its initial airing on August 9 , 2015 . The season 's story takes place in California and follows the interweaving stories of officers from three cooperating police departments ; when California Highway Patrol officer and war veteran Paul Woodrugh ( Kitsch ) Who created True Detective? BERTrandom History of time in the United States [SEP] The history of standard time in the United States began November 18 , 1883 , when United States and Canadian railroads instituted standard time in time zones . Before then , time of day was a local matter , and most cities and towns used some form of local solar time , maintained by some well known clock ( for example , on a church steeple or in a jeweler 's window ) . The new standard time system was not immediately embraced by all . Use of standard time gradually increased because of its obvious practical advantages for communication and travel . Standard time in time What form of time did most cities and towns use before standard? One Call Away ( Charlie Puth song ) [SEP] " One Call Away " is a song by American singer Charlie Puth for his debut album " Nine Track Mind " . It was released on August 20 , 2015 by Atlantic Records as the second single from the album , after the lead single " Marvin Gaye " . " One Call Away " is a gospel - infused pop soul song . It reached number 12 on the " Billboard " Hot 100 , making it Puth 's third top 40 single in the US and his third highest - charting single as a lead artist to date , behind " We Do n't Talk Anymore " and What is Charlie Puth's first album?

Cap of invisibility [SEP] In classical mythology , the Cap of Invisibility ( " ( H)a¨idos kunee¯n " in Greek , lit .

dog - skin of Hades ) is a helmet or cap that can turn the wearer invisible . It is also known as the Cap of Hades

, Helm of Hades , or Helm of Darkness . Wearers of the cap in Greek myths include Athena , the goddess of

wisdom , the messenger god Hermes , and the hero Perseus . The Cap of Invisibility enables the user to become

invisible to other supernatural entities , functioning much like the cloud of mist that the gods surround themselves

in to become undetectable . One ancient

What is the name given to a cap or helmet that renders the wearer unable to be seen in classical mythol-

ogy?

SDC

The Dark Side of the Moon [SEP] The Dark Side of the Moon is the eighth studio album by English rock band

Pink Floyd , released on 1 March 1973 by Harvest Records . It built on ideas explored in Pink Floyd 's earlier

recordings and performances , but without the extended instrumentals that characterised their earlier work . A

concept album , its themes explore conflict , greed , time , and mental illness , the latter partly inspired by the

deteriorating health of founding member Syd Barrett , who left in 1968 . Developed during live performances ,

Pink Floyd premiered an early version of " The Dark Side of the Moon

Which company released the album "The Dark Side of the Moon"?

The Boy in the Striped Pyjamas [SEP] The Boy in the Striped Pyjamas is a 2006 Holocaust novel by Irish novelist

John Boyne . Unlike the months of planning Boyne devoted to his other books , he said that he wrote the entire

first draft of " The Boy in the Striped Pyjamas " in two and a half days , barely sleeping until he got to the end .

He did , however , commit to nearly 20 years of research , reading and researching about the Holocaust as a

teenager before the idea for the novel even came to him . As of March 2010 , the novel had sold

How many days did it take John Boyne to write the first draft of The Boy in the Striped Pyjamas?

Table 8: Validation set examples of questions in different resources. Correct answers are highlighted in red.

Resource

Examples

ELECTRAfooled

Six ( TV series ) [SEP] Six ( stylized as SIX ) is an American television drama series . The series was ordered by the History channel with an eight - episode initial order . The first two episodes were directed by Lesli Linka Glatter . " Six " premiered on January 18 , 2017 . " Six " was renewed for a second season of 10 episodes on February 23 , 2017 , which premiered on May 28 , 2018 , with the second new episode airing during its regular timeslot on May 30 , 2018 . On June 29 , History announced they had cancelled the series after two seasons . The series chronicles the operations and daily lives of operators Who directed the first two episodes of six? Outer space [SEP] Outer space , or just space , is the expanse that exists beyond the Earth and between celestial bodies . Outer space is not completely empty -- it is a hard vacuum containing a low density of particles , predominantly a plasma of hydrogen and helium as well as electromagnetic radiation , magnetic fields , neutrinos , dust , and cosmic rays . The baseline temperature , as set by the background radiation from the Big Bang , is . The plasma between galaxies accounts for about half of the baryonic ( ordinary ) matter in the universe ; it has a number density of less than one hydrogen atom per cubic Half of the ordinary matter in the universe is comprised of what? Ode to Billie Joe [SEP] " Ode to Billie Joe " is a song written and recorded by Bobbie Gentry , a singer songwriter from Chickasaw County , Mississippi . The single , released on July 10 , 1967 , was a number one hit in the US and a big international seller . " Billboard " ranked the record as the No . 3 song of the year . It generated eight Grammy nominations , resulting in three wins for Gentry and one for arranger Jimmie Haskell . " Ode to Billie Joe " has since made " Rolling Stone" 's lists of the " 500 Greatest Songs of All Time " and the " 100 Greatest Country Songs of All Time " and " Pitchfork" What did "Billboard" rank as the No. 3 song of the year in 1967?

Sagrada Fam´ilia [SEP] The (; ; ) is a large unfinished Roman Catholic church in Barcelona , designed by Catalan architect Antoni Gaud´i ( 1852­1926 ) . Gaud´i 's work on the building is part of a UNESCO World Heritage Site , and in November 2010 Pope Benedict XVI consecrated and proclaimed it a minor basilica , as distinct from a cathedral , which must be the seat of a bishop . In 1882 , construction of Sagrada Fam´ilia started under architect Francisco de Paula del Villar . In 1883 , when Villar resigned , Gaud´i took over as chief architect , transforming the project with his architectural and engineering style What kind of unfinished church is the Sagrada Fam´ilia? ELECTRArandom Loyola Ramblers men 's basketball [SEP] The Loyola Ramblers men 's basketball team represents Loyola University Chicago in Chicago , Illinois . The Ramblers joined the Missouri Valley Conference on July 1 , 2013 , ending a 34-season tenure as charter members of the Horizon League . In 1963 , Loyola won the 1963 NCAA Men 's Division I Basketball Tournament ( then the " NCAA University Division " ) men 's basketball national championship under the leadership of All - American Jerry Harkness , defeating two time defending champion Cincinnati 60­58 in overtime in the title game . All five starters for the Ramblers played the entire championship game without substitution . Surviving team members were When did the Ramblers join the Missouri Valley Conference? The Walking Dead ( season 7 ) [SEP] The seventh season of " The Walking Dead " , an American post apocalyptic horror television series on AMC , premiered on October 23 , 2016 , and concluded on April 2 , 2017 , consisting of 16 episodes . Developed for television by Frank Darabont , the series is based on the eponymous series of comic books by Robert Kirkman , Tony Moore , and Charlie Adlard . The executive producers are Kirkman , David Alpert , Scott M. Gimple , Greg Nicotero , Tom Luse , and Gale Anne Hurd , with Gimple as showrunner for the fourth consecutive season . The seventh season received What was the Walking Dead's original source material?

Southern California Edison [SEP] Southern California Edison ( or SCE Corp ) , the largest subsidiary

of Edison International , is the primary electricity supply company for much of Southern California . It

provides 14 million people with electricity across a service territory of approximately 50,000 square miles .

However , the Los Angeles Department of Water and Power , San Diego Gas & Electric , Imperial Irrigation

District , and some smaller municipal utilities serve substantial portions of the southern California territory

. The northern part of the state is generally served by the Pacific Gas & Electric

How many people does SCE Corp provide with electricity?

SDC

Do n't Go Away [SEP] " Do n't Go Away " is a song by the English rock band Oasis from their third album

, " Be Here Now " , written by the band 's lead guitarist Noel Gallagher . The song was released as a

commercial single only in Japan , peaking at number 48 on the Oricon chart , and as a promotional single in

the United States , Japan and Europe . In the United States it was a success , hitting # 5 on the " Billboard "

Hot Modern Rock Tracks chart in late 1997 . It was the band 's last major hit in the United

What Oasis album is "Don't go away" from?

India national cricket team [SEP] The India national cricket team , also known as Team India and Men

in Blue , is governed by the Board of Control for Cricket in India ( BCCI ) , and is a full member of the

International Cricket Council ( ICC ) with Test , One Day International ( ODI ) and Twenty20 International

( T20I ) status . Although cricket was introduced to India by European merchant sailors in the 18th century

, and the first cricket club was established in Calcutta ( currently known as Kolkata ) in 1792 , India 's

national cricket team did not play its first Test match until 25 June 1932 at Lord 's

What does ODI stand for?

Table 9: Validation set examples of questions in different resources. Correct answers are highlighted in red.

