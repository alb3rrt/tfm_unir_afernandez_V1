Matrix factorisation and the interpretation of geodesic distance
Nick Whiteley, Annie Gray, and Patrick Rubin-Delanchy University of Bristol

arXiv:2106.01260v1 [stat.ML] 2 Jun 2021

Abstract
Given a graph or similarity matrix, we consider the problem of recovering a notion of true distance between the nodes, and so their true positions. Through new insights into the manifold geometry underlying a generic latent position model, we show that this can be accomplished in two steps: matrix factorisation, followed by nonlinear dimension reduction. This combination is effective because the point cloud obtained in the first step lives close to a manifold in which latent distance is encoded as geodesic distance. Hence, a nonlinear dimension reduction tool, approximating geodesic distance, can recover the latent positions, up to a simple transformation. We give a detailed account of the case where spectral embedding is used, followed by Isomap, and provide encouraging experimental evidence for other combinations of techniques.

1 Introduction

Assume we observe, or are given as the result of a computational procedure, data in the form of a

symmetric matrix A  Rn×n which we relate to unobserved vectors Z1, . . . , Zn  Z  Rd, where

d n, by

Aij = f (Zi, Zj) + Eij, 1  i  j  n,

(1)

for some real-valued symmetric function f which will be called a kernel, and a matrix of unobserved perturbations, E  Rn×n. As illustrative examples, A could be the observed adjacency matrix of a graph with n vertices, a similarity matrix associated with n items, or some matrix-valued estimator of covariance or correlation between n variates.
Broadly stated, our goal is to recover Z1, . . . , Zn given A, up to identifiability constraints. We seek a method to perform this recovery which can be implemented in practice without any knowledge of f , nor of any explicit statistical model for A, and to which we can plug in various matrix-types, be it adjacency, similarity, covariance, or other.
At face value, this may seem rather a lot to ask. Without assumptions, neither f , the Zi's, nor even d are identifiable [22]. However, under quite general assumptions, we will show that a practical solution is provided by the key combination of two procedures: i) matrix factorisation of A, such as spectral embedding, followed by ii) nonlinear dimension reduction, such as Isomap [53].
This combination is effective because of the following facts, only the first of which is already known. The matrix factorisation step approximates high-dimensional images of the Zi's, living near a d-dimensional manifold [41]. Under general assumptions, geodesic distance in this manifold equals Euclidean geodesic distance on Z, up to simple transformations of the coordinates, for example scaling. Thus a dimension reduction technique which approximates in-manifold geodesic distances can be expected to recover the Zi's, up to a simple transformation. If those assumptions fail, as they must with real data, we may still find that those approximate geodesic distances are useful because they reflect a geometry implicit in the kernel.
There are many works which address recovery of the Zi's when one does consider a particular matrix-type, an explicit statistical model, and/or specific form for f . When A is an adjacency matrix and f (Zi, Zj) is the probability of an edge between nodes i and j, the construct (1) is a latent position model [20] and various scenarios have been studied: Z is a sphere and f (x, y) is a function of x, y [9]; estimation by graph distance when f (x, y) is a function of x - y [10];

1

Figure 1: Illustration of theory. We find a precise relationship between the distance, along M, between (Zi) and (Zj), and the Euclidean geodesic distance, along Z, between Zi and Zj.
spectral estimation when f (x, y) is a possibly indefinite inner-product [49, 51, 11, 42]; inference under a logistic model via MCMC [20] or variational approximation [44]. The case where Z is a discrete set of points corresponds to the very widely studied stochastic block model [21], see [5, 30, 40, 49, 8, 13] and references therein; by contrast the methods we propose are directed at the case where Z is continuous. The opposite problem of estimating f , with Zi unknown but assumed uniform on [0, 1], is known as graphon estimation [18].
Concerning the case when A is a covariance matrix, Latent Variable Gaussian Process models [28, 27, 55] use f (Zi, Zj) to define the population covariance between the ith and jth variates under a hierarchical model. When f (x, y) = x, y , this reduces to a latent variable optimisation counterpart of Probabilistic PCA [54] and the maximum likelihood estimate of the Zi's is obtained from the eigendecomposition of the empirical covariance matrix, which in our setting could be A. When f is nonlinear, but fixed e.g. to a Radial Basis Function kernel, gradient and variational techniques are available [28, 27, 55]. See the same references for onward connections to kernel PCA [46].
Our contribution and precursors. It has been understood for several years that the highdimensional embedding obtained by matrix factorisation can be related via a feature map to the Zi of model (1) [24, 14, 31, 52, 58, 29]. However, it was only recently observed that the embedding must therefore concentrate about a low dimensional set, in the Hausdorff sense [41]. Our key mathematical contribution is to describe the topology and geometry of this set, proving it is a topological manifold and establishing the link between in-manifold geodesic distance and geodesic distance in Z. Riemannian geometry underlying kernels was sketched in [7] but without consideration of geodesic distances or rigorous proofs, and not in the context of latent position estimation. The work [12, 57] was an inspiration to us, suggesting Isomap as a tool for analysing spectral embeddings, under a latent structure model in which Z is a one-dimensional curve in Rd and f is the inner product. A key feature of our problem setup is that f is unknown, in which case the manifold is not available in closed form and typically lives in infinite-dimensional space. To our knowledge, we are the first to show why spectral embedding followed by Isomap might recover the true Zi's, or a useful transformation thereof, in general.
We complement our mathematical results with experimental evidence, obtained from both simulated and real data, suggesting that alternative combinations of matrix-factorisation and dimension-reduction techniques work too. For the former, we consider the popular node2vec [19] algorithm, a likelihood-based approach for graphs said to perform matrix factorisation implicitly [39, 59]. For the latter, we consider the popular t-SNE [34] and UMAP [35] algorithms. As predicted by the theory, a direct low-dimensional matrix factorisation, whether using spectral embedding or node2vec, is less successful.
2 Proposed methods and their rationale
2.1 Spectral embedding, as estimating (Zi)
In our mathematical setup (precise details come later) the kernel f will be nonnegative definite and  will be the associated Mercer feature map. It is well known that each point (Zi) then lives in an
2

infinite-dimensional Hilbert space, which will be denoted 2, and the inner-product (Zi), (Zj) 2 in this space equals f (Zi, Zj).
The spectral embedding procedure. For p  n, we define the p-dimensional spectral embedding of A to be X^ = [X^1, . . . , X^n] = U^ |S^|1/2  Rn×p, where |S^|  Rp×p is a diagonal matrix containing the absolute values of the p largest eigenvalues of A, by magnitude, and U^  Rn×p is
a matrix containing corresponding orthonormal eigenvectors, in the same order. The R packages
irlba and RSpectra provide fast solutions which can exploit sparse inputs (e.g., when A is a graph
adjacency matrix). One should think of X^i as approximating the vector of first p components of (Zi), denoted
p(Zi), up to orthogonal transformation, and this can be formalised to a greater or lesser extent depending on what assumptions are made. There are several situations, e.g. f any polynomial [41],
the cosine kernel used in Section 4.1, the degree-corrected [26] or mixed-membership [6] stochastic
block model, in which only the first p0 (say) components of (·) are nonzero, where typically p0  d. If, after n reaches p0, we embed into p = p0 dimensions, we have [17]

max QX^i - p(Zi)
i=1,...,n

= OP

(log n)c n1/2

,

(2)

for a universal constant c  1, orthogonal matrix Q, under regularity assumptions on the Zi's, f and E (that Zi are i.i.d., f (Zi, Zj) has finite expectation, and the perturbations Eij are independent and centered with exponential tails). This encompasses the case where A is binary, for example a
graph adjacency matrix [33, 42]. Similar results are available in the cases where A is a Laplacian
[42, 36], covariance matrix [16], or the matrix implicitly factorised by node2vec [59]. The methods of this paper are based, in practice, on the distances X^i - X^j , which are invariant to orthogonal transformations and so for purposes of validating X^i - X^j  p(Zi) - p(Zj) the presence of Q in (2) is immaterial.
For X^i to converge to (Zi) more generally, we must let its dimension p grow with n and, at least given the present state of literature, accept weaker consistency results, for example, convergence in Wassertein distance between QX^1, . . . QX^n and p(Z1), . . . , p(Zn) [29]. Uniform consistency results, in the style of (2), are also available for indefinite [42], bipartite, and directed graphs [23].
These are left for future work because of the complications of Q no longer being orthogonal.
Rank selection. In real data, where n is typically fixed, there is no `best' way of selecting p,
as discussed for example in [38]. The method of [60], based on profile-likelihood, provides a popular,
practical choice, taking as input the spectrum of A, and is implemented in the R package igraph.

2.2 Isomap, as estimating Zi
In Theorem 1 below we establish that under realistic and quite general assumptions on f ,  is a bi-Lipschitz homeomorphism, meaning M := (Z) and Z are topologically equivalent, M has Hausdorff dimension exactly equal to that of Z (as opposed to an upper bound [41]) and M can legitimately be called a topological manifold. We find precise relationships between geodesic distance in M between any (Zi) and (Zj), written DiMj , and the Euclidean geodesic distance in Z between Zi and Zj, written DiZj (illustrated in Figure 1). In Section 3.3, for various families of kernel, we show that DiMj is equal to DiZj up to some simple transformations, or even exactly equal in special cases. One can estimate DM from X^1, . . . , X^n (Steps 1-2 below), and through this attempt to reconstruct Z1, . . . , Zn, by nonlinear dimension reduction (Step 3).
Algorithm 1 Isomap procedure input p-dimensional points X^1, . . . , X^n
1: Compute the neighbourhood graph of radius : a weighted graph connecting i and j, with weight X^i - X^j , if X^i - X^j 
2: Compute the matrix of shortest paths on the neighbourhood graph, D^ M  Rn×n 3: Apply classical multidimensional scaling (CMDS) to D^ M into Rd
return d-dimensional points Z^1, . . . , Z^n

3

Dimension and radius selection. For visualisation in our examples we will pick d = 1 or d = 2. For other applications, d can be estimated as the approximate rank of D^ M after double-
centering [56], e.g. via [60], again. In practice, we suggest picking just large enough for the
neighbourhood graph to be connected or, if the data have outliers, a fixed quantile (such as 5%) of D^ M, removing nodes outside the largest connected component. The same recommendations apply
if the k-nearest neighbour graph is used instead of the -neighbourhood graph.

3 Theory

3.1 Setup and assumptions

The usual inner-product and Euclidean norm on Rd are denoted ·, · and · . 2 is the set of

x = [x(1) x(2) · · · ]

 RN such that

x 2 := (

 k=1

|x(k)

|2

)1/2

<

.

For x, y 

2,

x, y 2 :=

 k=1

x(k)

y(k).

With d  1, let Z be a compact subset of Rd, and let Z be a closed ball in

Rd, centered at the origin, such that Z  Z. Let f : Z × Z  R be a symmetric, continuous,

nonnegative-definite function.

By Mercer's Theorem, e.g., [47, Thm 4.49], there exist nonnegative real numbers (k)k1 and

functions (uk)k1, with each uk : Z  R, which are orthonormal with respect to the inner-product

(uj, uk)  Z~ uj(x)uk(x)dx and such that



f (x, y) = kuk(x)uk(y), x, y  Z,

(3)

k=1

where the convergence is absolute and uniform. For x  Z let (x) := [11/2u1(x) 12/2u2(x) · · · ] 

RN. The image of Z by  is denoted M. Observe from (3) that for any x, y  Z, f (x, y) =

(x), (y) 2 ,

(x)

2 2

= f (x, x),

and

the

latter

is

finite

for

any

xZ

since

f

is

continuous

and

Z

is compact, hence M  2.

The following definitions are standard in metric geometry [15]. For any a, b  2, a path in M

with end-points a, b is a continuous function  : [0, 1]  M such that 0 = a and 1 = b. With

n  1, a non-decreasing sequence t0, . . . , tn such that t0 = 0 and tn = 1, is called a partition. Given

a path  and a partition P = (t0, . . . , tn), define (, P) :=

n k=1

tk - tk-1

2. The length of

 (w.r.t. · 2) is l() := supP (, P), where the supremum is over all possible partitions. The

geodesic distance in M between a and b is defined to be the infimum of l() over all paths  in M

with end-points a, b. DiMj denotes geodesic distance in M between (Zi) and (Zj). Similarly a

path  in Z with end-points x, y is a continuous function  : [0, 1]  Z such that 0 = x, 1 = y,

and with (, P) :=

n k=1

tk - tk-1

the length of  (w.r.t.

· ) is l() := supP (, P). DiZj

denotes geodesic distance in Z between Zi and Zj. If Z is convex, DiZj = Zi - Zj .

Assumption 1. For all x, y  Z with x = y, there exists a  Z such that f (x, a) = f (y, a).

Assumption 2. f is C2 on Z and for every z  Z, the matrix Hz  Rd×d with elements

Hizj :=

2f x(i)y(j)

(z,z)

is positive-definite.

Assumption 1 is a non-degeneracy condition on the kernel and is used to show  is injective. Assumption 2 has various implications, loosely speaking it ensures a non-degenerate relationship between path-length in M and path-length in Z. Concerning our general setup, if f is not required to be nonnegative definite, but is still symmetric, then a representation formula like (3) is available under e.g. trace-class assumptions [41]. It is of interest to generalise our results to that scenario but technical complications are involved, so we leave it for future research.

4

3.2 Path-lengths in M and in Z

Theorem 1. Let assumptions 1 and 2 hold. Then  is a bi-Lipschitz homeomorphism between Z and M. Let a, b any two points in M such that there exists a path in M with end-points a, b of finite length. Let  be any such path and define  : [0, 1]  Z by t := -1(t). Then  is a path in Z with l() < . For any > 0 there exists a partition P such that for any partition P = (t0, . . . , tn) satisfying P  P,

n

1/2

l() -

tk - tk-1 , Htk-1 (tk - tk-1 )

.

(4)

k=1

If  and  are continuously differentiable, the following two equalities hold:

1

1

l() =

 t 2dt =

t, Ht t 1/2 dt.

(5)

0

0

The proof is in the appendix. In the terminology of differential geometry, the collection of
inner-products (x, y)  x, Hzy , indexed by z  Z, constitute a Riemannian metric on Z, see e.g. [37, Ch.1 and p.121] for background, and the right hand side of (5) is the length of the path  in Z
with respect to this Riemannian metric. The theorem tells us that finding geodesic distance in M,
i.e. minimising l() with respect to  for fixed end-points say a = (Zi), b = (Zj), is equivalent to minimizing path-length in Z between end-points -1(a) = Zi, -1(b) = Zj under the Riemannian metric. In section 3.3 we will show how, for various classes of kernels, this minimisation can be performed in closed form leading to explicit relationships between DiMj and DiZj. In Section 3.3 we focus on (5) rather than (4) only for ease of exposition, it is shown in the appendix that exactly
the same conclusions can be derived directly from (4). To avoid repetition Assumption 1 will be
taken to hold throughout section 3.3 without further mention.

3.3 Geodesic distance in M and in Z

Translation invariant kernels. Suppose Z  Rd is compact and convex, and f (x, y) = g(x - y) where g : Rd  R is C2. In this situation Hz is constant in z, and equal to the Hessian of -g evaluated at the origin. The positive-definite part of Assumption 2 is therefore satisfied if and only if g has a local maximum at the origin. We now use Theorem 1 to find geodesic distance in M between generic points a, b  M for this class of translation-invariant kernels. To do this we obtain a lower bound on l() over all paths  in M with end-points a, b, then show there exists such a path whose length achieves this lower bound.
Let G = V1/2 where VV is the eigendecomposition of the Hessian of -g evaluated at the origin, so Hz = GG for all z. Then from (5), for a generic path  in M with end-points a, b,

1

1

1

l() =

t, Ht t 1/2 dt =

t, GG t 1/2 dt =

G t dt

(6)

0

0

0

1

1



G tdt = G

tdt = G (1 - 0) = G [-1(b) - -1(a)] , (7)

0

0

where the inequality is due to the triangle inequality for the · norm. The right-most term in (7) is independent of , other than through the end-points a, b. To see there exists a path whose length equals this lower bound, take

~t := -1(a) + t[-1(b) - -1(a)], t  [0, 1],

(8)

which is well-defined as a path in Z, since for this class of kernels Z is assumed convex. With ~t := (~t), ~ is clearly a path in M. Differentiating ~t w.r.t. t and substituting into (6) shows l(~) = G [-1(b) - -1(a)] , as required. The following proposition summarises our conclusions in the case that a = (Zi) and b = (Zj), for any Zi, Zj.
Proposition 1. If Z is compact and convex, and f (x, y) = g(x - y) where g : Rd  R is C2 and has a local maximum at the origin, then DiMj is equal to Euclidean distance between Zi and Zj, up to their linear transformation by G. If g(x - y) depends on x - y only through x - y , then G is proportional to the identity matrix and DiMj  DiZj = Zi - Zj .

5

Inner-product kernels. Suppose Z = {x  Rd : x = 1} and f (x, y) = g( x, y ), where

g : R  R is C2 and such that g (1) > 0. In this case Hz = g (1)I+g (1)zz , and by a result of [25],

any kernel of the form f (x, y) = g( x, y ) on the sphere is positive-definite iif g(x) =

 n=0

anxn

for

some nonnegative (an)n0, implying g (1)  0. Assumption 2 then holds. To derive the geodesic

distance in M, first write out (5) for a generic path  in M with end-points a, b:

l() =

1
t, Ht t 1/2 dt =

1

g (1) t 2 + g (1) | t, t |2

1/2
dt.

(9)

0

0

Since Z is a radius-1 sphere centered at the origin we must have

t

=

1

for

all

t,

so

0

=

1 2

d dt

t 2 =

1d 2 dt

t, t

=

t, t .

Therefore the r.h.s.

of (9) is in fact equal to g (1)1/2

1 0

t dt.

This is

minimised over all possible paths  in Z with end-points -1(a), -1(b) when  is a shortest (with

respect to Euclidean distance) circular arc in Z, in which case

1 0

t dt = arccos

-1(a), -1(b) .

Thus we have:

Proposition 2. If Z = {x  Rd : x = 1} and f (x, y) = g( x, y ) where g : R  R is C2 and such that g (1) > 0, then DiMj = g (1)1/2DiZj = g (1)1/2 arccos Zi, Zj .

Additive kernels. Suppose Z is the Cartesian product of intervals Zi = [c-i , c+i ], i = 1, . . . , d

and f (x, y) =

d i=1

i

fi(x(i),

y(i)),

x

=

[x(1)

···

x(d)]

, where each i > 0 and each fi is positive-

definite and C2.

For this class of kernels Hz

is diagonal with Hizi = i

 2 fi  x(i)  y(i)

(z (i) ,z (i) )

where z(i)

is the ith element of the vector z. The positive-definite part of Assumption 2 thus holds if these

diagonal elements are strictly positive for all z  Z.

Let us introduce the vector of monotonically increasing transformations:

(z) := [1(z(1)) · · · d(z(d))] ,

i(z(i)) := i1/2

z(i)

2fi

1/2
d,

c- i x(i)y(i) (,)

(10)

and write the vector of corresponding inverse transformations -1. For a generic path  in M with end-points a, b and as usual t := -1(t), define t := (t). Then:

1

1

l() =

t, Ht t 1/2 dt =

t dt

0

0

(11)

1



t dt = 1 - 0 =   -1(b) -   -1(a) ,

(12)

0

where the second and last equalities hold due to the definition of t, and  denotes elementwise composition. The quantity   -1(b) -   -1(a) is thus a lower bound on path-length l() for
any path  in M with end-points a, b. To see that there exists a path whose length achieves this lower bound, and hence that it is the geodesic distance in M between a, b, define ~t :=   -1(~t) where ~t :=   -1(a) + t[  -1(b) -   -1(a)]. Differentiating ~t w.r.t. t and substituting into (11) yields l(~) =   -1(b) -   -1(a) , as required. Our conclusions in the case that
a = (Zi) and b = (Zj) are summarised by:

Proposition 3. If Z is the Cartesian product of d intervals Zi  R, i = 1, . . . , d, and f (x, y) =

d i=1

i

fi(x(i),

y(i))

where

for

each

i,

i

>

0,

fi

is

positive

definite

and

C2,

and

 2 fi  x(i)  y (i)

>
(z,z)

0 for all z  Zi, then DiMj is equal to the Euclidean distance between Zi and Zj up to their

coordinatewise-monotone

transformation

by

.

If

 2fi
i x(i)y(i)

(z,z)

is

a

positive

constant

over

all

z  Zi, and over all i = 1, . . . , d, then DiMj  DiZj = Zi - Zj .

4 Experiments
4.1 Simulated data: latent position network model
This section shows the theory at work in a pedagogical example, where A is an adjacency matrix. Consider an undirected random graph following a latent position model with kernel f (x, y) =

6

Figure 2: Simulated data example. a) Spectral embedding (first 3 dimensions), b) comparison of latent, approximate geodesic, and ambient distance and c) latent position recovery in the dense regime by spectral embedding followed by Isomap for increasing n. To aid visualisation, all plots in c) display a subset of 100 estimated positions corresponding to true positions on a sub-grid which is common across n. Estimated positions are coloured according to their true y-coordinate.
n{cos(x(1) -y(1))+cos(x(2) -y(2))+2}/2, operating on R2 ×R2. Here, the sequence n is a sparsity factor which will either be constant, n = 1, or shrinking to zero sufficiently slowly, reflecting dense (degree grows linearly) and sparse (degree grows sublinearly) regimes respectively. The kernel is clearly translation-invariant, satisfies Assumptions 1 and 2 and has finite rank, p = 5. The true latent positions Zi  R2 are deterministic and equally spaced points on a grid over the region Z = [- + 0.25,  - 0.25] × [- + 0.25,  - 0.25], this range chosen to give valid probabilities and an interesting bottleneck in the 2-dimensional manifold M. From Proposition 1, the geodesic distance between (Zi) and (Zj) on M is equal to the Euclidean distance between Zi and Zj, up to scaling, specifically DiMj = nDiZj/2.
Focusing first on the dense regime, for each n = 100, 400, 1600, 6400, we simulate a graph, and its spectral embedding X^1, . . . , X^n into p = 5 dimensions. The first three dimensions of this embedding are shown in Figure 2a), and we see that the points gather about a two-dimensional, curved, manifold. To approximate geodesic distance, we compute a neighbourhood graph of radius from X^1, . . . , X^n, choosing as small as possible subject to maintaining a connected graph. Figure 2b) shows that approximated geodesic distances roughly agree with the true Euclidean distances in latent space Z (up to the predicted scaling of 1/2), whereas there is significant distortion between ambient and latent distance ( X^i - X^j versus Zi - Zj ). Finally, we recover the estimated latent positions in R2 using Isomap, which we align with the original by Procrustes (orthogonal transformation with scaling), in Figure 2c). As the theory predicts, the recovery error vanishes as n increases.
In the appendix, we perform the same experiment in a sparse regime, nn = {log4 n}, chosen to ensure the spectral embedding is still consistent, and the recovery error still shrinks, but more slowly. We also implement other related approaches: UMAP, t-SNE applied to spectral embedding, node2vec directly into two dimensions and node2vec in ten dimensions followed by Isomap. Together, these results support the central recommendation of this paper: to use matrix factorisation (e.g. spectral embedding, node2vec), followed by nonlinear dimension reduction (e.g. Isomap, UMAP, t-SNE).
7

4.2 Real data: structural change in the global flight network
Air travel was, for several reasons, a prominent feature of the COVID-19 pandemic, and we will explore structural changes in the global flight network towards its start (the 11th March 2020 according to the WHO).
The dataset is a publically available, clean version [48] (with license details therein) of the crowdsourced OpenSky dataset. Six undirected graphs are extracted, for the months November 2019 to April 2020, connecting any two airports over the world for which at least a single flight was recorded (n  10, 000 airports in each). For each graph the adjacency matrix A is spectrally embedded into p = 10 dimensions, degree-corrected by spherical projection [32, 30, 45], after which we apply Isomap with chosen as the 5% distance quantile. Those parameters were chosen to ensure results were comparable across different months, could be obtained in a manageable amount of time (a few hours on a standard desktop, as long as p is small, and the same is a loose upper bound on compute time for the other experiments in the paper), and reflected reasonable, default choices.
The embedding for January is shown in Figures 3)a-b), before and after Isomap. Both recover real geographic features, notably the relative positioning of London, Paris, Madrid, Lisbon, Rome. However, the embedding of the US is warped, and only after using Isomap do we see New York and Los Angeles correctly positioned at opposite extremes of the country. In the appendix, we show the same figure applying t-SNE and UMAP to the spectral embedding, or using node2vec for the embedding step, either directly into two dimensions, or into ten followed by Isomap. Over these experiments, the conclusion is consistent with the theory: a plausible recovery of geography is only ever achieved by applying matrix factorisation, followed by dimension reduction. Figure 3c) shows the next three months (six in the appendix), aligned using two-dimensional Procrustes, showing an important structural change in April. A statistical analysis (in the appendix) of inter-continental geodesic distances suggests the change reflects severe flight restrictions in South America and Africa (latter less visible in the figures).
4.3 Real data: correlations between daily temperatures around the globe
In this example A is a correlation matrix and we demonstrate a simple model-checking diagnostic informed by our theory. The raw data consist of average temperatures over each day, for several thousand days, recorded in cities around the globe. These data are open source, originate from Berkeley Earth [1] and the particular data set analyzed is from [3]. We used open source latitude and longitude data from [4]. See those references for license details. Removing cities and dates for which data are missing yields a temperature time-series of 1450 common days for each of the n = 2211 cities. A is the matrix of Pearson correlation coefficients between the n time-series.
Figure 4b) shows the spectral embedding of A, with p = 2 and points coloured by the latitude of the corresponding cities. Two visual features are striking: the concentration of points around a curved manifold, and a correspondence between latitude and location on the manifold. Figure 4c) shows latitude against estimated latent positions from Isomap using the k-nearest neighbour graph with k = 200, which is roughly 10% of n, and with d = 1. A clear monotone relationship appears. Our theoretical results can explain this phenomenon. Notice that when d = 1, the additive structure of the kernel in Proposition 3 disappears. Hence Proposition 3 shows that in the case d = 1, for any kernel (meeting the basic requirements of section 3.1, including Assumptions 1 and 2 of course) DM ij is equal to Euclidean distance between Zi and Zj up to their monotone transformation by  defined in (10). In turn, this implies that if A did actually follow the model (1) for some f and some "true" latent positions Z1, . . . , Zn to which we had access, we should observe a monotone relationship between those true latent positions and the estimated positions Z^1, . . . , Z^n from Isomap.
The empirical monotonicity in Figure 4c) thus can be interpreted as indicating latitudes of the cities are plausible as "true" latent positions underlying the correlation matrix A, without us having to specify anything in practice about f . In further analysis in the appendix no such empirical monotone relationship is found between longitude and estimated latent position; this indicates longitude does not influence the correlations captured in A. A possible explanation is the daily averaging of temperatures in the underlying data: correlations between these average temperatures
8

Figure 3: Visualisation of the global flight network. a) Spectral embedding, b) spectral embedding followed by Isomap, both for January 2020, c) spectral embedding followed by Isomap for the three following months. The colours indicate continents (NA = North America, EU = Europe, AS = Asia, AF = Africa, OC = Oceania) and a spread of cities with high-traffic airports are labelled.

may be insensitive to longitude due to the rotation of the earth.

5 Conclusion
Our research shows how matrix-factorisation and dimension-reduction can work together to reveal the true positions of objects based only on pairwise similarities such as connections or correlations. For the sake of exposition and reproducibility, we have used public datasets which can be interpreted without specialist domain knowledge, but the methods are potentially relevant to any scientific application involving large similarity matrices.

References

[1] Berkeley Earth. http://berkeleyearth.org. Accessed: 2021-05-27.

[2] Countries slammed their borders shut to stop coronavirus. but is it doing any

good?

https://www.npr.org/sections/goatsandsoda/2020/05/15/855669867/

countries-slammed-their-borders-shut-to-stop-coronavirus-but-is-it-doing-any-goo.

Accessed: 2021-05-20.

[3] Climate change: Earth surface temperature data. https://www.kaggle.com/berkeleyearth/ climate-change-earth-surface-temperature-data. Accessed: 2021-05-27.

9

Figure 4: Temperature correlation example. a) Locations in degrees latitude and longitude of cities where temperatures were recorded, b) spectral embedding with p = 2, c) latitude of each city (vertical axis) against estimated latent position (horizontal axis) when d = 1. In all plots, points are coloured by latitude of the corresponding cities.

[4] Simplemaps free entire world database. free-country-cities. Accessed: 2021-05-27.

https://simplemaps.com/resources/

[5] Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of Machine Learning Research, 18(1):6446­6531, 2017.

[6] Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing. Mixed membership stochastic blockmodels. Journal of Machine Learning Research, 9(Sep):1981­2014, 2008.

[7] Shun-ichi Amari and Si Wu. Improving support vector machine classifiers by modifying kernel functions. Neural Networks, 12(6):783­789, 1999.

[8] Arash A Amini, Aiyou Chen, Peter J Bickel, and Elizaveta Levina. Pseudo-likelihood methods for community detection in large sparse networks. Annals of Statistics, 41(4):2097­2122, 2013.

[9] Ernesto Araya Valdivia and De Castro Yohann. Latent distance estimation for random geometric graphs. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ c4414e538a5475ec0244673b7f2f7dbb-Paper.pdf.

[10] Ery Arias-Castro, Antoine Channarond, Bruno Pelletier, and Nicolas Verzelen. On the estimation of latent distances using graph distances. Electronic Journal of Statistics, 15(1):722 ­ 747, 2021. doi: 10.1214/21-EJS1801. URL https://doi.org/10.1214/21-EJS1801.

[11] Avanti Athreya, Donniell E Fishkind, Minh Tang, Carey E Priebe, Youngser Park, Joshua T Vogelstein, Keith Levin, Vince Lyzinski, and Yichen Qin. Statistical inference on random dot product graphs: a survey. The Journal of Machine Learning Research, 18(1):8393­8484, 2017.

[12] Avanti Athreya, Minh Tang, Youngser Park, and Carey E Priebe. On estimation and inference in latent structure random graphs. Statistical Science, 36(1):68­88, 2021.

[13] Peter Bickel, David Choi, Xiangyu Chang, and Hai Zhang. Asymptotic normality of maximum likelihood and its variational approximation for stochastic blockmodels. Annals of Statistics, 41(4):1922­1943, 2013.

[14] Béla Bollobás, Svante Janson, and Oliver Riordan. The phase transition in inhomogeneous random graphs. Random Structures & Algorithms, 31(1):3­122, 2007.

[15] Dmitri Burago. A course in metric geometry, volume 33. American Mathematical Soc., 2001.

[16] Joshua Cape, Minh Tang, and Carey E Priebe. The two-to-infinity norm and singular subspace geometry with applications to high-dimensional statistics. The Annals of Statistics, 47(5): 2405­2439, 2019.

10

[17] Ian Gallagher, Andrew Jones, Anna Bertiger, Carey Priebe, and Patrick Rubin-Delanchy. Spectral embedding of weighted graphs. arXiv preprint arXiv:1910.05534, 2019.
[18] Chao Gao, Yu Lu, and Harrison H Zhou. Rate-optimal graphon estimation. Annals of Statistics, 43(6):2624­2652, 2015.
[19] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855­864, 2016.
[20] Peter D Hoff, Adrian E Raftery, and Mark S Handcock. Latent space approaches to social network analysis. Journal of the American Statistical Association, 97(460):1090­1098, 2002.
[21] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. Social networks, 5(2):109­137, 1983.
[22] Svante Janson and Sofia Olhede. Can smooth graphons in several dimensions be represented by smooth graphons on [0, 1]? arXiv preprint arXiv:2101.07587, 2021.
[23] Andrew Jones and Patrick Rubin-Delanchy. The multilayer random dot product graph. arXiv preprint arXiv:2007.10455, 2020.
[24] Olav Kallenberg. On the representation theorem for exchangeable arrays. Journal of Multivariate Analysis, 30(1):137­154, 1989.
[25] Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels. In Artificial intelligence and statistics, pages 583­591. PMLR, 2012.
[26] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in networks. Physical Review E, 83(1):016107, 2011.
[27] Neil Lawrence and Aapo Hyvärinen. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of machine learning research, 6(11), 2005.
[28] Neil D Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. In Nips, volume 2, page 5. Citeseer, 2003.
[29] Jing Lei. Network representation using graph root distributions. The Annals of Statistics, 49 (2):745­768, 2021.
[30] Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block models. Ann. Statist., 43(1):215­237, 2015. ISSN 0090-5364. doi: 10.1214/14-AOS1274. URL https://doi.org/10.1214/14-AOS1274.
[31] László Lovász. Large networks and graph limits. American Mathematical Society Colloquium Publications, volume 60. Amer. Math. Soc. Providence, RI, 2012.
[32] Vince Lyzinski, Daniel L. Sussman, Minh Tang, Avanti Athreya, and Carey E. Priebe. Perfect clustering for stochastic blockmodel graphs via adjacency spectral embedding. Electron. J. Stat., 8(2):2905­2922, 2014. doi: 10.1214/14-EJS978. URL https://doi.org/10.1214/14-EJS978.
[33] Vince Lyzinski, Minh Tang, Avanti Athreya, Youngser Park, and Carey E Priebe. Community detection and classification in hierarchical stochastic blockmodels. IEEE Transactions on Network Science and Engineering, 4(1):13­26, 2017.
[34] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov):2579­2605, 2008.
[35] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.
11

[36] Alexander Modell and Patrick Rubin-Delanchy. Spectral clustering under degree heterogeneity: a case for the random walk Laplacian. arXiv preprint arXiv:2105.00987, 2021.
[37] Peter Petersen. Riemannian geometry, volume 171 of Graduate Texts in Mathematics. Springer, 2006.
[38] Carey E Priebe, Youngser Park, Joshua T Vogelstein, John M Conroy, Vince Lyzinski, Minh Tang, Avanti Athreya, Joshua Cape, and Eric Bridgeford. On a two-truths phenomenon in spectral graph clustering. Proceedings of the National Academy of Sciences, 116(13):5995­6000, 2019.
[39] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying Deepwalk, LINE, PTE, and node2vec. In Proceedings of the eleventh ACM international conference on web search and data mining, pages 459­467, 2018.
[40] Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional stochastic blockmodel. Annals of Statistics, 39(4):1878­1915, 2011.
[41] Patrick Rubin-Delanchy. Manifold structure in graph embeddings. In Proceedings of the Thirty-fourth Conference on Neural Information Processing Systems, 2020.
[42] Patrick Rubin-Delanchy, Joshua Cape, Minh Tang, and Carey E Priebe. A statistical interpretation of spectral embedding: the generalised random dot product graph. arXiv preprint arXiv:1709.05506, 2020.
[43] Walter Rudin. Principles of mathematical analysis, volume 3. McGraw-hill New York, 1976.
[44] Michael Salter-Townshend and Thomas Brendan Murphy. Variational bayesian inference for the latent position cluster model for network data. Computational Statistics & Data Analysis, 57(1):661­671, 2013.
[45] Francesco Sanna Passino, Nicholas A Heard, and Patrick Rubin-Delanchy. Spectral clustering on spherical coordinates under the degree-corrected stochastic blockmodel. arXiv preprint arXiv:2011.04558, 2020.
[46] Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299­1319, 1998.
[47] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.
[48] Martin Strohmeier, Xavier Olive, Jannis Lübbe, Matthias Schäfer, and Vincent Lenders. Crowdsourced air traffic data from the opensky network 2019­2020. Earth System Science Data, 13(2):357­366, 2021.
[49] Daniel L Sussman, Minh Tang, Donniell E Fishkind, and Carey E Priebe. A consistent adjacency spectral embedding for stochastic blockmodel graphs. Journal of the American Statistical Association, 107(499):1119­1128, 2012.
[50] Wilson A Sutherland. Introduction to metric and topological spaces. Oxford University Press, 2009.
[51] Minh Tang and Carey E Priebe. Limit theorems for eigenvectors of the normalized laplacian for random graphs. The Annals of Statistics, 46(5):2360­2415, 2018.
[52] Minh Tang, Daniel L Sussman, and Carey E Priebe. Universally consistent vertex classification for latent positions graphs. The Annals of Statistics, 41(3):1406­1430, 2013.
[53] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319­2323, 2000.
12

[54] Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611­622, 1999.
[55] Michalis Titsias and Neil D Lawrence. Bayesian Gaussian process latent variable model. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 844­851. JMLR Workshop and Conference Proceedings, 2010.
[56] Warren S Torgerson. Multidimensional scaling: I. Theory and method. Psychometrika, 17(4): 401­419, 1952.
[57] Michael W Trosset, Mingyue Gao, Minh Tang, and Carey E Priebe. Learning 1-dimensional submanifolds for subsequent inference on random dot product graphs. arXiv preprint arXiv:2004.07348, 2020.
[58] Jiaming Xu. Rates of convergence of spectral methods for graphon estimation. In International Conference on Machine Learning, pages 5433­5442. PMLR, 2018.
[59] Yichi Zhang and Minh Tang. Consistency of random-walk based network embedding algorithms. arXiv preprint arXiv:2101.07354, 2021.
[60] Mu Zhu and Ali Ghodsi. Automatic dimensionality selection from the scree plot via the use of profile likelihood. Computational Statistics & Data Analysis, 51(2):918­930, 2006.

A Theory
Optionally include extra information (complete proofs, additional experiments and plots) in the appendix. This section will often be part of the supplemental material.

A.1 Supporting results and proof of Theorem 1

Lemma 1. If Assumption 1 holds,  is injective.

Proof. We prove the contrapositive. So suppose  is not injective. Then there must exist x, y,
with x = y, such that (x) = (y), and hence for any a, f (x, a) = (x), (a) 2 = (y), (a) 2 = f (y, a).

The inverse of  on M is denoted -1. Let Hz,z  Rd×d be the matrix with elements

Hizj,z

:=

2f x(i)y(i)

.
(z,z )

Lemma 2. If Assumption 2 holds, then for any x, y  Z, there exists z on the line segment with end-points x, y such that

(x) - (y)

2 2

=

x - y,

where the integral is element-wise.

1
Hz,y+s(x-y)ds (x - y) ,
0

Proof. Fix any x, y in Z. Observe from (3) and the definition of  that for any x, y  Z,

(x) - (y)

2 2

= f (x, x) + f (y, y) - 2f (x, y).

Now define

and so since f is symmetric,

g(u) := f (u, x) - f (u, y), g(x) - g(y) = (x) - (y) 22.

13

By the mean value theorem, there exists z on the line segment with end-points x, y (i.e. z  Z) such that

g(x) - g(y) = g(z), x - y = xf (z, y) - xf (z, x), x - y

where g is the gradient of u  g(u) (with x, y still considered fixed) and xf (z, u) is the gradient of x  f (x, u) evaluated at z (with u considered fixed). Now considering the vector-valued mapping u  xf (z, u) with z fixed, we have

xf (z, x) - xf (z, y) = Combining the above equalities gives:

1
Hz,y+s(x-y)ds (x - y).
0

(x) - (y)

2 2

=

xf (z, y) - xf (z, x), x - y

1

= x - y,

Hz,y+s(x-y)ds (x - y) .

0

Lemma 3. For any matrix B  Rd×d and z  Rd , | z, Bz |1/2 

B

1/2 F

z

, where

· F is the

Frobenius norm.

Proof.

z, Bz

1 =
2

z, (B + B

)z



z 2max 

z 21 2

B+B

F  z 2 B F,

where max is the maximum eigenvalue of the symmetric matrix (B + B )/2. Replacing B by -B and using B F = - B F yields the lower bound z, Bz  - z 2 B F.

Lemma 4. If Assumption 2 holds, then for any > 0 there exists  > 0 such that for any x, y  Z such that x - y   and any , z, z on the line segment with endpoints x, y,

x - y, (H - Hz,z )(x - y)  x - y 2.

Proof. For each i, j, since (z, z )  Hizj,z is assumed continuous on Z × Z, and Z is compact, by the Heine-Cantor theorem (z, z )  Hizj,z is in fact uniformly continuous on Z × Z. Fix any > 0. Using this uniform continuity, there exists  > 0 such that for any x, y  Z, if x - y  , then for
any , z, z on the line-segment with end-points x, y,

max
i,j=1,...,d

Hi,,j - Hiz,,jz

 d-1,

and so in turn

H, - Hz,z  ,
F

where · F is the Frobenius norm. Observing that H, = Hz, the result then follows from Lemma 3.

Proposition 4. If Assumptions 1 and 2 hold,  and -1 are each Lipschitz continuous with respect to the norms · on Z and · 2 on M.

Proof. As a preliminary note that for any z  Z, Hz is symmetric and positive-definite under

Assumption 2, and let mz in, mz ax be the minimum and maximum eigenvalues of the matrix Hz. Since

mz ax = Hz sp, the spectral norm of Hz, and the reverse triangle inequality for this norm states

| Hz sp - Hz sp|  Hz - Hz sp, the continuity in z of the elements of Hz under Assumption 2

implies continuity of z  mz ax. Similar consideration of mz in =

H-z 1

-1 sp

together with

H-z 1 - H-z 1 sp  H-z 1Hz - I sp H-z 1 sp  H-z 1 sp Hz - Hz sp H-z 1 sp

14

shows that z  mz in is continuous. Due to the compactness of Z, we therefore find that + := supzZ mz ax < , and - := infzZ mz in > 0.
Our next objective in the proof of the proposition is to establish the Lipschitz continuity of .
As a first step towards this, note that it follows from the identity

(x) - (y)

2 2

= f (x, x) + f (y, y) - 2f (x, y),

that the continuity of (x, y)  f (x, y) implies continuity in 2 of x  (x). Now fix any 1 > 0 and consider any x, y  Z. By combining Lemmas 2 and 4 there exists 1 > 0 such that if x - y  1, there exists z on the line segment with end-points x, y such that:

1

(x) - (y)

2 2

=

x - y,

Hz,y+s(x-y)dt (x - y)

0

1

= x - y, Hz(x - y) +

x - y, Hz,y+s(x-y) - Hz (x - y) ds

(13)

0

 (+ + 1) x - y 2.

(14)

On the other hand if x - y > 1,

(x) - (y) x-y

2

 c11-1,

(15)

where c1 := supx,yZ (x) - (y) 2 is finite since Z is compact and  has already been proved to be continuous in 2. Combining (14) and (15) we obtain

(x) - (y) 2  c11-1  (+ + 1)1/2 x - y ,

x, y  Z.

It remains to prove Lipschitz continuity of -1. Fix 2  (0, -). Since Z is compact and  is continuous, -1 is continuous on M [50, Prop. 13.26], and then also uniformly continuous by

the Heine-Cantor Theorem since M is compact. Putting this uniform continuity of -1 together

with Lemmas 2 and 4, via the identity (13), there exists 2 > 0 such that for any a, b  M, if

a - b 2  2 then

a-b

2 2



(-

-

2)

-1(a) - -1(b)

2.

On the other hand, if a - b 2 > 2,

-1(a) - -1(b) a-b 2

 c22-1,

where c2 := supx,yZ x - y is finite since Z is compact. Therefore

-1(a) - -1(b)  c22-1  (- - 2)-1/2 a - b 2,

a, b  M.

Lemma 5. If Assumptions 1 and 2 hold, then for any a, b  M and a path  in M with end-points a, b such that () < , the mapping  : [0, 1]  Z defined by t := -1(t) is a path in Z with end-points -1(a), -1(b), and l() < .
Proof. By Proposition 4, -1 is continuous, which combined with the continuity of t  t implies continuity of t  -1(t), so  is indeed a path in Z, and the end points of  are clearly -1(a), -1(b). Proposition 4 establishes that moreover -1 is Lipschitz continuous, and then l() <  implies l() <  due to the definition of path-length.
Lemma 6. For any a  0 and b such that |b|  a,
|a|1/2 - |b|1/2  (a + b)1/2  |a|1/2 + |b|1/2.

15

Proof. First prove the lower bound. For any a, b as in the statement, let c = a - |b|, so that c  0, and set x = |b|1/2 and y = c1/2. Since x, y  0, application of the Euclidean triangle inequality in R2 to the pair of vectors [x 0] , [0 y] gives the fact: (x2 + y2)1/2  x + y, hence a1/2 = (|b| + c)1/2  |b|1/2 + c1/2 = |b|1/2 + (a - |b|)1/2, or equivalently:

(a - |b|)1/2  a1/2 - |b|1/2.

(16)

By the reverse triangle inequality and the assumptions on a and b,

(a + b)1/2 = |a + b|1/2  (a - |b|)1/2.

(17)

Combining (16) and (17) completes the proof of the lower bound in the statement. For the upper-bound in the statement, let c = a1/2 and d = |b|1/2. Then

a + b  c2 + d2 = (c + d)2 - 2cd  (c + d)2,

which implies as required.

(a + b)1/2  a1/2 + |b|1/2

Proof of Theorem 1. By Lemma 1,  is injective; by Proposition 4,  and its inverse on M, namely -1, are Lipschitz; and by Lemma 5, for  and  as in the statement,  is a path as claimed with l() < .
For the remainder of the proof, fix any > 0. By the definition of the path-length l(), there
exists a partition P such that

l() - /2  (, P )  l().

(18)

Let P = (t0, . . . , tn) be any partition, and fix any k such that 1  k  n. By Lemma 2 there exists z on the line segment with end-points tk-1 , tk such that

tk - tk-1

2 2

=

=

(tk ) - (tk-1 )

2 2

1

tk - tk-1 ,

H ds 0

z,tk-1 +s(tk -tk-1 )

= ak + bk

(tk - tk-1 )

(19) (20)

where

ak := tk - tk-1 , Htk-1 (tk - tk-1 )

1

bk :=
0

 -  , H - H tk

tk-1

z,tk-1 +s(tk -tk-1 )

tk-1

(tk - tk-1 )

ds.

Under Assumption 2, Htk-1 is positive-definite, so ak  0, and by (19), ak + bk  0, so we must have |bk|  ak. Lemma 6 then gives

ak1/2 - |bk|1/2  (ak + bk)1/2  a1k/2 + |bk|1/2,

hence:

tk - tk-1 2 - a1k/2  |bk|1/2.

(21)

By Lemma 4, there exists  > 0 such that

max
k=1,...,n

tk - tk-1





|bk |1/2



2

1 l()

tk - tk-1

,

1  k  n.

(22)

Since  is a path, it is continuous on the compact set [0, 1], and then in fact uniformly continuous
by the Heine-Cantor Theorem. Hence there exists a suitably fine partition P ,  P such that if P = (t0, . . . , tn)  P ,, maxk=1,...,n tk - tk-1   and in turn from (22),

n

|bk |1/2



2

1 l()

n

tk - tk-1

, 2

k=1

k=1

(23)

16

where the final inequality holds due to the definition of l() as the length of . Combining (21) and (23), if again P  P ,,

n
(, P) - a1k/2
k=1

n

=

tk - tk-1

k=1

n
 |bk|1/2

k=1

2 - a1k/2

. 2

(24)

Recalling from (18) the defining property of P and using P  P ,  P , the triangle inequality for the · 2 norm gives
l() -  (, P )  (, P)  l(). 2
Combined with (24), we finally obtain that if P  P ,,
n
l() -  a1k/2  l() + 2
k=1

and the proof of (4) is completed by taking P as appears in the statement to be P ,. The first equality in (5) can be proved by a standard argument - e.g., [43, p.137]. The second
inequality in (5) is proved by passing to the limit of the summation in (4) along any sequence of partitions P(m) = (t(0m), . . . , t(nm(m) )), m  1, with P(m)  P , such that limm maxk=1,...,n(m) |t(km) - t(km-)1| = 0.

A.2 Deriving geodesic distances from (4) rather than from (5)
Recall from Section 3.3 that the general strategy to derive the geodesic distance associated with each family of kernels (translation invariant, inner-product, additive) is:
(i) identify a lower bound on l() which holds over all paths  in M which have generic end-points a, b  M in common, then

(ii) show there exists a path whose length is equal to this lower bound.
In Section 3.3 this strategy was executed for each family of kernels starting from the expression for l() given in (5). In the proofs of Lemmas 7-9 below we show how step (i) is performed if we start not from (5) but rather from (4), the latter being more general because continuous differentiability of the paths is relaxed to continuity. The key message of these three lemmas regarding step (i) is that we obtain exactly the same lower bounds on l() as are derived from (5) in Section 3.3. The reader is directed to Section 3.3 for the details of how Assumption 2 is verified for each family of kernels; to avoid repetition we don't re-state all those details here.

Lemma 7. Consider the family of translation invariant kernels described in Section 3.3 and let G be as defined there. For any a, b  M and any path   M with end-points a, b,
l()  G [-1(b) - -1(a)] .

If we define ~ to be the path in Z given by

~t := -1(a) + t[-1(b) - -1(a)], t  [0, 1],

then ~ defined by ~t := (~t) is a path in M with end-points a, b and l(~) = G [-1(b) - -1(a)] .
Proof. Applying Theorem 1, fix any > 0 and let P be a partition such that for any partition P = (t0, . . . , tn) satisfying P  P,

n

1/2

l() -

tk - tk-1 , Htk-1 (tk - tk-1 )

.

k=1

(25)

17

Recalling from Section 3.3 that for this family of translation invariant kernels Hz = GG for all z  Z, the triangle inequality for the · norm combined with (25) gives

l()  -

n
+
k=1 n

1/2

tk - tk-1 , Htk-1 (tk - tk-1 )

.

=- +

G (tk - tk-1 )

k=1

n

- +

G (tk - tk-1 )

k=1

= - + G (1 - 0)

= - + G [-1(b) - -1(a)] .

The proof of the lower bound in the statement is then complete since was arbitrary. To complete the proof of the lemma, observe that from the definition of ~ in the statement,

n

1/2

~tk - ~tk-1 , H~tk-1 (~tk - ~tk-1 )

k=1

n

=

(tk - tk-1)G [-1(b) - -1(a)]

k=1

n
= G [-1(b) - -1(a)] (tk - tk-1)

k=1

= G [-1(b) - -1(a)] ,

and the proof of the lemma is then complete, because in (25) being arbitrary implies l(~) = G [-1(b) - -1(a)] .

Lemma 8. Consider the family of inner-product kernels of the form f (x, y) = g( x, y ) as described in Section 3.3 where g (1) > 0 . For any a, b  M and any path   M with end-points a, b,

l()  g (1)1/2 arccos -1(a), -1(b) .

If ~ is a shortest circular arc in Z with end-points -1(a), -1(b), then ~ defined by ~t := (t) satisfies l(~) = g (1)1/2 arccos -1(a), -1(b) .

Proof. As usual, let  be the path in Z defined by t := -1(t). Then from the definition of
path-length and the triangle inequality for the · norm, for any  > 0, there exists a partition P such that for any P = (t0, . . . , tn) satisfying P  P ,

n



tk - tk-1

 l() -

.

g (1)1/2

k=1

(26)

Fix any > 0 and let P be as in Theorem 1 and then take P = (t0, . . . , tn) to be defined

by P = P  P , so by construction we have simultaneously P  P and P  P. Then from Theorem 1,

n
l() -

1/2

tk - tk-1 , Htk-1 (tk - tk-1 )

.

(27)

k=1

Combined with the fact that for this family of kernels Hz = g (1)I + g (1)zz where g (1) > 0 and

18

g (1)  0, we obtain

n

1/2

l()  - +

tk - tk-1 , Htk-1 (tk - tk-1 )

k=1

n

=- +

g (1) tk - tk-1 2 + g (1)| tk - tk-1 , z |2 1/2

k=1

n

 - + g (1)1/2

tk - tk-1

k=1

 - + g (1)1/2l() - ,

where the penultimate inequality uses g (1)  0 and the final inequality holds by taking P in (26) to be P. Since and  were arbitrary, we have shown l()  g (1)1/2l(). Recall that here  is a path in Z = { x  Rd : x = 1} with end-points -1(a), -1(b). Hence l() is lower-bounded by the Euclidean geodesic distance in Z between -1(a) and -1(b), which is arccos -1(a), -1(b)
because Z is a radius-1 sphere centered at the origin.
With ~ and ~ as defined in the statement, taking  in (27) to be ~, refining P and using ~t, ~t = 0 (see discussion in Section 3.3) we find l(~) = g (1)1/2l(~), where by definition of ~, l(~) = arccos -1(a), -1(b) .

Lemma 9. Consider the family of additive kernels described in Section 3.3. For any a, b  M and any path   M with end-points a, b,

l()    -1(b) -   -1(a) .

If we define ~t :=   -1(a) + t[  -1(b) -   -1(a)] and let ~ be defined by ~t :=   -1(t), then l(~) =   -1(b) -   -1(a) .

Proof.

The compactness of Z

and the continuity of z



2fi  x(i) y (i)

1/2 (z,z)

for each i = 1, . . . , d implies

the uniform-continuity of the latter by the Heine-Cantor theorem. Hence for any 1 > 0, there

exists 2 > 0 such that for all i = 1, . . . , d and z1(i), z2(i)  Zi,

|z1(i) - z2(i)|  2



i1/2

2fi 1/2

- 2fi 1/2

x(i)y(i) (z1(i),z1(i))

x(i)y(i) (z2(i),z2(i))

 1 . l()

(28)

Fix any > 0 and let P be as in Theorem 1. We now claim there exists a partition P = (t0, . . . , tn) satisfying simultaneously P  P and

max
i=1,...,d

max
k=1,...,n

|t(ki)

-

(i)
tk-1

|



2.

(29)

To see that such a partition exists, note that with t := -1(t), t  t is continuous on the compact set [0, 1] hence uniformly continuous by the Heine-Cantor theorem. Thus for any s, t  [0, 1]
sufficiently close to each other, s - t can be made less than or equal to 2, which implies maxi=1,...,d |s(i) - t(i)|  2. Thus starting from P , if we subsequently add points to this partition until maxk=1,...,n |tk - tk-1| is sufficiently small then we will arrive at a partition P with the required properties, as claimed.

19

Now with this partition P in hand, fix any i = 1, . . . , d and k = 1, . . . , n. We then have

t(ki)

-

 (i)
tk-1

=

i(t(ki))

-

i(t(ki)-1 )

= i1/2

t(ki)

2fi

1/2
d

t(ki)-1 x(i)y(i) (,)

= i1/2





t(ki)

2fi

t(ki)-1  x(i)y(i)

1/2
-
(,)

2fi  x(i) y (i)

1/2
 d
(t(ki)-1 ,t(ki)-1 )

+ i1/2

2fi  x(i) y (i)

1/2

( -  ) (i)

(t(ki)-1 ,t(ki)-1 )

tk

(i) tk-1







1  l()

+

i1/2

2fi  x(i) y (i)

1/2

(i) (i)

 ( -  ), (t(ki)-1 ,t(ki)-1 )

tk

tk-1

(30) (31)

where the upper bound is due to (28)-(29). Squaring both sides, summing over i and then applying the triangle inequality gives

H(1/2)
tk-1

(tk

-

tk-1 )



tk - tk-1

- 1 l()

tk - tk-1

,

where

H(1/2)
tk-1

is the

diagonal matrix with

ith diagonal element

equal to

i1/2

2fi  x(i) y (i)

1/2
.
(t(ki)-1 ,t(ki)-1 )

Summing over k = 1, . . . , n and using the definition of l() gives

n

n

H(1/2)
tk-1

(tk

-

tk-1 )

 -1 +

tk - tk-1 .

k=1

k=1

Combined with the relationship (4) from Theorem 1 and yet another application of the triangle inequality we thus find

n

l()  - - 1 +

tk - tk-1

k=1

 - - 1 + 1 - 0

= - - 1 +   -1(b) -   -1(a) .

The proof of the lower bound in the statement is complete since and 1 are arbitrary. In order

to complete the proof of the lemma, observe that (28) combined with the same decomposition in

(30)-(31)

yields

an

accompanying

lower

bound

on

t(ki)

-

 (i)
tk-1

,

from

which

it

follows

that

n k=1

H(1/2)
tk-1

(tk

-

tk-1 )

n
-
k=1

tk - tk-1

 1.

Substituting ~ as defined in the statement of the lemma in place of , and replacing  by ~ defined by ~t := -1(~t), then using the fact that 1 is arbitrary we find via (4) in Theorem 1 that
l(~) =   -1(b) -   -1(a) .

B Supplementary experiments
The R packages used in this paper are (with license details therein, see github repository https: //github.com/anniegray52/graphs for code): data.table, RSpectra, igraph, plotly, Matrix, MASS, irlba, ggplot2, ggrepel, umap, Rtsne, lpSolve, spatstat, ggsci, cccd, R.utils, tidyverse, gridExtra,
20

rgl, plot3D. The Python modules used in this paper are (with license details therein, see github repository for code): networkx, pandas, nodevectors, random.
Data on the airports (e.g. their continent) was downloaded from https://ourairports.com/ data/.
B.1 Supplementary figures for simulated data example

n=100

n=400

n=1600

n=6400

2

2

2

2

1

1

1

0

0

0

0

-1

-1

-1

-2

-2

-2

-2

-3

-3

-3

-2

0

2

-3 -2 -1 0 1 2

-3 -2 -1 0 1 2

-3 -2 -1 0

1

2

Figure 5: Latent position recovery in the sparse regime by spectral embedding followed by Isomap for increasing n and increasing sparsity. To aid visualisation, all plots display a subset of 100 estimated positions corresponding to true positions on a sub-grid which is common across n. Estimated positions are coloured according to their true y-coordinate.

B.2 Supplementary figures and discussion for flight network example
A gap appears to form between North and South America which, as a first hypothesis, we put down to the well-publicised suspension of all immigration into the US in April 2020. To measure this gap we use the Earth Mover's distance 1 between the point clouds belonging to the two continents, using the approximate geodesic distances of the -neighbourhood graph (i.e., before dimension reduction). While this distance does explode in April 2020, as shown in Figure 9 (Appendix), we found the proposed explanation to be incomplete, because Australia and New Zealand imposed similar measures at the same time, whereas the distance between Oceania and the rest of the world, computed in the same way, does not explode. Revisiting the facts [2], while the countries mentioned above closed their borders to nonresidents, the continents of South America and Africa arguably imposed more severe measures, with large numbers of countries fully suspending flights. This explanation seems more likely, as on re-inspection we find a large jump in Earth mover's distance, over April, between both of those continents and the rest of the world, as shown in Figure 10.
B.3 Supplementary figures and discussion for temperature correlation example
Further to the discussion in Section 4, Figure 11 illustrates two important findings in the case p = 2 and d = 1, under the setup for this temperature correlation example described in section 4: firstly that there is no clear relationship between longitude and position along the manifold, and secondly a non-monotone relationship between longitude and estimated latent position. Both these observations are in marked contrast with the results in Figure 4 for latitude.
We then consider the case p = 3 and d = 2. Figure 12 shows the spectral embedding. Again the point-cloud is concentrated around a curved manifold. In Figure 13 we plot latitude and longitude against each of the d = 2 coordinates of the estimated latent positions. As in the p = 2, d = 1 case, we find a clear monotone relationship between latitude and the first component of estimated position but no such relationship between longitude and the second component.

1with thanks to Louis Gammelgaard Jensen for providing code
21

Spectral embedding, followed by Isomap

Spectral embedding, followed by UMAP

Spectral embedding, followed by t-SNE

2

2

2

1

1

1

0

0

0

-1

-1

-1

-2

-2

-2

-3

-3

-3

-3

-2

-1

0

1

2

-2

-1

0

1

2

node2vec (2 dimensions)

node2vec (10 dimensions), followed by Isomap

2

2

2

1

1

1

-2

-1

0

1

2

Graph distance, followed by CMDS

0

0

0

-1

-1

-1

-2

-2

-2

-1.00

-0.75

-0.50

-0.25

-2

-1

0

1

2

-2

-1

0

1

2

Figure 6: Visualising latent position recovery using various methods. The first row contains spectral embedding followed by different nonlinear dimension reduction techniques and the second row contains node2vec, node2vec followed by Isomap, and we have attempted recovery using graph distances [10]. To aid visualisation, all plots will display a subset of 100 on a fixed sub-grid, coloured according to their true location.

22

a) Spectral embedding, followed by UMAP q q

London qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q

qq

qq qqqqqqqqqqqqq

qqq

q

New YSoaroCkDhPaiaclloaalgsoo qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qqqq

q

q qqqq

qqq

qqqqqqqq

q q

q

q

q

q

q q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q

q

q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqq qqqqqqqq

Dubai qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqq Tokyo q qqqqqqq q qqqqqqqqq

qq q

qqqqqqqqqqqqqqqqqqqqq qq

q

qqqqqqqqqqqqqqq

qqqqqqqqqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqqqqqqqqqqqqqqq q

q qq

q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqq

q qqq q

qq

q q

q

q

qqqqqqqq

q qqqq qqqqqqq

qqqqqqqq
q
qq qq qqq q

qqqqqqqqq

qqqqqqqqqq qqqqqqqqqqq qqq

qqqqq

qqqq

q

qqq qq

qqqq

qqq

Los Angeles qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qq qqqqqqq

qqqqqqqqqqqqqqqqqq q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qqqqqqq

qqqqq qqqqq

qqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqq qqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqq

qq

qqqqqqqq

qqqqqqqqq q

c) node2vec (2 dimensions)

a) Spectral embedding, qqqqqqqqqqq followed by t-SNE

q

q

qqqqqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqqqqqqqqqqqqq

q qqq

q

qqqqqqqqqq q qqqqqqqqqqq
q

qqq qqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qq qqqqqqqqqq

qqq

qqqqqqqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqq
qq

qq

q qq

qq qq

qqqqqqqqqqqqqqqqq

q

qqqqq qqqqqqqqqq qqqqqqqqqqq

New YoSrako PTaooklyoo LDousLboAannDidgaoelnllaess Chicago qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

d) node2vec (10 dimensions), followed by isomap

q q

qqq

qqqq q q

qqq

q

q

DubaLi LoonCsdhoAincnagDgeoalellsas q q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q qqqq q q qqqqq qqqq qqq q qqq q
q qqqqq

q

q qqq

q

q

q

q

q

q

q

q qq

q

q

q

q q

q
q q

q

q

q q

q

q q

q

q

q q

AtheLniDsLsKbuoeobnmnadpiotnoSBnauoPeaPnNroakesowlAoiYreosrk Tokyo ChiLcoasgoAngeles Dallas q

q q
q

q qq qq
q

q q
q
q q

q qq

q q

q

q

q qq

q

q

q

q

q q q qqqq
q qqq qq
q q
qq

q

qq

q

qq

qq

q

q qq

q q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q q

q
q q
q qq
q
q q
q q
q q
q
q q

q q

q

q

q

q

q

q

q q
q

q

qq

q

q qq

q
q qq
q

q

q

q

q

q q

qq q

q q

q

q

q

qq

q

q q q

q q

q

Figure 7: Visualisation of the global flight network over January 2020 by alternative combinations of techniques. The colours indicate continents (NA = North America, EU = Europe, AS = Asia, AF = Africa, OC = Oceania) and a spread of cities with high-traffic airports are labelled (to reduce clutter, only a selection of the cities in Figure 3 are shown).

23

Nov 19
NA EU AS SA AF OC

Dec 19

Feb 20

Mar 20

Jan 20 Apr 20

Figure 8: Visualisation of the global flight network over time: nonlinear dimension reduction of each spectral embedding using Isomap. The colours indicate continents (NA = North America, EU = Europe, AS = Asia, AF = Africa, OC = Oceania) and a spread of cities with high-traffic airports are labelled. An important structural change is observed in April 2020.
Earth mover's distance from N. to S. America

3

2.5

Distance

2

Nov 19 Dec 19 Jan 20 Feb 20 Mar 20 Apr 20
Figure 9: Earth mover's distance between North and South America, as inferred from the approximate geodesic distances given by the -neighbourhood graph. In each of 100 Monte Carlo iterations, the Earth Mover's distance is computed based on 100 points randomly selected from each continent. We plot the average, with 2 standard errors in either direction indicated by the vertical bars.
24

3.5

3

2.5

Distance

2

1.5

NA EU AS SA AF OC
Nov 19 Dec 19 Jan 20 Feb 20 Mar 20 Apr 20
Figure 10: Earth mover's distance from each continent to the rest of the world, as inferred from the approximate geodesic distances given by the -neighbourhood graph. In each of 100 Monte Carlo iterations, the Earth mover's distance is computed based on 100 points (airports) randomly selected from the continent of interest, and 100 points (airports) from the rest of the world. We plot the average, for each continent, with 2 standard errors in either direction indicated by the vertical bars.
25

Figure 11: Temperature correlation example. Top: city locations. Bottom left: spectral embedding with p = 2. Bottom right: true longitude (vertical axis) vs. estimated latent position (horizontal axis) with d = 1. In all plots, points are coloured by longitudes of the corresponding cities.
26

Figure 12: Temperature correlation example. Four views of the spectral embedding with p = 3. In all plots, points are coloured by latitudes of the corresponding cities.
27

Figure 13: Temperature correlation example. Results for spectral embedding with p = 3 followed by Isomap with d = 2. Left: latitude (vertical axis) vs. estimated latent coordinate Z^i(1) (horizontal axis) coloured by latitude. Right: longitude (vertical axis) vs. estimated latent coordinate Z^i(2) (horizontal axis) coloured by longitude.
28

