Parametrised branching processes: a functional version of Kesten & Stigum theorem
Cécile Mailler and Jean-François Marckert
June 4, 2021

arXiv:2106.01426v1 [math.PR] 2 Jun 2021

Abstract
Let (Zn, n  0) be a supercritical Galton-Watson process whose offspring distribution µ has mean  > 1 and is such that x(log(x))+dµ(x) < +. According to the famous Kesten & Stigum theorem, (Zn/n) converges almost surely, as n  +. The limiting random variable has mean 1, and its distribution is characterised as the solution of a fixed point equation.
In this paper, we consider a family of Galton-Watson processes (Zn(), n  0) defined for  ranging in an interval I  (1, ), and where we interpret  as the time (when n is the generation). The number of children of an individual at time  is given by X(), where (X())I is a càdlàg integer-valued process which is assumed to be almost surely non-decreasing and such that E(X()) =  > 1 for all   I. This allows us to define Zn() the number of elements in the nth generation at time .
Set Wn() = Zn()/n for all n  0 and   I. We prove that, under some moment conditions on the process X, the sequence of processes (Wn(),   I)n0 converges in probability as n tends to infinity in the space of càdlàg processes equipped with the Skorokhod topology to a process, which we characterise as the solution of a fixed point equation.

1 Introduction

The aim of this paper is to discuss some natural models of parameterised branching processes and

to introduce a functional version of the Kesten & Stigum theorem which is one of the prominent

results in branching processes theory.

The standard Galton-Watson (GW) process with offspring distribution  = (j, j  0), a prob-

ability distribution on N := {0, 1, 2, · · · } is an integer-valued Markov chain (Zn, n  0) such that

Z0 = 1 and, for any n  0,

Zn

Zn+1 =

X (k,n)

(1.1)

k=1

Department of Mathematical Sciences, University of Bath, Claverton Down, BA2 7AY Bath, UK.
c.mailler@bath.ac.uk. CM is grateful to EPSRC for support through the fellowship EP/R022186/1. CNRS, LaBRI ,Université Bordeaux, 351 cours de la Libération 33405 Talence cedex, France

1

where the (X(k,n), k, n  0) are i.i.d. random variables with distribution . It is standard to interpret a GW process as describing the evolution of a population structured in generations: for all n  0, Zn+1 is seen as the number of individuals in the (n + 1)-th generation of a population, and more precisely, X(k,n) is the number of children of the k-th individual of the n-th generation. From this classical point of view, the GW process is the sequence of generation sizes of a the genealogical tree of the branching process (also called the family tree).

Theorem 1.1 (Kesten & Stigum [11]). Consider (Zn, n  0) a GW process with offspring distribution , whose mean  is finite.
If the process is supercritical, that is  > 1, the sequence (Wn, n  0) defined by

Wn = Zn/n, for all n  0,

(1.2)

converges almost surely to a random variable W , and P(W > 0) > 0 iff E(X log+(X)) < + (where X  ). Moreover, in this case, EW = 1, and P(W = 0) = q where q is the extinction probability of the branching process Z, that is q = P(k : Zk = 0). The value of q is characterised as the smallest non-negative root of q = f(q) where f is the probability generating function of X   :

f(y) = E(yX ).

The original version of the theorem was written in the multi-type case (see Section 1.5 for references and more details). In fact the process (Wn, n  0) is a non-negative martingale so that the a.s. convergence of Wn to a random variable W is granted. Since branching processes can be decomposed at their first generation, the limit W is solution to a fixed point equation:

W (=d) -1 Z1 W (j)
j=1

(1.3)

where the W (j) are independent copies of W , independent of Z1. In other words, the characteristic function x  (x) = E(eixW ) (for x  R) is solution of

(x) = f((x/)), for all x  R.

(1.4)

However, the functional equation (1.4) (or distribution equation (1.3)) does not fully characterise , since for any constant c, cW is also solution of (1.3). By Seneta [21, Th. 3.1], the solution of (1.4) is unique up to constant factors. Furthermore, by Kesten-Stigum, E(X(log X)+) <   E(W ) = 1, which implies that, in that case, W is the unique solution of (1.4) with mean 1.

1.1 A process of GW processes: definition of the model
We mainly aim at addressing the following question: Kesten & Stigum's theorem is a "onedimensional result", since it concerns the limit of the one-dimensional sequence (Wn, n  0). There are some natural models in which appears a family of GW processes parameterised by a second parameter, which can be interpreted as a "time" parameter. (We give such an example after the definition.)

2

Definition 1.2. Let I  [0, ), and (X())I an almost surely non-decreasing, integer-valued process taking its values in the set of càdlàg functions D(I, R+) equipped with the Skorokhod topology

on all compact subsets of I. We define (Zn())I as the process satisfying Z0() = 1 for all   I,

and, for all n  0,

Zn()

Zn+1() =

X (k,n) ()

(1.5)

k=1

where (X(k,n), k, n  1) is a sequence of i.i.d. copies of the process X.

In standard GW processes, n is called the generation: this is the index n in Zn(). We choose to call  the time, and X the offspring process of Z.

A first example: Arguably the simplest of these is when the number of children of a node at time  is X(), where X is a simple Poisson process on [0, ). At any given time , the branching process (Zn(), n  0) is a simple Galton process whose offspring distribution is Poisson(). For each n,   Zn() is almost surely non-decreasing, since the number of children of each individual in the tree is non-decreasing as a function of . In fact, as  increases, the process of family trees forms a growing family of trees for the inclusion order. Consider a node of the tree u which, say, is created at time t. As a node of the tree at time t, its number of children is distributed as X(t), so that the subtree Tut rooted at u (at time t) has the same distribution as a global family tree T t of a Galton-Watson process with offspring distribution Poisson(t). More examples will be given in Section 2.

A motivation: This kind of model arises for example, when one studies the Erds-Renyi graph G(N, p) for p = /N and N large. A vertex u has a Binomial(N -1, p) random number of neighbors in the graph, approximately Poisson() distributed when N is large. For any fixed r > 0, the subgraph of G(N, p) induced by the vertices at graph distance smaller than r to u is well approximated by a Galton-Watson process with offspring distribution Poisson() (restricted to its r first generations). In many applications (starting from the study of coalescence processes, or as the study of the cluster sizes of G(N, p)), p is seen as a varying parameter: to each of the N (N - 1)/2 edges e of the complete graph KN , assign a weight we, where the we are i.i.d. uniform on [0, 1]. The graph G(N, p) obtained by keeping only the edges e of KN such that we  p has same distribution as G(N, p), and p  G(N, p) is a graph process which is non-decreasing for the inclusion order. Now, if one wants to study the evolution of G(N, /N ) in the ball of radius r around a given node, when N  +, for   [a, b], then one has to deal with our model: the offspring distribution of the involved nodes, asymptotically, are Poisson process X = (X(),   [a, b]).
Kesten & Stigum's theorem implies that the 1-dimensional marginals of the process (Wn(),   I)n0, for parameters  such that E(X()) > 1, converge almost surely. In our model, the (Wn()) for different values of  are coupled, so that a natural question is:

3

Does (Wn(),   I), seen as a sequence of processes indexed by I converges in distribution, or in a stronger sense, as n  +?

Of course, we are also interested in the description of the limit, if it exists. In our main result we assume some properties of the process X; we packed these hypothesis into two groups (HReg) and (HMom) ; one concerns "the regularity of X", and the other "some moments properties":

-------------------- (HReg) : a.s., the process X is càdlàg on an interval I  (1, +); on this interval X is a.s. non decreasing, takes its values in N = {0, 1, 2, · · · }, and, for any   I,

E(X()) = .

-------------------- Under this hypothesis, (Zn(), n  0)I is a process of GW processes, where for each  > 1, (Zn(), n  0) is a supercritical GW process, whose offspring distribution has mean . Analogously to (1.2), we define the process of processes (Wn(),   I)n1 by
Wn() = Zn() / n,   I, n  0.

Remark 1.3 (Comments on (HReg) ). (i) If we remove the non-decreasing property for X then, disappearance of individuals could occur when  grows; this leads to some complications since the identity of disappearing individuals in their generation matters. We prefer to avoid these complications, even if these models can be defined and investigated.

(ii) Any càdlàg, non decreasing and non negative process Y taking its values in N, defined on an interval I , and satisfying E(Y ()) < +, and   E(Y ()) continuous increasing, is the time-changed of a process X satisfying (HReg). It suffices to set X() = Y (g()) where g() = y if E(Y (y)) =  (that is g is the inverse of the map   E(Y ())).
For 1 < · · · < d  I, set X(j) := X(j) - X(j-1)

with, by convention, 0 = 0, and X(0) = X(0) = 0. Denote by Fac the factorial moments of the increments of X defined as usual by

d



Fac1,··· ,d (1, · · · , d) = E  (Xj )(j)

j=1

where (x)(r) = x(x - 1) · · · (x - r + 1); for example

Fac3,2,1(1, 2, 3) = E [(X(1))(X(1) - 1)(X(1) - 2)(X(2))(X(2) - 1)(X(3))] .

Convention: we often write Fac1,··· ,d instead of Fac1,··· ,d(1, . . . , d); all along the paper, when we need to fix some times, we always choose (1, . . . , d) so that this notation is not ambiguous.
When (HReg) holds, Fac1,0 = Fac1,0(1, 2) = 1 and Fac0,1 = Fac0,1(1, 2) = 2 - 1.

4

Lemma 1.4. Assuming (HReg) , the process ((Zn(),   I), n  0) is (a.s.) well defined, and for each n,   Zn() is a.s. càdlàg and non-decreasing.
Proof. (1.5) ensures that for each (, n), Zn+1() is a.s. finite, which implies it is well defined. The other properties are clearly inherited from those of X.

Good control on the increments of X will be needed to control the moments of increments of Wn, which is central in our proof of convergence of Wn, mainly, in the tightness argument.
-------------------- (HMom) : There exists   (1/2, 1) such that for any [a, b]  I, there exists a constant C such that, for all a  1  2  3  b

Facx,y,z(1, 2, 3)  C(3 - 1), for (x, y, z), 1  x + y + z  4, y  1 or z  1, (1.6)

Fac0,y,z(1, 2, 3)  C(3 - 1)2, for (y, z), 1  y  2 and 1  z  2.

(1.7)

-------------------- The following lemma, which we prove in Section 5.1, gives sufficient conditions for (1.6) and (1.7) to hold. These are convenient when checking (1.6) and (1.7) in practice.

Lemma 1.5. (HMom) is equivalent to the following condition: There exists   (1/2, 1) such that for any [a, b]  I, there exists a constant C such that for any a  1  2  3  b,

E (X(2))2(X(3))2  C (3 - 1)2

(1.8)

and

E (X(3))X(3)3  C (3 - 2).

(1.9)

1.2 Main results

For all time , we let q denote the non-increasing extinction probability of the process (Zn(), n  0). We start by stating the convergence of the finite dimensional distribution (FDD) convergence of the process Wn() when n  +: this is an almost sure convergence.

Proposition 1.6. Assume (HReg). For any d  1, for 1  · · ·  d in I, (Wn(i), 1  i  d) converges a.s. when n  + to some d-tuple of non-negative random variables (W (i), 1  i  d). Furthermore, if E(X(i) log+(X(i))) < +, then qi = P(W (i) > 0) > 0 and E(W (i)) = 1.

Proof. Kesten & Stigum's theorem implies the result for each marginal. Now, on any probability

space

on

which

are

defined

some

random

variables

, (i, i



0), , (i, i



0),

if

n

(as.)
---
n



and

n

-(-a-s.)
n



then

(n, n)

-(-a-s.)
n

(, ).

5

To state our main result, we use the following convention:

0 = X(0) = W (0) = Wn(0) = Z0(0) = 0

(1.10)

even if we use, in general X(), Wn() and W () for  > 1 elsewhere (notice, for example that Z0() = 1 for   I, but we set Z0(0) = 0). These conventions are only used to work more easily with increments (for example, Wn(i) = Wn(i) - Wn(i-1) = Wn(1) when i = 1).

Theorem 1.7. If the offspring process X satisfies (HReg) and (HMom), then

(Wn(),





I

)

--p-ro-b-a.
n+

(W

(),





I)

in D(I, R+) equipped with the Skorokhod topology on each compact subsets of I, where the process W has a distribution characterised by the following properties: · for any   I, E(W ()) = 1, · its FDD are solution to the following fixed point equation: for any d  1, any 1  · · ·  d in I,

W (1), . . . , W (d) (=d)

1 1

X (1 ) i=1

W

(i)(1),

.

.

.

,

1 d

X (d ) i=1

W

(i)(d)

,

(1.11)

where, on the right-hand side, W (i)(1), . . . , W (i)(d) i1 are i.i.d. copies of (W (1), . . . , W (d)), independent of X.

Remark 1.8. (i) We require I  (1, +) so that, for each   I, the GW process (Wn(), n  0) is supercritical, but the subcritical case can be treated too, but it is trivial. For  < 1, (Wn()) converges a.s. to 0, so that Wn()/n converges to 0 in D[0, 1) on all compact set, and this is true also at 1, if we exclude the case P(X(1) = 1) = 1.
(ii) The point of view "convergence of Fourier transforms" is discussed in Section 1.3.1.
(iii) The method we use to prove convergence uses a tightness argument: for any  > 0, there exists a compact K of D([a, b]) such that P(Wn  K)  1 -  for every n. This argument is not strong enough to prove almost sure convergence of (Wn), but it can be conjectured that almost sure convergence holds, possibly under additional regularity assumptions on X.
(iv) The sufficient condition (HMom) comes from our proof strategy using control of moments; it is probably not optimal. In Section 4 we explain that the moments can be exactly computed, but because of their complexity, this calculation does not lead to an explicit criterion (which, however would also need fourth moments for X, when it is not clear that they are needed).
(v) The process W is in D(I, R+), so that it has at most countably-many discontinuities (see Billingsley [4, Section 13]). As  grows, more and more individuals appear in the family tree. When a new node appears, it appears together with an infinite subtree with positive probability, which provokes a jump of the process W . In fact after leaving 0, the set of jumps of W is dense in I... so that W stops to be continuous as soon as it leaves 0.

6

To establish the convergence in D(I, R+) from Proposition 1.6, we mainly need a tightness argument (see Section 3.1), and a lemma to deal with the convergence in probability (Lemma 3.3).

1.3 On the identification of the limiting process

As usual when dealing with martingales, we know the existence of the limit before knowing anything about it. Using a branching property argument similar to the one leading to (1.4), it is possible to characterise the limit as the solution of a fixed point equation (as expressed in Theorem 1.7 and Proposition 1.10).
Indeed, as in the 1-D case, generation n + 1 is formed by the sum of the descendants after n generations of the children of the root: fix [a, b]  I, d  1 and (1, · · · , d) such that a  1  · · ·  d  b. For all n, jointly for 1  i  d,

X (i )

Zn+1(i) =

Zn(i)(i),

i=1

where (Zn(i) : n  0)i1 is a sequence of i.i.d. copies of (Zn : n  0), independent of the offspring process X. This implies that, jointly for 1  i  d,

Wn+1(i)

=

1 i

X (i )
Wn(i)(i),
i=1

(1.12)

where (Wn(i) : n  0)i1 is a sequence of i.i.d. copies of (Wn : n  0), independent of the offspring process X. Taking the limit as n  + (since this limit exists by Kesten & Stigum), we get that the limit satisfies (1.11) (see also Proposition 1.10). For the same reason as in the 1-dimensional case, Equation (1.11) does not characterise the law of (W (1), · · · , W (d)). The law of (W (1), · · · , W (d)) is characterised as the unique solution of (1.11) having constant mean 1 and finite second moments thanks to the following lemma (proved in Section 5) in which M2(1, . . . , 1) denotes the set of probability distributions on [0, )d having mean (1, . . . , 1) and whose marginals all have finite second moments.
Note that, under the assumptions of Theorem 1.7, W indeed has constant mean 1, and finite second moment since, by [14, Theorem 2.0], EX()2 < + implies EW ()2 < +.

Lemma 1.9. Assume (HReg) and EX()2 < + for all   I. Let d  1 and 1 < 1 < · · · < d in I. We define  = 1,...,d : M2(1, . . . , 1)  M2(1, . . . , 1) as

(µ)

=

Law

 1
 1

X (1 ) i=1

U1(i),

.

.

.

,

1 d

X (d ) i=1

 Ud(i)

,

where the ((U1(i), . . . , Ud(i)))i1's are i.i.d. copies of (U1, . . . , Ud)  µ, independent of the offspring process X. Then  is a contraction for the L2 Wasserstein metric, and in particular,  admits a
unique fixed point in M2(1, . . . , 1).

7

1.3.1 Convergence of the FDD of Wn with Fourier transforms
We introduce some tools that will play a role in the tightness proof and in some explicit computations that follow. For all sequences (yi, i  a, b ) indexed by any interval a, b = [a, b]  Z, the corresponding increment sequence is denoted

yi := yi - yi-1, for i  a + 1, b .

We often write y a, b instead of (ya, ya+1, · · · , yb). For all integers d  1 and real numbers 1 < 2 < · · · < d in I, consider the following generating function of the FDD of X and of its increments:





d

f 1,d (z 1, d ) := E  ziX(i) ,

j=1





d

f1,d (z 1, d ) := E  zjX(j) .

j=1

These generating functions are at least defined on BC(0, 1)d (and of course, each of them can be expressed with the other). Define the Fourier transform of (Wn(i), 1  i  d) by





d

(n1),d (x 1, d ) := E  exp i xjWn(j)  .

j=1

(1.13)

For any integers r and d such that 1  r  d, and any sequence (x1, . . . , xd) define

dr(x, ) = 0, · · · , 0, xr/r, · · · , xd/d .
r-1 terms

(1.14)

The following proposition provides a recursive way to compute (n).

Proposition 1.10. For any 1 < 2 < · · · < d in I, any x 1, d  Rd,

(0)1,d (x 1, d ) = exp [i(x1 + · · · + xd)] .

and for n  1,

(n1),d (x 1, d ) = f1,d (n1-,d1)(d1(x, )), · · · , (n1-,d1)(dd(x, )) .

(1.15)

Moreover, (n1),d converges pointwise on Rd to a function  1,d fixed point equation of

 1,d (x 1, d ) = f1,d  1,d (d1(x, )), · · · ,  1,d (dd(x, )) .

(1.16)

As discussed in the beginning of Section 1.3,  1,d is moreover the unique solution to (1.16) with mean (1, · · · , 1) if X has a finite variance.

8

1.4 On explicit computations

There are two characteristics of the limiting process W that are simple to compute: · the law of T(0,+)(W ) := inf{ : W > 0} the entrance time of W in (0, +) since,

P T(0,+)(W ) > x = qx, x  I

the extinction probability of the process (Zn(x))n0 (the smallest non-negative root of q = E(X(x)q)).

· the joint moments E

m j=1

Wkjj

for some fixed m, some fixed positive integers (k1, . . . , kj), fixed

time 1 < · · · < m for which this quantity exists. This is detailed in Section 4: it relies mainly on

Lemma 3.7 which allows to see that there are some polynomial relations between the E

m j=1

Wdjj

for dj  kj, which can be linearized in all generality. Another method consists in extracting the

moments using (1.16) (we present some of these computations in (3.4)).

The computation of the FDD of the processes W proves to be quite technical, since the main

tool we have are the Formulae (1.16), which are implicit. In dimension 1,2,3:

1 (x1)

=

f1 1

x1 1

1,2 (x1, x2)

=

f1,2 1,2

x1 , x2 1 2

, 1,2

0, x2 2

[3](x1, x2, x3)

=

f[3] [3]

x1 , x2 , x3 1 2 3

, [3]

0, x2 , x3 2 3

, [3]

0, 0, x3 3

These equations are related, by consistence; the last equation can be rewritten

(1.17) .

[3](x1, x2, x3) = f[3] [3]

x1 , x2 , x3 1 2 3

, 2,3

x2 , x3 2 3

, 3

x3 3

.

The identification of the marginal distributions is difficult too; write

 = q + (1 - q)

(1.18)

where  is the Fourier transform of L(W | W > 0). Since q is known (implicitly, in general), computing  is the only real issue: it is solution with mean 1/(1 - q) of

q + (1 - q)(x) = f [q + (1 - q)(x/)] ,

(1.19)

so that

(x)

=

f

[q

+

(1

- q)(x/)] 1 - q

-

q

,

and then, by expanding f(y) = m0 P(X = m)ym, we find

(x) = g((x/))

(1.20)

where g is the probability generating function of p() = (pm(), m  0) with

p0() = 0, and, for j  1, pj() = (1 - q)j-1

P(X = m)

m j

qm-j .

mj

(1.21)

In words,  is the Fourier transform of the limiting martingale of a second Galton-Watson process with offspring distribution p(), which naturally extincts with probability 0 since p0() = 0.

9

We will discuss three examples in Section 2.

Remark 1.11. An alternative equation on  can be written using the spinal decomposition of the

GW process (Zn(), n  0) conditioned on non extinction, which (Zn(), n  0) denotes. Indeed,



is

the

solution

of

Sp(x)/(1 - q)

=

1 i

d dx

(x)

with

(0)

=

1,

where

Sp

is

the

Fourier

transform of the limiting distribution of Zn()/n. Furthermore, by decomposition at the root, Sp

is characterised as the solution of sp(x) = sp(x/)f~((x/)) where f~ is the probability generating

function of X - 1, where P(X = k) = kP(X = k)/ (k  0). Using these two equations together,

we get another another fixed point equation that characterises . Except possibly in some particular

cases, the formula obtained this way is not more convenient than (1.19).

1.5 Discussion of the related literature
Branching processes have been widely studied in probability theory. They were originally introduced as models for the evolution of populations (see, e.g., Haccou & al. [7], and Kimmel & Axelrod [12]). They appear also as combinatorial structures called trees, which are one of the simplest models for complex networks. Simple families of trees such as uniform planar rooted binary trees with n internal nodes, uniform rooted planar trees with n nodes, and uniform rooted labeled trees with n are equal in distribution to Galton-Watson trees conditioned on having size n. Their asymptotic behaviour is thus well-known (Aldous [1], see also [17, 13]). We refer also to Devroye [6] where the theory of branching processes is applied to the analysis of models of random trees such as the binary search tree, Cayley trees, and Catalan trees. Branching processes are also a useful tool to study random graphs such as the Erds-Reyni random graph and scale-free random graphs such as the configuration model and the Barábasi & Albert model (see Bollobás & Riordan [5] for a survey on using branching processes to analyse random graphs). For a mathematical exposition of some of the existing results on branching processes, we refer the reader to, e.g., the surveys of Athreya and Ney [3], Asmussen and Hering [2], and Lyons and Peres [16], in chronological order.

Discussion on Bellman-Harris and Crump-Mode-Jagers processes: In this paper, we focus on discrete-time GW processes, meaning that for each , (Zn(), n  0) is a discrete-time Markov chain. We do not cover the case of continuous-time GW processes (in which each individual has an exponentially-distributed life-time and creates offspring at its death) or their age-dependent generalisations called Bellman-Harris processes (in which the life-time has a non-exponential distributions - see, e.g. [3, Chapter IV]). Another generalisation of continuous-time GW processes are the Crump-Mode-Jagers (CMJ) processes (see, e.g., Jagers [8], Nerman [19], or Jagers and Nerman [9]) in which individuals can create offspring during their whole life-time, for example according to a Poisson process. Most of these processes exhibit a martingale limit (in the CMJ case, only if the so-called Malthusian parameter exists, see [19]); as far as we know, none of these continuoustime branching processes and their martingale limits have been studied as processes indexed by a parameter as we do here.

10

Our model seen as a pruned multi-type Galton-Watson tree: It is possible to represent the family trees of our Galton-Watson processes at time 1  2  · · ·  k as pruned multi-type Galton-Watson trees (see, e.g., Athreya & Ney [3, Chapter V] for a survey, and Janson [10] for recent limiting theorems). Indeed, sample the GW tree (Zn(k), n  0), and, for each node u with offspring process Xu, for all 1  i  k, colour in colour i the children of u that appeared when the time parameter belongs to (i-1, i] (set 0 = 0). The number of the children of u of colour i is given by Xu(i) - Xu(i-1). To get the family tree at time i from this multi-type tree, we remove all the nodes of color  i + 1. However, it is unclear whether the theory of multi-type GW processes could be used to analyse the process (Zn(t), n  0)tI .
Discussion on smoothing (or fixed point) equations: Fixed point equations analogous to (1.11) are standard in the theory of branching processes (see, e.g., Liu [15] and the references therein). They are called fixed point or smoothing equations. In Lemma 1.9, we use the so-called contraction method (see, e.g., Rösler & Rüschendorf [20] for a survey, and Neininger & Sulzbach [18] where the contraction method is used on functional spaces) to show uniqueness of the solution with fixed mean and finite variance: it is quite straightforward in this case because almost sure convergence of (Wn())n1 as n tends to infinity is known a priori.

2 Three examples

2.1 The binary coupling
We call this the "binary" coupling because, for each time parameter , the GW family tree associated to (Zn(), n  0) is binary. Let U  Uniform[0, 1] and consider the càdlàg process (Bin(),   IBin), where

Bin() := 2 IU/2, for any   IBin := (1, 2].
The process Bin is a non-decreasing process that is constant in IBin, except at the random time  = 2U at which it jumps from 0 to 2. Moreover, since IU/2  Bernoulli(/2), we have
EBin() = .

The interval IBin is the range of time parameters  for which E(Bin()) > 1. Thus, (Bin())IBin can be used as the offspring process in Definition 1.2.
To describe the distribution of (Bin(1), · · · , Bin(d)) for 0  1 < · · · < d  2, set FI = inf{k : Bin(k) = 2}, the first index where Bin(k) is equal to 2; we have

P(FI = k) = (k - k-1)/2, for k  1, d

(2.1)

and P(FI = +) = P(Bin(d) = 0) = 1 - d/2.

11

For 1  · · ·  m elements of IBmin,

fB1in,··· ,m (x1, · · · , xm)

=

(1 - m/2) +

m

k - k-1 2

m
x2i .

k=1

i=k

Proposition 2.1. The process Bin satisfies (HReg) and (HMom), so that Theorem 1.7 applies when the offspring process X = Bin, on IBin = (1/2, 1].

Proof. It is straightforward to check that (HReg) holds. Now, we check (HMom) using Lemma 1.5: we have Fac0,2,2 = 0 (since either X(2) = 0 or X(3) = 0), which implies (1.8). To prove that (1.9) holds, write EBin((X(3))X(3)3) = 16 PBin(X(3) = 1, X(2) = 0) = 83.

We now comment on the properties of the limiting process (W (),   IBin). We first look at the one-dimensional marginal distributions. From Equation (1.4), we get that, for all x  R,   (1, 2],

(x)

=

1

-

 2

+

 2

(x/)2

.

In this case q = (2 - )/, and using (1.20) and (1.21) (or (1.19)),  is solution to

(x) = ( - 1) (x/)2 + (2 - ) (x/) ,

so that p() is the distribution ( - 1)2 + (2 - )1. Hence,  is the distribution of

Z

(=d)

(Z B-1

+ 

Z

)

+

(1

-

B-1

)

Z 

=

B-1

Z 

+Z 

(=d)

j1

B(j-) 1 Z (j ) j

where Z, Z , Z(1), Z(2), · · · are i.i.d. copies of Z, and the B(j-)1 are i.i.d. Bernoulli random variable with time parameter  - 1, all these random variables are independent (and the Zj have mean 1/(1-q)). We were not able to find an explicit solution of . The first moments of W conditioned to be > 0 are

1,

/2 -1

,

/2 ( - 1)2

,

(

+

3/2 1) ( -

1)3

,

(

+

1)

3 ( + ( - 1)4

5) /2 (2 +



+

1)

,

(2

+

15 1) (2

2 +

2 + 3  + 1)

 + 7 /2 ( + 1)2 (

-

1)5

and

E(W|W

>

0)

=

 2(-1)

,

and

its

variance

is

(2-) 4(-1)

which

goes

to



at

1,

and

is

0

at

2

(since

W2 = 2 a.s.).

We were not able to say something interesting on the 2-dimensional marginal distributions.

2.2 The geometric coupling
Geometric random variables (with support {0, 1, 2, · · · }) are important in GW theory because, conditionally on the total number of nodes n, the family tree of a GW process with this offspring distribution is uniform among the set of (unlabelled) trees with n nodes (and this holds for any parameter / {0, 1} of the geometric distribution).

12

Let Ber = (Ber(), 0    1), where Ber() = 1U and U  Uniform[0, 1]. Let (Ber(i), i  0) be a sequence of independent copies of Ber. Define

Geo()

=

inf

i  {0, 1, 2, · · · } : Ber(i)

1 1+

=1

, for   IGeo := (1, +),

the first success in this sequence of Bernoulli trials; we have P(Geo() = k) = k/(1 + )k+1, and

EGeo() = .

The inversion   1/(1 + ) allows to get a non-decreasing process, while the corresponding processes Ber(i) (1/(1 + )) are non-increasing. If the sequence (1, · · · , d) is non decreasing, then the sequence Geo(1), · · · , Geo(d) is a Markov chain. Indeed, given Geo(j-1),

Geo(j) (=d) Geo(j-1) + Ber

j - j-1 1 + j

(1 + Geo (j)),

where the random variables in the right hand side are all independent, so that

[Geo(1), · · · , Geo(d)] (=d)

j
G1 + Ber(i)
i=2

j - j-1 1 + j

(1 + Gi), 1  j  d

(2.2)

where the Gj are distributed as Geo(j), and all the variables Gj and Ber(i) are independent. For 1  · · ·  m elements of IGmeo



 

fG1e,o··· ,m (x1, · · · , xm)

=

G1

m
xi
i=1

m 1 + j-1


j=2

1 + j

+

j - j-1 1 + j

Gj

m

m

 xj

i=j

i=j

xj ,

where Gj(x) = 1/(1 + (1 - x)) is the generating function of Geo().

Proposition 2.2. The process Geo satisfies (HReg) and (HMom), so that Theorem 1.7 applies when the offspring process X = Geo, on IGeo = (1, +).
Proof. It is straightforward to check that (HReg) holds. We check (HMom) using Lemma 1.5: first note that Fac0,2,2 = 423(2)(3), which implies (1.8). For (1.9), we write EGeo((X(3))X(3)3) = (3)Q, where
Q = 24 33 + 18 (2 + 2) 32 + 2 6 22 + 12 2 + 7 3 + 6 23 + 12 22 + 7 2 + 1

is a polynomial which is bounded on any compact [a, b]  I. These moments are computed by some differentiations of fGe1o,3 .

We now comment on the properties of the limiting process. In particular, we look at its 1- and 2-dimensional marginals. By Equation (1.4), we get that, for all  > 1, for all x  R,

(x) = 1 +  - (x/) -1

(2.3)

13

and the only solution with mean 1 is

(x) = (1 -  + ix)/(1 -  + ix),

which we identify as the Fourier transform of the distribution of the random variable

Ber(p) Expo(p)

for

p

=



- 

1

=

1

-

q

where Expo(a) stands for an exponential random variable with parameter a (and mean 1/a), independent from the Bernoulli random variable.
Hence, the probability of extinction is 1/, and L(W | W > 0) is the law of Expo(p). The Fourier transform of the second dimensional distribution is a bit involved:

f1,,Ge2om(x1, x2)

=

(1

+

2

1 + 1 - 1x2 - 2x2) (1 + 1

-

1x1) .

(2.4)

A solution can be found:

f1,,Ge2om(x1, x2) = a + b1 (x1) + c2 (x2) + d1 (x1)2 (x2) and this is solution for all (a, b, c, d) such that

(2.5)

c = 1 - d, b = 1 - d, a = d - 1

and d is a root of the following quadratic polynomial

- x1x2 (2 - 1) (1 - 1) d2 + id 122x2 + 122x1 - 12x2 - 12x1 - i22 - 2x1 - 2x2 + 2 i2 + 2 i1x1x2 + i2x1x2
+ x1 + x2 - 2 i12 + i1 - 3 i12x1x2 + i122 - i - i122x2 - i122x1 - 2 12x1x2 + i1x1 + i2x2 + 122 + 122 - 12 - 2 12 - 22 + 1 + 2
This is not really informative, and we hope that some reader will succeed in finding a more classical representation of this distribution1.

2.3 The Poisson coupling

Take a standard Poisson process Poi := (Poi(),   IPoi) with intensity 1. We have

EPoi() = , for   IPoi := (1, +).

1To get this formula, we guess the form (2.5), so that finding (a, b, c, d) such that 1,2 (x1, x2) =

f1,,Po2i 1,2

, x1 x2
1 2

, 1,2

0,

x2 2

, and 1,2 (x1, x2) has the right marginal, is a matter to solve some polyno-

mial equations in (a, b, c, d) and coefficients in the set of rational fractions Q[[x1, x2, 1, 2]]. This can be done by

hand or using a computer algebra system, and the computation of a Kronur basis.

14

For any 1  · · ·  m elements of IPoi,

m

m

fP1o,i··· ,m (x1, · · · , xm) =

exp (k - k-1) - 1 + xj ,

k=1

j=k

since (Poi(k) - Poi(k-1), 1  k  m) are independent Poisson random variables with respective parameters k - k-1.

Proposition 2.3. The process Poi satisfies (HReg) and (HMom), so that Theorem 1.7 applies when the offspring process X = Poi, on IPoi = (1, +).

Proof. Again, it is straightforward to check that (HReg) holds. We now check (HMom) using Lemma 1.5: we have Fac0,2,2 = (2)2(3)2, which implies (1.8). For (1.9), we write EPoi((X(3))X(3)3) = (33 + 6 32 + 7 3 + 1)3. Since the polynomial in front of 3 is
bounded on any compact [a, b]  I, this implies (1.9). These moments can be computed by some differentiations of fPo1i,3 .

The extinction probability is solution of e(q-1) = q, which implies that q can be expressed

in terms of  using the LambertW function, or reciprocally, given q = q  (0, 1), the corresponding



is



=

ln(q) q-1

.

The

Fourier

transform



of

W ()

satisfies

(x)

=

exp [((x/) - 1)].

Let us turn our attention toward L(W | W > 0); using (1.21) and plugging P(X = m) =

me-/m!, we get p0() = 0 and for j  1

pj ()

=

(1

-

q)j-1 j!

j

e-

mj

m-j qm-j (m - j)!

=

[(1

- j

q)]j-1 !

e(q-1).

Hence  is solution to

(x)

=

-1 + q(x/) (-1 + q)q(x/)-1

=

q(1 - q-(x/)) .
-1 + q

From here (or using (1.18)) it is possible to extract the moments of :

1,

1

1 -

q

,

(1

-

 q) (

-

1)

,

(

+

2 ( + 2) 1) ( - 1)2 (1

-

q)

,

(

+

3 3 1) ( -

+ 5 2 1)3 (2

+6+6 +  + 1) (1

-

q)

,

·

·

·

We were not able to go further in the description of this distribution.

Open question 1. Find a simple description of each of the limit processes (W (),   I) in the binary, geometric, and Poisson cases.

Other cases can be interesting too, but we can expect that these three ones are likely to be the simplest, since they are the simplest model of GW trees.

15

3 Proof of Theorem 1.7

3.1 Convergence in D(I, R+): tightness under moments assumptions

We first give a characterization of convergence in D(I, R) taken in Billingsley [4, Section 13.5]:

Proposition 3.1. Consider a compact interval [a, b]  R, and (Yn(), n  0)[a,b] a sequence of processes such that:

(i)

(Yn(i), 1



i



d)

-(-d)
n

(Y (i), 1



i



d)

for

al l

d



1

and

1



2



···



d

in

the

set

of

continuity points of Y on [a, b].

(ii) Y (b - ) (d) Y (b) as   0, and

(iii) For all a  1  2  3  b and  > 0,

P min

|Yn(i)|, i  {2, 3}





(F (3)

- F (1))2 4

(3.1)

where   0,  > 1/2, and F is non-decreasing and continuous on [a, b].

In

this

case,

Yn

-(-d)
n

Y

in D([a, b], R).

Remark 3.2. By [4, Eq. (13.14)], a sufficient condition for the process Yn() = Wn() (for all n  0,   [a, b]) to satisfy (3.1) is

E |Wn(2)|2|Wn(3)|2  Const · (3 - 1)2.

In fact, we prove that the limit W is in D(I, R+) as a weak limit of elements of D(I, R+) for the Skorokhod topology on each compact.
To prove Theorem 1.7, we start by a lemma that shows that, in Proposition 3.1, if the convergence in Assumption (i) holds almost surely, then Yn converges in probability in D(I) to Y .
Lemma 3.3. Assume that a sequence of processes (Tn, n  0) is tight in D([a, b]), and that moreover, for any d  0 any a  1, · · · , d  b, the sequence (Tn(1), · · · , Tn(d)) converges a.s toward some random variables (T (1), · · · , T (d)) (that are, by construction, consistent). Under these assumptions, (Tn) converges in probability in D([a, b]) (equipped with the Skorokhod topology) to a càdlàg process T which coincides with T almost everywhere (it is determined by T , but can be different on a countable number of points).

We prove this lemma in Section 5.4. In Proposition 1.6 we stated the a.s. convergence of (Wn(i), 1  i  d) for all fixed 1 < · · · < d  I, and then characterised the limiting distribution (W (i), 1  i  d). By Theorem 1.7, the limiting process in probability of (Wn), which we call W  for the purpose of this remark, is a càdlàg process (as the limit of càdlàg processes in D(I)). From this convergence it can only be deduced that the FDD of W are given by those of (W (i), 1  i  d) almost everywhere, in fact at the a.s. continuity point of W  (the complement is a Lebesgue null subset of I).

16

To prove Theorem 1.7, it only remains to check that Assumptions (i), (ii) and (iii) of Proposition 3.1 are satisfied under the assumptions of Theorem 1.7. In Proposition 1.10, we have already proved that Assumption (i) of Proposition 3.1 holds. We prove (ii) in Section 3.2, and (iii) in Section 3.3. In the whole section, we assume that the assumptions of Theorem 1.7 hold.

3.2 The limiting process satisfies Condition (ii) of Proposition 3.1

Recall that, by Proposition 1.10,

1,2 (x1, x2) = f1,2 1,2

x1 , x2 1 2

, 1,2

0, x2 2

(3.2)

and 1,2 is the unique solution of this equation with mean (1, 1) and finite variance. Differentiating this equation several times in both variables gives equations for the moments of (W (1), W (2)). This can be extended to general d dimensional moments (see also Section 4). For all integers k1, k2  0, set

Mk1,k2 := E W (1)k1 W (2)k2 ,

and recall that, by assumption, M0,0 = M1,0 = M0,1 = 1. We first aim at proving that

E((W (2) - W (1))2) = M2,0 + M0,2 - 2M1,1 - 0,
12
since it would imply Assumption (ii) of Proposition 3.1. Differentiating (3.2), we get that

(3.3)



 

M2,0

=

(Fac2,0 + 1M2,0)/21



M0,2 = (2M0,2 + Fac0,2 + 2Fac1,1 + Fac2,0)/22



 

M1,1

=

(1M1,1 + Fac1,1 + Fac2,0)/(12),

(3.4)

where we have for all integers 1, . . . , d  0,

Fac 1,d

(

1, d

)

:=



y11

 ·

||
··

ydd

f

(y1

,

·

·

·

, yd)

[1,··· ,1]

From (3.4), we get

(3.5)

M2,0

=

1

Fac2,0 (1 -

1)

,

M0,2

=

Fac0,2

+ 2 Fac1,1 + 2 (2 - 1)

Fac2,0 ,

M1,1

=

Fac1,1 + 1 (2

Fac2,0 - 1)

.

(3.6)

Thus, by (3.3), we have, as 2  1,

E

(W (2) - W (1))2



Fac0,2 1(1 -

1)

.

Thus, for the right-hand side of (3.3) to tend to zero as 2  1, it is necessary and sufficient that Fac0,2  0, which is implied by (HMom), Equation (1.6).

17

3.3 The sequence (Wn) satisfies Condition (iii) of Proposition 3.1
The last remaining step to prove the tightness is the following result.
Theorem 3.4. If (HReg) and (HMom) hold, then for all [a, b]  I, there exists a constant CW > 0 such that, for all 1  2  3  [a, b],
sup E (Wn(2))2 (Wn(3))2  CW (3 - 1)2.
n0
By Remark 3.2, this theorem implies that the sequence (Wn) satisfies Condition (iii) of Proposition 3.1. The proof of Theorem 3.4 is quite long: it will last until the end of Section 3.

The important "S" notation: Fix an interval [a, b]  I and some 1  2  3  [a, b]. For r  {1, 2, 3} and j  {1, 2, 3}, set

Sr(Wn(j)) :=

X (r )

Wn(i)(j ),

i=1+X (r-1 )

(3.7)

where (Wn(i)(j) : n  0)i1 is a sequence of i.i.d. copies of (Wn(j) : n  0). Also recall that we set 0 = 0. For example, we can write

Wn+1(j )

=

1 j

X(j )
Wn(i)(j )
i=1

=

jS
=1

(Wn(j)) . j

(3.8)

We use the fact that, for all i = j, Wn(i) is independent of Wn(j), to get the following lemma:

Lemma 3.5. For any j,   {1, 2, 3}, Sr (Wn(j)) and Ss (Wn( )) are independent iff r = s. Moreover, Sr is linear in the following sense: for all constants c1, c2, c3,

3

3

Sr

cjWn(j) = cjSr (Wn(j)) .

j=1

j=1

Using this notation in (3.8), we get

Wn+1(2)

=

S1 (Wn(2)) 2

+

S2 (Wn(2)) 2

-

S1

(Wn(1)) , 1

Wn+1(3)

=

S1

(Wn(3)) 3

+

S2

(W (3)) 3

+

S3

(W (3)) 3

-

S1

(Wn(2)) 2

-

S2

(Wn(2)) . 2

Using (3.8) and the linearity of Sr, we can write

Wn+1(2)

=

1 2

[T1

+

T2

+

T3] ,

Wn+1(3)

=

1 3

T1 + T2 + T3 + T4 + T5

,

(3.9) (3.10)
(3.11) (3.12)

18

where we have set


 

T1

=

S1 (Wn(2)) ,





T1 = S1 (Wn(3)) ,

   

T2 = S1 T2 = S1 T4 = S2

-

2 1

Wn

(1

)

,

-

3 2

Wn

(2

)

,

-

3 2

Wn

(2

)

,

T3 = S2 (Wn(2)) , T3 = S2 (Wn(3)) , T5 = S3 (Wn(3)) .

(3.13)

The reason why we decompose Wn+1(2) and Wn+1(3) this way is because it maximises the number of 's; this is important because, intuitively, 's give terms that are small when |3 - 1| goes to zero.

Proof strategy: To prove Assumption (iii) of Proposition 3.1, we start by writing

E Wn+1(2)2Wn+1(3)2

=

1 (23)2

E
1i1 ,i2 3

Ti1 Ti2 Tj1 Tj2

.

1j1 ,j2 5

(3.14)

Note that there are 9 × 25 = 225 terms in this sum, which we call "T -moments" from now on.

Our strategy is to analyse the contribution of each of these T -moments. To do so, we will first

expand each of the 225 T -moments using Lemma 3.7 below. This will give

E

Wn+1(2)2Wn+1(3)2

=

1 (23)2 E

Wn(2)2Wn(3)2

+ Termm
m

(3.15)

where the index m ranges over several hundreds of values (there are more terms in this sum than the 225 initial terms of (3.14)). From now one, we call the Termm the "multinomials". Importantly, the sum in (3.15) is finite, and each of the multinomials satisfies

Termsm  cm(3 - 1)2

(3.16)

for a finite constant cm (which depends on m). To show (3.16), we do not treat the several hundred multinomials one by one. Instead, we partition them in several families, and show that all multinomials in each of these families satisfy (3.16).
Using (3.16) in (3.15), we get

E Wn+1(2)2Wn+1(3)2



1 (23)2 E

Wn(2)2Wn(3)2

+ cm(3 - 1)2,
m

(3.17)

which implies that for [a, b] fixed, there exist some universal constants A  (0, 1) and C > 0 such

that, for all 1 < 2 < 3  [a, b],

E(Wn+1(2)2Wn+1(3)2)  AE(Wn+1(2)2Wn+1(3)2) + C(3 - 1)2.

Iterating this formula (see Lemma 3.6 below, which can be applied since W0(2)W0(3) = 0 a.s.) allows us to conclude that

sup E Wn+1(2)2Wn+1(3)2  CW (3 - 1)2
n0
for a finite constant CW > 0, as claimed.

19

Lemma 3.6. Let (Un)n0 be a sequence of non-negative real numbers such that, for all n  0,

Un+1  AUn + B,

(3.18)

for

some

constants

A



[0, 1)

and

B

 0.

In

this

case,

for

al l

n



0,

Un



AnU0 +

B 1-A

.

Proof. Iterating (3.18), we get Un  AUn-1 + B  A(AUn-2 + B) + B  AnU0 + B

n-1 i=0

Ai

.

Thus, to conclude the proof, it only remains to prove (3.15) and (3.16). To do this, we first describe the multinomials that appear in (3.15); this is done in Section 3.4 by expanding each of the T -moments. We then show why each of these multinomials can be bounded by cm(3 - 1)2 (and thus why (3.16) holds); this is done in Section 3.4.2 by classifying the 225 of T -terms into four different classes.

3.4 Algebraic expansion of E(Wn+1(2)2Wn+1(3)2)
As already said, we write Facd1,d2,d3 instead of Facd1,d2,d3 (1, 2, 3). For a finite set B, we let Part(B, k) be the set of partitions of B into k non empty parts (the
parts must be disjoint, and their union must be B). For example,

Part({1, 2, 3, 4}, 3) = [{1}, {2}, {3, 4}], [{1}, {2, 3}, {4}], [{1}, {2, 4}, {3}], [{1, 2}, {3}, {4}], [{1, 3}, {2}, {4}], [{1, 4}, {2}, {3}] .

A partition is formally a set of sets. We consider the canonical representation of a partition as a sequence of sets, where the sequence is obtained by sorting the sets according to their smallest element, as done in the example above.
In the next lemma, we show how one can express each T -moment E(Ti1Ti2Tj1Tj2) as a linear combination of moments of (Wn(1), Wn(2), Wn(3)).

Lemma 3.7. Let n  1 and (Vn,j)j1 such that, for j  1, Vn,j  Wn( ), Wn( ) : 1   3 . The next formula hold if the moments involved are well defined. Assume that B1, B2 and B3 are disjoint set of indices. We have





3

d



E  S1(Vn,j)

S2 (Vn,k )

S3(Vn, ) =

Facd1,d2,d3

E  Vn,s

jB1

kB2

B3

d1 0 d2 0 d3 0

=1 [A1,...,Ad ] k=1
Part(B ,d )

sAk

(3.19)

We show on three particular examples how this formula can be applied to each of the T -moments: Example 1. We use Lemma 3.7 to expand
Q1 := E S1 (Wn(1))2 S1 (Wn(2)) S3 (Wn(3)) .

20

We set Vn,1 = Vn,2 = Wn(1), Vn,3 = Wn(2), and Vn,4 = Wn(3). We also set B1 = {1, 2, 3}, B2 = , and B3 = {4}. With this notation, Q1 is indeed equal to the left-hand side of (3.19).
We now look at the right-hand side of (3.19). First note that B1 can be partitioned into 1, 2, or 3 parts, i.e. d1 ranges from 1 to 3 in the right-hand side of (3.19). We have Part(B1, 1) = {[{1, 2, 3}]}, Part(B1, 2) = {[{1, 2}, {3}], [{1}, {2, 3}], [{1, 3}, {2}]}, and Part(B1, 3) = {[{1}, {2}, {3}]}. Similarly, d2 = 0 and (by convention) Part(B2, 0) = {[]}. Finally, d3 = 1, and Part(B3, 1) = {[{4}]}. Applying (3.19), we thus get

Q1 = Fac1,0,1 [E(Vn,1Vn,2Vn,3)] [E(Vn,4)] + Fac2,0,1 [E(Vn,1Vn,2)E(Vn,3) + E(Vn,1)E(Vn,2Vn,3) + E(Vn,1Vn,3)E(Vn,2)] [E(Vn,4)] + Fac3,0,1 [E(Vn,1)E(Vn,2)E(Vn,3)] [E(Vn,4)]
In the second term of the sum, since d1 = 2 in Facd1,d2,d3 we separate the product jB1 Vn,j into two independent non-empty products. There are three possible ways to do that, and they give the following sum of three terms: E(Vn,1Vn,2)E(Vn,3) + E(Vn,1)E(Vn,2Vn,3) + E(Vn,1Vn,3)E(Vn,2).
Example 2. We now show how to apply Lemma 3.7 to expand

Q2 := E (S2 (Wn(3)) S3 (Wn(1))) .

To do so, we set Vn,1 = Wn(3), Vn,2 = Wn(1), B1 = , B2 = {1}, and B3 = {2}. With these definitions, Q2 equals the left-hand side of (3.19). We now look at the right-hand side of (3.19): Since B1 is empty, and since both B2 and B3 have one element, the only possibility is d1 = 0 and d2 = d3 = 1. We thus get

E (S2 (Wn(3)) S3 (Wn(1))) = Fac0,1,1E(Wn(3))E(Wn(1)) = 0,

since E(Wn(1)) = 0. This is not surprising; indeed, we have

 X(2)

X (3 )



Q2 = E 

Wn(i)(3)

Wn(j)(1) .

i=X (1 )+1

j =X (2 )+1

Because the sequence (Wn(i)() : n  0,  > 1)i1 is a sequence of i.i.d. copies of (Wn() : n  0,  > 1), we indeed get

Q2 = E (X(2))(X(1)) E(Wn(3))E(Wn(1)) = 0.

Example 3. We show how to use Lemma 3.7 to expand
Q3 := E (S3 (Wn(3)) S3 (Wn(1))) .
We set Vn,1 = Wn(3), Vn,2 = Wn(1), B1 = B2 =  and B3 = {1, 2}, so that Q3 is indeed of the form of the left-hand side of (3.19). We have Part(B3, 1) = {[{1, 2}]} and Part(B3, 2) = {[{1}, {2}]}, so that d1 = 0, d2 = 0, 1  d3  2. We thus get E (S3 (Wn(3)) S3 (Wn(1))) = Fac0,0,1E (Wn(3)Wn(1)) + Fac0,0,2E (Wn(3)) E (Wn(1))

21

= Fac0,0,1E (Wn(3)Wn(1))

Again, this can be checked directly by computing

 X(3)

X (3 )



E

Wn(i)(3)

Wn(i)(1)

i=X (2 )+1

i=X (2 )+1

and by regrouping the terms involving the same (i) and the others. The advantage of Lemma 3.7 is to give a general formula that applies to all of the E(Ti1Ti2Tj1Tj2).

Proof of Lemma 3.7. Because of Lemma 3.5, conditionally on {Xj = xj, 1  j  3}, the three products inside the expectation on the left-hand side are independent. We thus need to calculate E jB S (Vn,j) | X( ) . Now recall that, by definition of S (see (3.7)), we have

X( )

X( )

X( )

S (Vn,j) =

Vn(,ij) =

···

Vn(,ij11) · · · Vn(,ijmm),

jB

jB i=X( -1)+1

i1=X( -1)+1 im=X( -1)+1

where m = m( ) is the cardinal of B = {j1, . . . , jm}. Shifting the indices from the range [X( -1)+ 1, X( )] to [1, X( )] does not affect the distribution of the right-hand side, implying that

x

x

E

S (Vn,j) | X( ) = x =

···

E Vn(,ij11) · · · Vn(,ijmm) .

jB

i1=1 im=1

We now re-write this sum by grouping the indices i1, . . . , im that are equal and using independence when the indices differ. For all k  {1, . . . , x}, group all j 's such that i = k into one (possibly empty) part. This forms a partition of B . We decompose the sum above depending on the number of non-empty parts in this partition of m, which we call d: this gives





d



E  S (Vn,j) | X( ) = x = x(x - 1) · · · (x - d + 1)

E  Vn,  .

jB

d1

[A1,··· ,Ad]Part(B ,d) k=1

Ak

The factor x(x - 1) · · · (x - d + 1) is the number of different choices for the common index i for the first, second, etc parts of the partition: there are x choices for the first part, x - 1 choices for the second part, and so on.

3.4.1 Expansion of E(Wn+1(2)2Wn+1(2)3): classification of the contributions
The aim of this section is to show how to apply Lemma 3.7 to each of the T -moments that appear in (3.14). This allows us to eventually write E(Wn+1(2)2Wn+1(3)2) as in (3.15), where each of the multinomials is a coefficient Facd1,d2,d3 times a product of "monomials" E( rAk Vn,r). Since we see E( rAk Vn,r) as a monomial, we will call |Ak| its "degree". If the degree is 1, then, because all the involved Vn,j belongs to 3i=1{Wn(i), Wn(i)} (see (3.11) and (3.12)), E(Vn,j) is either 1 or 0 (because E(Wn(i)) = 1 and E(Wn(i)) = 0). Monomials of degrees 2 correspond to correlations, and degree 3 and 4 are the most difficult monomials to handle in our analysis.

22

Graphical representation of the complete computation First, represent in an array, as in Fig. 1, the Ti and the Ti as defined in (3.13). With this graphical representation, each element

S1

S2

S3

T

Wn(2)

-

2 1

Wn(1

)

Wn(2)

T

Wn(3)

-

3 2

Wn(2

)

Wn(3)

-

3 2

Wn(2

)

Wn(3)

Figure 1 ­ On the top line T1, T2, T3 in this order (for example T1 = S1 (Wn(2))); on the second line, the T1, T2, T3, T4, T5 in this order, for example, T3 = S2 (Wn(3)).

(Ti1, Ti2, Tj1, Tj2) of the sum in (3.14) can be obtained by multiplying an ordered pair of elements (with repetition allowed) above the line and an ordered pair of elements below the line. Two
examples are given below: Each of the T -moments can be represented using this graphical tool,

S1 T Wn(2)

S2 Wn(2)

T

S1 T Wn(2)

S2

-

2 1

Wn

(1

)

T

-

3 2

Wn(2

)

-

3 2

Wn

(2)

S3

-

3 2

Wn(2

)

Wn(3)

S3

Figure 2 ­ Two examples of choices for (i1, i2, j1, j2): in the first case on the top, we have selected (T1, T3, T4, T5) and, in the second case on the bottom we have selected (T1, T2, T2, T2) (repetitions are allowed: here, we have chosen j1 = j2 = 2).

and this becomes useful when applying Lemma 3.7. Indeed, in this graphical representation, we can see that the four terms Ti1, Ti2, Tj1, Tj2 are partitioned into three (possibly empty) groups: the S1-group, the S2-group and the S3-group, represented graphically by the three rounded rectangles. In the right-hand side of (3.19), we consider all possible ways to refine this partition, meaning that each part of the chosen partition must be included in one of the three rounded rectangles, as for example in Fig. 3.
If, in a refined partition, S1 is split in d1 parts, S2 in d2 parts, and S3 in d3 parts, then the contribution of this partition  to the right-hand side of (3.19) is the multinomial

Facd1,d2,d3

E

e,

P  eP

where we sum on all the parts of the refined partition , and then multiply on all elements of this part P . Note that d1, d2  4, and d3  2, since there are maximum 4 terms in the rounded

23

S1 T Wn(2)

S2 Wn(2)

T

S1 T Wn(2)

S2

-

2 1

Wn

(1

)

T

-

3 2

Wn(2

)

-

3 2

Wn

(2)

S3

-

3 2

Wn(2

)

Wn(3)

S3

Figure 3 ­ Two refined partitions of, respectively, the top and bottom examples in Fig. 2.

rectangle associated to S1, resp. S2, and maximum 2 terms in the rounded rectangle associated
to S3. Furthermore, d1 + d2 + d3 = 4 since there are four terms in total: Ti1, Ti2, Tj1, and Tj2. For example, the contribution of the refined partition on the top of Fig. 3 is the multinomial

Fac1,1,1E [Wn(2)] E Wn(2)

-

3 2

Wn

(2)

E [Wn(3)] ,

and the contribution of the refined partition on the bottom of Fig. 3 is the multinomial

Fac2,0,0E Wn(2)

-

2 1

Wn

(1)

-

3 2

Wn(2

)

E

-

3 2

Wn(2)

.

From this graphical representation, one can see that the only way to get a term that contains E Wn(2)2Wn(3)2 is to have 4 terms of type S1 in the same part of the refined partition. This only occurs in the development of the T -moment E(T1, T1, T1, T1), and only for d1 = 1, d2 = d3 = 0. Thus, from (3.14), we get

E

Wn+1(2)2Wn+1(3)2

=

Fac1,0,0 (23)2 E

Wn(2)2Wn(3)2

+ Termm,
m

and the multinomial E Wn(2)2Wn(3)2 does not appear in any of the multinomials in the sum. Because Fac1,0,0 = 1, this gives (3.15).

3.4.2 Conclusion To conclude the proof, it only remains to bound all of the multinomials by cm(3 - 1)2, as announced in (3.16). To do so, we need the following lemma: Lemma 3.8. Fix a compact subinterval [a, b] of I. If (HReg) and (HMom) hold, then
(i) For any a  1  2  3  b,
2  (3 - 1), 3  (3 - 1).

24

(ii) For any k1, k2, k3  {0, 1, 2, · · · } such that k1 + k2 + k3  4,

M k1,k2,k3

:=

sup sup E
n a123b

Wn(1)k1 Wn(2)k2 Wn(3)k3

< +

(iii) If 1  k1 + k2  3 and k1 + k2 + j1 + j2 + j3  4 then, there exists a constant C  0, such that for all a  1  2  3  b,
sup E (Wn(2))k1 (Wn(3))k2 Wn(1)j1 Wn(2)j2 Wnj3 (3)  C(3 - 1).
n0

Proof. (i) is straightforward. We prove (ii) in Section 3.4.3, and (iii) in Section 3.4.4.
To prove the bounds of (3.16), we use the graphical representation of Section 3.4.1. Each of the multinomials comes from the expansion of a T -moment. We divide the T -moments into three groups: the T -moments that involve at least one element from the rounded rectangle associated to S3 (i.e. j1 = 5 or j2 = 5), the T -moments that involve no element from S3 but at least one element from S2, and finally, the T -moments that involve no element from S2 or S3.

T -moments that involve elements from S3. Note that the only term from S3 is T5, and it can appear once or twice in a T -moment E(Ti1Ti2Tj1Tj2). Apart from T5, only T3, which is an element from S2, does not contain the symbol .
Recall that, intuitively, the terms containing a  go to zero when 3 tends to 1. Therefore, intuitively, the worst possible case is E(T32(T5)2). We expand this using Lemma 3.7: we get a sum of multinomials, which are the product of a prefactors Fac0,y,z with 1  y, z  2 times a product of monomials of degree at most 4 in Wn(2) and Wn(3). By (1.7) and Lemma 3.8(ii) these multinomials are indeed all bounded by C(3 - 1)2 as claimed in (3.16).
Now assume that j1 = 5 or j2 = 5, and Ti1Ti2Tj1Tj2 = T32(T5)2. In view of Fig. 1 this means that one of Ti1, Ti2, Tj1, and Tj2 is either:
· a term of the form Wn( ); this term can be a S1 or a S2-term: in any case by Lemma 3.8(iii), any monomial containing such a term is bounded by C(3 - 1).
· a term that contains a j; using Lemma 3.8(ii), any monomial containing at least one of these terms is bounded in absolute value by C(3 - 1) for a universal constant C.
In both cases, the prefactor Facx,y,z with z  {1, 2} brings the extra term C(3 - 1) needed by (1.6) or (1.7).

Terms that involve no S3 terms but at least one S2-term. The only term in the S1 or S2 group that comes without any  is T3; thus, the "worst case" for a T -moment in this group is to have i1 = i2 = 3. All multinomials obtained when expanding such a T -moment come with a prefactor Facd1,d2,0 with 0  d1, d2  4 and d1 + d2  4. Also note that since the T -factor contains
25

at least one term from S2, it contains at most three terms from S1. This implies d2  1 and d1  3 (also, d3 = 0 because this T -moment contains no S3-term). We distinguish cases according to the value of d1. First note that, by (HMom), since d2 = 0, we have Facd1,d2,0  C(3 - 1).
· If d1 = 3, then the corresponding multinomial (without its prefactor) is a product of three expectations, each of one S1-term, times the expectation of an S2-term. All of the first three expectations contain a  and are thus bounded by C(3 - 1), by Lemma 3.8(i) and (iii). The fourth expectation is bounded by a constant by Lemma 3.8(ii) (and the triangular inequality if the term from S2 contains a ). In total, with its prefactor, the multinomial is thus bounded by C(3 - 1)2, as claimed.

· If d1 = 2, then the corresponding multinomial is a product of three (if d2 = 1) or four (if d2 = 2) expectations: two of these are expectations of S1-terms, the other one or two are expectations of S2-terms. We bound the expectations of S2-terms by constants using Lemma 3.8(ii) (and the triangular inequality if they contain 's). Among the two expectations of S1-terms, one is the expectation of one term from S1, and the other is the expectation of the product of either one or two terms from S1. The first of these two expectations is bounded by C(2 - 1) by Lemma 3.8(i) and (iii). The second can be bounded by a constant using the triangular inequality and Lemma 3.8(ii). Thus in total, with its prefactor, such a multinomial is bounded by C(2 - 1)2, as claimed.

· If d1 = 1, then the corresponding multinomial is the product of the expectation of a product of one, two or three S1-terms times the product of at least one expectation of the product of at most two S2-terms. The expectations of S2-terms can be bounded by constants using Lemma 3.8(ii) (and the triangular inequality to remove the 's). By Lemma 3.8(i) and (ii), the expectation of a product of one, two or three S1-terms is bounded by C(2-1). Together with the prefactor, this bounds the multinomial by C(2 - 1)2, as claimed.

· Finally, if d1 = 0, then the corresponding multinomial is a product of one, two, three or four expectations of S2-terms.

­ If d2 = 1, then the multinomial is one expectation of the product of four S2-terms;

among

those

four

terms,

two

are

from

{Wn

(3),

-

t3 t2

Wn(2

)}.

In other words, the

only

possible

multinomials

are

E(Wn(2)2Wn(3)2),

-

3 2

E(Wn(2

)3

Wn(3

)),

and

(3)2 2

E(Wn(2)4),

all

of

which

are

bounded

by

C (2

-

1),

by

Lemma

3.8.

­ If d2 = 2, then the multinomial is the product of two expectations of products of S2-

terms: these two products are either both products of two S2-terms, or one of them has

one term and the other three terms. In both cases, one can check that this multinomial

with its prefactor can be bounded by C(2 - 1)2.

­ If d2 = 3, then the multinomial is the product of the expectation of the product of two S2-terms times the product of two expectations of one S2-term each. At least one these

26

expectations contains a , and thus, by Lemma 3.8, it can be bounded by C(3 - 1), which, together with the prefactor, allows us to bound the monomial by C(2 - 1)2. ­ If d2 = 4, then the multinomial is the product of four expectations of one S2-term each. Two of these S2-terms contain a  (because they are from a T ), and are thus bounded by C(3 - 1) (by Lemma 3.8(i) and (iii)). The other two expectations are bounded by constants by Lemma 3.8(ii). With the prefactor, we get C(2 - 1)2, as claimed.
T -moments that involve no S3 term and no S2-terms. These T -moments only contain S1terms. These cases are a bit different from the previous ones because the prefactor Facx,0,0 is not small; however, by (HReg) , it is bounded by a constant C (since 1  [a, b]). Since we want to bound every contribution (in absolute value) up to a constant, we can ignore the factorial moments here. We look at the multinomials that come from the right-hand side of (3.19), and distinguish according to the value of 1  d1  4 (note that d2 = d3 = 0 since the T -moment has no S2 or S3-terms).
· If d1 = 1, then the corresponding multinomial is the expectation of the product of four S1terms.
­ If, among these four terms, at least two are from {T2, T2} (which contain t2 and t3 as factors, respectively), then, using Lemma 3.7(ii) to bound the rest of the expectation by a constant, we get that this monomial is bounded by C(3-1)2 and thus by C(3-1)2 as claimed.
­ If exactly one of the four S1-terms in the T -moment is from {T2, T2}, then the multinomial is a constant times t3 or t2 times an expectation of the form E(Wn(2)k1Wn(2)k2), with k1 + k2 = 3. By Lemma 3.7(iii) this last expectation is bounded by C(3 - 1), which, together with the t3 or t2 term gives C(3 - 1)2 as claimed.
­ Finally, if none of the four terms in the T -moment are T2 or T2, then the multinomial is Fac1,0,0E(Wn(2)2Wn(3)2), which gives the first term in (3.15).
· If d1  2, then the corresponding multinomial equals its prefactor times a product of at least two expectations of a product of one, two or three S1-terms. By Lemma 3.7(i) and (iii), each of these two expectations is bounded by C(3 - 1) and thus their product is bounded by C(3 - 1)2, as claimed.
This concludes the proof.
3.4.3 Proof of Lemma 3.8(ii)
The result is immediate when k1 + k2 + k3 = 1 or k1 + k2 + k3 = 0 since in this case M k1,k2,k3 = 1. We reason by induction and assume that M k1,k2,k3 < + for all non-negative k1, k2 and k3 such that k1 + k2 + k3  m, where m is some fixed integer in {1, 2, 3}. Take a triplet (k1, k2, k3) such
27

that k1 + k2 + k3 = m + 1; we only need to prove that M k1,k2,k3 < +. We set Mk(1n,)k2,k3 := E Wn(1)k1Wn(2)k2Wn(3)k3 ; by (3.8), for all n  0,



3

Mk(1n,+k21,)k3

=

E

 



k 

Sj (Wn( ))









=1 j=1

=

1

3 =1

k

k1,2 +k2,2 =k2

k1 k1

k1,3 +k2,3 +k3,3 =k3

k2 k1,2, k2,2

k3 k1,3, k2,3, k3,3

×E S1 (Wn(1))k1 S1 (Wn(2))k1,2 S2 (Wn(2))k2,2 S1 (Wn(3))k1,3 S2 (Wn(3))k2,3 S3 (Wn(3))k3,3 .

We use Lemma 3.7 to expand this expectation into a sum of multinomials. Note that there are

k1 + k1,2 + k1,3 S1-terms, k2,2 + k2,3 S2-terms and k3,3 S3-terms. Thus, the monomials appearing in

the right-hand side of (3.19) are at most of degree m + 1. The monomials with degree at most m are

uniformly bounded by the recurrence hypothesis. After expansion, we have a sum of multinomials

(products of monomials) with total degree m + 1. Bounding the monomials with degree at most m

by a constant leaves us with a constant c0 plus the contribution of monomials with degree m + 1.

Since 1/

3 =1

k

is also bounded on [a, b] we have for c = max{c0/

3 =1

k

, [3]



[a, b]},

Mk(1n,+k21,)k3  c

+ Fac1,0,0E

3 j=1

Wn(j

)kj

+ Ik1=0Fac0,1,0E

3 j=2

Wn

(j

)kj

3 =1

k

+ Ik1=k2=0Fac0,0,1E Wn(3)k3

and the reason for this is that the only terms with maximal degree comes from the cases where
there are only terms of type S1 or S2 or S3 (and for the second and third case, this can happen only if k1 = 0 and k1 = k2 = 0 respectively). Since Fac1,0,0 = 1, Fac0,1,0 = 2, Fac0,0,1 = 3, this gives

Mk(1n,+k21,)k3



c

+

Mk(1n,)k2,k3

1

+

Ik1=02 + Ik1=k2=03

3 =1

k

Since k1 + k2 + k3  2, the factor of Mk(1n,)k2,k3 is uniformly bounded by  1/a. Now, to conclude, we use Lemma 3.6 with U0 = 1, A = 1/a, B = c.

3.4.4 Proof of Lemma 3.8(iii)
First note that it is enough to prove the claim when k1 + k2 = 1. Indeed, the case k1 + k2  2 can be reduced to the k1 + k2 = 1 case by expanding k1 + k2 - 1 factor of the type Wn(j) using the triangular inequality. For example
|E((Wn(2))Wn(3)2Wn(3))|  |E(Wn(1)Wn(3)2Wn(3))| + |E(Wn(2)Wn(3)2Wn(3))|

28

is bounded from above by 2C(3-1) if each term in the right-hand side is bounded by C(3-1). If k1 + k2 = 1, then either k1 = 1 or k2 = 1, and we need to treat these two cases separately. We set
D
An = E Wn(2) Wn(mi )
i=1 D
Bn = E Wn(3) Wn(mi )
i=1
where 1  D  3 (even if the method that follows work for larger D when the corresponding moments exist), and the mi are, as usual, elements of {1, 2, 3}.

Control of An+1. We want to prove that for any choice of 0  D  3, any choices of (mi), there exists a constant C = CD,(mi) such that the corresponding sequence (An) satisfies

sup |An|  C(3 - 1) for all a  1  2  3  b.
n0

(3.20)

We give a proof by recurrence on the value of D: if D = 0 then An = 0 so that (3.20) holds for C = 0. Let us assume that we showed that supn |An|  CD ,(mi)(3 - 1) for all choices of (D , (mi)) with D  D - 1 for some D  {1, 2, 3}, and aim at proving the result for any (D, (mi)). Fix such a pair (D, (mi)).
We have by (3.13) and (3.8)

An+1

=

1 2 E

S1

Wn(2)

-

2 1

Wn(1)

+ S2 (Wn(2))

D mi S (Wn(mi ))

i=1 =1

mi

.

We now use the linearity of S1 and of the expectation and see An+1 as the sum of three expectations A(n1+)1, A(n2+)1 and A(n3+)1 that can be written as in the left-hand side of (3.19). We thus apply Lemma 3.7 to each of these three expectations, and get, from the right-hand side of (3.19), a sum of multinomials. Recall that a multinomial is a prefactor Fac times a product of monomials.
The maximum degree of a monomial in the expansion of An+1 is D + 1; such monomials form
a multinomial with their prefactor (i.e. they are not multiplied by another monomial). In the expansion of A(n1+)1, the only monomial of degree D + 1 comes from the partition that leaves all S1-terms in one part. The same is true for A(n2+)1. In A(n3+)1, we only get a multinomial of degree D + 1 if mi  2 for all 1  i  D, and it comes from the partition that leaves all S2-terms in the
same part. Thus, the only multinomials involving a monomial of degree D + 1 are

M1

=

Fac1,0,0

2

D i=1

mi

E

D
Wn(2) Wn(mi )
i=1

=

Fac1,0,0

2

D i=1

mi

An,

M2

=

-

Fac1,0,0

2

D i=1

mi

E

2 1

Wn(1)

D
Wn(mi )
i=1

,

29

M3

=

Fac0,1,0

2

D i=1

mi

E

Wn(2)

-

2 1

Wn(1)

D
Wn(mi )
i=1

Imi2,1iD

=

Fac0,1,0An

2

D i=1

mi

Imi2,1iD

-

Fac0,1,0

2

D i=1

mi

E

2 1

Wn

(1)

D i=1

Wn

(mi

)

Imi 2,1iD .

Note that |M2|  C2  C(3 - 1) because of Lemma 3.8(ii). Hence, the total contribution of the monomial An in M1 and M3 (and thus in the expansion of An+1) is

Fac1,0,0

+ Fac0,1,0Imi2,1iD

2

D i=1

mi

× An

All the other multinomials appearing in the expansion of An+1 (included the second term of M3) satisfy one of the following alternatives:

· Its prefactor is Facx,y,z with y  1 or z  1 (meaning that, in the right-hand side of (3.19), it comes from a triplet (d1, d2, d3) such that d2  1 or d3  1). By (1.6) for these values of (x, y, z),Facx,y,z  C(3 -1). Furthermore, all the monomials appearing in this multinomial can be bounded by constants by Lemma 3.8(ii).

· Its prefactor is Facx,0,0 (and x = 1 since this gives M1): in this case, either Wn(2) appears

in

a

monomial

of

degree

at

most

D - 1,

or

(2 1

)

Wn(1

)

appears

in

a

monomial

of

degree

at least one 1. Applying the induction hypothesis in the first case, and Lemma 3.8(i) in the

second case (and Lemma 3.8(ii) in both case to bound the other monomials involved in the

multinomial), we get that, in absolute value, this multinomial is bounded by C(3 - 1).

We thus get that

An+1

=

Fac1,0,0

+ Fac0,1,0

2

D i=1

Imi2,1iD mi

An

+

which gives by the triangular inequality

multinomials

(3.21)

|An+1|  |An|/a + |multinomials|

(3.22)

and all multinomials in the sum are bounded in absolute value by C(3 -1) for some C > 0 (which
can depend on the multinomial, but since there are finitely many of them, we can take the maximum constant for C). The bound by |An|/a comes from |D|  1 and Fac1,0,0 = 1, Fac0,1,0 = 2 - 1, and a  1  2  3  b. Since A0 = 0, we conclude by Lemma 3.6 that |An|  C (3 - 1) for
all a  1  2  3  b.

Control of Bn+1. We apply the same strategy as for An+1: we reason by recurrence over D. Again the case D = 0 is trivial since Bn = 0 in this case. After that the formula are a bit more

30

involved; let us have a glimpse on the differences with the An case. We group a bit the Ti defined in (3.13) and write

Bn+1

=

1 3 E

S1

Wn(3)

-

3 2

Wn(2)

+ S2

Wn(3)

-

3 2

Wn(2)

+ S3 (Wn(3))

× D mi S (Wn(mi )) .

i=1 =1

mi

Again, notice the presence of Wn(t3) in a S1 and a S2 terms, while the S3 terms concerns Wn(3). When one expands everything, and pack together the only terms ­ those of maximum degree­ that contain Bn as a factor, we observe that they can be produced only by S1-terms, and possibly S2-terms if all the mi  2. We then get, for M1, the contribution of these Bn terms

M1 = =

Fac1,0,0

3

D i=1

mi

+

Fac0,1,0

3

D i=1

mi

Imi2,1iD

E

D
(Wn(3)) Wn(mi )
i=1

3

1

D i=1

mi

+

3

2

D i=1

mi

Imi2,1iD

Bn.

The rest of the terms coming from the expansion of Bn+1 involves either Wn(3), or 3, and the possible terms avoiding this contains a S3 terms so that it comes with a prefactor Facx,y,z with z  1). This allows to write some equations similar to (3.21) and (3.22):

Bn+1

=

Fac1,0,0

+ Fac0,1,0

3

D i=1

Imi2,1iD mi

Bn

+

multinomials

from which we conclude for the same reasons as in the An case.

(3.23)

4 Exact computations of the moments of Wn and of W
In this short section, we would like to discuss the fact that the moments of Wn and of W can be computed (when they exist), and a closed formula for them can be derived. However, the formulae we obtain are so complicated that, despite important efforts, we were not able to find a way to present them in the paper: some matrices with large size and with involved coefficients enter into play in the formula expressing the moments E(Wn(2)2Wn(3)2) in terms of the moments of (X(1), X(2), X(3)). The obtained formulas are exact but we were unable to extract from them a simple criterion for the tightness.
We sketch the method allowing to get these close formulae: in principle, they can be used to treat some cases that are not covered by our Theorem 1.7 ((HMom) was derived working with inequalities, and it probably does not cover all the cases for which E((Wn(2))2(Wn(3))2)  C(3 - 1)2). We focus on the 3-dimensional moments, but the same method applies for any higher-dimensional moments.

31

We just sketch the ideas: (I) A non-linear recursion formula: Using (3.8), we write

3

3

Mn+1(k1, k2, k3) = E

Wn+1(j )kj = E

j=1

j=1

j S (Wn(j)) kj .

=1

j

By Lemma 3.7, Mn+1(k1, k2, k3) can thus be written as

3 j=1

-j kj

Fac1,0,0 × Mn(k1, k2, k3) plus a

sum of products of monomials of some Mn(d1, d2, d3) with (d1, d2, d3) < (k1, k2, k3) (the inequality

between vectors means non-strict inequality coordinate by coordinate and strictly smaller on at

least one entry).

We thus get a recursive equation that gives Mn+1(k1, k2, k3) in terms of Mn(k1, k2, k3) and of

lower order moments Mn(d1, d2, d3). This means that, in principle, one can calculate Mn(k1, k2, k3)

recursively, for arbitrary n and (k1, k2, k3). Unfortunately, the recursion formula is not linear in the

lower order moments, which makes this computation more complex.

(II) Conservation of degrees: When one uses Lemma 3.7 to expand Mn+1(k1, k2, k3), the

total degree in each multinomial on the right-hand side is k1 + k2 + k3. Similarly, if one uses

Lemma 3.7 to expand, e.g., Mn+1(d1, d2, d3)×Mn+1(d1, d2, d3) (applying Lemma 3.7 to both terms), then, after expansion, the total degree of each multinomial appearing in the expansion is d1 + d2 +

d3 + d1 + d2 + d3. In other words, the total degree of a multinomial is left unchanged by applying Lemma 3.7 to all its monomials. (This is true in all generality, even when multiplying more than

two monomials.)

(III) Linearising the recursion formula: A consequence of (I) and (II) is that it is possible

to linearise the induction formula of (I). The idea is that, although the formula for Mn+1(d1, d2, d3)

does not belong to the set of linear combinations of the Mn(d1, d2, d3), with (d1, d2, d3)  (d1, d2, d3), it belongs to the set of linear combinations of their products. Furthermore, for a fixed value of

d1 + d2 + d3, there are finitely many of these products. We thus take all these possible products

(i.e. all monomial or product of monomials with total degree d1 + d2 + d3) as a basis for this linear

representation. (In fact, we can just sequentially add the products into the basis while running the

computation to construct the smallest vector space that contains all necessary moments, and that

is, somehow, stable by our rewriting rules.)

Taking into account that E(Wn( )) = 1 simplifies a bit the formulas: some products of monomials of total degree 4 can be simplified. For example, E(Wn(1)3)E(Wn(1)) = E(Wn(1)3).
Applying (I-III) when calculating Mn+1(0, 2, 2) = E((Wn(2))2(Wn(3))2), we can write
this monomial as a linear combination of products of monomials of total degree 4. Because of
the simplifications due to E(Wn( )) = 1, we sometimes see products of smaller total degree. For
example, some of the products appearing when writing Mn+1(0, 2, 2) in term of Mn(0, 2, 2) are, among others Mn(1, 3, 0), Mn(0, 3, 0), Mn(1, 1, 0)Mn(0, 1, 1), and Mn(0, 2, 0)2. We give a name Pn(i) to each of the product of monomials that arises in this sum: for example, set Pn(1) = Mn(1, 2, 1), Pn(2) = Mn(0, 2, 0), Pn(3) = Mn(0, 2, 0)2, etc (we ignore the algebraic relation that can link these

32

products of moments). In the end, one can construct a basis of 41 of these products of monomials that allows to linearise the recursion of Mn(0, 2, 2) as in (III). If one defines Vn as the vector whose coordinates are the Pn(j), we eventually get that
Vn = AVn-1 + U,
for an explicit matrix A (whose coefficients are functions of the Facd1,d2,d3's) and a vector U whose coordinates are the P0(j).
The 41 × 41 matrix A can be diagonalised (in fact, up to relabelling the Pn(i)'s, it is triangular). This provides some explicit formulae for all Pn(i)'s by the standard mean of linear algebra. These formulae are explicit but giant! several pages in standard A4 format are needed to write down their expression: at the end, of course, all moments of interests can be expressed in terms of the moments of (X(1), X(2), X(3)).
The limiting moments P can also be computed: they are solution of
P = AP + U,
and since A is diagonalisable, they can be exactly computed, although again, the formula obtained doing this is huge and hard to manipulate.

5 Remaining proofs

5.1 Proof of Lemma 1.5
Since X is almost surely non-decreasing and integer-valued, one has

X(1)  X(2)  X(3).

(5.1)

· We start by proving ((1.6) and (1.7))  ((1.8) and (1.9)). If (1.7) holds, then

22

E (X(2))2(X(3))2 =

Fac0,y,z(1, 2, 3)  4(3 - 1)2.

y=1 z=1

So that (1.8) holds for C = 4C. Now, to prove that (1.9) holds, it suffices to express E((X(3))(1+ X33)) in terms of the factorial moments appearing in (1.6) and (1.7), which is possible:

E((X(3))X(3)3) = Fac0,0,4 + Fac0,0,1 + Fac3,0,1 + Fac0,3,1 + 7(Fac1,0,1 + Fac0,0,2 + Fac0,1,1)

+12(Fac1,1,1 + Fac0,1,2 + Fac1,0,2) + 6(Fac1,1,2 + Fac0,0,3 + Fac0,2,1 + Fac2,0,1)

+3(Fac0,1,3 + Fac2,1,1 + Fac1,2,1 + Fac2,0,2 + Fac0,2,2 + Fac1,0,3)

(5.2)

(This formula can be checked by hand; it follows from the fact that one can write x33(x3 - x2) on

the basis formed by

3 i=1

ni j=0

(xi

-

j),

and

it

can

be

computed

automatically,

using

a

computer

algebra system).

33

· We now prove that ((1.8) and (1.9))  ((1.6) and (1.7)). First, since Y is integer-valued, we have E(Y 2)  E(Y ) and E(Y 2)  E(Y (Y - 1)), and thus (1.8) implies (1.7) straightforwardly. Moreover, by (5.1),

C (3 - 2)  E((X(3))X33  E (X1j1 X2j2 X3j3 )X(3)

(5.3)

for all j1, j2, j3 such that 0  j1 + j2 + j3  3 (note that when j1 + j2 + j3 = 0, the right-hand side is zero). Each element Facx,y,z(1, 2, 3) with z  1 appearing in (1.6) can be expanded as a sum of terms of the form E[(X1j1X2j2X3j3)X(3)] (we write each X(3) and X(2) except one X(3) as a difference and then use of distributivity to expand). Therefore, (5.3) implies that Facx,y,z(1, 2, 3)  C(3 - 2) for all z  1. It only remains to treat the case z = 0; in this case, we apply (1.9) to (1, 2) instead of (2, 3) (this is allowed because 3 and 2 in (1.9) are just any numbers satisfying a  2  3  b). This gives E((X(2))X(2)3)  C (2 - 1). From here,
one can use the same arguments as in the case z  1, to prove that (1.6) holds when z = 0 and
y  1.

5.2 Proof of Lemma 1.9

Recall that the L2 Wasserstein metric is defined as follows: for any two probability distributions µ and  in M2(1, . . . , 1),

dW (µ, ) = inf

E

(U1, . . . , Um) - (U^1, . . . , U^m)

2 2

1/2 : (U1, . . . , Um)  µ, (U^1, . . . , U^m)  

.

Note that if E[(U1, . . . , Um)] = E[(U^1, . . . , U^m)], then

m

E

(U1, . . . , Um) - (U^1, . . . , U^m)

2 2

=

Var(Uk - U^k).

k=1

Thus, for all µ,   M2(1, . . . , 1), we have

dW

(µ), ()

2



m k=1

Var

 1
 k

X (k )
(Ui(i)
i=1

-

 U^i(i))

(5.4)

for all (U1, . . . , Um)  µ and (U^1, . . . , U^m)  , where ((U1(i), . . . , Um(i)), (U^1(i), . . . , U^m(i)))i1 are sequences of i.i.d. copies of ((U1, . . . , Um), (U^1, . . . , U^m)), independent from the offspring process X.
Using the law of total variance, we get that, for all 1  k  m,

Var

 1
 k

X (k )
(Uk(i)
i=1

-

 U^k(i))

=



EVar



1 k

X (k )
(Uk(i)
i=1

-

U^k(i))

 X (k )

+



VarE



1 k

X (k )
(Uk(i)
i=1

-

U^k(i))

 X (k )

34

=

1 2k E

X (k )Var(Uk

-

U^k)

= Var(Uk - U^k) , k

where we have used again that E[Uk] = E[U^k], and that EX(t) = t for all t > 1. Since the second term in Equation (5.4) can be treated similarly, we get

dW

(µ), ()

2  m Var(Uk - U^k) 

k=1

k

E[

(U1, . . . , Um) - (U^1, . . . , U^m) 1

2 2

]

.

Since this is true for all (U1, . . . , Um)  µ and (U^1, . . . , U^m)  , taking the infimum gives

dW

(µ), ()

1  1 dW

µ,  ,

which concludes the proof since 1 > 1.

5.3 Proof of Proposition 1.10

As already mentioned, the subtrees of the root are themselves independent GW trees, and this leads

us,

notably

to

(1.12),

which

says

that

Wn+1(i)

=

1 i

X (i ) i=1

Wn(i)(i),

jointly

for

1



i



d.

Hence,

Wn(m)

=

m X(k) Wn(j-,k1)(m) ,

k=1 j=1

m

d
= xmWn(m)
m=1

=

d k=1

X (k j=1

)


d

m=k

xm

Wn(j-,k1)(m m

)

 

The (Wn(j,k)(m), 1  m  d) are independent and this is true also, conditionally on the X( ). Hence taking in this last formula the operator E(exp(i ·)), the conclusion follows, as usual, by conditioning first by the values of (X(i), 1  i  d).

For the second statement, by Proposition 1.6, we know that the FDD of Wn converges, so that (n1),d converges simply, as n  + to the Fourier transform  1,d of a d dimensional distribution. Now, to conclude, it suffices to observe that x 1, d  f1,d (x 1, d ) is continuous on B(0, 1)n which contains the image set of the (n1),d .
5.4 Proof of Lemma 3.3
The following proof is original even if we suspect it may exist elsewhere in the literature. First, for  a D[a, b] process, denote by DP() the set of discontinuity points of , that is
t  DP() if (t) = (t-). According to Billingsley [4, p138], P(t  DP()) > 0 is possible for at most countably many t.
As a consequence there exists a deterministic countable dense set S, such that the set of continuity point of  contains S almost surely.

35

Under the hypothesis of the lemma, the statement holds in distribution (by Billingsley [4, Section

12]): the sequence of processes (Tn) converges in distribution in D([a, b]), and the FDD of the limit

process T at its continuity points are determined, on a dense subset of it, by taking the limit of

the FDD of Tn. To prove convergence in probability, we need more.

From the hypothesis, Tn converges to T on a dense subset of [a, b]. We want to prove that

P(d(Tn, T

)



)

-
n+

0

for

any

fixed



>

0,

where

T

is the càdlàg modification of T ,

d(f, g) = inf max{ - Id , f - g  }

where the infimum is taken on the set of strictly increasing and continuous functions such that (0) = 0 and (1) = 1, and Id(y) = y on [0, 1]. Since the sequence (Tn, n  0) is tight in D[a, b],
for each  > 0,

lim lim sup P(w (, Tn)  ) = 0
0+ n
where for a function f : [a, b]  R,

(5.5)

w (, f ) = inf max w([i-1, i), f )
(i) i

(5.6)

and w([c, d), f ) = sup{|f (x) - f (y)|, c  x, y < d}. The infimum in (5.6) is taken on the set of lists (0, · · · , v) where v is an integer, and the list satisfies: 0 = a, v = b, and for each i  {0, · · · , v - 1}, i+1 - i >  (this is called a -sparse sequence).
The intervals [i-1, i) defined by the (i) will be called (i)-intervals. Choose a small  > 0 and then, a  > 0 small enough, and N1 large enough such that for any n  N1

P(w (, Tn)  ) <  and P(w (, T )  ) < .

(5.7)

This is possible by (5.5), and since T is in D[a, b]. We may and will assume that

  .

(5.8)

Now, take (xk, k  0) a sequence in [a, b] such that {xk, k  0} is dense, and such that, the

points of {xk, k  0} are a.s. continuity points of T . Take a K large enough, such that the connected

components of [a, b] \ {x0, · · · , xK} have length < ; this is possible since {xk, k  0} is dense.

Since

(Tn(x1),

·

·

·

,

Tn(xK ))

-(-a-s.)
n

(T

(x1),

·

·

·

,

T

(xK )),

there

exists

N2

such

that

for

any

n



N2,

max |Tn(xi) - T (xi)|  .
1iK

(5.9)

Take any (fixed) n  max{N1, N2}; the event

E,n = {w (, Tn)  }  {w (, T )  }

36

has probability at least 1 - 2 by (5.7). When this event arises, there exists two -sparse sequences (i) and (ti) such that

max w([i-1, i), Tn)  2 and max w([ti-1, ti), T )  2.

i

i

Consider (xi, 0  i  K) the list obtained by sorting increasingly the sequence (xi, 0  i  K). Since consecutive elements of (xi, 0  i  K) are at most at distance , when consecutive elements of the list (i) (resp. (ti)) are at least at distance , between two consecutive xi and xi+1 can lie at most one element of (j), and at most one of (tj).
The main idea now is that Tn (resp. T ) may have big jumps of size >  at some of the elements of (j) (resp. (tj)) but since Tn and T are close at the (xj) and have small variations between the (j) (resp. (tj)), we can find a function close to the identity to synchronize the big jumps. The details are as follows.
We define a suitable function by working successively in each of the intervals [xi, xi+1]. Since the argument is the same in each interval, we choose an index i  {0, · · · , K - 1}, we write (x, x ) instead of (xi, xi+1), and work in [x, x ]. Three cases are possible: (a) there is no element of (j) or of (tj) in [x, x ], (b) there is a single element of (j) and a single element of (tj) in [x, x ], (c) there is a single element of (j) in [x, x ] but none of (tj), or vice-versa.
Case (a): Both x and x are in the same (j)-interval [k, k+1) and in the same (tj) interval [t , t +1) (for some k and ). Hence, w([x, x ], Tn)  w([k, k+1), Tn)  . Similarly, w([x, x ], T )  . Since |Tn(x) - T (x)|   by (5.9), we get supy[x,x ] |Tn(y) - T ((y))| = sup |Tn(y) - T ((y))|  3.
Case (b): Let  denote the element of the list (j) lying in [x, x ], and by t the element of (tj) lying in [x, x ]. Also let p and f denote the elements of (j) preceding and following , and tp and tf denote the element preceding and following t in (tj).
The jump of Tn at  and the one of T at t can be huge compared to , but they are almost equal. Indeed, since |Tn(x)-T (x)|   and |Tn(x )-T (x )|  , before both jumps and after both jumps, the two processes Tn and T are close to each other. More precisely, w([, x ], Tn)  w([, f ), Tn)  , w([t, x ], T )  , w([x, ), Tn)  , and w([x, t), T )  . This implies that

|Tn() - T (t)|  4, |Tn(-) - T (t-)|  4

where left limit is denoted by the "minus exponent", which implies that

|(Tn() - Tn(-)) - (T (t) - T (t-)|  8.

We need to use to synchronize the jump: take ­ [x, t] linearly onto [x, ], and ­ [t, x ] linearly onto [, x]. Globally, since |x - x |  , | (y) - y|  .

as the linear function by part that sends

37

We have on [x, x ],

max |Tn(y) - T ( (y))|  10.
y[x,x ]

(5.10)

Case (c): we take again (y) = y on [x, x ]. By symmetry assume that there is an element  of (j) in [x, x ] but none of (tj). In this case, by a similar argument to Case (b), one can see that the jump of Tn at  must be smaller than 4, and the conclusion follows. We thus have maxy[x,x ] |T (y) - T (x)|  6.
In total, we showed that on [x, x ]
max{| (u) - u|, u  [x, x ]}  max{|Tn(y) - T ( (y))|, y  [x, x ]}   + 10
so that by (5.8), this is smaller than 11. This implies that d(Tn, T )  10 with probability at least 1 - 2.

References
[1] D. Aldous. The continuum random tree. II. An overview. In Stochastic analysis (Durham, 1990), volume 167 of London Mathematical Society Lecture Note Series, pages 23­70. Cambridge Univ. Press, Cambridge, 1991.
[2] S. Asmussen and H. Hering. Branching processes, volume 3 of Progress in Probability and Statistics. Birkhäuser Boston, Inc., Boston, MA, 1983.
[3] K. B. Athreya and P. E. Ney. Branching processes. Dover Publications, Inc., Mineola, NY, 2004. Reprint of the 1972 original [Springer, New York; MR0373040].
[4] P. Billingsley. Convergence of probability measures. Wiley Series in Probability and Statistics: Probability and Statistics. John Wiley & Sons Inc., New York, second edition, 1999.
[5] B. Bollobás and O. Riordan. Random graphs and branching processes. In Handbook of large-scale random networks, pages 15­115. Springer, 2008.
[6] L. Devroye. Branching processes and their applications in the analysis of tree structures and tree algorithms. In Probabilistic methods for algorithmic discrete mathematics, pages 249­314. Springer, 1998.
[7] P. Haccou, P. Jagers, and V. A. Vatutin. Branching processes: variation, growth, and extinction of populations, volume 5 of Cambridge Studies in Adaptive Dynamics. Cambridge University Press, Cambridge; IIASA, Laxenburg, 2007.
[8] P. Jagers. Branching processes with biological applications. Wiley, 1975.
[9] P. Jagers and O. Nerman. The growth and composition of branching populations. Advances in Applied Probability, pages 221­259, 1984.
[10] S. Janson. Functional limit theorems for multitype branching processes and generalized Pólya urns. Stochastic Processes and their Applications, 110(2):177­245, 2004.
38

[11] H. Kesten and B. P. Stigum. A limit theorem for multidimensional galton-watson processes. Annals of Mathematical Statistics, 37(5):1211­1223, 10 1966.
[12] M. Kimmel and D. E. Axelrod. Branching processes in biology, volume 19 of Interdisciplinary Applied Mathematics. Springer-Verlag, New York, 2002.
[13] J.-F. Le Gall and G. Miermont. Scaling limits of random trees and planar maps. In Probability and statistical physics in two and more dimensions, volume 15 of Clay Mathematics Proceedings, pages 155­ 211. American Mathematical Society, Providence, RI, 2012.
[14] Q. Liu. The growth of an entire characteristic fonction and the tail probabilities of the limit of a tree martingale. In B. Chauvin, S. Cohen, and A. Rouault, editors, Trees, pages 51­80, Basel, 1996. Birkhäuser Basel.
[15] Q. Liu. Fixed points of a generalized smoothing transformation and applications to the branching random walk. Advances in Applied Probability, pages 85­112, 1998.
[16] R. Lyons and Y. Peres. Probability on trees and networks, volume 42 of Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, New York, 2016.
[17] J.-F. Marckert and A. Mokkadem. The depth first processes of Galton-Watson trees converge to the same Brownian excursion. The Annals of Probability, 31(3):1655­1678, 2003.
[18] R. Neininger and H. Sulzbach. On a functional contraction method. Annals of Probability, 43(4):1777­ 1822, 2015.
[19] O. Nerman. On the convergence of supercritical general (cmj) branching processes. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete, 57(3):365­395, 1981.
[20] U. Roesler and L. Rüschendorf. The contraction method for recursive algorithms. Algorithmica, 29(1):3­ 33, 2001.
[21] E. Seneta. On recent theorems concerning the supercritical Galton-Watson process. The Annals of Mathematical Statistics, 39(6):2098­2102, 1968.
39

Contents

1 Introduction

1

1.1 A process of GW processes: definition of the model . . . . . . . . . . . . . . . . . . . . . . . . 2

1.2 Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.3 On the identification of the limiting process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

1.3.1 Convergence of the FDD of Wn with Fourier transforms . . . . . . . . . . . . . . . . . 8

1.4 On explicit computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

1.5 Discussion of the related literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2 Three examples

11

2.1 The binary coupling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.2 The geometric coupling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.3 The Poisson coupling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

3 Proof of Theorem 1.7

16

3.1 Convergence in D(I, R+): tightness under moments assumptions . . . . . . . . . . . . . . . . 16

3.2 The limiting process satisfies Condition (ii) of Proposition 3.1 . . . . . . . . . . . . . . . . . . 17

3.3 The sequence (Wn) satisfies Condition (iii) of Proposition 3.1 . . . . . . . . . . . . . . . . . . 18 3.4 Algebraic expansion of E(Wn+1(2)2Wn+1(3)2) . . . . . . . . . . . . . . . . . . . . . . . 20
3.4.1 Expansion of E(Wn+1(2)2Wn+1(2)3): classification of the contributions . . . . . 22

3.4.2 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

3.4.3 Proof of Lemma 3.8(ii) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

3.4.4 Proof of Lemma 3.8(iii) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

4 Exact computations of the moments of Wn and of W

31

5 Remaining proofs

33

5.1 Proof of Lemma 1.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

5.2 Proof of Lemma 1.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

5.3 Proof of Proposition 1.10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

5.4 Proof of Lemma 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

40

