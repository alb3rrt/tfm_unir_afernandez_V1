arXiv:2106.00555v1 [math.AG] 1 Jun 2021

Tensor decomposition for learning Gaussian mixtures from moments
Rima Khouja, Pierre-Alexandre Mattei, Bernard Mourrain
Inria Sophia Antipolis, Universit´e C^ote d'Azur, 2004 route des Lucioles, B.P. 93, 06902 Sophia Antipolis, France
Abstract
In data processing and machine learning, an important challenge is to recover and exploit models that can represent accurately the data. We consider the problem of recovering Gaussian mixture models from datasets. We investigate symmetric tensor decomposition methods for tackling this problem, where the tensor is built from empirical moments of the data distribution. We consider identifiable tensors, which have a unique decomposition, showing that moment tensors built from spherical Gaussian mixtures have this property. We prove that symmetric tensors with interpolation degree strictly less than half their order are identifiable and we present an algorithm, based on simple linear algebra operations, to compute their decomposition. Illustrative experimentations show the impact of the tensor decomposition method for recovering Gaussian mixtures, in comparison with other state-of-the-art approaches.
1. Introduction
With the relatively recent evolutions of information systems over the last decades, many observations, measurements, data are nowadays available on a variety of subjects. However, too much information can kill the information and one of the main challenges remains to analyse and to model these data, in order to recover and exploit hidden structures.
To tackle this challenge, popular Machine Learning technologies have been developed and used successfully in several application domains (e.g. in image recognition [HZRS16]). These techniques can be grouped in two main classes: Supervised machine learning techniques are approximating a model by optimising the parameters of an enough general model (e.g. a Convolution Neural Network) from training data. Unsupervised machine learning techniques are deducing the parameters characterising a model directly from the given data, using an apriori knowledge on the model. The supervised approach requires annotated data, with a training step that can introduce some bias in the learned model. The unsupervised approach can be applied directly on a given data set avoiding the costly step of annotating data, but the quality of the output strongly depends on the type of models to be recovered.
We consider the latter approach and show how methods from effective algebraic geometry help finding hidden structure in data that can be modelled by mixtures of Gaussian distributions. The algebraic-geometric tool that we consider is tensor decomposition. It consists in decomposing a tensor into a minimal sum of rank-1 tensors. This decomposition generalises the rank decomposition of a matrix, with specific and interesting features. Contrarily to matrix rank decomposition, the decomposition of a tensor is usually unique (up to permutations) when the rank of the tensor, that is the minimal number of rank-1 terms in a decomposition, is small compared to the dimension of the space(s) associated to the tensor. Such a tensor is
June 2, 2021

called identifiable. This property is of particular importance when the decomposition is used to recover the parameters of a model. It guaranties the validity of the recovering process and its convergence when the number of data increases.
It has been shown in [COV16] that for symmetric tensors, if the rank of the tensor is strictly less than the rank rg of a generic tensor of the same size, then the tensor is generically identifiable, except in three cases. We show in Theorem 3.6 a more specific result: for a symmetric tensor T having a decomposition with r points, if the Hankel matrix associated to T in a degree strictly bigger than the degree of interpolation of the r points is of rank r, then the tensor is identifiable. We show in Proposition 3.3, that under some assumption on the spherical gaussian mixtures, a tensor of moments of order 3 of the distribution is identifiable and its decomposition allows to recover the parameters of the Gaussian mixture.
Several types of method have been developed to tackle the difficult problem of tensor decomposition. Direct methods based on simultaneous diagonalisation of matrices built from slices of tensors have been investigated for 3rd order multilinear tensors, e.g. in [Har70, SK90, LRA93, DL14] or for multilinear tensors of rank smaller than the lowest dimension in [DL06, LA14]. In his proof on lower bounds of tensor ranks, Strassen showed in [Str83, Theorem 4.1] that a 3rd order multilinear tensor is of rank r if it can be embedded into a tensor with slices of rank r matrices, which are simultaneously diagonalised.
For symmetric tensor decomposition, a method based on flat extension of Hankel matrices or commutation of multiplication operators has been proposed in [BCMT10] and extended to multi-symmetric tensors in [BBCM13]. This approach is closely related to the simultaneous diagonalisation of tensor slices, but follows a more algebraic perspective. Eigenvectors of symmetric tensors have been used to compute their decompositions in [OO13]. In [HKM18], Singular Value Decomposition and eigenvector computation are used to decompose a symmetric tensor, when its rank is smaller than the smallest size of its Hankel matrix in degree less than half the order of the tensor. In Section 3, we describe a new algorithm, involving Singular Value Decomposition and simultaneous diagonalisation, to compute the decomposition of an identifiable tensor, which interpolation degree is smaller that half the order of the tensor.
Numerical methods such as homotopy continuation have been applied to tensor decomposition in [HOOS19, BDHM17]. Distance minimisation methods to compute low rank approximations of tensors have also been investigated. Alternating Least Squares (ALS) methods, updating alternately the different factors of the tensor decomposition, is a popular approach (see e.g. [CC70, CHLZ12, Har70, KB09a]), but suffers from a slow convergence [EHK15, Usc12]. Other iterative methods such as quasi-Newton methods have been considered to improve the convergence speed. See e.g. [HH82, Paa99, PTC13, SL10, SVBDL13, TB06, BV18] for multilinear tensors. A Riemannian Newton iteration for symmetric tensors is presented in [KKM20]. In [KP09], a method for decomposing real even-order symmetric tensors, called Subspace Power Method (SPM), and similar to the power method for matrix eigenvector computation, is proposed. In these methods, the choice of the initial decomposition is crucial. In the applications of these algorithms, the initial point is often chosen at random, yielding approximate decompositions which can hardly be controlled. Tensor decomposition methods have numerous applications [KB09b]. Some of them were exploited more recently in Machine Learning. In [HK13], symmetric tensor decompositions for moment tensors are studied for spherical Gaussian mixtures. Moment methods have been further investigated for Latent Dirichlet Allocation models, topic or multiview models in [AGH+14, JGKA19]. In [RGL17], a tensor decomposition technique based on Alternate Least Squares (ALS) is used to initialise the Expectation
2

Maximisation (EM) algorithm, for a mixture of discrete distributions (which are not Gaussian distributions). An overview of tensor decomposition methods in Machine Learning can be found in [RSG17].
After reviewing Gaussian mixtures and moment methods in Section 2, we present in Section 3 an algebraic symmetric tensor decomposition method for identifiable tensors. In Section 4, we apply this algorithm for recovering Gaussian mixtures and show its impact on providing good initialisation point in the EM algorithm, in comparison with other state-of-the-art approaches.

2. Gaussian mixtures and high order moments In this section, we review Gaussian mixture models and their applications to clustering.

2.1. Gaussian mixtures Suppose that we wish to deal with some Euclidean data x  Rm, coming from a population
composed of r homogeneous sub-populations (often called clusters). A reasonable assumption is then that each sub-population can be modelled using a simple probability distribution (e.g. Gaussian). This idea is at the heart of the notion of mixture distribution. The prime example of mixture is the Gaussian mixture, whose probability density over Rm is defined as

r

p(x) = jN (x|µj, j),

(1)

j=1

where N (·|µ, ) denotes the Gaussian density with mean µ  Rm and definite positive covariance matrices   Sm++. The mixture is parametrised by a typically unknown  = (1, ..., r, µ1, ..., µr, 1, ..., r), composed of

·  = (1, ..., r), that belong to the r-simplex and correspond to the cluster proportions,

· µj and j, that correspond respectively to the mean and covariance of each cluster j  {1, ..., r}.

Gaussian mixtures are ubiquitous objects in statistics and machine learning, and owe their popularity to many reasons. Let us briefly mention a few of these.

Density estimation. If r is allowed to be sufficiently large, it is possible to approximate any probability density using a Gaussian mixture (see e.g. [NCNM20]). This motivates the use of Gaussian mixtures as powerful density estimators that can be subsequently used for downstream tasks such as missing data imputation [DZGL07], supervised classification [HT96], or image classification [SPMV13] and denoising [HBD18].

Clustering. Perhaps the most common use of Gaussian mixtures is clustering, also called unsupervised classification. The task of clustering consists in uncovering homogeneous groups among the data at hand. Within the context of Gaussian mixtures, each group generally corresponds to a single Gaussian distribution, as in Equation (1). If the parameters of a mixture are known, then each point may be clustered using the posterior probabilities obtained via Bayes's rule:

x



Rm, k



{1, ..., r},

Pr(x

belongs

to

cluster

j)

=

jN (x|µj, j) . p(x)

(2)

Detailed reviews on mixture models and their applications, notably to clustering, can be found in [FR02, BCMR19, MLR19].

3

2.2. Learning mixture models
The main statistical question pertaining mixture models is to estimate the parameters  = (1, ..., K, µ1, ..., µK, 1, ..., K) based on a data set x1, ..., xn. Typically, x1, ..., xn are assumed to be independent and identically distributed random variables with common density pdata. The problem of statistical estimation is then to find some  such that p  pdata. There are many approaches to this question, the most famous one being the maximum likelihood method. Maximum likelihood is based on the idea that maximising the log-likelihood function

n

() = log p(xi),

(3)

i=1

will lead to appropriate values of . One heuristic reason of the good behaviour of maximum likelihood is that () can be seen as a measure of how likely the observed data is, according to the mixture model p. This means that the maximum likelihood estimate will be the value of  that renders the observed data the likeliest. Another interesting interpretation of maximum likelihood in information-theoretic: when n - , maximising the log-likelihood is equivalent to minimising the Kullback-Leibler divergence (an information-theoretic measure of distance between probability distributions) between p and pdata, thus giving a precise sense to the statement p  pdata (see e.g. [Bis06, Section 1.6.1]). For more details on the properties of maximum likelihood, see e.g. [VdV98, Section 5.5].
In the specific case of a mixture model, performing maximum-likelihood is however complex for several reasons. Firstly, as shown for instance by [LC90], finding a global maximum is actually often ill-posed in the sense that some problematic values of  will lead to () =  while being very poor models of the data. While focusing on local rather global maxima will fix this first issue in a sense, iterative optimisation algorithms are likely to pursue these unfortunate global maxima. Because of the peculiarities of mixture likelihoods, the most popular algorithm for maximising () is the expectation maximisation (EM, [DLR77]) algorithm, an iterative algorithm specialised for dealing with log-likelihoods of latent variable models. The EM algorithm is usually preferred to more generic gradient-based optimisation algorithms [XJ96]. In a nutshell, at each iteration, the EM algorithm clusters the data using Equation (2), and then computes the means and covariances of each cluster. This iterative scheme is related to another popular clustering algorithm known as k-means (the close relationship between the two algorithms is detailed in [Bis06, Section 9]). A key issue when using the EM algorithm for a Gaussian mixture is the choice of initialisation. Indeed, a poor choice may lead to degenerate solutions, extremely slow convergence, or poor local optima (see [BC15] and references therein). We will see in this paper that good initial points can be obtained by using another estimation method called the method of moments (as was previously noted by [RGL17] in a context of mixtures of multivariate Bernoulli distributions).
The method of moments is a general alternative to maximum likelihood. The idea is to choose several functions g1 : Rm - Rq1, ..., gd : Rd - Rqd called moments, and to find  by attempting to solve the system of equations

 

Expdata [g1(x)]

=

Exp [g1(x)]

...

(4)

Expdata [gd(x)] = Exp [gd(x)].

4

Of course, since pdata is unknown, solving (4) is not feasible. However, one may replace the expected moments by empirical versions, and solve instead

1 n

n i=1

g1(xi)

=

Exp [g1(x)]

...

(5)

1
n

n i=1

gd(xi)

=

Exp [gd(x)].

A very simple example of this, in the univariate m = 1 case, when g1(x) = x, and g2(x) = x2. Then, solving (4) will ensure that the distributions of the model p and the data pdata have the same mean and variance. However, many very different distributions have identical mean and variance! A natural refinement of the previous idea is to consider also higher-order moments g3(x) = x3, g4(x) = x4, .... This will considerably improve the estimates found using the method of moments. This approach was pioneered by [Pea94] for learning univariate Gaussian mixtures. In the more general multivariate case m > 1, following [HK13], the moments chosen can be tensor products, as we detail in the next section in case of a Gaussian mixture with spherical covariances.

3. Learning structure from tensor decomposition

In this section, we describe the moment tensors revealing the structure of spherical Gaussian mixtures and how it can be decomposed using standard linear algebra operations.
Let X = (X1, . . . , Xm) be a set of variables. The ring of polynomials in X with coefficients in C is denoted C[X]. The space of homogeneous polynomials of degree d  N is denoted C[X]d. A symmetric tensor T of order d (with real coefficients) can be represented by an homogeneous polynomial of degree d in the variables X of the form

T (X) =

T

d 

X

||=d

where  = (1, . . . , n)  Nm, || = 1 + · · · + m = d, T  R, X11 · · · Xmm .

d 

=

, d!
1!···m!

X

=

A decomposition of T as a sum of dth power of linear forms is of the form

r

T (X) = i(i · X)d

(6)

i=1

where i = (i,1, . . . , i,m)  Cm and (i · X) =

m j=1

i,j

Xj

.

When

r

is

the

minimal

number

of

terms in such a decomposition, it is called the rank of T and the decomposition is called a rank

decomposition (or a Waring decomposition) of T (X).

We say that the decomposition is unique if the lines spanned by 1, . . . , r form a unique set of lines with no repetition. In this case, the decomposition of T is unique after normalisation of

the vectors i up to permutation (and sign change when d is even). A tensor T with a unique

decomposition is called an identifiable tensor. Then the Waring decompositions of T are of the

form T (X) =

r i=1

i-i d(i

i

·

X)d

for

i

=

0,

i



[r].

Given a random variable x  Rm, its moments are T = E[x1 1 · · · xmm] for  = (1, . . . , m) 

Nm. The symmetric tensor of all moments of order d of x is

E[(x · X)d] =

E[x1 1 · · · xmm ]

d 

X.

||=d

5

3.1. The structure of the moment tensor We aim at recovering the hidden structure a random variable, from the decomposition of its
dth order moment tensor. This is possible in some circumstances, that we detail hereafter.
Assumption 3.1. The random variable x  Rm is a mixture of spherical Gaussians of probability density (1) with parameters  = (1, ..., r, µ1, ..., µr, 12Im, , ..., r2Im) such that r < m.
Theorem 3.2 ([HK13]). Under the previous assumption, let
· ~ be the smallest eigenvalue of E[(x - E[x])  (x - E[x])] and v a corresponding unit eigenvector,

· M1(X) = E[(x · X)(v · (x - E[x]))2],

· M2(X) = E[(x · X)2] - ~2 X 2,

· M3(X) = E[(x · X)3] - 3 X 2M1(X).

Then ~2 =

r i=1

i i2

and

r

r

r

M1(X) = i i2 (µi · X), M2(X) = i (µi · X)2, M3(X) = i (µi · X)3. (7)

i=1

i=1

i=1

To analyse the properties of the decomposition (7), we introduce the apolar product on

tensors: For two homogeneous polynomials p(X) =

||=d

d 

pX

and q(X)

=

||=d

d 

qX

of degree d, in C[X]d, their apolar product is

d

p, q d :=

 p¯q.

||=d

The apolar norm of p is ||p||d = p, p d =

||=d

d 

p¯p.

The apolar product is invari-

ant by a linear change of variables of the unitary group Um: u  Um, p(u X), q(u X) d =

p(X), q(X) d. It also satisfies the following properties. For v  Cm, v(X)d = (v · X)d = (v1X1 + · · · +

vmXm)d, p  C[X]d, q  C[X]d-1, we have :

· (v · X)d, p d = p(v¯),

·

p, Xiq

d=

1 d

Xip, q

d-1.

For an homogeneous polynomial T of degree d  N (or equivalently a symmetric tensor of order d), we define the Hankel operator of T in degree k  d as the map

HTk,d-k : p  C[X]d-k  [ T, X p d]||=k  Csk

where sk =

m+k-1 k

= dim C[X]k is the number of monomials of degree k in X. The matrix of

HTk,d-k in the basis (X)||=d-k is

HTk,d-k = ( T , X+ d)||=k,||=d-k.

From

the

properties

of

the

apolar

product,

we

see

that

HT1,d-1

:

p



1 d

[

XiT, p

d-1]1im.

For

  Cm and k  N, let (k) = ()||=k. We also check that if T = ( · X)d with v  Cm, then

H(k,·dX-)kd = ¯(k)  ¯(d-k) is of rank 1 and its image is spanned by the vector ¯(k).

6

Proposition 3.3. Assume that r < m, wi > 0 for i  [r] and µ1, . . . , µr  Rm are linearly independent. The symmetric tensor M3(X) is identifiable, of rank r and has a unique Waring decomposition satisfying (7).

Proof. Assume that M3(X) has a decomposition of the form (7). Since the vector µ1, . . . , µr

are linearly independent, by a linear change of coordinates in Glm, we can further assume that

µ1 = e1, . . . , µr = er are the first r vectors of the canonical basis of Rm. In this coordinate

system, M3(X) =

r i=1

Xi3

and

the

matrix

HM1,23

in

a

convenient

basis

has

a

r

×r

identity

block

and

zero

elsewhere.

Thus

HM1,23

is

of

rank

r.

Its

kernel

of

dimension

1 2

m

(m

+

1)

-

r

is

spanned

by the polynomials XiXj with (i, j) = (k, k) for k  [r]. The kernel of HM1,23 is thus the space of

homogeneous polynomials of degree 2, vanishing at e1, . . . , er  Rn.

If M3(X) can be decomposed as M3(X) =

r i=1

i

(µi

· X)3

with

i



C,

µi



Cm

and

r

< r,

then HM1,23, as a sum of r

<

r

matrices

i

H 1,2 (µi·X)3

of

rank

1,

would

be

of

rank

smaller

than

r

< r,

which is a contradiction. Thus a minimal decomposition of M3(X) is of length r and r is the

rank of M3(X).

Let us show that the decomposition (7) of M3(X) is unique up to a scaling of the vector µi, i.e.

that M3(X) is identifiable. For any Waring decomposition M3(X) =

r i=1

i

(µi·X)3,

the

vectors

µ1, . . . , µr are linear independent, since µi spans imH(1µ,2i·X)3 and HM1,23 =

r i=1

iH(1µ,2i·X)3

is

of

rank r. As µ1, . . . , µr can be transformed into e1, . . . , er by a linear change of variables, ker HM1,23

is also the vector space of homogeneous polynomials of degree 2, vanishing at µ1, . . . , µr  Cm.

Therefore, the set of {µ1, . . . , µr} coincides, up to a scaling, with the set of points {µ1, . . . , µr}

of another Waring decomposition of M3(X) =

r i=1

i

(µi

·

X)3.

This shows that M3(X) is

identifiable.

Therefore, a Waring decomposition of M3(X) is of the form M3(X) =

r i=1

~i

(µ~i

·

X)3

with

~i = -3i, µ~i = iµi and i = 0 for i  [r]. As µ~1, . . . , µ~r are linearly independent, the

homogeneous polynomials (µ~1 · X)2, . . . , (µ~r · X)2 are also linearly independent in C[X]2 (by a

linear change of variables, they are equivalent to X12, . . . , Xr2). Consequently, the relation

r

r

M2(X) = i(µi · X)2 = i~i(µ~i · X)2

i=1

i=1

defines uniquely 1, . . . , r, and M3(X) has a unique Waring decomposition, which satisfies the relations (7).

Under Assumption 3.1, the hidden structure of the random variable x can thus be recovered
using Algorithm 1. This yields the parameters i  R+, µi  Rm, i  R+ for i  [r] of the Gaussian mixture x. In the experimentation, the moments involved in the tensors Mi will be approximated by
empirical moments and we will compute an approximate decomposition of the empirical moment tensor M^3(X).

3.2. Decomposition of identifiable tensors

We describe now an important step of the approach, which is computing a Waring decom-

position of a tensor. In this section, we consider a tensor T  C[X]d of order d  N with a

Waring decomposition of the form T =

r i=1

i

(i

·

X)d

with

i



C, i



Cm,

that

we

recover

by linear algebra techniques, under some hypotheses.

7

Algorithm 1 Recovering the hidden structure of a Gaussian mixture Input: The moment tensors M1(X), M2(X), M3(X).

· Compute a Waring decomposition of M3(X) to get ~i  R, µ~i  Rm, i  [r] such that

M3(X) =

r i=1

~i

(µ~i

·

X)3.

·

Solve the system

r i=1

~i

(µ~i

·

X)2i

µi = -i 1µ~i  Rm such that M3(X) =

= M2(X) to get i  R and

r i=1

i

(µi

·

X)3

and

M2(X)

=

i = 3i ~i  R+,

r i=1

i

(µi

·

X)2.

· Solve the system

r i=1

i(µi

·

X)i2

=

M1(X)

to

get

i



R+.

Output: i  R+, µi  Rn, i  R+ for i  [r].

Definition 3.4. The interpolation degree () of  = {1, . . . , r}  Cm is the smallest degree k of a family of homogenous interpolation polynomials u1, . . . , ur  C[X]k at the points  (ui(j) = i,j for i, j  [r]).

For any d  (), there exists a family (u~i)i[r] of interpolation polynomials of degree d,

obtained

from

an

interpolation

family

(ui)i[r]

in

degree

()

as

u~i

=

u (·X)d-()
(·i)d-() i

for

a

generic

  Cm such that  · i = 0 for i  [r].

Notice that if the points  = {1, . . . , r} are linearly independent (and therefore r  n),

then () = 1 since a family of linear forms interpolating  can be constructed.

If k  (), then the evaluation map e(k) : p  C[X]k  (p(1), . . . , p(r))  Cr is surjective. Its kernel is the space of homogeneous polynomials of degree k vanishing at . Any supple-

mentary space admits a basis u1, . . . , ur, which is an interpolating family for  in degree k. A property of the interpolation degree is the following:

Lemma 3.5. For k > (), the common roots of ker e(k) is the union ri=1C i of lines spanned by 1, . . . , r  Cm.

Proof. As () + 1 is the Castelnuovo-Mumford regularity of the vanishing ideal I() = {p 
C[X] | p homogeneous, p() = 0 for   } [Eis05][Ch.4], it is generated in degree k > () and the common roots of ker e(k) = I()k is ri=1C i.

Hereafter, we show that tensors T such that rank HTk,d-k = r for k > () + 1 are identifiable and we describe a numerically robust algorithm to compute their Waring decomposition.
Let U = (U,j)||=k,j[r]  Csk×r be such that im U = im HTk,d-k and Ui = (Uei+,j)||=k-1,j[r] be the submatrices of U with the rows indexed by the monomials divisible by Xi for i  [n].

Theorem 3.6. Let T  C[X]d with a decomposition T =

r i=1

i

(i

·

X)d

with

i



C

and

i = (i,1, . . . , i,n)  Cm such that rank HTk,d-k = r for some k  [(1, . . . , r) + 1, d]. Then T is

identifiable of rank r and there exist invertible matrices E  Csk×sk, F  Cr×r such that

Et Ui F =

i 0

(8)

with i = diag(¯1,i, . . . , ¯r,i) for i  [n]. For any pair (E, F ), which diagonalises simultaneously

[U1, . . . , Um] as in (8), there exist unique 1, . . . , r  C such that T = ¯i = ((1)i,i, . . . , (m)i,i).

r i=1

i

(i

·

X)d

with

8

Proof. From the decomposition of T , we have for k  d that

r

HTk,d-k =

i ¯i(k)  ¯i(d-k)

i=1

is a linear combination of r Hankel matrices ¯i(k)  ¯i(d-k) of rank 1. If T is of rank r < r, then using its decomposition of rank r , HTk,d-k would be of rank  r < r, which is a contradiction.
This shows that T is of rank r. As rank HTk,d-k = r, we deduce that the image of HTk,d-k is spanned by ¯1(k), . . . , ¯r(k) and
there exists an invertible matrix F  Cr×r such that

U F = [¯1(k), . . . , ¯r(k)]

For any polynomial p  C[X]k, which coefficient vector in the monomial basis (X)||=k is denoted [p], we have [p]tU F = [p(¯1), . . . , p(¯r)]t. This shows that U  = {p  C[X] | [p]tU = 0}
is ker e(¯k). By Lemma 3.5 since k  (¯), the common roots of the homogeneous polynomials in ker e(¯k) are the scalar multiples of ¯. Consequently, the set of lines spanned by the vectors  of a Waring decomposition of T is uniquely determined as the conjugate of the zero locus of

U   C[X]k and T is identifiable. For any p  C[X]k-1 represented by its coefficient vector [p] in the monomial basis (X)||=k-1,

we have

[p]tUiF = [xip]tU F = [¯1,i p(¯1), . . . , ¯r,i p(¯r)]t.

(9)

Let E be the coefficient matrix of a basis u1, . . . , ur, vr+1, . . . , vsk-1 of C[X]k-1, such that
u1, . . . , ur is an interpolating family for ¯ = {¯1, . . . , ¯r} and vr+1, . . . , vsk-1 is a basis of ker e(¯k-1). The matrix E is invertible by construction, and we deduce from (9) that

EtUiF =

diag(¯1,i, . . . , ¯r,i) 0

.

Let us show conversely that for any pair of matrices (E , F ), which diagonalises simultane-

ously [U1, . . . , Um] as in (8) with i = diag(¯1,i, . . . , ¯r,i), there exist unique 1, . . . , r  C such

that T =

r i=1

i

(i

·

X)d

.

Let u1, . . . , ur, vr+1, . . . , vsk-1  C[X] be the polynomials corresponding to the columns of E . Then for a generic  = (1, . . . , r)  Cm, we have

m

m

diag(( · ¯1), . . . , ( · ¯r)) =

i[u1, . . . , ur]tUiF = i[u1, . . . , ur]tUiF (F -1F )

i=1

i=1

= [( · ¯j) ui(¯j)]i,j[r]F -1F = diag(( · ¯1), . . . , ( · ¯r)) [ui(¯j)]i,j[r]F -1F .

As   Cm is generic and  · ¯i = 0 for i  [r], we deduce that  = [ui(¯j)]i,j[r]F -1F is a

diagonal and invertible matrix and that i = ¯ i,ii with i,i = 0.

Then we have (i · X)d weights 1, . . . , r and the

= ¯ di,i (i · X)d and T = decomposition are unique,

r i=1

i

(i

since for

· X)d with i = ¯ -i,idi. The d > (¯) there exist interpo-

lation polynomials u1, . . . , ur in degree d such that (i · X)d, uj d = uj(¯i) = i,j so that the

polynomials (i · X)d = ¯ di,i (i · X)d are linearly independent. This concludes the proof of the

theorem.

This leads to Algorithm 2 to compute a Waring decomposition of an identifiable tensor T .

9

Algorithm 2 Decomposition of an identifiable tensor Input: T  C[X]d, which admits a decomposition with r points  = {1, . . . , r} and k > ().

· Compute the Singular Value Decomposition of HTk,d-k = U S V t; · Deduce the rank r of HTk,d-k, take the first r columns of U and build the submatrices Ui
with rows indexed by the monomials (XiX)||=k-1 for i  [n];

· Compute a simultaneous diagonalisation of the pencil [U1 . . . , Um] as EtUiF =

diag(¯1,i, . . . , ¯r,i) 0

and deduce the points i = (i,1, . . . , i,n)  Cm for i  [r];

· Compute the weights 1, . . . , r by solving the linear system T =

r i=1

i

(i

·

X)d;

Output: i  C, i  Cm s.t. T =

r i=1

i

(i

·

X)d.

4. Numerical experimentations
The model used in this section is the Gaussian Mixture Model (GMM) with differing spherical covariance matrices. Recall that if x = (x1, . . . , xn) is a sample of n independent observations from r multivariate Gaussian mixture with differing spherical covariance matrices of dimension m, and h = (h1, h2, . . . , hn) is the latent variable that determine the component from which the observation originates, then:
xi | (hi = k)  Nm(µk, k2Im) where,
r
Pr(hi = k) = k, for k  [r], such that k = 1.
k=1
The aim of statistical inference is to find the unknown parametrs µk, k2 and wk, for k  [r] from the data x. This can be done by finding the maximum likelihood estimation (MLE) i.e. finding the optimal maximum of the likelihood function associated to this model. The expectation maximisation algorithm (EM) [DLR77], usually used for finding MLEs, is an iterative algorithm in which the initialisation i.e. the initial estimation of the latent parameters is crucial, since various initialisations can lead to different local maxima of the likelihood function, consequently, yielding different clustering partition. Thus, in this section we compare the clustering results obtained by different initialisation of the EM algorithm against the initialisation by the method of moments through examples of simulated (subsection 4.1) and real (subsection 4.2) datasets. We fix a maximum of 100 iterations of the EM algorithm. The different initialisation considered in this section are the following:
· The k-means method [Mac67] according to the following strategy: The best partition obtained out of 50 runs of the k-means algorithm.
· The method of moments, where Algorithm 2 is applied to the empirical moment tensor corresponding to M3(X) (see Theorem 3.2), with less than 5 Riemannian Newton iterations [KKM20] to reduce the distance between the empirical moment tensor and its decomposition.
10

· The Model-based hierarchical agglomerative clustering algorithm (MBHC) [VD00, Fra98].
· The emEM strategy [BCG03] as in [LIL+15] which makes 5 iterations for each of 50 short runs of EM, and follows the one which maximises the log-likelihood function by a long run of EM.

The k-means, MBHC and emEM are common strategies for initialising the EM algorithm for

GMMs. The comparison among the different EM initialisation strategies is based on three

measures: The Bayesian Information Criterion (BIC) [Sch78, FR98], the Adjusted Rand Index

(ARI) [HA85], and the error rate (errorRate). The BIC is a penalized-likelihood criterion given

by the following formula

BIC = -2 (^) + log(n),

where is the log-likelihood function , ^ is the MLE which maximises the log-likelihood function and  is the number of the estimated parameters. This criterion measures the quality of the model such that for comparing models the one with the largest BIC value among the other models is the most fitted to the studied dataset. The ARI criterion measures the similarity between the estimated clustering obtained by the applied model and the exact true clustering. Its value is bounded between 0 and 1. The more this measure is close to 1 the more the estimated clustering is accurate. The error rate measure can be viewed as an alternative of the ARI. In fact this criterion measures the minimum error between the predicted clustering and the true clustering, and thus low error rate means high agreement between the estimated and the true clustering. The former criteria as well as the EM algorithm are used from the tools of the package mclust [SFMR16] in R programming.

4.1. Simulation
We performed 100 simulations from each of the two models described in examples 4.1 and 4.2. We counted the instances where each of the considered initialising strategies for the EM could find throughout the 100 simulated data and among the other initialisation methods the largest BIC, the highest ARI, ARI 0.99 (as in this case the clustering obtained is the most accurate) and the lowest errorRate. The values of the BIC, ARI, errorRate and consumed time of the different considered initialisation strategies for one dataset sampled according to the model of Example 4.1 (resp. 4.2) are presented in Table 1 (resp. 3), and Figure 1 (resp. 2) shows a two-dimensional visualisation of the observations according to the first four features, the observations in the upper panels are labeled according to the actual clustering, while they are labeled in the lower panels according to the clustering obtained by the EM algorithm initialised by the method of moments. In order to have an estimation about the numerical stability of the obtained results, we repeat the same numerical experiment for each example 20 times and we compute the means (Table 2, 4) and the variances (values in parentheses in Table 2, 4) of the 20 percentages obtained of each of the BIC, ARI, ARI 0.99 and errorRate values for the different initialising strategies. As we mentionned before the initialising strategies considered in this comparision against the method of moments are common and have, in general, good numerical behavior. Nevertheless, we cannot expect all the initialisation strategies that exist for the EM algorithm to work well in all the cases [BCG03, MM10]. Herein, these two examples are chosen in such a way to present some cases where the common initialising strategies k-means, MBHC and emEM have some difficulties to provide a good initialisation to the EM algorithm for the GMMs with

11

differing spherical covariance matrices, or in other words where the initialisation by the method of moments outperforms the other considered initialisations. For instance, we put in each of these two examples one cluster of small size (the blue cluster in Figure 1, the red cluster in Figure 2), we want to make the clusters overlap, since these initialising strategies could misscluster the dataset if the clusters are intersecting. We notice that this choice of the mean vectors and the different variances in each of the two examples yields a dataset with the expected clustering characteristic.
Example 4.1. In the first simulation example, a multivariate dataset (m=6) of n=1000 observations generated with r=4 clusters according to the following parameters:
· The probability vector:  = (0.2782, 0.0139, 0.3324, 0.3756)T .
· The mean vectors: µ1 = (-5.0, -9.0, 8.0, 8.0, 2.0, 5.0)T , µ2 = (-7.0, 6.0, -1.0, 6.0, -8.0, -10.0)T , µ3 = (-4.0, -10.0, -5.0, 1.0, 5.0, 4.0)T , µ4 = (-6.0, 6.0, 5.0, 4.0, -1.0, -1.0)T .
· The variances: 12 = 1.5, 22 = 2.5, 32 = 5.0, 42 = 15.0.

Table 1: Numerical results of one data set of Example 4.1

Method

BIC

ARI errorRate time(s)

em km -29590.48 0.8281 0.168 0.045

em mom -29492.11 1.0

0.0

0.547

em mbhc -29594.97 0.8574 0.099 0.287

em emEM -29593.18 0.8366 0.132 0.171

Method em km em mom em mbhc em emEM

Table 2: Estimation of the stability of Example 4.1 results

BIC

ARI

ARI  0.99

errorRate

38.35% (37.82) 47.6% (21.41) 48.85% (21.61) 47.6% (21.2)

74.8% (41.01) 88.75% (15.36) 83.4% (18.36) 88.60% (14.46)

10.75% (12.41) 15.9% (17.57) 15.55% (22.99) 15.9% (19.46)

7.3% (8.43) 14.5% (8.05) 12.6% (17.83) 14.95% (7.52)

Example 4.2. In the second simulation example, a multivariate dataset (m=5) of n=1000 observations generated with r=3 clusters according to the following parameters:
· The probability vector:  = (0.0930, 0.2151, 0.6918)T . · The mean vectors: µ1 = (7.0, -4.0, -4.0, -6.0, -4.0)T , µ2 = (2.0, -4.0, -6.0, -10.0, -3.0)T ,
µ3 = (4.0, -4.0, -5.0, 6.0, 1.0)T . · The variances: 12 = 5.0, 22 = 10.0, 32 = 15.0.

12

Figure 1: Scatterplot matrix for the sampled dataset of Example 4.1 projected onto the first four variables (features): upper panels show scatterplots for pairs of variables in the original clustering; lower panels show the clustering obtained by applying the EM algorithm initialised by the method of moments.
The Table 2, 4 show that in Example 4.1, 4.2 the best results among the considered initialising strategies are for the method of moments. In fact, in the former two tables we see that the method of moments found throughout the 100 simulated datasets, in average (by runing the numerical experiment 20 times), the largest BIC, highest ARI, ARI 0.99 and lowest errorRate among the other initialisation strategies in more instances than all the other considered initialisation method, implying in this context marked outperformance for the moments initialisation method. Note that the consumed time (see. Table 1, 3) tends to be higher in the method of moments than in the other initialisation strategies. This is expected since stochastic approaches (to which the methods k-means, MBHC and emEM belong) outperform the deterministic approaches (as the method of moments) in this term.
13

Table 3: Numerical results of one data set of Example 4.2

Method

BIC

ARI errorRate time(s)

em km -28360.30 0.4352 0.309 0.051

em mom -28246.02 0.9498 0.03 0.504

em mbhc -28358.67 0.3197 0.384 0.292

em emEM -28360.42 0.4408 0.296 0.141

Table 4: Estimation of the stability of Example 4.2 results

Method

BIC

ARI

ARI  0.99 errorRate

em km 0.45% (0.576) 0.05% (0.05) 0.0% (0.0) 0.1%(0.095)

em mom 50.0% (18.63) 92.35% (9.82) 0.0% (0.0) 92.1% (7.46)

em mbhc 49.35% (19.82) 2.45% (3.63) 0.0% (0.0) 2.45% (2.58)

em emEM 0.3% (0.326) 5.2% (4.48) 0.0% (0.0) 5.9% (5.36)

Figure 2: Scatterplot matrix for the sampled dataset of Example 4.2 projected onto the first four variables (features): upper panels show scatterplots for pairs of variables in the original clustering; lower panels show the clustering obtained by applying the EM algorithm initialised by the method of moments.
14

4.2. Real data In this subsection we present two examples of real datasets, for which we know already their
number of clusters, and we report the different BIC, ARI and errorRate values as well as the consumed time attained by the EM algorithm initialised by the different considered initialisation strategies and used with the GMM of different spherical covariance matrices. The explored real data are: The famous iris data [Fis36, DT17] widely used as an example of clustering to test the algorithms and the olive oil dataset [AM14].
Example 4.3. The iris dataset contains four physical measurements (length and width of sepals and petals) for 50 samples of three species of iris (setosa, virginica and versicolor). The number of features is n = 4 and the number of clusters is r = 3. The four initialisation strategies yield the same BIC value. The ARI and the errorRate values

Table 5: Numerical results of Example 4.3
Method BIC ARI errorRate time(s) em km -1227.67 0.6199 0.167 0.007 em mom -1227.67 0.6410 0.153 0.203 em mbhc -1227.67 0.6199 0.167 0.007 em emEM -1227.67 0.6302 0.160 0.045
are slightly better with the moment initialisation among the other considered initialisation strategies. On the other hand, the consumed time is clear higher in the moment method initialisation.
Example 4.4. The olive oil data set contains the chemical composition (8 chemical properties) of 572 olive oils. They are derived from three different macro-areas in Italy (South, Sardinia and Centre North). The dataset contains as well nine regions from which the olive oils were taken in Italy. Thus we can cluster this dataset according to the macro-areas (r = 3) or the region (r = 9). As the number of features in this dataset is n = 8, we choose r = 3, so that the condition r  n for the method of moment is verified. The results show that the MBHC initialisation strategy yields the largest BIC, the highest

Table 6: Numerical results of Example 4.4

Method

BIC

ARI errorRate time(s)

em km -10948.64 0.4018 0.262 0.021

em mom -10946.46 0.4532 0.210 0.508

em mbhc -10625.59 0.5003 0.185 0.080

em emEM -10948.72 0.4040 0.260 0.087

ARI and the lowest errorRate values among the other initialisation strategies. Nevertheless, the initialisation by the moment method comes to be the second after the MBHC strategy in terms of the BIC, ARI and errorRate values, while the K-means and the emEM initialisation strategies attain almost the same values of the previously mentionned criteria.
This shows that for these datasets which are not well fitted by the mixture of spherical Gaussians, the moment method can still give good initialisations for the EM algorithm, in comparision with the common initialisation strategies.

15

5. Conclusion
In the context of unsupervised machine learning, the type of models to be recovered plays an important role. For Gaussian mixture models, where iterative methods such as Expectation Maximisation algorithms are applied, the choice of the initialisation is also crucial to recover an accurate model of a given dataset. We demonstrated in the experimentations that tensor decomposition techniques can provide a good initial point for the EM algorithm, and that the moment tensor method outperforms the other state-of-the-art strategies, when datasets are well represented by spherical gaussian mixture models. For that purpose, we presented a new tensor decomposition algorithm adapted to the decomposition of identifiable tensors with low interpolation degree, which applies to the 3rd order moment tensors associated to the data distributions.
References
[AGH+14] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15:2773­2832, 2014.
[AM14] Adelchi Azzalini and Giovanna Menardi. Clustering via nonparametric density estimation: The R package pdfcluster. Journal of Statistical Software, Articles, 57(11):1­26, 2014.
[BBCM13] A. Bernardi, J. Brachat, P. Comon, and B. Mourrain. General tensor decomposition, moment matrices and applications. Journal of Symbolic Computation, 52:51­71, May 2013.
[BC15] Jean-Patrick Baudry and Gilles Celeux. Em for mixtures. Statistics and computing, 25(4):713­726, 2015.
[BCG03] Christophe Biernacki, Gilles Celeux, and G´erard Govaert. Choosing starting values for the em algorithm for getting the highest likelihood in multivariate gaussian mixture models. Comput. Stat. Data Anal., 41(3­4):561­575, January 2003.
[BCMR19] Charles Bouveyron, Gilles Celeux, T Brendan Murphy, and Adrian E Raftery. Modelbased clustering and classification for data science: with applications in R, volume 50. Cambridge University Press, 2019.
[BCMT10] Jerome Brachat, Pierre Comon, Bernard Mourrain, and Elias Tsigaridas. Symmetric tensor decomposition. Linear Algebra and its Applications, 433(11-12):1851­1872, December 2010.
[BDHM17] Alessandra Bernardi, Noah S. Daleo, Jonathan D. Hauenstein, and Bernard Mourrain. Tensor decomposition and homotopy continuation. Differential Geometry and its Applications, 55:78­105, December 2017.
[Bis06] Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006.
[BV18] Paul. Breiding and Nick. Vannieuwenhoven. A Riemannian trust region method for the canonical tensor rank approximation problem. SIAM Journal on Optimization, 28(3):2435­2465, 2018.
16

[CC70] J. Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling via an n-way generalization of "eckart-young" decomposition. Psychometrika, 35(3):283­319, Sep 1970.
[CHLZ12] Bilian Chen, Simai He, Zhening Li, and Shuzhong Zhang. Maximum block improvement and polynomial optimization. SIAM Journal on Optimization, 22(1):87­107, 6 2012.
[COV16] Luca Chiantini, Giorgio Ottaviani, and Nick Vannieuwenhoven. On generic identifiability of symmetric tensors of subgeneric rank. Transactions of the American Mathematical Society, 369(6):4021­4042, Nov 2016.
[DL06] Lieven De Lathauwer. A Link between the Canonical Decomposition in Multilinear Algebra and Simultaneous Matrix Diagonalization. SIAM Journal on Matrix Analysis and Applications, 28(3):642­666, January 2006.
[DL14] Ignat Domanov and Lieven De Lathauwer. Canonical Polyadic Decomposition of ThirdOrder Tensors: Reduction to Generalized Eigenvalue Decomposition. SIAM Journal on Matrix Analysis and Applications, 35(2):636­660, January 2014.
[DLR77] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1­38, 1977.
[DT17] D. Dheeru and E. Karra Taniskidou. UCI machine learning repository. https://archive. ics.uci.edu/ml/index.php, 2017.
[DZGL07] Marco Di Zio, Ugo Guarnera, and Orietta Luzi. Imputation through finite Gaussian mixture models. Computational Statistics & Data Analysis, 51(11):5305­5316, 2007.
[EHK15] Mike Espig, Wolfgang Hackbusch, and Aram Khachatryan. On the convergence of alternating least squares optimisation in tensor format representations. arXiv preprint arXiv:1506.00062, 2015.
[Eis05] David Eisenbud. The Geometry of Syzygies: A Second Course in Commutative Algebra and Algebraic Geometry. Springer, 2005. OCLC: 249751633.
[Fis36] R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2):179­188, 1936.
[FR98] C. Fraley and A. E. Raftery. How many clusters? which clustering method? answers via model-based cluster analysis. The Computer Journal, 41(8):578­588, 1998.
[FR02] Chris Fraley and Adrian E Raftery. Model-based clustering, discriminant analysis, and density estimation. Journal of the American statistical Association, 97(458):611­631, 2002.
[Fra98] Chris Fraley. Algorithms for model-based gaussian hierarchical clustering. SIAM Journal on Scientific Computing, 20(1):270­281, 1998.
[HA85] Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification, 2(1):193­218, 1985.
[Har70] Richard Harshman. Foundations of the parafac procedure: Models and conditions for an "explanatory" multi-modal factor analysis. UCLA Working Papers in Phonetics, 16:1­84, 1970.
17

[HBD18] Antoine Houdard, Charles Bouveyron, and Julie Delon. High-dimensional mixture models for unsupervised image denoising (HDMI). SIAM Journal on Imaging Sciences, 11(4):2815­2846, 2018.
[HH82] Chikio Hayashi and Fumi Hayashi. A new algorithm to solve parafac-model. Behaviormetrika, 9(11):49­60, Jan 1982.
[HK13] Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical gaussians: Moment methods and spectral decompositions. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science, ITCS '13, pages 11­20, New York, NY, USA, January 2013. Association for Computing Machinery.
[HKM18] Jouhayna Harmouch, Houssam Khalil, and Bernard Mourrain. Structured low rank decomposition of multivariate Hankel matrices. Linear Algebra and Applications, 542:161­185, April 2018.
[HOOS19] Jonathan D. Hauenstein, Luke Oeding, Giorgio Ottaviani, and Andrew J. Sommese. Homotopy techniques for tensor decomposition and perfect identifiability. Journal fu¨r die reine und angewandte Mathematik (Crelles Journal), 2019(753):1­22, August 2019.
[HT96] Trevor Hastie and Robert Tibshirani. Discriminant analysis by Gaussian mixtures. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):155­176, 1996.
[HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­778, 2016.
[JGKA19] Majid Janzamin, Rong Ge, Jean Kossaifi, and Anima Anandkumar. Spectral Learning on Matrices and Tensors. Foundations and Trends® in Machine Learning, 12(5-6):393­536, 2019.
[KB09a] Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455­500, September 2009.
[KB09b] Tamara G. Kolda and Brett W. Bader. Tensor Decompositions and Applications. SIAM Review, 51(3):455­500, August 2009.
[KKM20] Rima Khouja, Houssam Khalil, and Bernard Mourrain. A Riemannian Newton Optimization Framework for the Symmetric Tensor Rank Approximation Problem. working paper or preprint, July 2020.
[KP09] Joe Kileel and Jo~ao M. Pereira. Subspace power method for symmetric tensor decomposition and generalized PCA. 2019-12-09.
[LA14] Xavier Luciani and Laurent Albera. Canonical Polyadic Decomposition based on joint eigenvalue decomposition. Chemometrics and Intelligent Laboratory Systems, 132:152­ 167, March 2014.
[LC90] Lucien Le Cam. Maximum likelihood: an introduction. International Statistical Review/Revue Internationale de Statistique, pages 153­171, 1990.
18

[LIL+15] R´emi Lebret, Serge Iovleff, Florent Langrognet, Christophe Biernacki, Gilles Celeux, and G´erard Govaert. Rmixmod: The R package of the model-based unsupervised, supervised, and semi-supervised classification Mixmod library. Journal of Statistical Software, 67(6):1­ 29, 2015.
[LRA93] S. E. Leurgans, R. T. Ross, and R. B. Abel. A Decomposition for Three-Way Arrays. SIAM Journal on Matrix Analysis and Applications, 14(4):1064­1083, October 1993.
[Mac67] J. Macqueen. Some methods for classification and analysis of multivariate observations. In In 5-th Berkeley Symposium on Mathematical Statistics and Probability, pages 281­297, 1967.
[MLR19] Geoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite mixture models. Annual review of statistics and its application, 6:355­378, 2019.
[MM10] Volodymyr Melnykov and Ranjan Maitra. Finite mixture models and model-based clustering. Statistics Surveys, 4(none):80 ­ 116, 2010.
[NCNM20] TrungTin Nguyen, Faicel Chamroukhi, Hien D Nguyen, and Geoffrey J McLachlan. Approximation of probability density functions via location-scale finite mixtures in Lebesgue spaces. arXiv preprint arXiv:2008.09787, 2020.
[OO13] Luke Oeding and Giorgio Ottaviani. Eigenvectors of tensors and algorithms for Waring decomposition. Journal of Symbolic Computation, 54:9­35, July 2013.
[Paa99] Pentti Paatero. The multilinear engine--a table-driven, least squares program for solving multilinear problems, including the n-way parallel factor analysis model. Journal of Computational and Graphical Statistics, 8(4):854­888, 1999.
[Pea94] Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71­110, 1894.
[PTC13] Anh-Huy. Phan, Petr. Tichavsky´, and Andrzej. Cichocki. Low complexity damped gauss­ newton algorithms for candecomp/parafac. SIAM Journal on Matrix Analysis and Applications, 34(1):126­147, 2013.
[RGL17] Matteo Ruffini, Ricard Gavalda, and Esther Lim´on. Clustering patients with tensor decomposition. In Machine Learning for Healthcare Conference, pages 126­146. PMLR, 2017.
[RSG17] Stephan Rabanser, Oleksandr Shchur, and Stephan Gu¨nnemann. Introduction to Tensor Decompositions and their Applications in Machine Learning. arXiv:1711.10781 [cs, stat], November 2017. Comment: 13 pages, 12 figures.
[Sch78] Gideon Schwarz. Estimating the Dimension of a Model. Annals of Statistics, 6(2):461­464, July 1978.
[SFMR16] Luca Scrucca, Michael Fop, T. Brendan Murphy, and Adrian E. Raftery. mclust 5: clustering, classification and density estimation using Gaussian finite mixture models. The R Journal, 8(1):289­317, 2016.
[SK90] E. Sanchez and B. Kowalski. Tensorial resolution: A direct trilinear decomposition. undefined, 1990.
19

[SL10] Berkant. Savas and Lek-Heng. Lim. Quasi-newton methods on grassmannians and multilinear approximations of tensors. SIAM Journal on Scientific Computing, 32(6):3352­3393, 2010.
[SPMV13] Jorge S´anchez, Florent Perronnin, Thomas Mensink, and Jakob Verbeek. Image classification with the Fisher vector: Theory and practice. International journal of computer vision, 105(3):222­245, 2013.
[Str83] V. Strassen. Rank and optimal computation of generic tensors. Linear Algebra and its Applications, 52-53:645­685, July 1983.
[SVBDL13] Laurent. Sorber, Marc. Van Barel, and Lieven. De Lathauwer. Optimization-based algorithms for tensor decompositions: Canonical polyadic decomposition, decomposition in rank - (lr, lr, 1) terms, and a new generalization. SIAM Journal on Optimization, 23(2):695­720, 2013.
[TB06] Giorgio Tomasi and Rasmus Bro. A comparison of algorithms for fitting the parafac model. Comput. Stat. Data Anal., 50(7):1700­1734, April 2006.
[Usc12] Andr´e. Uschmajew. Local convergence of the alternating least squares algorithm for canonical tensor approximation. SIAM Journal on Matrix Analysis and Applications, 33(2):639­652, 2012.
[VD00] Shivakumar Vaithyanathan and Byron Dom. Model-based hierarchical clustering. In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence, UAI '00, page 599­608, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.
[VdV98] Aad W Van der Vaart. Asymptotic statistics. Cambridge university press, 1998. [XJ96] Lei Xu and Michael I Jordan. On convergence properties of the em algorithm for gaussian mixtures. Neural computation, 8(1):129­151, 1996.
20

