Addressing the Long-term Impact of ML Decisions via Policy Regret
David Lindner1 , Hoda Heidari2 , Andreas Krause1 1ETH Zurich
2Carnegie Mellon University {lindnerd, krausea}@ethz.ch, hheidari@cmu.edu

arXiv:2106.01325v1 [cs.LG] 2 Jun 2021

Abstract
Machine Learning (ML) increasingly informs the allocation of opportunities to individuals and communities in areas such as lending, education, employment, and beyond. Such decisions often impact their subjects' future characteristics and capabilities in an a priori unknown fashion. The decision-maker, therefore, faces explorationexploitation dilemmas akin to those in multi-armed bandits. Following prior work, we model communities as arms. To capture the long-term effects of ML-based allocation decisions, we study a setting in which the reward from each arm evolves every time the decision-maker pulls that arm. We focus on reward functions that are initially increasing in the number of pulls but may become (and remain) decreasing after a certain point. We argue that an acceptable sequential allocation of opportunities must take an arm's potential for growth into account. We capture these considerations through the notion of policy regret, a much stronger notion than the often-studied external regret, and present an algorithm with provably sub-linear policy regret for sufficiently long time horizons. We empirically compare our algorithm with several baselines and find that it consistently outperforms them, in particular for long time horizons.
1 Introduction
Machine learning (ML) systems increasingly inform or make high-stakes decisions about people, in areas such as credit lending [10], education [31], criminal justice [4], employment [38], and beyond. These ML-based decisions can negatively impact already-disadvantaged individuals and communities [1, 7, 40]. This realization has spawned an active area of research into quantifying and mitigating the disparate effects of ML [12, 17, 28]. Much of this work has focused on the immediate predictive disparities that arise when supervised learning techniques are applied to batches of training data sampled from a fixed underlying population [12, 17, 28, 45]. While such approaches capture important types of disparity, they fail to account for the long-term effects of present decisions on individuals and communities. Recent work has ad-

vocated for shifting the focus to societal-level implications of ML in the long run [11, 19, 21, 30, 32].
In many real-world domains, decisions made today correspond to the allocation of opportunities and resources that impact the recipients' future characteristics and capabilities. In such settings, we argue that a socially and ethically acceptable allocation of opportunities must account for the recipients' long-term potential for turning resources into social utility. As an example, consider the following stylized scenario: Suppose a decision-maker must allocate funds to several communities, all residing in one city, at the beginning of every fiscal period. The communities have distinct racial and wealth compositions, and for historical reasons, they initially have different capabilities to turn their allocated funds into economic prosperity and welfare for members of the community and the city. The decision-maker does not know ahead of time how the economic capabilities of each community will evolve in response to the funds allocated to it. Moreover, he/she can only observe the return on each possible allocation strategy after employing it. While the decision-maker does not know the precise return-on-investment or reward curves associated with each community in advance, domain knowledge may provide him/her with information about the general shape of such curves. For instance, he/she may be able to reliably assume that reward curves are often initially increasing with diminishing marginal returns; and if investment continues beyond a point of saturation, they exhibit decreasing returns to additional investments.
How should a just-minded decision-maker allocate funds in this hypothetical example? Should he/she always aim for equal allocation of funds in every fiscal period to ensure a form of distributive equality today, or are there cases1 in which he/she should additionally take each community's potential for growth into account and allocate funds proportionately? Note that in this example, a myopic decision maker might neglect disadvantaged communities with high longterm potential to turn funds into welfare, and as a result, amplify disparities between advantaged and disadvantaged communities over time. If the decision-maker aims to maximize the city's long-term economic welfare and prosperity, he/she should prioritize communities that produce higher returns on
1For example, such considerations may come to the fore once all communities have received a reasonable minimum budget.

Reward Per-Step Regret # Pulls Arm 1

Arm 1 Arm 2
0.8

0.6

0.4 0

5000 10000 15000 20000 Number of Pulls

0.15

0.10

0.05

0.00 0

5000 10000 15000 20000 Time-Horizon

20000 15000 10000 5000
0 0

5000 10000 15000 20000 Time-Horizon

Figure 1: The left plot shows a single-peaked bandit with two reward functions, modeling the evolution of rewards in the number of times each arm is pulled. For long time horizons, the optimal strategy is to play Arm 2 because it has a higher asymptotic reward. However, bandit algorithms that maximize external regret fail to recognize this because the initial reward of Arm 2 is smaller than the initial and asymptotic reward of Arm 1. The middle plot shows the regret of a greedy-selection strategy ( ), EXP3 [3] ( ), which minimizes external regret, as well as D-UCB [15] ( ), SW-UCB [15] ( ), and R-EXP3 [3] ( ), three bandit algorithms designed for nonstationary bandits. All of these algorithms fail on the single-peaked bandit. We propose SPO ( ) which achieves sub-linear policy regret in single-peaked bandits settings. The right plot shows how often each algorithm pulls the first arm. The plot shows that SPO stops pulling Arm 1 much earlier than the other algorithms, and is much closer to the optimal policy ( ). For more details on our experiments, see Section 5.

investment over time. Aside from the utilitarian argument for this objective, it can also be justified through the classic fitness argument to justice and fairness, which states that resources and opportunities must be allocated to those who make the best use of them [34, 39].2 This objective motivates the algorithmic question we focus on in this work: how should the decision-maker choose a sequence of allocations to ensure that communities receive funds in proportion to their relative potential for producing high reward for society in the long-run?
Motivated by the above example and numerous other realworld domains in which similar concerns arise,3we study a multi-armed-bandit (MAB) setting in which communities correspond to arms and the reward from each arm evolves every time the decision-maker pulls that arm. We consider a decision-maker who aims to maximize the overall reward obtained within a set time-horizon, but because he/she does not know how the reward curves evolve, he/she is bound to incur some regret. We formulate the decision-maker's goal in this sequential setting as achieving low policy regret [2]. As Figure 1 shows, conventional no external regret algorithms ignore the impact of their decisions today on the evolution of rewards, so they are prone to spending many of their initial pulls on arms that exhibit high immediate rewards but lack adequate potential for growth.
2We emphasize that in many domains, considerations such as need and rights should take precedence to fitness as defined in our stylized example. In certain domains, however, fitness can be one of the key criteria in determining whether an allocation is morally acceptable. For the sake of simplicity and concreteness, we solely focus on this particular factor. It is worth noting that our model and findings are equally applicable to settings in which needs or entitlements change in response to the sequence of allocation decisions.
3Additional real-world examples that fit into our model include: allocating policing resources to neighborhoods to maximize safety; allocating funds to research institutions to maximize scientific discoveries and innovations; allocating loans to students to maximize the rate of graduation/ loan pay-back.

Technical findings. We study single-peaked bandits, a new MAB setting with reward functions that are initially increasing and concave in the number of pulls but can become decreasing at some point (Section 2). We introduce SinglePeaked Optimism (SPO), a novel algorithm that considers potential long-term effects of pulling different arms (Section 3). We prove that SPO achieves sub-linear policy regret if rewards can be observed free of noise (Section 3.1). Further, we present an LP-based heuristic that effectively handles noisy reward observations (Section 4). We empirically compare SPO with several standard no-external-regret algorithms and additional baselines, and find that SPO consistently performs better, in particular, for long time horizons (Section 5).
Broader implications. Our work takes conceptual and technical steps toward modeling and analyzing the long-term implications of ML-informed allocations made over time. From a conceptual point of view, our work showcases the importance of accounting for domain knowledge (here, the general shape of reward curves) and social-scientific insights (e.g., the dynamic by which communities evolve in response to allocation policies) to formulate ML's long-term impact. Our results draw attention to the necessity of understanding social dynamics of a domain for designing allocation algorithms that improve equity and fairness in the long-run. From a technical perspective, we believe our work can serve as a stepping stone toward designing and analyzing better nopolicy regret algorithms for domain-specific reward curves beyond those considered here. Our work is directly applicable to a specific class of reward functions which generalizes and subsumes those in prior work (e.g., [18]). Finally, our novel approach to handling noise allows utilizing the proposed algorithm in more practical settings where observed rewards are expected to be noisy.
1.1 Related Work
Much of the existing work on the social implications of ML focuses on disparities in a model's predictions [12, 17, 28, 45]. However, these approaches are only suited to evaluate

one-shot decision scenarios. In contrast, we formalize disparities that arise when making a sequence of allocation decisions. Recent work has initiated the study of longer-term consequences and effects of ML-based decisions on people, communities, and society. For example, Liu et al., 2018 [30] and Kannan et al., 2019 [26] study how a utility-maximizing decision-maker may interpret and use ML-based predictions. Dong et al., 2018 [11], Hu et al., 2019 [21], and Milli et al., 2019 [32] address strategic classification, a setting in which decision subjects are assumed to respond strategically and untruthfully to the choice of the classification model, and the goal is to design classifiers that are robust to strategic manipulation. Hu and Chen, 2018 [20] study the impact of enforcing statistical parity on hiring decisions made in a temporary labor market that precedes a permanent labor market. Mouzannar et al., 2019 [35] and Heidari et al., 2019 [19] model the dynamics of how members of a population react to a selection rule by changing their qualifications (defined in terms of true labels or feature vectors). However, none of the prior articles investigate the community-level implications of ML-based decision-making policies over multiple time-steps.
Another conceptually-relevant line of work studies fairness in online learning [22, 24, 25]. Joseph et al., 2016 [24], for example, study fairness in the MAB setting, where arms correspond to socially salient groups (e.g., racial groups), and pulling an arm is equivalent to choosing that group (e.g., to allocate a loan to). They consider an algorithm fair if it never prefers a worse arm to a better one, that is, the arm chosen by the algorithm never has a lower expected reward than the other arms. Similar to Joseph et al., 2016 [24], in our running example, each arm corresponds to a community. However, instead of imposing short-term notions of fairness, we focus on longer-term implications and disparities arising from present decisions.
Our model is based on the MAB framework, which has been established as a powerful tool for modeling sequential decision-making, and has been used successfully for many decades and across a wide range of real-world domains [6, 16]. From a technical perspective we study a nonstationary bandit problem. In nonstationary bandits (with limits to the change of the reward distributions), modified versions of common bandit algorithms have strong theoretical guarantees and good empirical performance. For example, if the reward distributions only change a small number of times, variants of the upper confidence bound algorithm (UCB) such as discounted or sliding-window UCB perform well [15]. Similarly, if there is a fixed budget on how much the rewards can change, R-EXP3, a variant of the popular EXP3 algorithm for adversarial bandits [3], guarantees low regret [5]. In this work, we do not restrict how much the rewards can change, but instead restrict the functional shape of the reward functions to be first increasing and concave before switching to decreasing. This is somewhat similar to rotting bandits [29] and recharging bandits [27], but distinct from them in crucial ways. In contrast to rotting bandits, where the rewards decrease when pulling an arm more often, our reward functions first increase and only later decrease with the number of pulls. In contrast to recharging bandits, where rewards are increasing and concave in the amount of time an arm has

not been pulled, we consider a bandit setting with rewards depending on the number of times an arm has been pulled. We consider reward functions that exhibit a "unimodal" shape. However, our setting is very different from unimodal bandits, which are stationary bandit models with a unimodal structure across arms [9, 44].
The setting by Heidari et al., 2016 [18] is closest to ours. They consider two separate models, one with rewards that are increasing and concave, and another with decreasing rewards in the number of pulls of an arm. While [18] provides different algorithms for these two cases, we present a single algorithm that can adapt to both settings and beyond, while matching the respective asymptotic policy regret bounds in [18] (cf. Appendix B). Additionally, in contrast to [18] which primarility studies noise-free observations, we provide an effective heuristic for handling noise.
For an extended discussion of prior work, see Appendix A.

2 The Single-Peaked Bandit Setting

We consider a multi-armed bandit (MAB) with arms

{ 1, . . . , N }, corresponding, e.g., to the different commu-

nities in our introductory example. At each time step t =

1, 2, . . . T , the decision-maker pulls one arm and observes its

immediate reward, e.g., the short-term outcome of an invest-

ment in a community. The decision-maker aims to achieve

the highest cumulative reward within the fixed time horizon

T , e.g., he/she wants to get the best total return-on-investment

over T years. Each arm i has an underlying reward function

fi : { 1, . . . , T }  [0, 1]. When the decision-maker pulls arm i for the m-th time (1  m  T ), he/she observes re-

ward fi(m). Later, in Section 4, we study noisy reward observations of the form fi(m) + i, but for now, let's assume

observed rewards are noise-free. We denote the cumulative

reward of arm i after m pulls by Fi(m) =

m t=1

fi

(t).

A deterministic policy  is a sequence of mappings

(1, . . . , T ) from observed action-reward histories to arms, where t maps histories of length (t - 1) to the next arm to

be pulled:

t : { 1, 2, . . . , N }t-1 × [0, 1]t-1  { 1, 2, . . . , N } .

The cumulative reward of a policy only depends on how

often it pulls each arm, so it is determined by a tuple

(nT1 (), . . . , nTN ()) where nTi () denotes how often  pulls

arm i within the time horizon T . Note that

N i=1

nTi

()

=

T

.

We can write the cumulative reward of a policy as:

N nTi ()

N

rT () =

fi(t) = Fi(nTi ()).

i=1 t=1

i=1

Let  denote the space of all possible deterministic policies, and OPT  argmax rT () be an optimal policy, that is, a policy achieving the highest possible cumulative reward. The
decision-maker does not know the reward functions (fi's) in advance, so he/she cannot find an optimal policy ahead of
time. Instead, he/she can aim to design a (possibly stochastic) policy that minimizes the policy regret: rT (OPT) - ErT (). Given a fixed set of reward functions, we say an algorithm A

that follows policy A,T over time horizon T has sub-linear policy regret, if

lim rT (OPT) - ErT A,T = 0.

T 

T

It is in general impossible to achieve sub-linear policy regret in an adversarial bandit setting [2], and we have to make additional assumptions about the shape of the reward functions fi. In this work, we assume that the underlying reward functions are initially increasing and concave, then decreasing.

Definition 1 (Single-peaked bandit). We call fi(.) a singlepeaked reward function, if there exists a tipping points m¯ i such that fi(m) increases monotonically in m and is concave up to m  m¯ i, and then decreases monotonically for m > m¯ i. We call a bandit with single-peaked reward functions a
single-peaked bandit.

Note that bandits with monotonically increasing or de-
creasing reward functions are single-peaked bandits with m¯ i =  and m¯ i = 0, respectively.

3 SPO: A New No-Policy-Regret Algorithm

Our algorithm operationalizes the principle of optimism in the face of uncertainty, which has been successfully applied with different interpretations to a wide range of MAB problems [6]. Our interpretation of the principle is as follows: At each time step, pull the arm with the highest optimistic future reward. The reward functions of a single-peaked bandit are first increasing and concave, then become decreasing. Therefore, we can define the future optimistic reward in the increasing phase using concavity and in the decreasing phase using monotonicity of the reward function. In the increasing concave phase, we estimate the optimistic future reward as

T

pTi (ni, t) =

min {1, (fi(ni) + i(ni) · (s - t))} ,

s=t+1

where i(ni) = fi(ni) - fi(ni - 1). Defined this way, pTi (ni, t) is a linear optimistic approximation of future rewards from arm i after it has been pulled ni times within the
first t pulls. Similarly, for the decreasing phase, we can define

pTi (ni, t) = fi(ni) · (T - t).
In the increasing phase, we use the fact that the reward will increase at most linearly, and in the decreasing phase we use that it will at best remain constant.
The Single-Peaked Optimism algorithm (SPO, Algorithm 1) performs two main steps at every round t:
1. Pull the arm that maximizes pTi (ni, t) where ni is the number of times the algorithm has pulled arm i so far.
2. Update the optimistic future rewards pTi (ni, t).
For technical reasons, we add an initial phase in which we pull each arm log(T ) times, which only adds sub-linear policy regret, but simplifies the analysis (see Appendix C).
Our analysis formalizes the observation that while SPO may initially overestimate the future reward of an arm that grows at a high rate, it will stop pulling that arm as soon as it ceases to live up to the optimistic expectations.

Algorithm 1 The Single-Peaked Optimism (SPO) algorithm.

function SINGLE-PEAKED OPTIMISM

Ninit  max(log(T ), 2) for arm i in 1, . . . , N do

initial phase

pull it Ninit times

observe the rewards fi(1), . . . , fi(Ninit) ni  Ninit
end for

t  Ninit · N while t  T do

main phase

pT1 , . . . pTN  UPDATEOPTIMISTICREWARD let i  argmaxi pTi (break ties arbitrarily) pull arm i and observe fi (ni + 1) ni  ni + 1 tt+1

end while

end function

function UPDATEOPTIMISTICREWARD

for arm i in 1, . . . , N do

if fi(ni)  fi(ni - 1) then

pTi 

T s=t+1

min{1,

(fi

(ni)+

(fi(ni) - fi(ni - 1)) · (s - t))}

else

pTi  fi(ni) · (T - t) end if

end for

return pT1 , . . . pTN end function

3.1 Regret Analysis
Next, we present our main theoretical result, which establishes the sub-linear policy regret of SPO. All omitted proofs and technical material can be found in Appendix C.
Theorem 1. [informal statement] For any (noise-free) single-peaked bandit, SPO achieves sub-linear policy regret.
The proof consists of several steps: we first observe that all single-peaked reward functions have finite asymptotes (Lemma 3, Appendix C), which follows from the monotone convergence theorem. Then, we show that for sufficiently large time horizons, always pulling the single arm with the highest asymptote would lead to sub-linear policy regret (Lemma 4, Appendix C). Finally, the key step of the proof is to show that SPO pulls all arms with suboptimal asymptotes less than linear in T . Together these steps imply the sub-linear policy regret of SPO.
4 An LP-based Heuristics to Handle Noise
So far we have assumed the decision-maker can observe rewards free of noise. In this section, we describe how to find an upper bound on the future reward from noisy observations. This allows us to extend SPO to noisy observation.
Assume that when pulling arm i for the n-th time, we observe f^i(n) = fi(n) + i(n) where i is a random noise term. We start by assuming that the magnitude of the noise is bounded |i(n)|  ¯i, and ¯i is known for each arm. We

1

1

Reward Reward

Solution to ( )

True reward function

Observations

0

T

Number of Pulls

Upper bound

True reward function

Observations

0

T

Number of Pulls

Figure 2: An illustration of 5 noisy reward observations from an arm, along with the true reward values which lie within the depicted confidence intervals. The dashed red curves specify our upper bound on cumulative future rewards obtained by solving ( ). The left plot shows an instance in which ( ) has a feasible concave and increasing solution. Note that the upper-bound estimate can be lower than the true reward function for past observation, but it is indeed an upper bound for future observations. The right plot shows an instance in which ( ) is not feasible because the reward curve has entered its decreasing phase. In this case the last observation provide an upper bound on the cumulative future reward.

can then define Lji = f^i(n) - ¯i and Uij = f^i(n) + ¯i to obtain confidence intervals for the true reward fi(n) such that fi(n)  [Lji , Uij] with probability 1.
We first extend our algorithm to this case of bounded noise,
and then relax this assumption to confidence intervals that
contain the true value with probability less than 1.

Decreasing phase. For arms in their decreasing phase, we define the optimistic future return as pTi (ni, t) = Uij · (T - t) using the confidence interval [Lji , Uij].
Increasing phase. For arms in their increasing phase, we combine our noise confidence intervals with our knowledge that the function is concave. Concretely, we find the monotone concave function with the highest cumulative future reward that can explain past observations. We can phrase this as solving the following linear program (LP) for each arm i:

n+T -t

maximizev

vj

j=n+1

subject to 0  vj  1,

j = 1, . . . , T

()

Lji  vj  Uij , vj  vj+1,

j = 1, . . . , n j = 1, . . . , T - 1

vj  2vj-1 - vj-2, j = 3, . . . , T

where n is the number of times arm i has been pulled up

to time t. The optimization variables vj correspond to the values of the reward function fi after j pulls of arm i. The

constraints encode that the true reward function is bounded,

consistent with past observations, increasing, and concave, in

that order. Hence, a feasible solution to the LP corresponds to

a possible reward function and an optimal solution provides a

tight upper bound on future rewards.

Theorem 2. Let fi : N+  [0, 1] be a concave, increasing

function with confidence bounds L1i , Ui1, . . . , Lni , Uin  [0, 1]

such that fi(j)  [Lji , Uij] for 1  j  n. Let V  =

n+T -t j=n+1

vj

be the solution to (

).

Then,

n+T -t j=n+1

fi(j)



V . Furthermore, there exists a concave, increasing function,

fi : N+  [0, 1], such that

n+T -t j=n+1

fi(j)

=

V

.

We can extend SPO to noisy observations, by solving the LP ( ) every time we update the future optimistic reward for a given arm. If the LP does not have a feasible solution, we can infer that the arm is in its decreasing phase, and use a corresponding upper bound. Figure 2 illustrates both cases.

Unbounded noise. We can readily extend this approach to unbounded noise with confidence intervals.

Corollary 1. Let fi : N+  [0, 1] be a concave, increasing

function. Suppose that for any  > 0 and observation f^i(ni)

we can find a fi(ni)  [Lni i

(co)n, fiUdinein(ce)]inwteitrhvaplro[Lbani bi i(lit)y,

Uini ()] such that at least 1 - . Let

V  be the solution to ( ). Then for any > 0, we can choose

 such that

n+T -t j=n+1

fi

(j

)



V

with

probability

at

least

1- .

The proof sketch goes as follows: The probability that

within the remainder of time horizon T , at least one true re-

ward value falls outside of its confidence interval is upper bounded by 1 - (1 - )T . For the given , we can choose





1

-

e

1 T

log(1-

),

so

that

the

probability

of

any

true

reward

being outside its confidence interval is bounded by . More

precisely, we can write:

1 - (1 - )T  1 -

1-

1

-

1
eT

log(1-

)

T

= 1 - elog(1- ) =

With the above choice for , the optimistic future reward is estimated correctly with probability at least 1 - .

5 Experiments
In this section, we empirically investigate the effectiveness of our noise-handling approach on several datasets.4
4Code to reproduce all of our experiments can be found at https://github.com/david-lindner/single-peaked-bandits.

Reward Per-Step Regret
Reward Per-Step Regret

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2 Arm 3 Arm 4
500 1000 1500 2000 Number of Pulls

0.5 0.4 0.3 0.2 0.1 0.0
0

500 1000 1500 2000 Time-Horizon

(a) FICO Dataset

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1

Arm 2

0.4

Arm 3

Arm 4

0.3

0.2

0.1

1000

2000

Number of Pulls

0.0

3000

0

1000

2000

3000

Time-Horizon

(b) Recommender System Simulations

D-UCB [15] One-Step-Optimistic

SW-UCB [15] Greedy

EXP3 [3] SPO (ours)

R-EXP3 [5]

Figure 3: Results of our simulation experiments with (a) the FICO credit scoring dataset, and (b) synthetic recommender system data. In both cases, the left plot shows the reward functions of the bandit, and the right plot shows the per-step regret, i.e., the policy regret divided by T . The x-axes of the regret plots show the time horizon T , discretized in 100 points, where each point corresponds to a single experiment. The per-step regret is averaged over 30 random seeds. SPO outperforms all baselines and is the only algorithm that achieves low policy regret for long time horizons.

Setup. We consider three datasets: (1) a set of synthetic reward functions, (2) a simulation of a user interacting with a recommender system, and (3) a dataset constructed from the FICO credit scoring data. We compare SPO with six baselines: (1) a greedy algorithm that always pulls the arm that provided the highest reward at the last pull, (2) a one-stepoptimistic variant of SPO that pulls the arm with the highest upper bound on the reward at the next pull, (3) EXP3, a standard no-external-regret algorithm for adversarial bandits [3], (4) R-EXP3, a modification of EXP3 for non-stationary bandits [5], (5) discounted UCB (D-UCB), and (6) sliding window UCB (SW-UCB), two adaptations of UCB to nonstationary bandits [15].
Illustrations on synthetic data. We first perform a series of experiments on single-peaked bandits with two arms, and synthetic reward functions with Gaussian noise. To this end we define a class of single-peaked functions and combine them into multiple single-peaked bandits with two arms each. In Figure 1 we highlight one experiment in which algorithms that minimize external regret fail. The figure shows how SPO avoids this kind of failure by minimizing policy regret. In Appendix D we provide more detailed results comparing SPO to the baselines on various synthetic reward functions, including the monotonic functions proposed by Heidari et al., 2016 [18], and evaluate the effect of varying the observation noise. We find that SPO matches the performance of the baselines in all cases and significantly outperforms them in some. Further, we show that SPO can also handle stationary MABs, where arms have fixed reward distributions.
FICO credit lending data. Motivated by our initial example of a budget planner in Section 1, we simulate a credit lending scenario based on the FICO credit scoring dataset from 2003 [36]. We pre-process the data using code provided by previous work [17, 30]. The dataset contains four ethnic groups: 'Asian', 'Black', 'Hispanic' and 'White'. Each group has a distribution of credit-scores and an estimated mapping from credit-scores to the probability of repaying a loan. We use this group-level data to simulate a population

of individuals applying for a loan. Each individual belongs to one ethnic group and has a credit score sampled from the group's distribution and a probability of repaying a loan.
We consider a hypothetical decision-making scenario in which at each round, there is exactly one loan applicants from each group. In each time step, the decision-maker (i.e., a bank) can approve only one loan applicant. We are interested in the long-term impact of decision-maker's choices on the underlying groups. As discussed in Section 1, we argue that a fair decision-maker will allocate loans according to the groups' long-term potential to turn them into welfare/prosperity, which is measured by the per-group reward functions in this simulation. In other words, policy regret is our measure of long-term disparity and achieving low policy regret improves the fairness of resource allocation decisions.
To simulate this situation based on the FICO dataset, we first sample N applicants from each group. We assume the decision-maker always approves the loan of the applicant with the highest credit score within a given group; hence, we order the applicants decreasing by their credit score. Thereby, we reduce the problem to the decision-maker deciding between four arms to pull, each corresponding to one group. We interpret pulling an arm as approving the load on the highest scoring applicant within the corresponding group. However, the credit scores do not directly correspond to the reward of pulling an arm. Rather we want to define a reward function that quantifies the benefit/loss of giving out a loan to a group.5 We follow Liu et al., 2018 [30], and measure the impact on a group as the change in mean credit score for this group. Liu et al.'s model assumes an increase in credit score of 75 points for a repaid loan and a decrease of 150 points for a defaulted loan, while the credit score is always being clipped to the range [300, 850]. Finally, we rescale the rewards to [0, 1].
The resulting reward functions increase at first because the first individuals in each group are highly credit-worthy and them paying back their loan increases the mean credit score
5We also investigated a variant of this setting considering only the utility of a loan to the decision-maker, see Appendix D.

for the group. The reward functions are concave because as the decision-maker gives more loans to a group he/she starts to give loans to less creditworthy individuals. Eventually, the reward functions start to decrease because giving loans to individuals who cannot pay them back decreases the average credit score of the group. Hence, this setup can be approximately modelled as a single-peaked bandit.
Figure 3a shows results of running SPO in this setup. Overall, SPO strictly outperforms the baselines over long time horizons. For short time horizons we find that simple greedy approaches or UCB variants can perform favorably.
Synthetic recommender system data. Recent work shows that strategies minimizing external regret can perform poorly in the context of recommender systems, due to negative feedback loops [8, 23, 33, 42]. Here, we focus on one concrete problem that can arise: many recommender systems exhibit a bias towards recommending particularly engaging or novel content that leads to high instantaneous reward, disregarding the long-term benefit and cost to the user. We argue that this situation is analogous to our example of a budget maker, and that a recommender system should aim to maximize it's users' long-term benefit.
Motivated by this observation, we simulate a system that recommends content, e.g., articles or videos, to a user and receives feedback about how much the user engaged with the content. For simplicity, we assume the user's engagement with a piece of content is driven by two factors only: (i) the user's inherent preferences, and (ii) a novelty factor which makes new content more engaging to the user. We assume that the user's inherent preferences stay constant, but the novelty factor decays when showing an item more often. Note that an algorithm that minimizes external regret would show content with high novelty and neglect content that is a better match for the user's inherent preferences. An algorithm that minimizes policy regret would select the content that best matches the user's inherent preferences in the long-run.
We simulate the user's feedback with a reward function fi for each item that can be recommended. Each item has an inherent value v to the user, a novelty factor n, and decay factors  and c. The reward is fi(0) = 0 for never showing an item, and subsequent rewards are defined as
fi(t) = fi(t - 1) + n · t - c · (fi(t) - v).
The second term in the expression models the novelty of an item which decays when showing it more often. The third term models the tendency of the reward to move towards how much the user values the item inherently. The resulting rewards increase at first due to the novelty of an item and decrease later as the novelty factor decays. For simplicity we model all effects that are not captured by this stylized model as Gaussian noise on the observed rewards.
Figure 3b shows that SPO significantly outperforms the baselines for long time horizons, at the cost of worse performance for short time horizons. This results indicates that if a decision-maker acts on a short time-horizon classical bandit algorithms perform well. However, if the decisionmaker aims to achieve a good long-term impact, SPO is preferable. We present results on additional instances of the recommender system simulation in Appendix D.

6 Conclusion
Motivated by several real-world domains, we studied single-peaked bandits in which the reward from each arm is initially increasing then decreasing in the number of pulls of the arm. We introduced Single-Peaked Optimism (SPO), an algorithm that achieves sub-linear policy regret in single-peaked bandits. Our findings highlight the importance of understanding the long-term implications of ML-based decisions for impacted communities and society at large, and utilizing domain knowledge, e.g., regarding social- and population-level dynamics stemming from decisions today, to design appropriate sequences of allocations that do not amplify historical disparities.
Limitations. We argued that single-peaked bandits are a useful model to provide insights about allocation decisions in a range of practical domains, e.g., allocating loans to communities, allocating funds to research institutions, or allocating policing resources to districts. However, single-peaked bandits can also be too restrictive in domains where the evolution of rewards are more nuanced, e.g., if rewards can later increase again after first decreasing. We emphasize that singlepeaked reward functions are one among many reasonable classes of reward functions that are interesting to study from an algorithmic perspective. We consider our work as an starting point to look into more complex dynamics in future work.
Future work. We hope that our work draws the research community's attention to the study of policy regret for typical reward-evolution curves. Additional directions for future work include (1) establishing regret bounds for settings with arbitrary noise distributions, (2) providing instance-specific (and potentially tighter) regret bounds for Single-Peaked Optimism, and finally (3) more broadly characterizing the limits of "optimism in the face of uncertainty" principle in achieving low policy regret.
Acknowledgements
Lindner was partially supported by Microsoft Swiss JRC. This work was in part done while Heidari was a postdoctoral fellow at ETH Zurich. Heidari acknowledges partial support from NSF IIS2040929. Any opinions, findings, and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation, or other funding agencies.
References
[1] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.
[2] Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive adversary: from regret to policy regret. In ICML, 2012.
[3] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48­77, 2002.
[4] Richard Berk and Jordan Hyatt. Machine learning forecasts of risk to inform sentencing decisions. Federal Sentencing Reporter, 27(4):222­228, 2015.

[5] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Optimal exploration­exploitation in a multi-armed bandit problem with non-stationary rewards. Stochastic Systems, 9(4):319­337, 2019.
[6] Se´bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends® in Machine Learning, 5(1):1­ 122, 2012.
[7] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability, and Transparency, 2018.
[8] Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. How algorithmic confounding in recommendation systems increases homogeneity and decreases utility. In Conference on Recommender Systems, 2018.
[9] Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In ICML, 2014.
[10] Will Dobbie, Andres Liberman, Daniel Paravisini, and Vikram Pathania. Measuring bias in consumer lending. Technical report, National Bureau of Economic Research, 2018.
[11] Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In Conference on Economics and Computation, 2018.
[12] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In ITCS, 2012.
[13] Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Eduardo Scheidegger, and Suresh Venkatasubramanian. Decision making with limited feedback: Error bounds for predictive policing and recidivism prediction. In Conference on Fairness, Accountability, and Transparency, 2017.
[14] Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Eduardo Scheidegger, and Suresh Venkatasubramanian. Runaway feedback loops in predictive policing. In Conference on Fairness, Accountability, and Transparency, 2018.
[15] Aure´lien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In International Conference on Algorithmic Learning Theory, 2011.
[16] John Gittins, Kevin Glazebrook, and Richard Weber. Multiarmed bandit allocation indices. John Wiley & Sons, 2011.
[17] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems, 2016.
[18] Hoda Heidari, Michael J. Kearns, and Aaron Roth. Tight policy regret bounds for improving and decaying bandits. In IJCAI, 2016.
[19] Hoda Heidari, Vedant Nanda, and Krishna P Gummadi. On the long-term impact of algorithmic decision policies: Effort unfairness and feature segregation through social learning. In ICML, 2019.
[20] Lily Hu and Yiling Chen. A short-term intervention for longterm fairness in the labor market. In WWW, 2018.
[21] Lily Hu, Nicole Immorlica, and Jennifer Wortman Vaughan. The disparate effects of strategic manipulation. In Conference on Fairness, Accountability, and Transparency, 2019.
[22] Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in reinforcement learning. In ICML, 2017.
[23] Ray Jiang, Silvia Chiappa, Tor Lattimore, Andra´s Gyo¨rgy, and Pushmeet Kohli. Degenerate feedback loops in recommender systems. In Conference on AI, Ethics, and Society, 2019.
[24] Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, 2016.

[25] Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Fair algorithms for infinite contextual bandits. In Conference on Fairness, Accountability, and Transparency, 2017.
[26] Sampath Kannan, Aaron Roth, and Juba Ziani. Downstream effects of affirmative action. In Conference on Fairness, Accountability, and Transparency, 2019.
[27] Robert Kleinberg and Nicole Immorlica. Recharging bandits. In Annual Symposium on Foundations of Computer Science (FOCS), pages 309­319, 2018.
[28] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In ITCS, 2017.
[29] Nir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. In Advances in Neural Information Processing Systems, 2017.
[30] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In ICML, 2018.
[31] Frank Marcinkowski, Kimon Kieslich, Christopher Starke, and Marco Lu¨nich. Implications of AI (un-) fairness in higher education admissions: the effects of perceived AI (un-) fairness on exit, voice and organizational reputation. In Conference on Fairness, Accountability, and Transparency, 2020.
[32] Smitha Milli, John Miller, Anca D Dragan, and Moritz Hardt. The social cost of strategic classification. In Conference on Fairness, Accountability, and Transparency, 2019.
[33] Martin Mladenov, Elliot Creager, Omer Ben-Porat, Kevin Swersky, Richard Zemel, and Craig Boutilier. Optimizing longterm social welfare in recommender systems: A constrained matching approach. In ICML, 2020.
[34] Herve´ Moulin. Fair division and collective welfare. MIT press, 2004.
[35] Hussein Mouzannar, Mesrob I Ohannessian, and Nathan Srebro. From fair decision making to social equality. In Conference on Fairness, Accountability, and Transparency, 2019.
[36] US Federal Reserve. Report to the congress on credit scoring and its effects on the availability and affordability of credit. Board of Governors of the Federal Reserve System, 2007.
[37] Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527­535, 1952.
[38] Javier Sa´nchez-Monedero, Lina Dencik, and Lilian Edwards. What does it mean to 'solve' the problem of discrimination in hiring? social, technical and legal perspectives from the uk on automated hiring systems. In Conference on Fairness, Accountability, and Transparency, 2020.
[39] Michael J Sandel. Justice: What's the right thing to do? Macmillan, 2010.
[40] Latanya Sweeney. Discrimination in online ad delivery. Queue, 11(3):10, 2013.
[41] Cem Tekin and Mingyan Liu. Online learning of rested and restless bandits. Transactions on Information Theory, 58(8):5588­5611, 2012.
[42] Romain Warlop, Alessandro Lazaric, and Je´re´mie Mary. Fighting boredom in recommender systems with linear reinforcement learning. In Advances in Neural Information Processing Systems, 2018.
[43] Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of applied probability, pages 287­298, 1988.
[44] Jia Yuan Yu and Shie Mannor. Unimodal bandits. In ICML, 2011.
[45] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In WWW, 2017.

A More Details on Related Work
In this section we give more details on related work.
External vs. policy regret. The vast majority of existing algorithms for the well-studied MAB setting aim to minimize the so-called external regret. External regret is the difference between the total reward collected by an online algorithm and the maximum reward that could have been collected by pulling the best/optimal arm on the same sequence of rewards as the one generated by the online algorithm. Existing work on MAB often focuses on one of the following two settings: first, the statistical setting in which the reward from each arm is assumed to be sampled from a fixed, but unknown distribution [3, 16, 37], and second, the adversarial setting where an adversary--who is capable of reacting to the choices of the decision-maker--determines the sequence of rewards [3]. As discussed by Arora et al., 2012 [2], the notion of external regret fails to capture the actual regret of an online algorithm compared to the optimal sequence of actions it could have taken when the adversary is adaptive. Policy regret is defined to address this counterfactual setting. Arora et al. show that if the adaptive adversary has bounded memory, then a variant of traditional online learning algorithms can still guarantee nopolicy-regret. If the adversary does not have bounded memory, as is the case in our setting, no algorithm can achieve no-policy-regret in general. Hence, additional assumptions are required, such as the ones we make when defining singlepeaked bandits.
Restless and rested bandits. Nonstationary bandits are called restless if the rewards of an arm can change in each timestep, or rested if the reward of an arm can only change when the arm is pulled [41]. Restless bandits model changes in the environment due to external effects [43], whereas rested bandits model changes to the environment caused by interactions of the decision maker with the environment. Such feedback effects are the focus of our work, and hence we consider a rested nonstationary bandit.
Feedback loops as a source of unfairness. At a high-level, feedback loops occur when outputs of a system influence its inputs in future rounds, and this cause-and-effect circuit amplifies or inhibits the system [13, 14, 30].
Ensign et al., 2017 [13], for example, study feedback loops in the context of recidivism prediction, that is, predicting if an inmate will re-offend within a fixed time after being released, and predictive policing, that is, allocating police patrols to city districts based on historical crime data. In both cases, the decision-maker faces a partial monitoring problem: the decision-maker only receives feedback if he/she takes specific actions (e.g., release an inmate or send a patrol). Ensign et al., 2018 [14] show that commonly-used algorithms for predictive policing suffer from a specific type of feedback loop: if police patrols are assigned based on historical crime data, the assignment rate of police officers to neighborhoods can quickly become highly disproportionate to the actual crime rates of the neighborhoods.
Feedback loops can also occur in recommender systems [8, 23, 33, 42], which motivated some of the simulation experiment in Section 5. Jiang et al., 2019 [23] argue that feed-

back loops can give rise to "echo chamber" and "filter bubble effects", and Chaney et al., 2018 [8] argue that they lead to more homegenity and lower utility for the users. Warlop et al., 2018 [42] suggest that minimizing external regret is suboptimal for recommender systems because the user's preferences can change depending on what is recommended to them, and Mladenov et al., 2020 [33] propose to maximize the long-term social welfare instead of short-term reward. These interpretations of feedback loops in recommender systems are in line with the main conceptual argument of our work: when deploying ML-based systems to make decisions that affect humans, one should focus on maximize their longterm well-being instead of sacrificing it for short-term reward.
We argue that these different kinds of feedback loops are a consequence of deploying algorithms that minimize external regret instead of policy regret, and, therefore, do not take into account the long-term effects of their decisions.
B Monotone Bandits
In this section, we describe bandits with monotonically increasing or decreasing reward functions, as described by Heidari et al., 2016 [18]. We show that these are special cases of single-peaked bandits, and that SPO achieves the same asymptotic policy regret as the algorithms introduced by Heidari et al..
B.1 Increasing Reward Functions
Heidari et al. consider reward functions that are monotonically increasing and concave which correspond to singlepeaked bandits with t¯i =  for every arm. With this observation we receive sub-linear policy regret of SPO in this setting as a corollary of Theorem 1.
Lemma 1. For any set of bounded, concave and increasing reward functions f1, . . . fN , the policy regret of SPO is sublinear.
This results shows that our algorithm matches the asymptotic performance of Algorithm 1 by Heidari et al. in the case of increasing reward functions.
B.2 Decreasing Reward Functions
Further, Heidari et al. study bandits with monotonically decreasing reward functions, which are single-peaked bandits with t¯i = 0. Heidari et al. show that for decreasing reward functions the optimal policy in hindsight always pulls the arm with the highest instantaneous reward. This immediately gives a constant-policy-regret algorithm that greedily pulls the arms that gave the highest reward at the last pull.
It is straightforward to show that in the case of monotonically decreasing reward functions, SPO is equivalent to the greedy algorithm after the initial phase of pulling all arms. In particular, this implies that it achieves sub-linear policy regret for this case as well.
Lemma 2. For any set of monotonically decreasing reward functions f1, . . . , fN and a time horizon T , SPO greedily pulls the arms that gave the highest reward at the last pull after its initial phase. Therefore, it achieves sub-linear policy regret.

Proof. After pulling every arm log(T ) times, SPO will always update the optimistic future reward to be

pTi (ni, t) = fi(ni) · (T - t)

after i has been pulled ni times because the reward functions
are decreasing. At each time step, SPO then pulls an arm j  argmaxi pTi (ni, t). For fixed T and t

argmax pTi (ni, t) = argmax fi(ni)

i

i

Hence, at each time step SPO pulls an arm that gave the highest reward at the last pull. Heidari et al. show that this behavior is optimal. Therefore SPO only incurs policy regret of order O(log(T )) during its initial phase. In particular, the policy regret is sub-linear.

While SPO achieves sub-linear regret, it does not match the performance of a greedy algorithm which achieves constant policy regret Heidari et al.. In situations where the reward functions are known to be decreasing a greedy approach is therefore preferrable. If, however, reward functions might be increasing or decreasing SPO can still achieve sub-linear regret.

C Proofs of Theoretical Statements
This section contains detailed proofs for all results presented in the main paper.
C.1 Noise-free Observations
Definition 2. We call arms with a single-peaked reward function with t¯i =  asymptotically increasing, and all other single-peaked arms asymptotically decreasing.
Lemma 3. All arms i of a single-peaked bandit have an asymptote, i.e., a finite limit ai = limt fi(t), which is bounded between 0 and 1, i.e., 0  ai  1.
Proof. This follows from the monotone convergence theorem:
Theorem (Monotone Convergence). If {an} is a monotone sequence of real numbers (an  an+1 or an+1  an for all n  1) then this sequence has a finite limit if and only if it is bounded.
In our case, all fi are bounded between 0 and 1. Asymptotically increasing arms are monotone and asymptotically decreasing arms are monotone for all t > t¯i. Hence the theorem applies to both cases and gives a finite limit ai between 0 and 1 for each arm.
Lemma 4. Let W = argmaxi ai. For any k  W let kT be the policy that pulls the k-th arm T times and does not pull any other arm. Then, kT achieves sub-linear policy regret.
Proof. Let nTi denote the number of times OPT pulls arm i for time horizon T . Then
N
rT (OPT) = Fi(nTi )
i=1

Observe that

lim fi(t)  a
t

And because fi(t) = Fi(t) - Fi(t - 1) also

lim
t

(Fi(t)

-

Fi(t

-

1))



a

Hence for any  > 0 there exists a t such that for any t > t and all i
Fi(t) - Fi(t - 1)  a +  Fi(t)  Fi(t - 1) + a + 
Then with C2i = Fi(t)

Fi(t)  C2i + (t - t)(a + )

Let C2 = maxi{C2i} and consider a general t  1. Then because Fi is increasing

Fi(t)  C2 + max(0, t - t)(a + )  C2 + t · (a + )

We can use this to upper bound the reward of the optimal policy

N
rT (OPT) = Fi(nTi )
i=1
N
 (C2 + nTi · (a + ))
i=1
= N · C2 + T · (a + )

where in the last step we use

N i=1

nTi

=

T.

Dividing by T

and taking the limit T   gives us

lim rT (OPT)  a +  T  T Because this has to hold for all  > 0, it implies

lim rT (OPT)  a

(1)

T  T

Not consider the policy kT . It only pulls arm k and therefore achieves reward

rT (kT ) = Fk(T )

Because k  W we have

lim
t

fk (t)

=

a

and this means

lim
t

(Fk (t)

-

Fk (t

-

1))

=

a

In particular for any  > 0 there exists a t such that for t > t

Fk(t) - Fk(t - 1)  a -  Fk(t)  Fk(t - 1) + a -  Fk(t)  Fk(t) + (t - t)(a - ) Fk(t)  Ck + t · (a - )

where we defined Ck = Fk(t) - t · (a - ). Therefore we get
rT (kT ) = Fk(T )  Ck + T · (a - )
and lim rT (kT )  a -  T  T
Because this has to hold for all  > 0 we end up with

lim rT (kT )  a

(2)

T  T

Combining eqs. (1) and (2) gives

lim rT (OPT) - rT (kT ) = lim rT (OPT) - lim rT (kT )

T 

T

T  T

T  T

 a - a = 0

which gives the desired result

lim rT (OPT) - rT (kT ) = 0

T 

T

because

rT (OPT) - rT (kT )  0 T

Theorem 1. [informal statement] For any (noise-free) single-peaked bandit, SPO achieves sub-linear policy regret.

Theorem (formal statement of Theorem 1). Let f1, . . . , fN

be the arms of a single-peaked bandit. Denote the SPO al-

gorithm by A. Then SPO achieves sub-linear policy regret,

i.e.,

lim rT (OPT) - rT (A,T ) = 0.

T 

T

Proof. Let W = argmaxi ai, let arm k be in W and kT be the policy that only pulls arm k. We will show that

lim rT (kT ) - rT (A,T ) = 0

T 

T

which implies the theorem because of lemma 4 and

lim rT (OPT) - rT (A,T ) =

T 

T

= lim rT (OPT) - rT (kT ) + rT (kT ) - rT (A,T ) = 0

T 

T

T

We now go on to show two things: (i) pulling arms i  W adds sub-linearly to the reward difference between iT and A and (ii) the number of times A pulls arms i / W is sub-linear in T .
To show (i) we modify a Lemma by Heidari et
al., 2016 [18] to hold for single-peaked reward functions in-
stead of monotonically increasing reward functions.

Lemma 5 (Adaptation of Lemma 2 in Heidari et al., 2016 [18]). Let k  W and let nTi denote the number of times A pulls arm i. Then

lim Fk(T ) - iW Fi(nTi ) = 0

T 

T

holds for single-peaked bandits.

Proof of Lemma. If i  W is pulled o(T ) times, it can not

cause the algorithm to suffer linear policy regret. Now con-

sider a subset W of W consisting of any suboptimal arm i

which is pulled (T ) times by the algorithm.

Given that all arms not in W are pulled less than T =

o(T ) times, we have that T - iW nTi = T = o(T ).

Let w  W

be

the

arm

for

which

Fi(nTi ) nTi

is

the

smallest,

i.e. the arm with optimal asymptote that has the lowest aver-

age reward. We have

Fk(T ) -
iW

Fi(nTi ) = Fk(T ) -
iW

nTi

·

Fi(nTi ) nTi

< Fk(T ) -
iW

nTi

·

Fw (nTw ) nTw

=

Fk(T ) -

(T

-

T

)

·

Fw (nTw ) nTw

=T ·

Fk (T T

)

-

Fw (nTw ) nTw

+T

·

Fw (nTw ) nTw

T ·

Fk (T T

)

-

Fw (nTw ) nTw

+ o(T )

where

in

the

last

step

we

used

that

Fw (nTw ) nTw

is

constant

and

T = o(T ). In total we now have

Fk(T ) -

Fi(nTi )  T ·

iW

Fk (T T

)

-

Fw (nTw ) nTw

+ o(T )

It only remains to show that

T·

Fk(T ) T

-

Fw (nTw ) nTw

= o(T )

We have limT 

Fk(T ) T

=

limT 

Fw (nTw ) nTw

=

a

and there-

fore

T·
limT 

Fk (T T

)

-

Fw (nTw nTw

)

T

=

limT 

Fk(T ) T

-

Fw (nTw ) nTw

=

0

which means T ·

- Fk(T )
T

Fw (nTw ) nTw

= o(T ). Together this

gives the desired result.

Lemma 5 shows that whenever A pulls an arm in W it incurs sub-linear regret, which concludes step (i).
It remains to show (ii), that is the number of times A pulls arms i / W is sub-linear in T . For this we use the initial phase in which the algorithm pulls each arm log(T ) times.
Importantly, this phase only adds sub-linear regret, and after this phase we can assume T to be large enough such that for every arm i

|fi(log(T )) - ai|  

(3)

where



=

mini

a -ai 4

nTi is the number of times the algorithm pulls arm i and

nTi = nTi,1 + nTi,2 = log(T ) + nTi,2

Let i / W .

We

will

show

that

limT 

nTi T

= 0 by us-

ing

limT 

nTi T

=

limT 

log(T )+nTi,2 T

=

limT 

. nTi,2
T

We show that lim supT  nTi,2 = 0 which implies

limT 

nTi,2 T

=

0.

Assume this was not the case, then we could find an infinite

subsequence of T 's, which we label {ij} j=1 such that for every j  1

ni,ij2  1

Dropping the indices, we just write  to mean ij for an arbitrary j.
Let i be the last timestep at which the algorithm pulls arm i within time horizon  and si = nk(i ) the number of times it pulls the arm k up until the last pull of i.
Now, at timestep i the algorithm pulls arm i, and because ni,2  1, this implies that i has the highest optimistic future
reward. In particular:

pi (ni - 1, i )  pk(si , i )

No matter weather k is asymptotically increasing or asymptotically decreasing, we can always lower-bound its optimistic future reward by

pk(si , i )  ( - i ) · fk(si )

Now, there are two cases: either arm i is asymptotically increasing or it is asymptotically decreasing.

First case: i is asymptotically increasing. We have

pi (ni - 1, i )



=

min {1, (fi(ni - 1) + i(ni - 1) · (t - i ))}

t=i +1





(fi(ni - 1) + i(ni - 1) · (t - i ))

t=i +1

 -i

=fi(ni - 1) · ( - i ) + i(ni - 1) ·

t

t=1

=fi(ni

-

1)

·

(

-

i )

+

1 2

i

(ni

-

1)

·

(

-

i )(

-

i

+

1)

where i(t) = fi(t) - fi(t - 1).

Combining the inequalities gives

fi(ni

-

1)

·

(

-

i

)

+

1 2

i(ni

-

1)

·

(

-

i )(

-

i

+

1)

 ( - i ) · fk(si ) Dividing by  - i gives

fi(ni

-

1)

+

1 2

·

i(ni

-

1)

·

(

-

i

+

1)



fk(si )

and subtracting fi(ni - 1) gives

1 2

·

i(ni

-

1)

·

(

-

i

+

1)



fk(si )

-

fi(ni

-

1)

bWoeuncdanthueppr.ehr.sb. ouusnindgtheeq.l.3h.asn. dbyfi21(nui(-ni1)-1)ai·

 and by

lower

fk(si ) - fi(ni

- 1)



a -  - ai



3(a - ai) 4



a

- ai 2

which together gives

i(ni - 1) ·   a - ai

Reintroducing the indices of  , the full statement is that for any j we have

i(ni ij - 1) · ij  a - ai

(4)

In the next step, we use a small technical Lemma, that is proven after the main proof.
Lemma 6. Let {nT } T =1 be a positive sequence with limT  nT = , fi a bounded and increasing reward function. Then
lim i(nT ) · T = 0
T 
where i(t) = fi(t) - fi(t - 1).
Because limT  nTi = limT  log(T ) + nTi,2 = . We can apply Lemma 6 to get limT  i(nTi - 1) · T = 0. In particular, this holds for the sub-sequence {ij} j=1:

lim
j

i

(ni ij

- 1)

·

ij

=

0

But together with eq. 4 this would mean a = ai which is a contradiction to i / W . Therefore, A drops increasing arms with suboptimal asymptotes in sub-linear time. This concludes the discussion of the first case.
Second case: i is asymptotically decreasing. We have

pi (ni - 1, i ) = ( - i ) · fi(ni - 1)

and the resulting inequality is

( - i ) · fi(ni - 1)  ( - i ) · fk(si )

which after dividing by ( - i ) leaves us just with

fi(ni - 1)  fk(si )

(5)

However |fk(si ) - ak|   and |fi(ni - 1) - ai|   imply

fk(si ) - ak   ak - fk(si )   fi(ni - 1) - ai   ai - fi(ni - 1)  

And combining this with eq. 5 we get

ai +   ak - 

ai + 2 ·   ak

ai

+

2

·

min
j

a

- 4

aj

 ak

ai

+2·

a

- ai 4



ak

ai

+

a 2

-

ai 2



ak

ai 2



ak

-

a 2

ai  a 22

where in the last step we used k  W and therefore ak = a. But this implies ai  a, which is a contradiction to the assumption i / W . This shows that we also get a contradiction

and (ii) holds for asymptotically increasing as well as asymp-

totically decreasing arms. This concludes the proof.

Lemma 6. Let {nT } T =1 be a positive sequence with limT  nT = , fi a bounded and increasing reward function. Then
lim i(nT ) · T = 0
T 
where i(t) = fi(t) - fi(t - 1).
Proof. This result, which is also used by Heidari et al., 2016 [18], follows from the reward function f being increasing and bounded between 0 and 1. Observe that

i(T )  1
T =1 
i(T ) · t  t
T =1
lim i(T ) · t = 0
T 
lim i(nT ) · t = 0
T 
where in the last step we replaced the T with nT which is possible because limT  nT = .
Hence, the sequence {i(nT ) · t} T =1 converges to 0 for any fixed t. For the next step, recall Lebesgue's Dominated Convergence Theorem.

Theorem (Lebesgue's Dominated Convergence). Let {fn} n=1 be a sequence of complex-valued measurable functions on a measure space (S, , µ). Suppose that
the sequence converges pointwise to a function f and is
dominated by some integrable function g in the sense that |fn(x)|  g(x) for all numbers n in the index set of the sequence and all points x  S. Then f is integrable and

lim |fn - f | dµ = 0
n S

We can apply this theorem here by considering the sequence {fT (t)} T =1 = {i(nT )} T =1 which is define on S = N+ and µ is the counting measure. We just showed that
this sequence converges pointwise to the function f (t) = 0. Because i(nT )  1, the sequence is also dominated by g(t) = t which is integrable.

The theorem then gives



lim i(nT ) · t = 0
T  t=1

For any T > 0



i(nT ) · T  i(nT ) · t

t=1

because i(nT ) · T is just one term of the sum and all terms are non-negative.

It follows that


lim i(nT ) · T  lim i(nT ) · t = 0

T 

T 

t=1

Because f is increasing we also have

lim i(nT ) · T  0
T 

and therefore

lim i(nT ) · T = 0
T 

C.2 Handling Noise

Theorem 2. Let fi : N+  [0, 1] be a concave, increasing

function with confidence bounds L1i , Ui1, . . . , Lni , Uin  [0, 1]

such that fi(j)  [Lji , Uij] for 1  j  n. Let V  =

n+T -t j=n+1

vj

be the solution to (

).

Then,

n+T -t j=n+1

fi

(j

)



V . Furthermore, there exists a concave, increasing function,

fi : N+  [0, 1], such that

n+T -t j=n+1

fi(j)

=

V

.

Proof. For convenience we restate the LP:

n+T -t

maximizev

vj

j=n+1

subject to 0  vj  1,

j = 1, . . . , T

()

Lji  vj  Uij ,

j = 1, . . . , n

vj  vj+1,

j = 1, . . . , T - 1

vj  2vj-1 - vj-2, j = 3, . . . , T

We show that v1, . . . , vT is a feasible solution to the LP if and only if fi defined by v1 = fi(1), v2 = fi(2), . . . , vT = fi(T ) is a concave increasing reward function. This implies the claim of the lemma and in particular that the optimal value
of the LP provides an upper bound on the future optimistic reward of any reward function fi consistent with the past observations.
To show the claim, we verify that the constraints are equivalent to fi being bounded, increasing and concave:

· 0  fi(j)  1 for j = 1, . . . , T means that fi is bounded
· Lji  fi(j)  Uij for j = 1, . . . , n restricts fi to functions consistent with the past observations

· fi(j)  fi(j + 1) for j = 1, . . . , T - 1 means that fi is increasing

· fi(j)-fi(j -1)  fi(j -1)-fi(j -2) for j = 3, . . . , T is the concavity of fi

Hence, every fi gives a feasible solution to the LP. The objec-

tive of this solution is

n+T -t j=n+1

vj

=

n+T -t j=n+1

f (j),

which

is the cumulative future return of fi.

D Additional Experiments
In this section, we provide more extensive empirical results from the simulations discussed in the main text of the paper.
In Appendix D.1, we discuss increasing and concave reward functions and compare SPO to an algorithm proposed by Heidari et al., 2016 [18] for this specific setting. In Appendix D.2, we consider more synthetic single-peaked reward functions. In Appendix D.3 we consider constant rewards, which corresponds to stationary MABs with Gaussian observations. In Appendix D.5, we consider different instances of the recommender system simulation. In Appendix D.4, we provide a variation of the reward functions from the FICO dataset that only considers the bank's utility.

D.1 Increasing Reward Functions
For monotonically increasing and concave reward functions we choose a set of synthetic functions that Heidari et al., 2016 [18] use. We compare their algorithm, which is restricted to increasing, concave reward functions, to ours by plotting the per-step-regret of both algorithms. The results are shown in Figures 4 to 6. Additionally we choose one of the reward function to test the robustness of the algorithms to noisy observations, shown in Figure 7. Figure 4 shows SPO perform comparably to Heidari et al., 2016 [18]'s algorithm. In all other experiments SPO outperforms their algorithm significantly. Their algorithm fails especially when adding noise as in Figure 7. EXP3 and R-EXP3 perform poorly on all experiments, while D-UCB and SW-UCB work well in some of them, but fail in others.
D.2 Single-Peaked Reward Functions
Here we consider more synthetic single-peaked reward functions to evaluate SPO on, similar to Figure 1 in the main text. The experimental setup is as before with two reward functions f1 and f2. We consider three sets of reward functions with different levels of Gaussian noise.
The results of these experiments are shown in Figures 8 to 10. SPO compares favorably to the baselines. While the baselines perform comparably in some cases, they fail in others, e.g., in Figure 10.
D.3 Gaussian Multi-armed Bandits
One might wonder if SPO can also be used for classical multiarmed bandits for which the rewards are drawn from a fixed distribution for each arm. Of course, one would expect SPO to perform worse in this situation than algorithms designed for the simple MAB problem. However, in some situations there might be little a priori information on the shape of the reward functions, and then it is beneficial to have an algorithm that can handle different situations such as increasing, decreasing or constant rewards.
To test whether SPO can handle simple MABs, we consider constant reward functions with Gaussian noise. This

corresponds to a MAB setup where the reward of each arm is drawn from a Gaussian distribution with a fixed mean. We evaluate SPO on sets of arms with randomly sampled means, and compare it to UCB. Figure 11 shows that SPO can still find the optimal arm in this situation, but takes longer than UCB which is d.
D.4 FICO Dataset
Figures 12 and 13 show additional results for our experiments using the FICO dataset.
Figure 12 shows the same results presented in the main paper, but for different simulated noise levels. The general observation that SPO outperforms all baselines holds across different levels of noise. As expected its advantage shrinks a bit for high noise levels.
In a separate experiment, we consider reward functions defined by the banks's utility of giving a loan. We use the model by Liu et al., 2018 [30], which assumes a utility of +1 for a repaid loan and -4 for a defaulted loan. In this case, the reward functions are decreasing almost everywhere. For monotonically decreasing reward functions Heidari et al., 2016 [18] show that greedy approaches are optimal. This is also what we see in the results in Figure 13: both the greedy and the one-step-optimistic algorithm achieve no-policy-regret everywhere. However, our algorithm also achieves good performance and outperforms some of the other baselines significantly.
D.5 Simulated Recommender System
We randomly generate 3 instances of the simulated recommender system described in the main text. Figures 14 to 16 show the reward functions and experimental results for different levels of simulated Gaussian noise. The results across all instances and noise levels are consistent with the results we report in the main paper, and they show that SPO outperforms all alternatives significantly except for very short time horizons.

Reward

Reward

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.3

0.2

0.1

0.0 0

2500 5000 7500 10000 Time-Horizon

(a)  = 0.1

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.25 0.20 0.15 0.10 0.05 0.00
0

2500 5000 7500 10000 Time-Horizon

(b)  = 0.5

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.20 0.15 0.10 0.05 0.00
0

2500 5000 7500 10000 Time-Horizon

(c)  = 1

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.20 0.15 0.10 0.05 0.00
0

2500 5000 7500 10000 Time-Horizon

(d)  = 5

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

Reward

Reward

Reward

Reward

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.4 0.3 0.2 0.1 0.0
0

2500 5000 7500 10000 Time-Horizon

(a)  = 0.03

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.4 0.3 0.2 0.1 0.0
0

2500 5000 7500 10000 Time-Horizon

(b)  = 0.1

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.4 0.3 0.2 0.1 0.0
0

2500 5000 7500 10000 Time-Horizon

(c)  = 0.4

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.3

0.2

0.1

0.0 0

2500 5000 7500 10000 Time-Horizon

(d)  = 1

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

Reward

Reward

Figure 4: The left-hand plots show the increasing reward functions defined by f1(t) = 1 - t-0.5, f2(t) = 0.5 - 0.5t- where  = 0.1, 0.5, 1, 5 (from top to bottom). The middle plots show the perstep policy regret achieved by SPO ( ) compared to the algorithm proposed by Heidari et al., 2016 [18] for increasing rewards ( ), EXP3 ( ), R-EXP3 ( ), D-UCB ( ), and SW-UCB ( ). The right-hand plots show the policies these algorithms choose in comparison to the optimal policy ( ).

Figure 5: The left-hand plots show the reward functions given by

f1(t) = min

1,

1 1000

, f2(t) = min

0.5, 0.5

t 1000

where

 = 0.03, 0.1, 0.4, 1 (from top to bottom). The middle plots show

the per-step policy regret achieved by SPO ( ) compared to the

algorithm proposed by Heidari et al., 2016 [18] for increasing re-

wards ( ), EXP3 ( ), R-EXP3 ( ), D-UCB ( ), and

SW-UCB ( ). The right-hand plots show the policies these al-

gorithms choose in comparison to the optimal policy ( ).

Reward

Reward

0.6

0.4

0.2
0.0 0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.125 0.100 0.075 0.050 0.025 0.000
0

2500 5000 7500 10000 Time-Horizon

(a)  = 0.1

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

0.6

0.4

0.2
0.0 0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.100 0.075 0.050 0.025 0.000
0

2500 5000 7500 10000 Time-Horizon

(b)  = 0.5

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

0.6

0.4

0.2
0.0 0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.15

0.10

0.05

0.00 0

2500 5000 7500 10000 Time-Horizon

(c)  = 1

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

0.6

0.4

0.2
0.0 0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.25 0.20 0.15 0.10 0.05 0.00
0

2500 5000 7500 10000 Time-Horizon

(d)  = 5

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

Reward

Reward

Figure 6: The left-hand plots show the reward functions given by f1(t) = 1 - t-0.1, f2(t) = 0.5 - 0.5t- where  = 0.1, 0.5, 1, 5 (from top to bottom). The middle plots show the per-step policy regret achieved by SPO ( ) compared to the algorithm proposed by Heidari et al., 2016 [18] for increasing rewards ( ), EXP3 ( ), R-EXP3 ( ), D-UCB ( ), and SW-UCB ( ). The right-hand plots show the policies these algorithms choose in comparison to the optimal policy ( ).

Reward

Reward

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.3
0.2
0.1
0.0 0

2500 5000 7500 10000 Time-Horizon

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

(a) Noise-free observations

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.4 0.3 0.2 0.1 0.0
0

2500 5000 7500 10000 Time-Horizon

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

(b) Noise with  = 0.01

1.00

0.75 0.50

Arm 1 Arm 2

0.25

0.00 0

2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.4 0.3 0.2 0.1 0.0
0

2500 5000 7500 10000 Time-Horizon

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

(c) Noise with  = 0.05

1.0 0.5 0.0
0

Arm 1 Arm 2
2500 5000 7500 10000 Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

2500 5000 7500 10000 Time-Horizon

# Pulls Arm 1

10000 8000 6000 4000 2000
0 0

2500 5000 7500 10000 Time-Horizon

(d) Noise with  = 0.1

Reward

Reward

Figure 7: The left-hand plots show the increasing reward functions defined by f1(t) = 1 - t-0.5, f2(t) = 0.5 - 0.5t- where  = 0.1. We add Gaussian noise to the observations, with  = 0, 0.01, 0.05, 0.1. The middle plots show the per-step policy regret achieved by SPO ( ) compared to the algorithm proposed by Heidari et al., 2016 [18] for increasing rewards ( ), EXP3 ( ), R-EXP3 ( ), D-UCB ( ), and SW-UCB ( ). The right-hand plots show the policies these algorithms choose in comparison to the optimal policy ( ).

Reward

Reward

0.8 0.6 0.4 0.2
0

Arm 1 Arm 2
5000 10000 15000 20000 Number of Pulls

Per-Step Regret

0.15

0.10

0.05

0.00 0

5000 10000 15000 20000 Time-Horizon

# Pulls Arm 1

8000 6000 4000 2000
0 0

(a) Noise-free observations

5000 10000 15000 20000 Time-Horizon

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2
5000 10000 15000 20000 Number of Pulls

Per-Step Regret

0.15

0.10

0.05

0.00 0

5000 10000 15000 20000 Time-Horizon

# Pulls Arm 1

8000 6000 4000 2000
0 0

(b) Noise with  = 0.01

5000 10000 15000 20000 Time-Horizon

1.00 0.75 0.50 0.25 0.00
0

Arm 1 Arm 2
5000 10000 15000 20000 Number of Pulls

Per-Step Regret

0.15 0.10 0.05 0.00
0

5000 10000 15000 20000 Time-Horizon

# Pulls Arm 1

8000 6000 4000 2000
0 0

(c) Noise with  = 0.05

5000 10000 15000 20000 Time-Horizon

Reward

f1(t) = - 0.0015 · e-0.01·(t-600)+ f2(t) = - 0.005 · e-0.009·(t-500)+

-0.95 e-(0.011)·(t-600) + 1 + 1
-0.7 + 0.8
e-(0.0099)·(t-500) + 1

Figure 8: The left-hand plots show the single-peaked reward functions f1 and f2 with simulated Gaussian noise. The middle plots show the per-step policy regret achieved by SPO ( ) compared to EXP3 ( ), R-EXP3 ( ), D-UCB ( ), SW-UCB ( ), a one-step-optimistic ( ), and a greedy algorithm ( ). The right-hand plots show the policies these algorithms choose in comparison to the optimal policy ( ).

Reward

Reward

Arm 1

0.8

Arm 2

0.6

0.4

0.2

0

5000 10000 15000 20000

Number of Pulls

Per-Step Regret

0.4 0.3 0.2 0.1 0.0
0

5000 10000 15000 20000 Time-Horizon

# Pulls Arm 1

6000

4000

2000

0 0

5000 10000 15000 20000 Time-Horizon

(a) Noise-free observations

0.8 0.6 0.4 0.2 0.0
0

Per-Step Regret

Arm 1 Arm 2
0.3
0.2
0.1

# Pulls Arm 1

6000 4000 2000

5000 10000 15000 20000 Number of Pulls

0.0 0

5000 10000 15000 20000 Time-Horizon

0 0

(b) Noise with  = 0.01

5000 10000 15000 20000 Time-Horizon

1.00 0.75 0.50 0.25 0.00
0

Arm 1 Arm 2
5000 10000 15000 20000 Number of Pulls

Per-Step Regret

0.4 0.3 0.2 0.1 0.0
0

5000 10000 15000 20000 Time-Horizon

# Pulls Arm 1

6000 4000 2000
0 0

5000 10000 15000 20000 Time-Horizon

(c) Noise with  = 0.05

Reward

f1(t)

=

-

0.0015

·

e-0.003·(t-600)

+

-0.95 e-(0.004)·(t-600)

+

1

+

1

f2(t)

=

-

0.008

·

e-0.011·(t-400)

+

-0.6 e-(0.012)·(t-400)

+

1

+

0.8

Figure 9: The left-hand plots show the single-peaked reward functions f1 and f2 with simulated Gaussian noise. The middle plots show the per-step policy regret achieved by SPO ( ) compared to EXP3 ( ), R-EXP3 ( ), D-UCB ( ), SW-UCB ( ), a one-step-optimistic ( ), and a greedy algorithm ( ). The right-hand plots show the policies these algorithms choose in comparison to the optimal policy ( ).

Reward

Reward

0.8 0.6 0.4
0

Arm 1 Arm 2
5000 10000 15000 20000 Number of Pulls

Per-Step Regret

0.15

0.10

0.05

0.00 0

5000 10000 15000 20000 Time-Horizon

# Pulls Arm 1

20000 15000 10000 5000
0 0

5000 10000 15000 20000 Time-Horizon

(a) Noise-free observations

1.0 0.8 0.6 0.4
0

Arm 1 Arm 2
5000 10000 15000 20000 Number of Pulls

Per-Step Regret

0.15

0.10

0.05

0.00 0

5000 10000 15000 20000 Time-Horizon

# Pulls Arm 1

20000 15000 10000 5000
0 0

5000 10000 15000 20000 Time-Horizon

(b) Noise with  = 0.01

1.0 0.8 0.6 0.4
0

# Pulls Arm 1

Per-Step Regret

Arm 1 Arm 2

0.15

0.10

0.05

15000 10000 5000

5000 10000 15000 20000 Number of Pulls

0.00 0

5000 10000 15000 20000 Time-Horizon

0

0

5000 10000 15000 20000

Time-Horizon

(c) Noise with  = 0.05

Reward

f1(t)

=

-

0.0015

·

e-0.01·(t-600)

+

-0.5 e-(0.011)·(t-600)

+

1

+

1

f2(t)

=

-

0.005

·

e-0.009·(t-500)

+

-0.2 e-(0.0099)·(t-500)

+

1

+

0.8

Figure 10: The left-hand plots show the single-peaked reward functions f1 and f2 with simulated Gaussian noise. The middle plots show the per-step policy regret achieved by SPO ( ) compared to EXP3 ( ), R-EXP3 ( ), D-UCB ( ), SW-UCB ( ), a one-step-optimistic ( ), and a greedy algorithm ( ). The right-hand plots show the policies these algorithms choose in comparison to the optimal policy ( ).

Cumulative Regret

250 200 150 100 50
0 0

2000

4000

Time-Horizon

(a)  = 0.01

Cumulative Regret

200

100

0 0

2000

4000

Time-Horizon

(b)  = 0.05

300

200

100

0 0

2000

4000

Time-Horizon

(c)  = 0.1

Cumulative Regret

Figure 11: Regret of SPO ( ) compared to UCB ( ) on a Gaussian multi-armed bandit for different time horizons. We show the mean of the regret over 30 instances of MABs with 10 arms with means uniformly sampled between 0 and 1.

Reward

Reward

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2 Arm 3 Arm 4
500 1000 1500 2000 Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

500 1000 1500 2000 Time-Horizon

(a) Noise-free observations

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2 Arm 3 Arm 4
500 1000 1500 2000 Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

500 1000 1500 2000 Time-Horizon

(b) Noise with  = 0.01

1.00 0.75 0.50 0.25 0.00
0

Arm 1 Arm 2 Arm 3 Arm 4
500 1000 1500 2000 Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

500 1000 1500 2000 Time-Horizon

(c) Noise with  = 0.05

Reward

Figure 12: The left-hand plots show the reward functions defined as the utility of receiving a loan for different social groups using the FICO dataset, as described in the main text. We add simulated Gaussian noise with different variances. The right-hand plots show the per-step policy regret achieved by SPO ( ) compared to EXP3 ( ), R-EXP3 ( ), D-UCB ( ), SW-UCB ( ), a one-stepoptimistic ( ), and a greedy algorithm ( ).

Reward

Reward

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2 Arm 3 Arm 4

0.100 0.075

Per-Step Regret

0.050

0.025

500 1000 1500 2000 Number of Pulls

0.000 0

500 1000 1500 2000 Time-Horizon

(a) Noise-free observations

1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2 Arm 3 Arm 4
500 1000 1500 2000 Number of Pulls

Per-Step Regret

0.10 0.08 0.06 0.04 0.02 0.00
0

500 1000 1500 2000 Time-Horizon

(b) Noise with  = 0.01

1.00 0.75 0.50 0.25 0.00 -0.25
0

Arm 1 Arm 2 Arm 3 Arm 4
500 1000 1500 2000 Number of Pulls

Per-Step Regret

0.10 0.08 0.06 0.04 0.02 0.00
0

500 1000 1500 2000 Time-Horizon

(c) Noise with  = 0.05

Reward

Figure 13: The left-hand plots show the reward functions defined as the bank's utility of giving a loan. We add simulated Gaussian noise with different variances. The right-hand plots show the per-step policy regret achieved by SPO ( ) compared to EXP3 ( ), R-EXP3 ( ), D-UCB ( ), SW-UCB ( ), a one-stepoptimistic ( ), and a greedy algorithm ( ).

Reward

Reward

1.0 0.8 0.6 0.4 0.2 0.0
0
1.0 0.8 0.6 0.4 0.2 0.0
0

Per-Step Regret

Arm 1

Arm 2

0.6

Arm 3

Arm 4

0.4

0.2

1000

2000

3000

Number of Pulls

0.0 0

1000

2000

3000

Time-Horizon

(a) Noise-free observations

Arm 1 Arm 2 Arm 3 Arm 4

1000

2000

3000

Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

1000

2000

3000

Time-Horizon

(b) Noise with  = 0.01

1.00 0.75 0.50 0.25 0.00
0

Arm 1 Arm 2 Arm 3 Arm 4

1000

2000

3000

Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

1000

2000

3000

Time-Horizon

(c) Noise with  = 0.05

Reward

Reward

Reward

1.0 0.8 0.6 0.4 0.2 0.0
0
1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1

0.6

Arm 2

Arm 3

Arm 4

0.4

Per-Step Regret

0.2

1000

2000

3000

Number of Pulls

0.0 0

1000

2000

3000

Time-Horizon

(a) Noise-free observations

Arm 1

Arm 2

0.4

Arm 3

Arm 4

0.3

Per-Step Regret

0.2

0.1

1000

2000

3000

Number of Pulls

0.0 0

1000

2000

3000

Time-Horizon

(b) Noise with  = 0.01

1.00 0.75 0.50 0.25 0.00
0

Arm 1

Arm 2

0.4

Arm 3

Arm 4

0.3

Per-Step Regret

0.2

0.1

1000

2000

3000

Number of Pulls

0.0 0

1000

2000

3000

Time-Horizon

(c) Noise with  = 0.05

Reward

Figure 14: The left-hand plots show the reward function of instance A of the simulated recommender system with simulated Gaussian noise. The right-hand plots show the per-step policy regret achieved by SPO ( ) compared to EXP3 ( ), R-EXP3 ( ), D-UCB ( ), SW-UCB ( ), a one-step-optimistic ( ), and a greedy algorithm ( ).

Figure 15: The left-hand plots show the reward function of instance B of the simulated recommender system with simulated Gaussian noise. The right-hand plots show the per-step policy regret achieved by SPO ( ) compared to EXP3 ( ), R-EXP3 ( ), D-UCB ( ), SW-UCB ( ), a one-step-optimistic ( ), and a greedy algorithm ( ).

Reward

Reward

1.0 0.8 0.6 0.4 0.2 0.0
0
1.0 0.8 0.6 0.4 0.2 0.0
0

Arm 1 Arm 2 Arm 3 Arm 4

1000

2000

3000

Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

1000

2000

3000

Time-Horizon

(a) Noise-free observations

Arm 1 Arm 2 Arm 3 Arm 4

1000

2000

3000

Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

1000

2000

3000

Time-Horizon

(b) Noise with  = 0.01

1.00 0.75 0.50 0.25 0.00 -0.25
0

Arm 1 Arm 2 Arm 3 Arm 4

1000

2000

3000

Number of Pulls

Per-Step Regret

0.5 0.4 0.3 0.2 0.1 0.0
0

1000

2000

3000

Time-Horizon

(c) Noise with  = 0.05

Reward

Figure 16: The left-hand plots show the reward function of instance C of the simulated recommender system with simulated Gaussian noise. The right-hand plots show the per-step policy regret achieved by SPO ( ) compared to EXP3 ( ), R-EXP3 ( ), D-UCB ( ), SW-UCB ( ), a one-step-optimistic ( ), and a greedy algorithm ( ).

