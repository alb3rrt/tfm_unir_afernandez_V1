MINIMALIST: Mutual INformatIon Maximization for Amortized Likelihood Inference from Sampled Trajectories

arXiv:2106.01808v1 [cs.LG] 3 Jun 2021

Giulio Isacchini Max Planck Institute for Dynamics
and Self-organization, Am Faßberg 17, 37077 Göttingen, Germany
Laboratoire de physique
de l'École Normale Supérieure, CNRS, PSL University, Sorbonne Université,
and Université de Paris, 75005 Paris, France
giulioisac@gmail.com

Natanael Spisak Laboratoire de physique
de l'École Normale Supérieure, CNRS, PSL University, Sorbonne Université,
and Université de Paris, 75005 Paris, France
natanael.spisak@gmail.com

Armita Nourmohammad Max Planck Institute for Dynamics and Self-organization,
Am Faßberg 17, 37077 Göttingen, Germany Department of Physics, University of Washington, 3910 15th Ave Northeast, Seattle, WA 98195, USA
Fred Hutchinson Cancer Research Center, 1100 Fairview ave N, Seattle, WA 98109, USA
armita@uw.edu

Thierry Mora Laboratoire de physique
de l'École Normale Supérieure, CNRS, PSL University, Sorbonne Université,
and Université de Paris, 75005 Paris, France
thierry.mora@gmail.com

Aleksandra M. Walczak Laboratoire de physique
de l'École Normale Supérieure, CNRS, PSL University, Sorbonne Université,
and Université de Paris, 75005 Paris, France
aleksandra.walczak@phys.ens.fr

Abstract
Simulation-based inference enables learning the parameters of a model even when its likelihood cannot be computed in practice. One class of methods uses data simulated with different parameters to infer an amortized estimator for the likelihood-toevidence ratio, or equivalently the posterior function. We show that this approach can be formulated in terms of mutual information maximization between model parameters and simulated data. We use this equivalence to reinterpret existing approaches for amortized inference, and propose two new methods that rely on lower bounds of the mutual information. We apply our framework to the inference of parameters of stochastic processes and chaotic dynamical systems from sampled trajectories, using artificial neural networks for posterior prediction. Our approach
These authors contributed equally These authors contributed equally
Preprint. Under review.

provides a unified framework that leverages the power of mutual information estimators for inference.

Introduction
Model-based Bayesian inference relies on the probabilistic description of a process. Traditional methods rely on computing the likelihood of the observed data given the model parameters, in order to maximize or sample from the posterior. For many models, in particular with multiple interacting degrees of freedom or hidden variables, the likelihood function may be impractical to evaluate. In cases where drawing data from the generative process is possible, simulation-based inference techniques can be used as a powerful alternative approach for characterizing the underlying model.
Population genetics provides many examples of such problems. The observed quantities in this context are often based on sequencing data and are "far" from the quantities described by population dynamics models: it is in principle possible to write down likelihood functions, but they typically depend on a number of hidden variables that need to be marginalized out, making their evaluation impractical. Approximate Bayesian Computation (ABC) was first used for posterior inference in the context of population genetics [1], and since then numerous new approaches to simulation-based inference have been developed to answer particular questions of phylodynamics and sequencing data analysis.
More broadly, methods for simulation-based inference can be organized in two classes [2, 3]. In the first class, observations and simulated data are compared within the inference process; examples include ABC [4, 5] and Probabilistic Programming [6, 7]. The methods belonging to the second class proceed in two stages. First, they use a large number of simulations to learn an approximate model for the likelihood function [8, 9], or alternatively the posterior function [10], or the likelihood ratio [11, 12]) that is amortized over the simulation examples. The amortized model is then used to evaluate the likelihood of observations and to evaluate the posterior for model parameters. With theoretical developments in machine learning and improvements in computing power, this class of models has seen a renewed interest in the past years.
Here, we propose a method to evaluate the likelihood-to-evidence ratio, by mapping the problem onto the inference of a Boltzmann-Gibbs density function. We show that the maximum-likelihood estimation of this density (where here likelihood refers to the meta-likelihood of simulated data given a density function proposal) is equivalent to the maximization of mutual information between parameters of the simulation and the simulated data. We use this equivalence to develop an inference technique for posterior prediction, based on the optimization of artificial neural networks. We compare our method to recent work by Hermans et al. [12], where a model of the likelihood-to-evidence ratio is learned through the optimization of a binary classifier operating on simulated data. We assess the accuracy of our method to infer the parameters of 3 types of stochastic processes (Ornstein-Uhlenbeck model, birth-death dynamics, and the Susceptible-Infected-Recovered model of epidemiology) as well as one example of chaotic dynamical system, the Lorenz attractor.

Methods

We aim to estimate parameters  of a model given a set of data x obtained from stochastic simulation of that model, P (x|), with a prior P (). First, we reinterpret the likelihood-to-evidence ratio in
terms of a Boltzmann-Gibbs density

P (x|) P (x)

=

P (x, ) P (x)P ()



Pjoint(x, ) Pindep(x, )

=

1 Z

e-E(x,)

,

(1)

where the "energy" E(x, ) captures how the joint distribution of data and parameters Pjoint(x, ) =

P (x|)P () deviates from the independent distribution Pindep(x, ) = P (x)P (). The energy

E(x, ) is generally a non-linear function describing the dependence between data and parameters.

Z is the "partition function", which ensures that the probability density P (x, ) is normalized. E

and Z are each defined up to constants (additive for E, multiplicative for Z) that compensate each

other. Given the energy function, E(x, ) we recover the posterior probability density P (|x) =

1 Z

e-E(x,)

P

().

2

A

Pjoint <latexit sha1_base64="dpwRKCaj3dSAWkJYyDpicmAlN+s=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoPgKexGQY9BLx4jmAckS5idTJIx81hnZgNhyXd48aCIVz/Gm3/jJNmDJhY0FFXddHdFMWfG+v63l1tb39jcym8Xdnb39g+Kh0cNoxJNaJ0ornQrwoZyJmndMstpK9YUi4jTZjS6nfnNMdWGKflgJzENBR5I1mcEWyeFtW7a0QI9KibttFss+WV/DrRKgoyUIEOtW/zq9BRJBJWWcGxMO/BjG6ZYW0Y4nRY6iaExJiM8oG1HJRbUhOn86Ck6c0oP9ZV2JS2aq78nUiyMmYjIdQpsh2bZm4n/ee3E9q/DlMk4sVSSxaJ+wpFVaJYA6jFNieUTRzDRzN2KyBBrTKzLqeBCCJZfXiWNSjm4KFfuL0vVmyyOPJzAKZxDAFdQhTuoQR0IPMEzvMKbN/ZevHfvY9Ga87KZY/gD7/MH2H2SJg==</latexit>

J
<latexit sha1_base64="sMKk44QuIYrSDVLLHZjAqNYer0g=">AAACB3icbVDLSsNAFJ3UV62vqEtBgkWoICWpgm6EohtxVcE+oAlhMp22QycPZm7EErJz46+4caGIW3/BnX/jpM1CWw9cOJxzL/fe40WcSTDNb62wsLi0vFJcLa2tb2xu6ds7LRnGgtAmCXkoOh6WlLOANoEBp51IUOx7nLa90VXmt++pkCwM7mAcUcfHg4D1GcGgJFfft30MQ4J5cpNe2EnlwWXHNgwpYJcd2amrl82qOYExT6yclFGOhqt/2b2QxD4NgHAsZdcyI3ASLIARTtOSHUsaYTLCA9pVNMA+lU4y+SM1DpXSM/qhUBWAMVF/TyTYl3Lse6ozu1rOepn4n9eNoX/uJCyIYqABmS7qx9yA0MhCMXpMUAJ8rAgmgqlbDTLEAhNQ0ZVUCNbsy/OkVataJ9Xa7Wm5fpnHUUR76ABVkIXOUB1dowZqIoIe0TN6RW/ak/aivWsf09aCls/soj/QPn8AvCmZNw==</latexit>

= {(xi, i)}

I
<latexit sha1_base64="ePQHK0+2qZmI6udAFvOpCj6NSqg=">AAACD3icbVDLSgNBEJz1GeMr6tHLYFASkLAbBb0IQS96i2AekA3L7GQ2GTL7YKZXDMv+gRd/xYsHRbx69ebfOJvkoIkFDUVVN91dbiS4AtP8NhYWl5ZXVnNr+fWNza3tws5uU4WxpKxBQxHKtksUEzxgDeAgWDuSjPiuYC13eJX5rXsmFQ+DOxhFrOuTfsA9TgloySkc2T6BASUiuUnxhZ2UHhx+bMOAAXESO+IlXk7LduoUimbFHAPPE2tKimiKulP4snshjX0WABVEqY5lRtBNiAROBUvzdqxYROiQ9FlH04D4THWT8T8pPtRKD3uh1BUAHqu/JxLiKzXyXd2ZXa9mvUz8z+vE4J13Ex5EMbCAThZ5scAQ4iwc3OOSURAjTQiVXN+K6YBIQkFHmNchWLMvz5NmtWKdVKq3p8Xa5TSOHNpHB6iELHSGauga1VEDUfSIntErejOejBfj3fiYtC4Y05k99AfG5w88EJwk</latexit>

= {(xi, (i))}

sample <latexit sha1_base64="+qfakRwcSn0MvUPZfGzxr/dbqaM=">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZkq6LLoxmUF+4DOUDJppg1NMiHJCGXob7hxoYhbf8adf2PazkJbD1w4nHNvcu+JFWfG+v63t7a+sbm1Xdop7+7tHxxWjo7bJs00oS2S8lR3Y2woZ5K2LLOcdpWmWMScduLx3czvPFFtWCof7UTRSOChZAkj2DopzEMtkMFCcTrtV6p+zZ8DrZKgIFUo0OxXvsJBSjJBpSUcG9MLfGWjHGvLiHuvHGaGKkzGeEh7jkosqIny+c5TdO6UAUpS7UpaNFd/T+RYGDMRsesU2I7MsjcT//N6mU1uopxJlVkqyeKjJOPIpmgWABowTYnlE0cw0cztisgIa0ysi6nsQgiWT14l7XotuKzVH66qjdsijhKcwhlcQADX0IB7aEILCCh4hld48zLvxXv3Phata14xcwJ/4H3+ADU/kcs=</latexit>

shue <latexit sha1_base64="VztlJcZYZQzckvNvsgWGk39DQ4U=">AAAB9HicbVDLSgMxFL3xWeur6tJNsAiuykwVdFl047KCfUA7lEyaaUOTzJhkCmXod7hxoYhbP8adf2PazkJbD1w4nHNvcu8JE8GN9bxvtLa+sbm1Xdgp7u7tHxyWjo6bJk41ZQ0ai1i3Q2KY4Io1LLeCtRPNiAwFa4Wju5nfGjNteKwe7SRhgSQDxSNOiXVSkHW1xGaYRpFg016p7FW8OfAq8XNShhz1Xumr249pKpmyVBBjOr6X2CAj2nLq3it2U8MSQkdkwDqOKiKZCbL50lN87pQ+jmLtSlk8V39PZEQaM5Gh65TEDs2yNxP/8zqpjW6CjKsktUzRxUdRKrCN8SwB3OeaUSsmjhCqudsV0yHRhFqXU9GF4C+fvEqa1Yp/Wak+XJVrt3kcBTiFM7gAH66hBvdQhwZQeIJneIU3NEYv6B19LFrXUD5zAn+APn8AAVeSQA==</latexit>

B

Pjoint <latexit sha1_base64="dpwRKCaj3dSAWkJYyDpicmAlN+s=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoPgKexGQY9BLx4jmAckS5idTJIx81hnZgNhyXd48aCIVz/Gm3/jJNmDJhY0FFXddHdFMWfG+v63l1tb39jcym8Xdnb39g+Kh0cNoxJNaJ0ornQrwoZyJmndMstpK9YUi4jTZjS6nfnNMdWGKflgJzENBR5I1mcEWyeFtW7a0QI9KibttFss+WV/DrRKgoyUIEOtW/zq9BRJBJWWcGxMO/BjG6ZYW0Y4nRY6iaExJiM8oG1HJRbUhOn86Ck6c0oP9ZV2JS2aq78nUiyMmYjIdQpsh2bZm4n/ee3E9q/DlMk4sVSSxaJ+wpFVaJYA6jFNieUTRzDRzN2KyBBrTKzLqeBCCJZfXiWNSjm4KFfuL0vVmyyOPJzAKZxDAFdQhTuoQR0IPMEzvMKbN/ZevHfvY9Ga87KZY/gD7/MH2H2SJg==</latexit>

density <latexit sha1_base64="e1KyaE+2pT5gDDyZ757yyKvF0zk=">AAAB9HicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGC/YA2lM1m0y7d3cTdTSGE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDMvSDjTxnW/nbX1jc2t7dJOeXdv/+CwcnTc1nGqCG2RmMeqG2BNOZO0ZZjhtJsoikXAaScY3838zoQqzWL5aLKE+gIPJYsYwcZKft5XAoVUamay6aBSdWvuHGiVeAWpQoHmoPLVD2OSCioN4Vjrnucmxs+xMoxwOi33U00TTMZ4SHuWSiyo9vP50VN0bpUQRbGyJQ2aq78nciy0zkRgOwU2I73szcT/vF5qohs/ZzJJDZVksShKOTIxmiWAQqYoMTyzBBPF7K2IjLDCxNicyjYEb/nlVdKu17zLWv3hqtq4LeIowSmcwQV4cA0NuIcmtIDAEzzDK7w5E+fFeXc+Fq1rTjFzAn/gfP4AHeaSUw==</latexit>

simulator <latexit sha1_base64="TAPW5YnJ68FuRZD5vxAbAGs3A/E=">AAAB+HicbVDLSgNBEOz1GeMjqx69DAbBU9iNgh6DXjxGMA9IljA7mSRDZmaXeQhxyZd48aCIVz/Fm3/jJNmDJhY0FFXddHfFKWfaBMG3t7a+sbm1Xdgp7u7tH5T8w6OmTqwitEESnqh2jDXlTNKGYYbTdqooFjGnrXh8O/Nbj1RplsgHM0lpJPBQsgEj2Dip55eyrhJIM2E5Noma9vxyUAnmQKskzEkZctR7/le3nxArqDSEY607YZCaKMPKMMLptNi1mqaYjPGQdhyVWFAdZfPDp+jMKX00SJQradBc/T2RYaH1RMSuU2Az0sveTPzP61gzuI4yJlNrqCSLRQPLkUnQLAXUZ4oSwyeOYKKYuxWREVaYGJdV0YUQLr+8SprVSnhRqd5flms3eRwFOIFTOIcQrqAGd1CHBhCw8Ayv8OY9eS/eu/exaF3z8plj+APv8wc+j5N4</latexit>

(xi, j) <latexit sha1_base64="9bjyfSncME+SUW7zoN1ZhtP9ypU=">AAACCXicbVA9SwNBEN3zM8avqKXNYhAiSLiLgpZBG8sIRoVcOPY2E7O6u3fszonhSGvjX7GxUMTWf2Dnv3ETU6jxwcDjvRlm5sWpFBZ9/9Obmp6ZnZsvLBQXl5ZXVktr6+c2yQyHJk9kYi5jZkEKDU0UKOEyNcBULOEivjke+he3YKxI9Bn2U2grdqVFV3CGTopKtHIXid0Qe4Asut4JrVC0EeWhUVToDqSDqFT2q/4IdJIEY1ImYzSi0kfYSXimQCOXzNpW4KfYzplBwSUMimFmIWX8hl1By1HNFNh2PvpkQLed0qHdxLjSSEfqz4mcKWv7KnadimHP/vWG4n9eK8PuYTsXOs0QNP9e1M0kxYQOY6EdYYCj7DvCuBHuVsp7zDCOLryiCyH4+/IkOa9Vg71q7XS/XD8ax1Egm2SLVEhADkidnJAGaRJO7skjeSYv3oP35L16b9+tU954ZoP8gvf+Bb9/mb4=</latexit>



Pindep

I(
<latexit sha1_base64="YCCr1SYdb5ZRJe+DV6Dst562isg=">AAACQXicbVA9SwNBFNzzM8avqKXNYhBikXCngpaiBIyVgtHA3SXsbV6SJXsf7u4FwnF/zcZ/YGdvY6GIrY17SQo1GVgYZubx3o4XcSaVab4Yc/MLi0vLuZX86tr6xmZha/tOhrGgUKchD0XDIxI4C6CumOLQiAQQ3+Nw7/UvMv9+AEKyMLhVwwhcn3QD1mGUKC21Co1ayYl67MCBh5gNyo5PVM/zkmraGlFKeHKV2tVmFnIxLmOHh108K1ZLbWgm5XE0dVuFolkxR8DTxJqQIprgulV4dtohjX0IFOVEStsyI+UmRChGOaR5J5YQEdonXbA1DYgP0k1GDaR4Xytt3AmFfoHCI/X3REJ8KYe+p5PZvfK/l4mzPDtWnVM3YUEUKwjoeFEn5liFOKsTt5kAqvhQE0IF07di2iOCUKVLz+sSrP9fniZ3hxXrqHJ4c1w8O5/UkUO7aA+VkIVO0Bm6RNeojih6RK/oHX0YT8ab8Wl8jaNzxmRmB/2B8f0Dj8Ww5w==</latexit>

)

EJ [E ]

log EI [e E ]

energy <latexit sha1_base64="gSr4NXp7uDeZMR41lZsbg2kCNO8=">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGC/YAmlM122i7dbMLuRgihf8OLB0W8+me8+W/ctjlo64OBx3szzMwLE8G1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1Q6pRcIktw43AbqKQRqHATji5m/mdJ1Sax/LRZAkGER1JPuSMGiv5ua8ighLVKJv2K1W35s5BVolXkCoUaPYrX/4gZmmE0jBBte55bmKCnCrDmcBp2U81JpRN6Ah7lkoaoQ7y+c1Tcm6VARnGypY0ZK7+nshppHUWhbYzomasl72Z+J/XS83wJsi5TFKDki0WDVNBTExmAZABV8iMyCyhTHF7K2FjqigzNqayDcFbfnmVtOs177JWf7iqNm6LOEpwCmdwAR5cQwPuoQktYJDAM7zCm5M6L86787FoXXOKmRP4A+fzB0E8kdM=</latexit>

Figure 1: (A) Schematic of the method. We first sample parameters  from the prior P () and then sample observations x from the simulator P (x|) to obtain pairs (xi, i). In order to generate pairs (xi, j) from the independent distribution we shuffle the two initial vectors. Both sets are used to infer a model of energy E by maximizing a log-likelihood in (5) with  parameters of the artificial
neural network used for the inference. (B) Distribution of energies of independent and joint pairs. Pairs from the joint distribution have lower energy E, while pairs from the independent distribution
have higher energy, as the majority of these independent samples are relatively unlikely under a joint
model (2).

We will now describe an amortized inference scheme to learn a model of the energy function from simulated data, relying on the flexibility of artificial neural networks. Specificially, we approximate the energy E by a multi-layered network E characterized by a set of parameters . Under a given model E, the joint distribution is approximated as:

Pjoint(x, )

=

1 Z

e-E



(x,)

Pindep

(x,

).

(2)

We simulate samples from the joint distribution, denoted J = {(xi, i)}Ni=1 by drawing a model parameter from a prior distribution, i  P () and simulating xi  P (x|i).

To learn the neural network parameters , we need to maximize the (meta-)log-likelihood of the simulated sample J under a given model E:

L(; J ) = N EJ [log Pjoint] = N -EJ [E] - log Z + EJ [log Pindep] .

(3)

where EJ [·] denotes the empirical average over samples J . Note that this meta-likelihood is distinct from the original likelihood P (x|) as it involves drawing both model parameters and data together, multiple times, given a model of the energy E parametrized by . The partition function is approximated using importance sampling on samples drawn from Pindep:

Z = e-E(x,)Pindep(x, )dxd  EI [e-E ],

(4)

where EI[·] is the counting measure over a large set I of independently drawn parameter and data pairs (x, )  Pindep(x, ) = P ()P (x). In practice, I may be obtained by shuffling the indices of J [12], I = {(xi, (i))}, where  is a random permutation of N elements, possibly multiple times. Counting all possible combinations, the set I can have maximal size max(NI) = N 2 - N under a fixed simulation budget. We denote the relative size of the two sets by k = NI/N .
With this estimate of Z, and noting that the last term of eq. 3 does not depend on , the problem boils down to maximizing:

I(; I, J )  -EJ [E] - log EI [e-E ] N  = 

Pjoint(x,

)

log

Pjoint(x, ) Pindep(x, )

dx

d



I,

(5)

where I = Pjoint(x, ) log[Pjoint(x, )/Pindep(x, )] dx d is the mutual information between x and . This lower bound is known as the Donsker-Varadhan representation [13] of the Kullback-
Leibler divergence used in the definition of the mutual information. This bound was extensively
studied in Ref. [13] as an estimate of the mutual information from discrete samples drawn from

3

joint distributions. Here, we will use this representation to learn the energy function E, which approximates the likelihood-to-evidence ratio. We refer to this method as Mutual Information Neural Estimation (MINE).

The connection to mutual information estimation established above opens the possibility of employing other empirical mutual information estimates to perform simulation-based-inference. An alternative lower bound to I, first introduced in [14], is the so called f-divergence representation (FDIV),

Lf (; I, J )  -EJ [E] - EI [e-E-1].

(6)

This estimator defines an alternative objective function to (5) that can be used to infer an optimal energy model E. Note that in the limit of infinite data N  , and when the class of models {E} is able to represent the true E exactly, the maxima of (5) and (6) are both reached at the true value of E(x, ) where they give the true value of I. Outside of this limit, using one of these objective functions may prove more beneficial. In particular, the second term of Lf (; I, J ) and its gradients may be reliably estimated by averaging over small batches, unlike the second term of I(; I, J )
because of the logarithm, giving FDIV an advantage for stochastic gradient descent algorithms. While the Donsker-Varadhan bound on the mutual information is tighter, i.e. Lf (; I, J )  I(; I, J ) holds for N   [13], it is unclear whether it might produce a more reliable estimate of E(x, ).

A third alternative is to use the original approach for the likelihood-to-evidence ratio estimation

proposed in Ref. [12]. We introduce their method in a more general setting in order to clarify its

relationship to our likelihoods. The energy E(x, ) may be rewritten in terms of a classifier between

the two hypotheses of (x, ) originating from the joint or independent distribution in a mixture

Pmix

=

1 k+1

Pjoint

+

k k+1

Pindep

(in Ref.

[12] k

=

1).

We define:

d(x, )



P (joint|x, )

=

Pjoint(x, ) Pjoint(x, ) + kPindep(x, )

=

1

+

1 kZ eE (x,)

.

(7)

The classifier is parametrized by a neural network, d = d and is trained by minimizing the binary cross entropy

S(; I, J ) = -EJ [log d] - kEI[log 1 - d ].

(8)

Similarly to objectives (5) and (6), in the N   limit and when the class of {d} models contains the true d, this cross entropy is minimized at the true value d(x, ). In Ref. [15] it was shown that
it can also be used as an estimator of mutual information by computing the mean logarithm of the predicted likelihood-to-evidence ratio, EJ [log(kd/(1 - d))]. For high-dimensional random variables, this sets a tighter lower bound and a more accurate estimate of the mutual information than
the f-divergence estimator. However, its accuracy was not directly compared to the proposed estimator
in eq. 5. We will refer to the inference approach based on minimizing the binary cross-entropy loss in
eq. 8 as BCE.

The three methods described so far are connected to the Noise Contrastive Estimation (NCE) framework for density ratio approximation introduced in Refs. [16, 17]. The NCE methods are used for density estimation by comparing the data to a contrastive set of samples from a reference noise distribution. These techniques have been already applied for simulation-based inference, particularly for sequential posterior inference [18], see Supplementary Material for a more detailed discussion.

Finally, once an energy model E has been trained by optimizing I(; I, J ), Lf (; I, J ), or S(; I, J ) over , the posterior of parameters given an observation x may be calculated as P (|x) = (1/Z)e-E(x,)P (). When  is of high dimension, scanning the posterior for all possible values of  may be impractical. In that case, we generate samples of  from the posterior using a Markov-Chain
Monte-Carlo method with Metropolis-Hasting acceptance probability:

( -  ) = min

1,

q( |)P ( ) q(| )P ()

e-(E(x,

)-E(x,))

,

(9)

with q an ergodic Markov transition probability in the parameter space. This procedure generalizes in a straightforward way to the case of multiple observations drawn with the same set of parameters.

In the next section we will use four different tasks to compare the inference accuracy based on the three introduced approaches, MINE (eq. 5), FDIV (eq. 6), and BCE (eq. 8).

4

x n n Posterior Density

A Ornstein Uhlenbeck C ×102 BirthDeath

3

1.2

2

1

1.0

E
1.0

×102

SIR

0.5

G Lorenz attractor
S I R

0

0.0

0.0 0.2 0.4

0.0 0.2 0.4

0

20 40

B0

time

D2

time

F0

time

H ×10 1
3

2

true value 210

10 22

20 10

1

0

1 30

40

Figure 2: Example trajectories (A,C,E,G) and posterior inference (B,D,F,H) for Ornstein-Uhlenbeck (A,B), Birth-Death (C,D), SIR (E,F) and Lorenz attractor (G,H). Trajectories are simulated with the parameter marked in red on the posterior plots. Circles indicate the discrete observations used for inference. Posteriors where estimated over 10 trajectories.

Experiments

We set out to examine the 3 presented methods for estimating the likelihood-to-evidence ratio (MINE, FDIV, and BCE) to infer the parameters of simple dynamical models from discrete samples of their trajectories. We chose 4 contexts that together encompass the range of difficulties in the inference of model parameters: (i) the stochastic birth-death process, (ii) the epidemiological Susceptible-InfectedRecovered (SIR) process, (iii) the multidimensional Ornstein-Uhlenbeck process, and (iv) the chaotic system of Lorenz attractor. Example trajectories of each model are shown in Fig. 2.

Ornstein-Uhlenbeck process. The Ornstein-Uhlenbeck (OU) process is a multidimensional

Markov process driven by additive Gaussian white noise. It is applied in many branches of sci-

ence, notably to describe the velocity of a Brownian particle [19], the fluctuations of interest rates

[20] or evolution of continuous phenotypic traits [21, 22]. The trajectories are a solution of a

stochastic differential equation:



dx = - (x - µ) dt + 2dW,

(10)

where µ is the stationary mean and  is the damping matrix, assumed to be symmetric. W stands for the multidimensional Wiener process, and  is the noise amplitude. We use the Euler-Maruyama integration scheme to obtain the numerical solutions of this equation [23]. The corresponding Fokker-Planck equation for this process can be solved to obtain the true posterior (see Supplementary Material).

In one dimension, we infer the mean µ and the noise strength , setting  = 1 and using uniform

priors P (µ) = U(-10, 10) and P () = U(0, 2). In higher dimensions, we fix µ = 0,  = I, where I is the identity matrix, and infer the damping matrix parametrized as  = I + g, where g is a

Gaussian orthogonal matrix and < 1, which ensures that the damping matrix is positive definite

(see Supplementary Material for more details). Since  is symmetric, in the d-dimensional case we

have

d 2

parameters to infer. The prior is given by the Gaussian Orthogonal Ensemble distribution

density

P (g)

=

1 z(d)

e-

d 4

Tr(g2

)

.

Birth-death process. The birth-death process is a discrete one-dimensional Markov process with multiplicative demographic noise. The number of individuals n is subject to variation due to stochastic

5

birth and death events occurring at rates n and n, respectively,

n -n- n + 1, n -n n - 1.

(11)

We use the Gillespie algorithm to sample trajectories from this process [24]. We parametrize the process with the average exponential drift  =  - , and the noise timescale  =  + . We use uniform priors for both of these variables: P () = U(-2, 2) and P () = U(2, 20).

SIR model. The Susceptible-Infected-Recovered (SIR) model is a staple of epidemiological modelling. Any member of susceptible population S can be infected at rate  upon a contact with one of I infected individuals. The infected individuals can become resistant R at a rate :

S + I - 2I, I - R.

(12)

We simulate the trajectories of the SIR model using the Gillespie algorithm [24]. We infer the rates  and  under uniform priors P () = P () = U(0, 1) given samples from the (S, I) trajectories.

Lorenz attractor. The Lorenz system is a 3-dimensional chaotic system governed by the equations,

x = (y - x), y = x( - z) - y, z = xy - z.

(13)

We simulate this deterministic process starting from a random position (x0 + , y0, z0), where  is the noise in the initial position drawn from a uniform distribution,   U(-0.1, 0.1). We fix the parameters  = 10 and  = 8/3 and set out to infer . The ensemble of trajectories starting in the vicinity of x0 diverge with the characteristic time set by the inverse of the largest Lyapunov exponent of the system  = (). We start sampling from the trajectories at a random initial time drawn from a Gamma distribution, t0  (k = 5,  = 2). We then take 5 samples from each trajectory at time windows that are larger than the characteristic time for chaotic divergence t = 2-1. We set  0.905, which is the Lyapunov exponent for  = 28, a transition point where some but not all the solutions of the Lorenz system are chaotic. We infer the parameter  in a chaotic regime using a uniform prior P () = U(30, 40).

The artificial neural networks used for all 3 methods were multilayer perceptrons [25] with two hidden layers and hyperbolic tangent activation function. This architecture choice was found to be expressive enough across tasks and, thanks to its simplicity, we could perform a well grounded comparison of the three methods without advanced regularisation techniques (see Supplementary Material for details on hyperparameters choices). The methods were implemented using Tensorflow [26, 27] with extensive use of Numpy [28] and Scipy [29] libraries.

Results

Given enough data and a powerful enough neural network, we expect the optima of the objective functions I, Lf and S to converge, and the estimated energy function should approach the true value.
To study the convergence of the posterior functions we choose a hypothesis  and simulate M trajectories, x1:M = {xm}M m=1, xm  P (x|) with M = 2 for SIR, and M = 5 for the other tasks. We evaluate the posteriors

P^l(|x1:M )

=

1 (Zl)M

exp

M

-

El(xm, )

m=1

P (),

(14)

with l indexing one of the three methods (MINE, FDIV, or BCE) and  is the optimal one for each method (we dropped the explicit dependency on  in P^l for ease of notation). The posteriors converge when the amortized inference is done on a training set with at least N = 107 samples (and 107 samples for validation); see Fig. 3. We define a reference posterior P (|x1:M )  P^l(|x1:M ) l, obtained with N = 107 as the average over three estimators. In the case of Ornstein-Uhlenbeck process, where the true posterior P (|x1:M ) can be calculated analytically, P (|x1:M ) agrees with
the analytical prediction. This first result confirms the validity of the proposed methods.

With reducing sample size N , the amortized posteriors differ. To study the performance of the three methods under different simulation budgets N for each task we simulate Ntot = 2 × 107 samples

6

Posterior Density

Posterior Density

1A.0 ×1O0r1nstein Uhlenbeck
MINE FDIV BCE 0.5 true

C ×10 2 BirthDeath
7.5
5.0

E ×10 2 SIR
7.5 5.0

true value

2.5

2.5

0.0 10

0.0 10 2

0.0 20

B ×10 2
7.5

D ×10 2

F4 ×10 1

5.0

2

2

2.5

0.0

0

0

0

22

20 0

G

×10

Lorenz attractor
1

3

2

1

0

1 30

40

1

Figure 3: Convergence of the posteriors for Ornstein-Uhlenbeck (A,B), Birth-Death (C,D), SIR (E,F)
and Lorenz attractor (G). In order to evaluate the posteriors we first choose a reference hypothesis : for Ornstein-Uhlenbeck µ = 5 and  = 1, for Birth-Death  = 10 and  = 0.2, for SIR  = 0.6 and  = 0.2, for Lorentz attractor  = 35. We show posteriors P (|x1:M ) calculated using models trained with N = 107 trajectories. For Ornstein-Uhlenbeck process the exact posterior density P (|x1:M ) is also shown.

J = {(xi, i)}. We perform the inference of the amortized likelihood-to-evidence ratio with varying simulation budgets, where both the training and the validation data are equal-sized subsamples of J with N  {104, 105, 106}. Inference and comparison are performed 10 times on independent subsamples of J . In order to obtain samples from the independent set I we shuffle the joint samples k = 5 times, and so NI = 5N . A larger shuffled data NI can improve the inference but at the cost of computing power, which sets a trade-off between performance and training time.
We compare the accuracy of the 3 inference methods (MINE, FDIV, BCE) for the 4 tasks (OU, Birth-death, SIR, Lorenz) based on the three following metrics:
Global comparison. The first metric used for the benchmark is the mutual information given a density estimator, computed with eq. 5. For each N , it is evaluated on test data composed of the remaining Ntot - N samples. Unlike the other two comparisons (see below), it is a global metric that tests the approximation of the likelihood-to-evidence ratio over all  and x values. For this reason we use it to perform hyperparameter tuning for each task and each objective with N = 105, see Supplementary Material.
For all four tasks the value of the estimated mutual information grows with the simulation budget and yields comparable performances for the 3 methods (Fig. 4 A, D, G, J). For the Ornstein-Uhlenbeck process, the BCE method reaches consistently higher values of mutual information. For the Lorentz attractor, the MINE method is significantly outperformed by the other methods in the low data limit (N = 104).
Posterior comparison. The objective of simulation-based inference is to find the posterior distribution over model parameters. To characterize the inference accuracy as a function of the simulation budget N and the method l, we evaluate the Jensen-Shannon divergence between the inferred and the reference posterior DJS(P (|x1:M ), P^lN (|x1:M )) (where DJS(p, q) = (1/2) [p(x) log(p(x)/m(x)) + q(x) log(q(x)/m(x))]dx with m(x) = (p(x) + q(x))/2), by scanning through the parameter space with the prior P (). A larger Jensen-Shannon divergence indicates a larger deviation between the inferred posterior and the reference (i.e., a lower performance).
All method show comparable performances, and the Jensen­Shannon divergence decays as a function of the simulation budget N (Fig. 4 B, E, H, K, and Supplementary Material for the Ornstein-Uhlenbeck
7

mutual information [bits]

DJS(Posteriors) [bits]

A Ornstein Uhlenbeck
3 2
MINE 1 FDIV
BCE
0 104 105 106 B ×10 1
6
4
2

D BirthDeath
2.0 1.5 1.0 0.5
0.0 104 105 106 E ×10 1
2
1

0 104 105 106

C
1.00

×10

1

0.75

0.50

0.25

0.00 104 105 106
N

0 104 105 106 F 4 ×10 2
3 2 1
0 104 105 106
N

G

SIR

2
1
0 104 105 106 H ×10 1
4
2
0 104 105 106 I ×10 1

4 2
0 104 105 106
N

J Lorenz attractor
3
2
1
0 104 105 106 K
1.00 0.75 0.50 0.25
0.00 104 105 106 L ×10 1
3 2 1
0 104 105 106
N

DJS(Likelihoods) [bits]

Figure 4: Benchmarking of the methods. We compare the three objectives I, eq. 5 (MINE), Lf , eq. 6 (FDIV), and S, eq. 7 (BCE) for 3 different metrics. We perform 10 replicates of the inference and comparison for simulation budgets N  {104, 105, 106} for 4 systems: Ornstein-Uhlenbeck
(A,B,C), Birth-Death (D,E,F), SIR (G,H,I) and Lorenz attractor (J,K,L). In the first row we compare the mutual information on held out test data using eq. 5 with the estimated E. For the following two metrics we need to instead choose an hypothesis , see Fig 3. for the exact values. In the second row we compare the Jensen-Shannon divergence DJS(P (|x1:M ), P^lN (|x1:M )) between the reference and inferred posteriors. In the last row we compare the Jensen-Shannon divergence DJS(P (x|), P^lN (x|)) using sampled trajectories from the simulator P (x|) and the inferred distribution P^l(x|).

process with d > 1). At N = 104 the accuracy of the posterior inference is significantly decreased in all methods, as reflected by the large variance of the DJS.
Likelihood comparison. The third metric is the Jensen-Shannon divergence DJS(P (x|), P^lN (x|)) between the true and approximated likelihood for a given model . This DJS cannot be directly evaluated by summing over x, because it is typically of high dimension. We thus rely on samples from these two distributions and infer an additional classifier to estimate DJS; see Supplementary Material. The performance of the 3 methods are comparable (Fig. 4 C, F, I, L). For the Ornstein-Uhlenbeck process, the BCE infers more accurate likelihood functions at N = 104 and N = 105 but it is outperformed by MINE at higher simulation budgets.
The results of the benchmark shown in Fig 4 suggest that all estimators show reliable performances across different tasks and simulation budgets. While the first metric is global and the two other metrics are local, they draw a consistent picture. A higher simulation budget enhances the performance of all methods. The BCE method tends to perform better at the lowest simulation budget. All three methods perform similarly in the middle and high data regime.
8

Conclusion
We analysed the problem of inferring an amortized estimator for the likelihood-to-evidence ratio over model parameters, using simulated data. We showed that this inference can be performed by maximization of the mutual information between simulated data and parameters of the model. This formulation captures an intuition that inference can be performed when we are able to extract the dependence between parameters and observed data, as measured by the mutual information.
The link between the field of mutual information estimation and simulation-based inference was first suggested in Ref. [30]. We formalized the relation within our framework, and tested its applicability on a variety of examples by proposing two methods (MINE and FDIV) for simulation-based inference. The formalism opens up possibilities for using algorithms and techniques developed in the context of mutual information estimation [31] for inverse problems.
The likelihood function we propose is equivalent to the mutual information bound analysed in Ref. [13]. However, while in [13] the focus is on the estimation of the absolute value of this quantity, we are interested in the inferred energy function that can be used to evaluate the posterior distribution for model parameters. Previous work that used classifiers for simulation-based inference [12] also fits naturally within our framework since logistic regression is linked to mutual information estimation [15]. Our methods rely on two lower bound estimators of mutual information, which are based on (i) the Donsker-Varadhan [13], and (ii) f-divergence representations of the Kullback-Leibler divergence [14]. It would be interesting to explore other known mutual information estimators for simulation-based inference [31].
We showed that our mutual information-based methods (MINE and FDIV), implemented in flexible neural networks, can reliably infer the posterior of the parameters and give consistent results with the previously proposed classifier-based technique (BCE) [12], when the simulation budget is sufficient. We benchmarked the three approaches and found that in their performances are comparable in intermediate data regime, while in the low data regime the classifier-based method performs consistently better. The main limitation of the two proposed objective functions I and Lf is that they require large simulation budgets for accurate inference.
Our choice to implement the neural network as a multilayer perceptron with two hidden layers was motivated by having a simple and reliable architecture to better focus on relative performance of the different objective functions. For the specific task of inference of model parameters from discrete samples of trajectories, absolute performance could be increased by choosing network architectures adapted to the data structure such as convolutional and recurrent layers.
Existing approaches to simulation-based inference, such as ABC, suffer from the need to define ad-hoc summary statistics to be matched between data and model. An important property of mutual information is its invariance upon reparametrization of its variables. This enables inference and comparison of different parametrizations of the observed data, as different choices can be evaluated using the absolute value of the mutual information. A specific application that could be interesting to explore is inference for population genetics models, where the choice of summary statistics to use for ABC analysis has been always critical and the ability to flexibly compare different parametrization choices could be greatly helpful. Another possibility would be to explore more principled regularization techniques such as the information bottleneck method [32]. This approach could be used to infer summary statistics of the data that are maximally informative of the parameters of the model. Then the summary statistics could be added as additional variables for the observations of related tasks, such as model extensions, in a transfer learning fashion.
In conclusion, our work helps to clarify the link between mutual information estimation and simulation based inference. We believe that this connection can be a fruitful source of improved methods for amortized inference.
Code availability. The code for the algorithms presented in this paper is available at github.com/ statbiophys/MINIMALIST
9

Acknowledgements
This work was supported by the DFG grant (SFB1310) for Predictability in Evolution (AN, AMW, GI), the MPRG funding through the Max Planck Society (AN, GI), the Royalty Research Fund from the University of Washington (AN), European Research Council grant, ERCCOG n. 724208 (AMW, TM, GI, NS).
References
[1] S Tavaré, DJ Balding, RC Griffiths, and P Donnelly. Inferring coalescence times from dna sequence data. Genetics, 145(2):505--518, February 1997.
[2] Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy of Sciences, 117(48):30055­30062, 2020.
[3] Jan-Matthis Lueckmann, Jan Boelts, David S. Greenberg, Pedro J. Gonçalves, and Jakob H. Macke. Benchmarking simulation-based inference, 2021.
[4] Mark A. Beaumont, Wenyang Zhang, and David J. Balding. Approximate Bayesian Computation in population genetics. Genetics, 162(4):2025­2035, 2002.
[5] S. A. Sisson, Y. Fan, and M. A. Beaumont. Overview of Approximate Bayesian Computation, 2018.
[6] Andrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, and Sriram K. Rajamani. Probabilistic programming. In Future of Software Engineering Proceedings, FOSE 2014, page 167­181, New York, NY, USA, 2014. Association for Computing Machinery.
[7] Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang, and David M. Blei. Edward: A library for probabilistic modeling, inference, and criticism, 2017.
[8] Simon N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466(7310):1102­1104, Aug 2010.
[9] George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 837­848. PMLR, 16­18 Apr 2019.
[10] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1530­1538, Lille, France, 07­09 Jul 2015. PMLR.
[11] Gilles Louppe Kyle Cranmer, Juan Pavez. Approximating likelihood ratios with calibrated discriminative classifiers. 2015.
[12] Joeri Hermans, Volodimir Begy, and Gilles Louppe. Likelihood-free MCMC with amortized approximate ratio estimators, 2020.
[13] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. MINE: Mutual information neural estimation, 2018.
[14] XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847­5861, Nov 2010.
[15] Sudipto Mukherjee, Himanshu Asnani, and Sreeram Kannan. CCMI : Classifier based Conditional Mutual Information Estimation, 2019.
[16] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models, Mar 2010.
[17] Michael U. Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. J. Mach. Learn. Res., 13(null):307­361, February 2012.
10

[18] David Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transformation for likelihood-free inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2404­2414. PMLR, 09­15 Jun 2019.
[19] G. E. Uhlenbeck and L. S. Ornstein. On the Theory of the Brownian Motion. Phys. Rev., 36:823­841, Sep 1930.
[20] Oldrich Vasicek. An equilibrium characterization of the term structure. Journal of Financial Economics, 5(2):177­188, 1977.
[21] Luigi L Cavalli-Sforza and Anthony WF Edwards. Phylogenetic analysis. models and estimation procedures. American journal of human genetics, 19(3 Pt 1):233, 1967.
[22] Joseph Felsenstein. Phylogenies and quantitative characters. Annual Review of Ecology and Systematics, 19(1):445­471, 1988.
[23] Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations. Stochastic Modelling and Applied Probability. Springer-Verlag Berlin Heidelberg, 1992.
[24] Daniel T Gillespie. Exact stochastic simulation of coupled chemical reactions. The journal of physical chemistry, 81(25):2340­2361, 1977.
[25] D. Rumelhart, Geoffrey E. Hinton, and R. J. Williams. Learning internal representations by error propagation. 1986.
[26] Martín Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.
[27] Francois Chollet et al. Keras, 2015.
[28] Charles R. Harris et al. Array programming with NumPy. Nature, 585(7825):357­362, September 2020.
[29] Pauli Virtanen et al. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261­272, 2020.
[30] Conor Durkan, Iain Murray, and George Papamakarios. On contrastive learning for likelihoodfree inference, 2020.
[31] Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A. Alemi, and George Tucker. On variational bounds of mutual information, 2019.
[32] Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pages 368­377, 1999.
[33] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling, 2016.
[34] Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3698­3707, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
[35] George Papamakarios and Iain Murray. Fast -free inference of simulation models with bayesian conditional density estimation. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.
[36] Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan Öcal, Marcel Nonnenmacher, and Jakob H Macke. Flexible statistical inference for mechanistic models of neural dynamics. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[37] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding, 2019.
[38] Rmsprop. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_ lec6.pdf.
[39] Kwanghee Choi and Siyeong Lee. Regularized mutual information neural estimation, 2020.
11

Supplementary material

1 Different objective functions share an optimum

We study the following objective funtions:

I(; I, J ) = -EJ [E] - log EI [e-E ],

Lf (; I, J ) = -EJ [E] - EI [e-E-1],

S(; I, J ) = -EJ [log d] - kEI[log 1 - d ]

=

-EJ

log

1

+

1 kZ eE 

- kEI

log

k

+

k Z -1 e-E 

.

(S15) (S16) (S17)
(S18)

In the infinite data limit the empirical averages converge and we can rewrite all objectives as functional of the energy model:

I(E) = - E(x, )Pjoint(x, ) dx d - log e-E(x,)Pindep(x, ) dx d, (S19)

Lf (E) = -

E(x, )Pjoint(x, ) + e-E(x,)-1Pindep(x, ) dx d,

(S20)

S(E) = -

log

1

+

1 kZ eE  (x,)

Pjoint(x,

)

+k

log

k

+

k Z -1 e-E  (x,)

Pindep(x,

)

dxd.

(S21) (S22)

In this limit the 3 optima of the objective functions are equivalent and recover the likelihood-toevidence ratio. To see this, we take the functional derivative with respect to the energy model E,

I E(x, )

= -Pjoint(x, ) +

e-E



(x,)

1 Pindep

(x,



)

dx

d

e-E

(x,)

Pindep

(x,

),

(S23)

Lf E(x, )

=

-Pjoint(x, ) + e-E(x,)-1Pindep(x, ),

(S24)

S E(x, )

=

k

ZeE(x,)Pjoint(x, ) - Pindep(x, ) 1 + kZeE(x,)

1

+

 log Z E(x, )

,

(S25)

and we find they vanish at energies EI , Ef and ES respectively:

EI

=

- log

Pjoint(x, ) Pindep(x, )

-

log

Z,

Ef

=

- log

Pjoint(x, ) Pindep(x, )

-

1,

ES

=

- log

Pjoint(x, ) Pindep(x, )

-

log

Z.

(S26) (S27) (S28)

All three are equal to the logarithm of the likelihood-to-evidence ratio up to constant factors. We note
that the second derivatives are different in the 3 cases and therefore convergence to the optima EI , Ef and ES will in general be different.

2 Noise Contrastive Estimation and mutual information

In this section we show how our work fits within the framework of Noise Contrastive Estimation

(NCE) and how it relates to the existing contrastive learning approaches to simulation-based inference.

The NCE methods estimate a probability density p(y) by comparison to a reference noise distribution

q(y) [16, 17] :

p(y)

=

1 Z

e-E(y)q(y),

(S29)

12

which reduces the problem to approximating the density ratio. The original method [16] consists of
the inference of the density ratio model using logistic regression (minimizing binary cross entropy) on samples from both distributions, p, q. This framework encompasses the likelihood-to-evidence ratio inference problem where p(y) = Pjoint(x, ) and q(y) = Pindep(x, ) and one minimizes S(; I, J ) to find E.

An alternative approach proposed in the Noise Contrastive Estimation literature [33, 34] focuses on the estimation of conditional probability functions

p(y|z)

=

Z

1 (z)

e-E(z,y)

q(y),

(S30)

where now the partition function explicitly depends on the conditioned variable z. The new density
ratio can be inferred by optimizing the the so-called ranking objective [33]. This objective function is typically used to rank a positive sample from the target distribution p(y|z) above k samples from the reference noise q(y) for the input z [34].

In simulation-based inference, this family of methods has been used for posterior estimation, where p(y|z) = P (|x) is the unknown posterior and q(y) = P () is the prior. In our notations the ranking
objective function reads

Lr(; J ) = EJ

log

e-E  (x,)

e-E(x,) +

k j=1

e-E(x,j )

k
P (i)di .
i=1

(S31)

This method is known as the Sequential Neural Posterior Estimation (SNPE) proposed in [18], building on the work in Refs. [35, 36]. It's useful to note that the ranking loss Lr(; J ) has also been used to construct a high-bias and low-variance estimator of mutual information [37, 30].

Ref. [30] proposes that also the binary classification approach introduced in [12] is a special case of the above inference for k = 1. However, the ranking objective Lr(; J ) with k = 1 is distinct from S(; I, J ) and the two methods cannot be identified as one. In Ref. [34] the cross entropy has been compared to the ranking loss and shown to generically outperform it in the context of Neural
Language Processing.

3 The Ornstein-Uhlenbeck process in dimension d

The trajectories x(t) are solutions to a stochastic differential equation 
dx = - (x - µ) dt + 2dW,

(S32)

where x is a d-dimensional coordinate, µ its long-term average,  is a d × d damping matrix,  is the noise strength, and W is a d-dimensional Wiener process. From a trajectory x(t) we sample n values every t so that x = {xi = x(it)}. To find the analytical expression for the likelihood of these observations we write the corresponding Fokker-Planck equation for the density P = P (xi, t + t|xi-1, t),

dP dt

= - [ (x - µ) P ] + T 2P,

(S33)

solved with a multivariate Gaussian distribution density

P (xi, t + t|xi-1, t) =

1

e , -

1 2

(xi

-

xi

)T -1(xi-

xi

)

(2)d det 

(S34)

with mean

xi = e-txi-1 + (1 - e-t)µ,

and a covariance matrix given by

(S35)

t

=2

ds e(s-t)T eT (s-t).

(S36)

0

Both expressions simplify when we set  = I and µ = 0. For symmetric  ( = T ) we can find an orthogonal eigenbasis r() in which the damping matrix is diagonal,

 = r()  r()T ,

(S37)

13

where  is a diagonal matrix and r()r()T = I. The covariance matrix is also diagonal in this basis, which allows us to compute the integral when  = I so that

(t) = r() -1(1 - e2t)r()T .

(S38)

To ensure that  is symmetric and positive definite (which is required so that the trajectories don't diverge and a steady state exists) we choose the following parametrization:

 = I + (d)g,

(S39)

where g is a random matrix from the Gaussian Orthogonal Ensemble with density

P

(g)



e-

d 4

Tr(g 2 ) .

(S40)

The eigenvalues of g can be both positive and negative, in particular the lowest eigenvalue is

distributed according to g = 2d1/6. Choosing

the Tracy-Widom law with mean µg = 2d and standard (d) = (µg + 2g)-1 ensures that the eigenvalues of  are all

deviation of positive with

good confidence.

4 Neural network architecture and learning hyperparameters
The I(; I, J ) objective function is invariant with respect to a global shift in energy, I(E + E0) = I(E), since any shift E0 can be incorporated in the partition function Z to obtain the same likelihoodto-evidence ratio. We choose an energy gauge in which the "free energy" vanishes, - log Z = 0. As suggested in [39] we do so by adding a regularization term of the form -Z(log Z)2 to the likelihood function. Since the constraint Z = 1 may be satisfied by adding the right constant E0 to the energy function, this regularization does not affect the result of the optimization. We fixed the strength of this term to Z = 10-3.
To perform the benchmark of the methods we used the same neural network architecture for all three objective functions: a multilayer perceptron [25] with two hidden layers of 50 nodes each. Each node processes a linear combination of the inputs and adds a constant term (bias). A hyperbolic tangent activation function is then applied to the result of this linear map. Between the second hidden layer and the output of the network we do not apply the activation function. We implemented L2 regularization on network weights, with regularization strength 2. We optimized the network weights using stochastic gradient descent and the RMSprop [38] optimization algorithm with learning rate lr and size of mini batches b.
We tuned the hyperparameter by inference of 5 replicate models on N = 105 training data for each objective function and combination of hyperparameters, 2  {10-4, 10-5, 10-6}, lr  {10-2, 10-3, 10-4} and b  {103, 104}. We evaluated the mutual information estimate I(; I, J ) on N = 105 independent samples (test set). For each of the three methods, we chose hyperparameters for which the mutual information was highest.

5 Methods for likelihood comparison

We outline here the method for calculating the Jensen-Shannon divergence between two distributions for which an analytical density is not known but instead we can sample from the two distributions. This will be the case for the likelihood comparison where we will compare the true likelihood and an inferred model of the likelihood.

We generate M = 5×104 samples {xm}M m=1  P (x|) from the true simulator. In order to generate samples for the inferred estimators {x^m}M m=1  P^lN (x|) we perform rejection sampling on samples from the marginal probability P (x). To produce samples from P (x) we discard the parameters  from the samples {(xi, i)}Ni=t1ot . Rejection sampling is based on the identity P (x|) = P (x)Z-1e-E(x,) where the likelihood-to-evidence ratio is approximated by an estimator. For each simulation budget N and method l we generate {x^m}M m=1 samples by rejection sampling with acceptance probability e-El /Zl. In the last row of Fig. 4 the methods are compared using this metric.

By mixing the samples from P (x|) and P^lN (x|) in equal proportion we construct an ensemble of

samples

from

Pmix(x|)

=

1 2

(P

(x|)

+

P^lN

(x|

)).

We

then

train

two

classifiers,

one

between

sam-

ples from P (x|) and Pmix(x|), and the second between samples from Pmix(x|) and P^lN (x|).

14

We again exploit the fact that an optimal classifier is the ratio of the two likelihoods and we can read off the two corresponding Kullback-Leibler divergences, DKL(P ||Pmix) and DKL(P^lN ||Pmix) from it's estimate (5). The value of the Jensen-Shannon divergence is then the average

DJS(P ||P^lN )

=

1 2

DKL(P ||Pmix) + DKL(P^lN ||Pmix)

.

(S41)

An alternative measure of performance of the model of the likelihood is the AUROC characteristic of
the optimal classifier between samples from the true and an inferred likelihood, which we will use below. The best models should result in indistinguishable sets of samples for which AUROC = 1/2. A failure to capture the mutual information between parameters  and simulated data x make the two sets distinguishable and AUROC = 1.

6 Supplementary benchmark results

Fig. S1 presents the comparison of the objective functions applied to the 4 tasks: the Ornstein-

Uhlenbeck process in dimension 1 (A), the birth-death process (B), the SIR process (C) and the

Lorenz attractor (D), using the AUROC between the true and estimated likelihood as a measure of

performance. The results are consistent with the Jensen-Shannon divergence metric presented in

the main text (Fig. 4). Fig. S2 presents the comparison of the objective functions for the task of

infering the damping matrix elements in the high-dimensional Ornstein-Uhlenbeck process. We

compare the performance in terms of inferred posterior divergence from the analytical prediction. We

compute the Jensen-Shannon divergence independently for the marginal posterior distributions over

each element of the damping matrix (a total of

d 2

unique elements in dimension d). We present the

results independently for diagonal and off-diagonal elements of the matrix  (in dimension d there

are d diagonal and d(d - 1)/2 off-diagonal elements).

AUROC

A1.0

B1.0

C1.0

D1.0

0.9

MINE FDIV

0.9

0.9

0.9

0.8

BCE 0.8

0.8

0.8

0.7

0.7

0.7

0.7

0.6

0.6

0.6

0.6

0.5 104 105 106 0.5 104 105 106 0.5 104 105 106 0.5 104 105 106

N

N

N

N

Figure S1: We compare the three objective functions I (MINE), Lf (FDIV), and S (BCE) using the Area Under the Receiver Operating Characteristic (AUROC) of a classifier trained to distinguish samples from the simulator P (x|) and samples from the inferred estimators P^(x|) for a specific hypothesis . AUROCs closer to 1/2 mean that the model is performing well, as its samples are
indistinguishable from samples from the true distribution. This metric gives a consistent picture with respect to the DJS metric presented in the main text (Fig. 4C, F, I, L).

15

DJS(Posteriors) [bits] DJS(Posteriors) [bits]

1.0

d=1

0.8

0.6

0.4

0.2

0.0 104

105

106

N

1.0

d = 2 diagonal

0.8

0.6

0.4

0.2

0.0 104

105

106

1.0 d = 2 offdiagonal

0.8

0.6

0.4

0.2

0.0 104

105

106

sims

1.0

d = 3 diagonal

0.8

0.6

0.4

0.2

0.0 104

105

106

1.0 d = 3 offdiagonal

0.8

0.6

0.4

0.2

0.0 104

105

106

N

1.0

d = 4 diagonal

0.8

0.6

0.4

0.2

0.0 104

105

106

1.0 d = 4 offdiagonal

0.8

0.6

0.4

0.2

0.0 104

105

106

N

1.0

d = 5 diagonal

0.8

0.6

0.4

0.2

0.0 104

105

106

1.0 d = 5 offdiagonal

0.8

0.6

0.4

0.2

0.0 104

105

106

N

Figure S2: The Ornstein-Uhlenbeck process in d  1. We compare the three objective functions I (MINE), Lf (FDIV), and S (BCE) for dimension d = 1, 2, 3, 4, 5. We perform 10 replicates of the inference with simulation budgets N = 104, 105, 106 for changing dimension d = 1, 2, 3, 4, 5. We compare the posterior with the analytical prediction for a hypothesis (g)ij = -1. We compute the Jensen-Shannon divergence DJS(P (|x1:M ), P^lN (|x1:M )) between the true and inferred posteriors for each element of the damping matrix  independently. We show the summary statistics for diagonal
and off-diagonal terms.

16

