Deep Personalized Glucose Level Forecasting Using Attention-based Recurrent Neural Networks

Mohammadreza Armandpour Department of Statistics Texas A&M University College Station, USA armand@stat.tamu.edu

Brian Kidd Department of Statistics Texas A&M University College Station, USA
bkidd@stat.tamu.edu

Yu Du Biometrics Department Eli Lilly and Company
Indianapolis, USA du yu@lilly.com

Jianhua Z. Huang Department of Statistics Texas A&M University College Station, USA jianhua@stat.tamu.edu

arXiv:2106.00884v1 [cs.LG] 2 Jun 2021

Abstract--In this paper, we study the problem of blood glucose forecasting and provide a deep personalized solution. Predicting blood glucose level in people with diabetes has significant value because health complications of abnormal glucose level are serious, sometimes even leading to death. Therefore, having a model that can accurately and quickly warn patients of potential problems is essential. To develop a better deep model for blood glucose forecasting, we analyze the data and detect important patterns. These observations helped us to propose a method that has several key advantages over existing methods: 1- it learns a personalized model for each patient as well as a global model; 2- it uses an attention mechanism and extracted time features to better learn long-term dependencies in the data; 3- it introduces a new, robust training procedure for time series data. We empirically show the efficacy of our model on a real dataset.
Index Terms--Time series forecasting, Recurrent neural network, Personalized blood glucose prediction, Machine learning for diabetes
I. INTRODUCTION
In 2017, more than 400 million people worldwide were living with diabetes. This number is expected to rise to 693 million by 2045 [1]. Predicting blood glucose (BG) levels would help diabetics both understand and control their BG levels to avoid complications [2]. Machine learning algorithms have been widely applied to the prediction of glucose level with varying levels of success. Most of the early works provide prediction based on a few observations per day [3]. Recently, with the emergence of continuous glucose monotoring (CGM) devices, people can measure their blood glucose every 5 minutes, leading to much more data for models to utilize.
The predictive models using only CGM data can be split into two prediction tasks: predicting risk of upcoming related health problems and forecasting BG levels. Some algorithms try to do hypo or hyperglycemia forecasting in the near future [4], [5]. For predicting BG levels, the goal is not only to provide a final value prediction but also forecast trajectories. This allows patients to not only have an alert of possible BG events but also to understand the trend leading to it. Some trends require more immediate action to alleviate (e.g. the trend is continually accelerating downward). In this work, we will focus on this more challenging and important task of trajectory prediction.
For these tasks, many models require added information (such as carbohydrate intake, insulin level, exercise, etc.)

for prediction purposes [4], [6]­[9]. Although having more information can potentially improve the performance of the prediction, it necessitates that the patient collects the information. The data-collection process relies mainly on the subjective inputs provided by the person who wears a CGM device. This is an extra burden on the user and often results in inaccurate and noisy data. It has been shown that this data is often difficult to systematically collect and use in broader applications [10], [11]. Hence, it is of significant importance to have a model that can both provide reliable prediction based solely on previous glucose levels and also take advantage of extra information whenever available. Our proposed method falls in this category. We train our model just based on the history of BG, as our application does not provide any exogenous variables. However, this information can be easily appended to the inputs of our model.
Various previous works have investigated methods for predicting BG levels. Some use classical approaches, like ARIMA [11]­[14] and Random Forests [9], [15], [16]. Deep learning methods have shown competitive advantages to the classical models in many areas both for probabilistic [17], [18] and deterministic [19] prediction. Forecasting BG levels is no exception; two studies have shown the superior performance [20], [21] of deep learning. The downside is that they usually need lots of training examples to perform well. To overcome this, we develop a deep personalized forecasting model; it leverages data across different people to provide forecasting specific to each patient with fewer observations. This improves on the state of the art deep learning methods for BG levels [20], [21]. They either ignore the individual variability in the forecasting or need lots of training examples per person to learn separate models for each patient.
After explaining our deep personalized model, we proceed by analyzing the data and showing the existence of long term dependencies within collected BG samples. The standard deep learning solution with recurrent neural networks (RNNs) cannot capture such a long dependency well because of the known problem of vanishing and exploding gradients [22]. To overcome this issue, we add an attention mechanism and time features to help the model capture this long term structure. Also, we observe high fluctuation in variables during training of the model, which is the result of high variance

Fig. 1: One day of CGM data for two different patients
of batch gradients. To alleviate this issue, we exploit a more robust training procedure which happened to improve the final result as well. Our main model is designed to provide BG forecasting, without the need for the user to manually enter any input, like carbohydrate intake or eating time. The model implicitly learns these events based on the previous behavior of each patient. We demonstrate the efficacy of our method over the existing state-of-the-art technique [20] on a real dataset of BG values for different users provided by [20].
II. METHODOLOGY
The goal of this section is to both provide some intuition behind our model and develop it in detail. We start by emphasizing the need for personalization, followed by a mathematical formulation of our personalized forecasting problem. Then we describe a deep learning component to capture the long term dependencies in our application and conclude the section with two further additions (time features and robust training).
A. Personalization
We start by motivating the importance of having a deep personalized model for the BG forecasting problem. Figure 1 shows the BG levels of two different diabetic patients over the course of one day. The plots illuminates the systematically different behavior of the two patients. The BG level of the second patient is clearly smoother and has less fluctuation than the first patient. This phenomena clearly suggests a single model for forecasting both patients is insufficient. On the other side, although deep learning based approaches have shown significant improvement over the traditional methods, in the existing literature they either ignore the individual identity and provide a generic model [20] or they learn an entirely new model for different patients [23]. The latter is not an efficient solution because deep learning based models need lots of training examples; hence, we need many BG observations for each patient to provide useful predictions, which is often not possible. In the following, we propose our deep personalized solution that alleviates the aforementioned problem by leveraging information across the different patients.
To accomplish the personalization while also allowing the model to share parameters across patients, we need to give the model information about which person is relevant for the specific BG measurements. The traditional way of giving

the identity of the person to the model is by using a onehot encoded vector. This one-hot encoded vector is large, sparse, and inefficient. Also, it does not represent any semantic information about the person. To overcome this inefficiency, we represent each person with a latent vector. The idea of using latent vectors, or embeddings, to represent identities in the model has shown great success in a wide range of topics, from social network analysis [24]­[26] to natural language processing [27]. Some methods learn the embedding vectors manually using feature engineering, others use pre-trained embeddings from other tasks, and still others learn them by adding an embedding layer to the model, which we use in this paper. We will provide the details about our method and how we use the embedding layer in the following section.
B. Deep Personalized Forecasting
In the following, we formulate learning a personalized forecasting model as a maximum likelihood optimization problem. Let V to be the set of individuals and g : V  Rd be the embedding layer from one-hot encoded vectors to latent feature representations with dimension d, which we aim to learn. Let {x(1v:T)v }vV be a set of univariate time series, where x(1v:T)v = (x(1v), x(2v), . . . , x(Tvv)), and x(tv)  R denotes the value of time series of person v at time step t.
We proceed by defining our objective function based on the log-probability of observing x(Tv+) 1:T + given the history of the time series of person v:

max
,g

log p(x(Tv+) 1:T + |x(1v:T) ; g(v), ).

vV T Tv -

The  refers to the set of learnable parameters of the

model, which are shared between patients and learned jointly;

g(v) is the embedding of person v. However, the model still

theoretically needs to take as much historical data as possible

into account, which is both unrealistic and computationally

infeasible. Intuitively, BG levels from weeks ago shouldn't

influence today's BG levels. We formulate this assumption by

only allowing the BG levels to depend on the most recent t0 observations, or, mathematically,

(x(Tv+) 1:T + |x(Tv-) t0+1:T )  x(1v:T) -t0 ,

(1)

where  symbolizes independence and (A|B) refers to

conditioning random variable A respect to the B. Then, define a summary vector zT(v)  RD:

zT(v) = f (xT -t0+1, xT -t0+2, . . . , xT , g(v)),

(2)

such that:

(x(Tv+) 1:T + |zT(v))  x(v).

(3)

The above assumption allows us to further simplify each term in the objective function:

log p(x(Tv+) 1:T + |x(1v:T) ; g(v), )

= log p(x(Tv+) 1:T + |x(Tv-) t0+1:T ; g(v), )

= log p(x(Tv+) 1:T + |zT(v); g(v), ).

(4)

Encoded

Decoder RNN

Decoder RNN

....

Decoder RNN

Information

one-hot encoded
vector

User Embedding

Encoder RNN

Encoder RNN

....

Encoder RNN

Fig. 2: Deep Personalized Model

With the chain rule, we can further decompose the above objective term as:

log p(x(Tv+) 1:T + |zT(v); g(v), )



=

log p(x(Tv+) i|x(Tv+) 1:T +i-1, zT(v); g(v), ).

(5)

i=1

With this simplified objective, we define our deep architectures for learning both the summary vector zT(v) and  for obtaining the log probability.
To learn zT(v), we use a bidirectional RNN [28] (BiRNN) that sequentially reads the time series data of each individual
as input. We employ BiRNN to capture both forward and backward trend information in the latent variable zT(v). More specifically, for any 0 < j  t0, we set

hj = BiRNN([x(Tv-) t0+j ||g(v)], hj-1).

(6)

The || is the concatenation operation, and hi refers to the concatenation of hidden states from both the forward and backward paths. Then we let:

zT(v) = tanh(W ht0 ), where W  RD×N , ht0  RN (7)

and by tanh we refer to the hyperbolic tangent function. Lastly, we need to provide a parametric model for

log p(x(Tv+) i|x(Tv+) 1:T +i-1, zT(v); g(v), ).

(8)

First, we approximate it by:

log p(x(Tv+) i|x^(Tv+) 1:T +i-1, zT(v); g(v), ),

(9)

where x^ refers to our models approximation of the true x. We make another summary vector s(i-v)1 to capture the useful information from x^(Tv+) 1:T +i-2 for predicting x(Tv+) i. In
the following we explain the rest of the mathematical details:

s(iv) = RNN([g(v)||x^(Tv+) i-1], s(i-v)1),

(10)

s(0v) = zT(v), x^(Tv) = x(Tv)

(11)

x^(Tv+) i = Q([s(iv)||g(v)]),

(12)

Fig. 3: Autocorrelation graphs of randomly sampled time series over the course of one day. The dashed and solid horizontal lines correspond to the 99% and 95% confidence intervals for the correlation values around zero.

where the RNN just has the forward path and Q is a fully connected neural network with no activation function at the last layer. Figure 2 shows a summarized version of our model.
Our model can also be considered as a recurrent autoencoder, where zT(v) is the encoded vector of the last t0 time series observations. The decoder is made by putting a fully connected NN on top of the hidden states of the decoder RNN. The term g(v) acts as an embedding vector for each patient, which is fed as input features to both the encoder and decoder RNN's. This lets the encoder and decoder behave differently for different patients and provides the opportunity for the RNN's in the model to show different behavior across different people.

C. Capturing Long Range Dependencies

In this section, we first investigate any possible time dependencies in the data and discuss possible explanations for these trends. Then, we propose a method to capture the patterns within the data with two key components: multi-head attention and added time features.
Following the statistical time-series literature, we use autocorrelation as a metric for time dependencies in the data. Autocorrelation, also known as serial correlation, is the correlation of a signal with its delayed version, defined as:

R( )

=

E[(xt

-

µ)(xt+ 2

-

µ)] ,

(13)

where xt is the time series signals, µ and 2 are its mean
and variance. In practice, we consider the empirical unbiased estimator (using the sample estimates of µ and 2) to calculate

the autocorrelation.

A plot of the autocorrelation for a few randomly sampled

time series is shown in Figure 3. The behavior is similar for

these samples and those not shown; these are just represen-

tative of a common behavior. For the first few minutes (up

to an hour and a half sometimes), there is significant positive

correlation, as there is rarely a drastic change in BG in a short time interval. Therefore, our model needs to be able to capture this short-term trend.
Later in the autocorrelation plot, some significant negative auto-correlation emerges. Capturing these much longerterm dependencies is a challenging task for standard models, like linear dynamical systems, hidden Markov models, and ARIMA. RNN's have been shown to perform better with long-term dependencies, but there is still the vanishing and exploding gradient problem [22]. Another issue is high variance in when the trend emerges, with significant negative auto-correlation appearing after a lag as large as 200 (16 hours). This irregularity in the pattern of negative correlation discouraged us from using convolutional modeling with fixed dilation [29] because convolutional models look for significant signals in fixed observation windows. Our solution captures this variability in negative trend by adaptively paying attention to different parts of the time series.
1) Multi-head Attention Layer: We start by describing a single-head attention layer. The input to this layer is the set of all hidden states of the encoder H = [h1, h2, . . . , ht0 ] and the decoder hidden state at the previous time s(i-v)1. The output is a weighted sum of encoder hidden states across the whole encoding time. We mathematically describe the attention mechanism below:

eij = b(hj , s(i-v)1),

(14)

This eij indicates the importance of the j'th hidden state of the encoder to the i'th output of the decoder. To make coefficients easily comparable across different hidden states, we normalize them using the softmax function across all choices of j:

ij = softmaxj(eij) =

exp(eij) , s exp(eis)

(15)

and we model the attention mechanism a as following:

eij = b(hj , s(i-v)1) = tanh(rT W [hj ||s(i-v)1]),

(16)

where r is a vector that needs to be learned and rT represents the transpose of r. Once obtained, the normalized attention coefficients are used as the weights for combining the hidden states. To obtain the final output of the attention layer, we apply an activation function  as:

ai = ( ij hj ).

(17)

j

In our experiments, we chose tanh as our activation function.
To extend the model to a multi-head attention model, we learn a different r(k), W (k) for each attention head. To be more precise, for the k'th head, there are separate r(k), W (k)'s used to get e(ijk) and i(jk). Then, we let the output be

1

ai

=

( K

i(jk)hj ).

(18)

kj

Encoded

Decoder RNN

Decoder RNN

....

Decoder RNN

Information

one-hot encoded
vector

User Embedding

Multi-head Attention layer

Encoder RNN

Encoder RNN

....

Encoder RNN

Fig. 4: Deep personalized multi-head attention model

Then the attention vector ai is fed as an input feature to the decoder RNN at step i, as well as the fully connected NN. Analytically, we let:

s(iv) = RNNatt([ai||g(v)||x^(Tv+) i-1], s(i-v)1),

(19)

s(0v) = zT(v),

(20)

x^(Tv+) i = Qatt([ai||s(iv)||g(v)||x^(Tv+) i-1]),

(21)

where the RNNatt and Qatt are again a forward path recurrent

unit and a fully connected NN. We illustrate our final model

architecture in the figure 4.

The encoder in this new model does not have the burden

of summarizing all the useful information in just a single vector zT(v). The decoder can decide which parts of the input
time series to pay attention to, instead of working with just

a summarized vector that comes from the encoder. Lastly, in

the new model, the gradient can pass through skip connections

between encoder and decoder with the attention layer, which

improves training and diminishes the exploding and vanishing

gradient problem. 2) Time Features: We explain why time is an important

variable in the BG prediction task and how we can leverage it

in our model. BG level is highly influenced by activities, such

as eating and exercising, so a model that can better estimate

eating time and/or exercising routine can provide more accu-

rate forecasting. People tend to follow certain activity patterns

based on environment; time is the simplest proxy for these

patterns without more information.

Based on our observations from the data, we consider

three time features. First, the hour of the day is important to

capture scheduled daily patterns, like eating times and workout

routines. Second, the day of the week is similarly considered

as schedules sometimes vary slightly by day but remain fairly

constant from week to week. Third, an indicator of weekend or

not weekend is a crucial added emphasis on the day indicator,

as schedules are often much less predictable during weekends

(e.g. when there is no work or when travelling). We simply

append these three time features to the input of each recurrent unit in our final model.
D. Robust Training
It's well known that training recurrent neural networks is a challenging task, and there have been several attempts to solve those problems, including teacher forcing [30] and gradient clipping [22]. Standard RNN's use the model output from a prior time step as an input for predicting the next one when forecasting multiple steps into the future. Teacher forcing works by using the actual or expected output from the training dataset at the current time step as an input in the next time step instead. In training of our model, we try teacher forcing with an adaptive probability; however, we find the model becomes more fragile and gets worse results than when not using teacher forcing.
The next method we examine to aid training is gradient clipping. This forces the gradient values (element-wise) to have a maximum magnitude, "clipping" it to be that maximum if it is exceeded. Applying clipping to stabilize the batch gradients helps our RNN learn better. We further improve training by adaptively decaying the clipping threshold during training, as gradients should ideally shrink as training occurs and the model approaches an optimum.
Even with adaptive gradient clipping, we still observe large variance in the parameters' gradients for different minibatch samples. Consequently, the model parameters fluctuate dramatically and are numerically unstable. By looking closer at the loss term for each time series sample in a mini-batch, we noticed the distribution of loss terms usually has some extreme outliers. The loss term outliers can overwhelm the total loss (i.e. the average) for the whole mini-batch. Hence, training based on a whole mini-batch is in fact just training on some "overwhelming" samples, so the model cannot properly learn the true general trends.
To circumvent this problem, we only consider the lower -quantile of the loss terms related to the different samples within each mini-batch. Then, after removing the upper 1 -  quantile of loss terms, we take the average as the loss of the whole mini-batch. In this way, we prevent the whole objective from being controlled by a few outliers. An added benefit is that it should ignore some of the unpredictable behavior (e.g. unexpected intake of a sugary food) during gradient calculation, making training more robust to unpredictable breaks in pattern. Empirically, we emphasize the improvement from this procedure in the experiments.
Some other methods, like the current state of the art model from [20], consider the BG level as a discrete variable, so their model must also output a discrete variable. They use the cross-entropy loss for training to accommodate the discrete variables. This strategy also potentially avoids the outlier loss terms; however, the model does not have the right direction for the gradient for training the model when the distance between the estimated value and true value is large.

III. EXPERIMENTS
In this section, we illustrate the performance of our model on real CGM data of 38 diabetes patients. To evaluate the performance of the model at the forecasting task, we need to decide on the length of the prediction window. Most of the existing works only considered 30 minute forecasts [4], [7], [8], [20], but we also consider one hour forecasts. This gives the patients more time to take proactive action in preventing hyper/hypo-glycemia by administering insulin or eating, which have delayed effects. However, as some methods were developed for the thirty minute intervals, we show that our method improves in the 15, 30, 45, and 60 minute forecasts.
Although our main goal is to provide forecasting trajectories of BG levels, it is important to evaluate the model performance when the patient experiences hypoglycemia (< 70 mg/dl) or hyperglycemia (> 180 mg/dl) separately. In our experiments, we include this by reporting the results for four different scenarios based on the BG value at forecasting time: Full (no constraint on BG level), Events (either hypoglycemia or hyperglycemia), Hypo (hypoglycemia), and Hyper (hyperglycemia).
We use the median of absolute percentage error (APE) and root mean square error (RMSE) over the forecasting window as two error metrics for the predictions. For i'th time step into future, we define APE as the median of |x(Tv+) i - x^(Tv+) i|/x(Tv+) i, and RMSE as the square root of v(x(Tv+) i - x(^v)T +i)2/n, where x(Tv+) i denotes the true value and n denotes the number of time series. Following [20] and unlike most of the previous work, we don't just calculate the error based on a single point in the future; instead, we use the average error based on all points in the forecast (e.g. six points for the 30 minute window). We prefer this method of evaluation as it is valuable to let a patient know about the trend as well. For example, if a patient's BG level rises toward the hyperglycemia range with an accelerating rate, the patient needs to be more alert to taking proactive action than the case when the rate is slowing.
A. Data Description
To be consistent with previous work, we use the raw CGM data provided by [20]. Some of the CGM observations show drastic fluctuations of more than 40 mg/dl, which is physiologically unrealistic, so we removed them and considered them as errors. Unlike [20] we have not done polynomial interpolation for missing values for the large gaps. The data after cleaning consists of 399,302 observations (through time) for 38 patients. We temporally split the data to three sections: training, validation and testing. In fact, for each patient, we divided the data to these three sections with no overlap and a ratio of 20:1:1 with training first, validation second, and the testing data were the most recent observations. We use the validation set for early stopping and hyper-parameter tuning, but we do not use it for training the model.
B. Compared Algorithms
We compared our method to both classical statistical methods and deep learning based approaches, including the state

APE / RMSE
Algorithms
ARIMA RF: Rec RF: MO PolySeqMo Our Method (MSE) Our Method (Robust)

Full
3.31 / 5.98 3.43 / 6.17 3.97 / 7.12 3.07 / 5.55 3.02 / 5.40 2.90 / 5.26

15 Minutes

Event

Hypo

3.27 / 7.54 3.07 / 6.58 3.35 / 7.62 2.96 / 6.72 3.03 / 6.64 2.90 / 6.32

7.83 / 6.13 5.91 / 4.57 5.73 / 4.31 7.27 / 6.32 8.07 / 6.23 6.36 / 4.92

Hyper
3.13 / 7.68 3.02 / 7.55 3.14 / 7.90 2.48 / 7.30 2.59 / 6.65 2.57 / 6.62

Full
5.89 / 11.21 5.55 / 10.54 6.13 / 11.46 4.90 / 8.97 4.79 / 9.18 4.47 / 8.91

30 Minutes

Event

Hypo

5.43 / 11.87 5.22 / 11.47 5.23 / 11.56 4.84 / 10.42 4.82 / 10.36 4.55 / 10.30

12.14 / 9.41 11.31 / 7.73 12.09 / 9.23 13.38 / 11.97 11.89 / 9.99 9.96 / 7.50

Hyper
4.98 / 12.83 4.75 / 12.37 4.90 / 12.12 4.03 / 10.32 4.08 / 11.19 4.04 / 10.89

TABLE I: The median forecasting errors, in terms of APE and RMSE, for 15 and 30 minute prediction windows

APE / RMSE
Algorithms
ARIMA RF: Rec RF: MO PolySeqMo Our Method (MSE) Our Method (Robust)

Full
7.84 / 14.63 7.11 / 14.10 8.06 / 14.85 6.83 / 12.32 6.44 / 12.60 6.27 / 11.74

45 Minutes

Event

Hypo

6.93 / 15.89 6.89 / 15.96 6.91 / 15.75 6.46 / 14.52 6.65 / 15.25 6.21 / 14.09

16.32 / 12.91 15.08 / 10.41 17.09 / 13.58 18.51 / 17.30 15.69 / 13.09 13.08 / 9.81

Hyper
6.84 / 18.16 6.77 / 18.46 6.39 / 16.90 5.42 / 14.20 5.63 / 15.44 5.45 / 15.04

Full
9.85 / 17.65 9.04 / 17.15 10.22 / 18.27 8.55 / 15.68 8.17 / 15.67 7.92 / 14.81

60 Minutes

Event

Hypo

8.91 / 19.86 8.97 / 20.36 8.61 / 19.90 8.27 / 18.81 8.29 / 19.37 8.01 / 17.80

19.94 / 14.53 18.84 / 12.43 21.64 / 17.36 22.86 / 21.87 18.72 / 16.26 15.97 / 12.57

Hyper
8.51 / 22.17 8.68 / 23.41 7.99 / 21.58 6.77 / 18.30 6.99 / 19.22 6.82 / 19.07

TABLE II: The median forecasting errors, in terms of APE and RMSE, for 45 and 60 minute prediction windows

of the art method. We did not compare with models that were already shown to be inferior for BG prediction. During training, we allow each algorithm to utilize up to the previous 190 data points (approximately 16 hours).
· ARIMA: A traditional statistical method for time series prediction is auto-regressive integrated moving average (ARIMA), and they have been previously used in forecasting BG level [4]. As the name suggests, there are three parts to the model: an auto-regressive part, an integration, or differencing, part, and a moving average part. The auto-regressive (AR) part is simply regressing the current value on the past p values of the time series. The integrated part is designed to account for non-stationary trends. Lastly, the moving average piece models is a regression of the current value on the past q error terms. Combining these correctly can be quite powerful for consistent trends.
· Random Forests: Random forests are a standard baseline ensemble method for prediction. It builds many prediction trees and then averages the predictions from each tree to get a final ensembled prediction. They also have been used for BG forecasting [9]. There are two possible implementations for predicting multiple time points into the future. One way is for the RF to output the whole prediction window time series as a vector (RF: MO for multi-output). An alternative to attempt to incorporate time in the prediction is to recursively do one-step ahead prediction with a sliding window (so that the input dimension remains consistent) and the output is one dimensional (RF: Rec).
· PolySeqMo: The current state of the art model [20] uses a recurrent auto-encoder model and has shown superior results to the standard statistical methods. PolySeqMo uses an encoder to summarize the history of BG levels; it uses the encoded vector to learn the coefficients of

a polynomial with the decoder. The method then uses the coefficients to provide forecasting based on the fitted polynomial. The model does not provide personalized forecasting but instead learns a generic model for the entire population. Since they got the best result when they fixed the degree of the polynomial to one (i.e. linear forecasting), we follow the same setup for comparison. They fix the prediction window to 6 time lags (30 minutes); however, we changed the prediction window to 12 (one hour) for a longer comparison as well.
C. Implementation Details
We implement our model in Pytorch and use RAdam [31] for the optimization with the default parameters. For our method, we do adaptive gradient clipping with an initial value of 2 and a decay rate of 0.99 for each epoch, and we do early stopping based on the validation set. For the encoder, we use a bidirectional Gated Recurrent Unit (GRU) [32] with 120 hidden dimensions and for the decoder we use a feed forward GRU with 30 hidden dimensions. A single layer fully connected neural network with 60 hidden units transforms the decoder output into the explicit forecast values. Our embedding consists of applying a linear transformation to the one-hot patient encoding vector to obtain a 5 dimensional embedding. We initialize each parameter in our model with a normal distribution with mean 0 and standard deviation 0.1. The top 10 percent of loss terms are not used while training the model ( = 0.9).
To be able to capture long-term dependencies, we set the encoder length to be 190 in our experiment and for the forecasting window, we use 12 time steps (one hour) into the future. We followed the choice in [20] to use a 10-step input size for the RF baselines, which are implemented using

(a) The first quartile of APE

(b) The median of APE

(c) The third quartile of APE

(d) The first quartile of RMSE

(e) The median of RMSE

(f) The third quartile of RMSE

Fig. 5: Ablation study forecasting results in terms of APE and RMSE for 15, 30, 45, and 60 minutes into the future

ScikitLearn [33]. The code, the trained models, and the data will be (is) available at the Github page of the authors1.
D. Results
Tables I and II summarize the forecasting errors of the different methods for 15, 30, 45, and 60 minutes into the future. The variance of the reported medians in the tables is less than 0.01 since we had a large (over 3,000) test set. As is shown in the "Full" column, our method performs best overall for predicting BG levels in the future. Our method also performs the best in term of predicting the hypoglycemia APE for more than 15 minutes into the future. Short term health complications of hypoglycemia can be deadly, so having an early warning is crucial. Hypoglycemia forecasting is the most difficult to predict in terms of APE (as it has the largest values), but our method improves on it by about 15% for 30 minutes or longer into the future. In the cases that our method is not better than the others, the difference is often insignificant.
To highlight the importance of robust training, we compare our full robust method to our method without robust training (Our Method (MSE)). Tables I and II clearly show that adding robust training improves performance in all cases. Even without the robust training, our method performs better than PolySeqMo in many scenarios. It is also of note that due to the categorical training of PolySeqMo, our robust training addition is not directly applicable.
PolySeqMo has another disadvantage besides the overall performance: their best results come from only fitting a linear forecast. As we argued in the introduction, knowing the trend is quite important to understand the urgency of required action. Our method can capture general trends (not limited to polynomial trends) due to the direct use of added inputs and both GRU and NN layers for decoding.
1https://github.com/rezaarmand/glucose att rnn

E. Ablation Study
Finally, we provide an ablation study to stress the importance of using both an attention mechanism and personalized embeddings. We add two additional versions of our method: one without attention mechanism ('W/O Att') and the other by excluding embedding vectors ('W/O Embed'). For comparison, we plot these alongside both our full model and the previously described PolySeqMo model in Figure 5. The result is based on using the entire test data set and correspond to the "Full" columns in Tables I, II. The plots provide quartile estimates to show the variability of errors and clearly affirm that all parts of our model are necessary to better capture the long-term dependencies in blood glucose levels.
IV. CONCLUSION
In this paper, we study the problem of BG trajectory forecasting without any need of exogenous information, such as carbohydrate intake or exercise. Our algorithm has three main components that we motivate by analyzing real data. One component is using the idea of embedding vectors to provide personalized prediction. By having shared parameters across the patients, the model can alleviate the need for a huge set of observations for each patient. Another component is designed to capture long term dependencies within the time series. To achieve this better than vanilla recurrent auto-encoders, we add an attention mechanism and provide time features as inputs. Our method reserves the ability to take other information as inputs to make more informative blood glucose forecasting. The last component is a more robust training algorithm to further improve our results.
Compared to a state of the art method, PolySeqMo (which has already shown effectiveness over other widely used methods), our approach demonstrated a consistent improvement for forecasting window of all sizes up to one hour, indicated by smaller error metrics on a real world large dataset consisting

of 400K continuous blood glucose measurements. Such results highly suggest the efficacy of our proposed approach in forecasting blood glucose levels and more generally in providing better care for people living with diabetes.
For future work, we would investigate the probabilistic forecasting solutions to the problem. The idea is to not only provide a point estimate of the future values but also a confidence bound about how certain the model is about its estimations. We plan to do that by changing the objective to a log-normal loss function and learning the variance of the estimated forecast with a recurrent autoencoder or using Bayesian neural network.
V. ACKNOWLEDGEMENTS
The authors thank Ran Duan for many useful discussions and guidance. The authors thank the anonymous reviewers and area chair for their valuable comments and suggestions that have helped us to improve the paper.
REFERENCES
[1] N. Cho, J. Shaw, S. Karuranga, Y. Huang, J. da Rocha Fernandes, A. Ohlrogge, and B. Malanda, "Idf diabetes atlas: Global estimates of diabetes prevalence for 2017 and projections for 2045," Diabetes research and clinical practice, vol. 138, pp. 271­281, 2018.
[2] M. A. Atkinson, G. S. Eisenbarth, and A. W. Michels, "Type 1 diabetes," The Lancet, vol. 383, no. 9911, pp. 69­82, 2014.
[3] D. J. Albers, M. Levine, B. Gluckman, H. Ginsberg, G. Hripcsak, and L. Mamykina, "Personalized glucose forecasting for type 2 diabetes using data assimilation," PLoS computational biology, vol. 13, no. 4, p. e1005232, 2017.
[4] K. Plis, R. Bunescu, C. Marling, J. Shubrook, and F. Schwartz, "A machine learning approach to predicting blood glucose levels for diabetes management," in Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
[5] S. Oviedo, J. Veh´i, R. Calm, and J. Armengol, "A review of personalized blood glucose prediction strategies for t1dm patients," International journal for numerical methods in biomedical engineering, vol. 33, no. 6, p. e2833, 2017.
[6] J. F. de Canete, S. Gonzalez-Perez, and J. Ramos-Diaz, "Artificial neural networks for closed loop control of in silico and ad hoc type 1 diabetes," Computer methods and programs in biomedicine, vol. 106, no. 1, pp. 55­66, 2012.
[7] K. Turksoy, E. S. Bayrak, L. Quinn, E. Littlejohn, D. Rollins, and A. Cinar, "Hypoglycemia early alarm systems based on multivariable models," Industrial & engineering chemistry research, vol. 52, no. 35, pp. 12 329­12 336, 2013.
[8] C. Zecchin, A. Facchinetti, G. Sparacino, and C. Cobelli, "Jump neural network for real-time prediction of glucose concentration," in Artificial Neural Networks. Springer, 2015, pp. 245­259.
[9] B. Sudharsan, M. Peeples, and M. Shomali, "Hypoglycemia prediction using machine learning models for patients with type 2 diabetes," Journal of diabetes science and technology, vol. 9, no. 1, pp. 86­90, 2014.
[10] C. Zecchin, A. Facchinetti, G. Sparacino, and C. Cobelli, "How much is short-term glucose prediction in type 1 diabetes improved by adding insulin delivery and meal content information to cgm data? a proofof-concept study," Journal of diabetes science and technology, vol. 10, no. 5, pp. 1149­1160, 2016.
[11] C. Novara, N. M. Pour, T. Vincent, and G. Grassi, "A nonlinear blind identification approach to modeling of diabetic patients," IEEE Transactions on Control Systems Technology, vol. 24, no. 3, pp. 1092­ 1100, 2015.
[12] M. Eren-Oruklu, A. Cinar, and L. Quinn, "Hypoglycemia prediction with subject-specific recursive time-series models," 2010.
[13] M. Otoom, H. Alshraideh, H. M. Almasaeid, D. Lo´pez-de Ipin~a, and J. Bravo, "Real-time statistical modeling of blood sugar," Journal of medical systems, vol. 39, no. 10, p. 123, 2015.

[14] D. Boiroux, A. K. Duun-Henriksen, S. Schmidt, K. Nørgaard, S. Madsbad, O. Skyggebjerg, P. R. Jensen, N. K. Poulsen, H. Madsen, and J. B. Jørgensen, "Overnight control of blood glucose in people with type 1 diabetes," IFAC Proceedings Volumes, vol. 45, no. 18, pp. 73­78, 2012.
[15] E. I. Georga, V. C. Protopappas, D. Polyzos, and D. I. Fotiadis, "Evaluation of short-term predictors of glucose concentration in type 1 diabetes combining feature ranking with regression models," Medical & biological engineering & computing, vol. 53, no. 12, pp. 1305­1318, 2015.
[16] J. I. Hidalgo, J. M. Colmenar, G. Kronberger, S. M. Winkler, O. Garnica, and J. Lanchares, "Data based prediction of blood glucose concentrations using evolutionary methods," Journal of medical systems, vol. 41, no. 9, p. 142, 2017.
[17] V. L. Guen and N. Thome, "Probabilistic time series forecasting with structured shape and temporal diversity," NeurIPS 2020, 2020.
[18] R. Kurle, S. S. Rangapuram, E. de Be´zenac, S. Gu¨nnemann, and J. Gasthaus, "Deep rao-blackwellised particle filters for time series forecasting," Advances in Neural Information Processing Systems, vol. 33, 2020.
[19] S. Wu, X. Xiao, Q. Ding, P. Zhao, Y. Wei, and J. Huang, "Adversarial sparse transformer for time series forecasting," Advances in Neural Information Processing Systems, vol. 33, 2020.
[20] I. Fox, L. Ang, M. Jaiswal, R. Pop-Busui, and J. Wiens, "Deep multi-output forecasting: Learning to accurately predict blood glucose trajectories," in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018, pp. 1387­1395.
[21] Q. Sun, M. V. Jankovic, L. Bally, and S. G. Mougiakakou, "Predicting blood glucose with an lstm and bi-lstm based deep neural network," in 2018 14th Symposium on Neural Networks and Applications (NEUREL). IEEE, 2018, pp. 1­5.
[22] R. Pascanu, T. Mikolov, and Y. Bengio, "On the difficulty of training recurrent neural networks," in International conference on machine learning, 2013, pp. 1310­1318.
[23] T. Zhu, K. Li, P. Herrero, J. Chen, and P. Georgiou, "A deep learning algorithm for personalized blood glucose prediction." in KHD@ IJCAI, 2018, pp. 64­78.
[24] P. D. Hoff, A. E. Raftery, and M. S. Handcock, "Latent space approaches to social network analysis," Journal of the american Statistical association, vol. 97, no. 460, pp. 1090­1098, 2002.
[25] W. Hamilton, Z. Ying, and J. Leskovec, "Inductive representation learning on large graphs," in Advances in Neural Information Processing Systems, 2017, pp. 1024­1034.
[26] M. Armandpour, P. Ding, J. Huang, and X. Hu, "Robust negative sampling for network embedding," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 3191­3198.
[27] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their compositionality," in Advances in neural information processing systems, 2013, pp. 3111­3119.
[28] M. Schuster and K. K. Paliwal, "Bidirectional recurrent neural networks," IEEE transactions on Signal Processing, vol. 45, no. 11, pp. 2673­2681, 1997.
[29] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, "Wavenet: A generative model for raw audio," arXiv preprint arXiv:1609.03499, 2016.
[30] R. J. Williams and D. Zipser, "A learning algorithm for continually running fully recurrent neural networks," Neural computation, vol. 1, no. 2, pp. 270­280, 1989.
[31] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, "On the variance of the adaptive learning rate and beyond," arXiv preprint arXiv:1908.03265, 2019.
[32] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, "Empirical evaluation of gated recurrent neural networks on sequence modeling," arXiv preprint arXiv:1412.3555, 2014.
[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al., "Scikit-learn: Machine learning in python," Journal of machine learning research, vol. 12, no. Oct, pp. 2825­2830, 2011.

