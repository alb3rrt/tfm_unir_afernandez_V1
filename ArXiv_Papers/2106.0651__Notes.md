
# Asymptotics of representation learning in finite Bayesian neural networks

[arXiv](https://arxiv.org/abs/2106.0651), [PDF](https://arxiv.org/pdf/2106.0651.pdf)

## Authors

- Jacob A. Zavatone-Veth
- Abdulkadir Canatar
- Cengiz Pehlevan

## Abstract

Recent works have suggested that finite Bayesian neural networks may outperform their infinite cousins because finite networks can flexibly adapt their internal representations. However, our theoretical understanding of how the learned hidden layer representations of finite networks differ from the fixed representations of infinite networks remains incomplete. Perturbative finite-width corrections to the network prior and posterior have been studied, but the asymptotics of learned features have not been fully characterized. Here, we argue that the leading finite-width corrections to the average feature kernels for any Bayesian network with linear readout and quadratic cost have a largely universal form. We illustrate this explicitly for two classes of fully connected networks: deep linear networks and networks with a single nonlinear hidden layer. Our results begin to elucidate which features of data wide Bayesian neural networks learn to represent.

## Comments

12+28 pages, 2+1 figures

## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{zavatoneveth2021asymptotics,
      title={Asymptotics of representation learning in finite Bayesian neural networks}, 
      author={Jacob A. Zavatone-Veth and Abdulkadir Canatar and Cengiz Pehlevan},
      year={2021},
      eprint={2106.00651},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

