Granger Causality from Quantized Measurements 
Salman Ahmadi, Girish N. Nair, Erik Weyer
Department of Electrical and Electronic Engineering, University of Melbourne, VIC 3010, Australia

arXiv:2106.01513v1 [eess.SY] 3 Jun 2021

Abstract
An approach is proposed for inferring Granger causality between jointly stationary, Gaussian signals from quantized data. First, a necessary and sufficient rank criterion for the equality of two conditional Gaussian distributions is proved. Assuming a partial finite-order Markov property, conditions are then derived under which Granger causality between them can be reliably inferred from the second order moments of the quantized processes. A necessary and sufficient condition is proposed for Granger causality inference under binary quantization. Furthermore, sufficient conditions are introduced to infer Granger causality between jointly Gaussian signals through measurements quantized via non-uniform, uniform or high resolution quantizers. This approach does not require the statistics of the underlying Gaussian signals to be estimated, or a system model to be identified. No assumptions are made on the identifiability of the jointly Gaussian random processes through the quantized observations. The effectiveness of the proposed method is illustrated by simulation results.
Key words: Causal inference; quantization; system identification.

1 Introduction
Causal inference is the determination of the qualitative cause-and-effect (or input versus output) relationships between two or more signals over time. Sometimes these relationships are obvious beforehand, but in many critical applications they are not. For instance, in environmental monitoring the direction in which a pollutant spreads may be unknown to begin with, making it difficult to determine a priori which measurements are inputs and which are outputs. In large manufacturing plants, the root cause of alarm signals is commonly obscured by complex feedback loops. A misunderstanding of the correct causal relationships not only reduces the accuracy of the subsequently identified model, but could mislead decision-makers into poorly founded interventions.
In 1963, the econometrician C. Granger introduced a definition of causality in terms of statistical prediction [22], inspired by the work of N. Wiener [52]. A signal x is said
 This work was partially supported by the Australian Research Council via Future Fellowship grant FT140100527. The material in this paper was partially presented at the 58th IEEE Conf. Decision and Control (CDC), December, 2019, Nice, France [3] and will also partially appear in IFAC-PapersOnLine, after the recent cancellation of the 24th Int. Symp. Mathematical Theory of Networks and Systems (MTNS 2020) [2]. Corresponding author: Salman Ahmadi.
Email addresses: ahmadis@student.unimelb.edu.au (Salman Ahmadi), gnair@unimelb.edu.au (Girish N. Nair), ewey@unimelb.edu.au (Erik Weyer).

to cause another signal z if at some time, the optimal expected prediction error for a future value of z is reduced by knowledge of past x and z, as compared to if the past values only of z are known. In subsequent work [23, 24], Granger proposed a looser definition in terms of conditional probabilities, whereby x is said to cause z if, at some time, a future z and past x are conditionally dependent given past z; i.e., given past z, past x can still influence the future of z. In the case of jointly Gaussian processes under a meansquare error prediction error, the first definition coincides with the second. These definitions allow causality from z to x as well, which would reflect mutual coupling between the two processes as they evolve over time.
As defined above, Granger causality is a `non-interventionist' notion based on signals rather than systems. This suits applications where signals can only be measured, and cannot easily be adjusted through experiments. In [11], a connection with linear systems theory is introduced through the idea of feedback-freeness for wide-sense stationary random processes, which is shown to be equivalent to Granger non-causality under linear minimum mean-square error prediction. A more restrictive version of feedback-free random processes having block-diagonal innovation covariance is introduced in [10] and its equivalence to strictly causal linear systems is discussed. The relation between Granger non-causality and a linear time-invariant state space representation with a star graph structure as a network topology is considered in [29], and it was shown that Granger noncausality is equivalent to the existence of such a representation. Granger causality between processes corrupted by additive noise or affected by filtering or sampling has also

Preprint submitted to Automatica

4 June 2021

been investigated [4, 6, 20, 37, 44, 45].
In this paper, we focus on the effects of quantization. We investigate methods for inferring Granger causality between two jointly Gaussian, stationary signals using quantized measurements. Evidently, the nonlinearity introduced by the quantizers moves this problem beyond the linear systems realm of the literature above.
Our main contributions are as follows:
· Assuming a partial finite-order Markov property, we introduce a causality matrix comprising joint process covariances, and show that Granger causality is characterized in terms of its rank. The basis of our analysis is a necessary and sufficient rank condition for two conditional Gaussian distributions to be identical (Theorem 2.1). This extends a recent result of [46, 47] on Gaussian conditional independence. To the best of our knowledge, this causality matrix has not been previously studied in the system identification or causality literature.
· We then consider binary quantization, and derive a necessary and sufficient condition to infer Granger causality between Gaussian signals from binary measurements (Theorem 3.1). This holds even though the variances of the unquantized signals may not be identifiable.
· Next we consider multi-level quantizers and derive sufficient conditions under which Granger causality between jointly Gaussian signals can be inferred from the second order moments of their quantized versions (Theorem 4.2). This uses a perturbation analysis of the causality matrix introduced above, bounds on covariances of quantization error terms combined with the Eckart-Young-Mirsky approximation theorem. Exploring this result, we derive sufficient conditions for three cases: a) non-uniformly quantized measurements with finite number of quantization levels (Proposition 4.1), b) uniform quantization with infinitely many quantization levels (Proposition 5.1), c) high resolution quantization (Proposition 5.2).
Unlike much of the literature on causal inference, e.g. [21, 32, 43], our approach does not require the statistics of the underlying Gaussian signals to be estimated, or a system model to be explicitly identified.
Preliminary versions of these results have been accepted or presented in [2, 3]. Here, we provide full proofs for the binary and non-uniform quantization cases, improving the results compared with [2], and refine our previous results on high resolution quantization in [3]. We also introduce new results on uniform quantization with infinitely many levels. Moreover, we investigate the required ergodicity properties of the quantized data to be able to estimate their covariances. Further, we present a numerical example.

The paper is organized as follows. In Section 2, we characterize Granger causality between a pair of jointly Gaussian signals in terms of the rank of a special matrix of covariances. In Section 3 we derive necessary and sufficient conditions for Granger causality using binary quantized signals. In Section 4, we investigate the effects of multi-level quantization and derive sufficient conditions for inferring Granger causality between the unquantized signals from the statistics of the quantized data. Using different techniques, we focus in Section 5 on infinite-level uniform quantization, including the high-resolution case. The empirical estimation of second order moments of the quantized processes is discussed in Section 6. Some simulation results are presented in Section 7 demonstrating the proposed methods. Section 8 concludes the paper.

Notation: Throughout this paper, we denote the random

tpthhreeocecemosnspvsteyengstmieoqenunsetsnthc(axetk)wxnkn=he=nb(yxkx>)nnkn=, 1aonrwdnh(ex<nk)1nk=.1Sib1my, iaxlannr.dlyWe, qexunuasiless

the empty sequence when n < 1. When clear from context,

the full sequence (xk)k=1 is written as x. For a parameter a, the overline and underline (a, a) denote known upper and

lower

bounds,

respectively.

We

also

denote

lim
y0

g(y) f (y)

=

0

by

the short-hand notation f (y)  g(y).

2 Granger causality investigation
Let us first begin with the definition of causality between discrete-time stochastic processes, in terms of conditional independence. Let x, z be discrete-time random processes on the time-axis k = 1, 2, . . . . In 1980, Granger [23] defined that x does not cause z if:
P(zk+1|xk, zk) = P(zk+1|zk), with probability (w.p.) 1, k = 1, 2, . . . ,
where P(·|·) denotes conditional probability measure. Otherwise, if for some k  1 there is a nonzero probability that P(zk+1|xk, zk) = P(zk+1|zk), then x causes z.
In other words, x causes z if and only if (iff) there is a nonzero chance that at some time k + 1, the value of z could still be stochastically influenced by past x, even if all past values of z up to time k are known. If x does not cause z, the values of z are always conditionally independent of past x, given past z.
Now let us assume the following:
Assumption 2.1 (Partial Markov-m) The random process z is partially Markov of order m  1 in x and z, that is, P(zk+1|xk, zk) = P(zk+1|xkk-m+1, zkk-m+1), k  1.
Remark 2.1 This is strictly weaker than being joint Markov for the same order m, and allows the x-process to potentially have a much higher-order dependence on the past.

2

For instance, let us consider xk+1 = a0xk + a1xk-1 + b0zk + b1zk-1 + wk and zk+1 = c0xk + d0zk + vk where wk and vk are independent white noise and a1 or b1 are nonzero. In this example, x and z are joint Markov of order two while the process z is partially Markov of order one. This example can easily be extended so that x and z are joint Markov of arbitrarily high or infinite order, while z remains partial Markov-1.
With this restriction, we define Granger causality as follows:
Definition 2.1 (Granger Causality) The random process x is said to not Granger cause (GC) z if:
P(zk+1|xkk-m+1, zkk-m+1) = P(zk+1|zk), w.p. 1, k  m. (1)
Otherwise, if for some k  m there is a nonzero probability that P(zk+1|xkk-m+1, zkk-m+1) = P(zk+1|zk), then x is said to GC z.
Remark 2.2 We assess Granger causality definition from time m rather than 1 to avoid technical issues about initial conditions. Different authors have treated the initial condition differently; see e.g. [19]. In [6, 11, 45], time starts from negative infinity while Granger [23] considers the starting time k = 1. In any case, it makes no practical difference because when we infer Granger causality, we need a large number of data points.
As the conditioning term on the RHS is no longer nested (i.e. included) in that of the LHS, this is no longer a conditional independence relationship. In much of the literature e.g. [31, 40, 41], it is assumed that under non-causality the process z is also Markov-m, so that zk-m can be dropped from the RHS. However, finite-order partial Markovianity does not generally imply finite-order marginal Markovianity. In this paper, we do not make any a priori assumption on the Markovianity of z, but show that for jointly Gaussian signals, (1) and Assumption 2.1 imply that z is marginally Markov-m, under a mild additional requirement.
For jointly Gaussian processes, it turns out that (1) can be expressed in terms of a rank condition. We first present this condition for general jointly Gaussian random vectors A, B,C, D with joint covariance matrix  . The covariance matrices between subsets of variables are denoted by matching subscripts, for instance A,B = BT,A is the crosscovariance matrix between A and B, while [CD],[CD] is the covariance matrix for the random vector [C D]. 1
Theorem 2.1 Let A, B,C, D be jointly Gaussian random vectors with joint covariance matrix  , and suppose that the random vector [B C D] has positive definite covariance. Then the following statements are equivalent:
1 For convenience we adopt this mild abuse of notation, rather than [CT DT ]T .

1. The conditional distributions P(A|C, B) and P(A|C, D) are identical.
2.

rank A,B A,C A,D = #C,

(2)

C,B C,C C,D

where #C is the dimension of the random vector C. 3. P(A|C, B) = P(A|C, D) = P(A|C).

PROOF. See Appendix A.

Remark 2.3 The second item in this result is a variation of a
recent rank formula for Gaussian conditional independence, due to Sullivant [46, 47]. Under joint Gaussianity, the positive definiteness of [BCD],[BCD] excludes degenerate cases where deterministic (affine) relationships exist between B, C and D. With additional analysis, it can be shown that if D is constant (requiring this positive definiteness to be relaxed),
then the formula of [46, 47] for conditional independence can be recovered as a special case. However, this extension is not needed for our purposes. The third item in this result shows that P(A|C, B) = P(A|C, D) if and only if conditional independence between A, B and between A, D immediately follow given C.

Now suppose that x, z are jointly stationary Gaussian sig-

nals satisfying Assumption 2.1. Further assume that they are

each scalar-valued, for simplicity, and that there is never a

d(mete+rm1)in×is(tic+remla)ticoanusshaiplitbyemtwaeterinxxCkk-Gxm+z(1ma,nd),zk

. Define  m as:

the

CGxz(m, ) := z,x~ z,z~ z,zo ,

(3)

where z := zkk+-1m+1, z~ := zkk-m+1, zo := zkk--m+1, and x~ := xkk-m+1. The covariances z,x~ and z,z~ are (m + 1) × m matrices, and z,zo is an (m + 1) × ( - m) matrix. The causality matrix depends on the cross-covariances between x and z and the autocovariances of z, but not on the autocovariances
of x. We have the following:

Theorem 2.2 Let x, z be jointly stationary scalar Gaussian signals satisfying Assumption 2.1. Further assume that there
is no deterministic relationship between any of the components of xkk-m+1 zk at any time k  m.

i) x does not Granger cause z if and only if

rankCGxz(m, k) = m, k  m.

(4)

ii) If x does not Granger cause z, then z is marginally Markov-m, i.e.

P(zk+1|zk) = P(zk+1|zkk-m+1).

(5)

3

iii) If P(zk+1|xkk-m+1, zkk-m+1) = P(zk+1|zkk-m+1) then P(zk+1|zk) = P(zk+1|zkk-m+1).
PROOF. See Appendix B.
Remark 2.4 A distinguishing feature of this result is that it allows Granger causality to be inferred directly from the second-order statistics of the signals, without having to analyze or fit a linear dynamical model as is often done in the literature.
Remark 2.5 The assumption of no deterministic relationship between any of the components of xkk-m+1 zk in Theorem 2.2 is more relaxed than Axiom B in Granger's paper [23] which assumes no deterministic relationship between any of the components of xk zk . For instance, the process xk+1 = xk-m+1 where x1, ..., xm are independent and identically distributed is covered by Theorem 2.2 but not by [23]. This is a stationary random process which evolves periodically from its initial conditions. The last item in Theorem 2.2 does not require the assumption of no deterministic relationship between any of the component of xkk-m+1 zk at any time k  m.
Remark 2.6 One of the implications of the last item in Theorem 2.2 is that the last block column in (3) is not needed when the rank condition in (4) is evaluated, that is (4) can be replaced by rankCGxz(m, m) = m. However the last block column is kept since it is used in the following Sections when sufficient conditions in terms of the smallest singular value are derived.
Now we investigate Granger causality from signals quantized by binary quantizers.
3 Granger causality under binary quantization
In this Section, we consider zero-mean jointly Gaussian stationary signals x and z passing through binary quantizers with zero thresholds. If a mean is nonzero, this is equivalent to assuming that the respective quantizer threshold is set equal to it. The observed signals are xQ and zQ, respectively. We are interested in revealing whether or not x Granger causes z, using the statistics of the quantized signals only.
In the causality matrix CGxz(m, ) defined in (3), covariances (xz(.) and zz(.)) and the variance of signal z (z2) are required. However, the variance of zero-mean Gaussian signals passed through a binary quantizer with threshold zero cannot be estimated, e.g. [35]. Hence, some of the elements of CGxz(m, ) cannot be estimated.
In order to solve this issue, a modified matrix is constructed. We know that if B is a nonsingular matrix, then rank(CGxz(m, )B) = rankCGxz(m, ). Let us construct a

matrix

B

:=

diag(

1 x z

,

.

.

.

,

1 x z

,

1 z2

,

.

.

.

,

1 z2

)

where

the

num-

bers of

1 xz

and

1 z2

on

the diagonal of the matrix

B

is equal to

the number of columns of the matrix z,x~ and [z,z~ z,zo ],

respectively. Now instead of determining the rank of the

causality matrix CGxz(m, ), the rank of

RGxz(m, ) := CGxz(m, )B,

(6)

whose elements are the cross-correlation coefficients between signals x and z and the auto-correlation coefficients of signals z is obtained. The relation between auto-correlation coefficients of a zero-mean Gaussian signal passing through the binary quantizer with zero threshold and outputs -1 and +1 and the covariance of output of the binary quantizer can be estimated through Van Vleck's formula [49].

Suppose that two zero-mean jointly Gaussian random vari-
ables w1 and w2 with correlation coefficient w1w2 = 1 are quantized by binary quantizers with zero thresholds and the
outputs are wQ1  {-1, +1} and wQ2  {-1, +1}, respectively:

wQi =

-1 +1

wi wi

< >

0 0

,

i = 1, 2.

(7)

It can be shown that a relation similar to Van Vleck's formula holds between auto-/cross-correlation coefficients of the jointly Gaussian random variables and the covariances of the quantized variables as follows:

w1w2 = sin

 2

wQ1 wQ2

.

(8)

In fact, in order to investigate Granger causality in this Section, the elements of the causality matrix are modified to construct the matrix RGxz(m, ) depending just on correlation coefficients since the variances of zero-mean Gaussian signals after binary quantization with the threshold zero are not identifiable while the rank of RGxz(m, ) equals the rank of the causality matrix.

Using Theorem 2.2 and Remark 2.6, the following necessary and sufficient condition on Granger causality between signals quantized by binary quantizers can be stated:

Theorem 3.1 Let x, z be jointly zero-mean Gaussian, sta-

tionary, scalar random processes and z be partial Markov-

m. Further assume that there is no deterministic relationship between any of the components of xkk-m+1 zk at any time k  m.

Then x does not Granger cause z if and only if the matrix

RRGGxxzz((mm,,

m) m)

(6) are

is not auto-

full and

rank. The elements of the matrix cross-correlation coefficients as-

4

sociated with CGxz(m, m) (xz and zz) obtained as follows:

xz(i) = sin

 2

xQ

zQ

(i)

, i = 1 - m, . . ., m - 1, m,

(9)

zz( j) = sin

 2

zQ

zQ

(

j)

,j

= 1, 2, . . . , m,

(10)

zz(0) = 1,

(11)

where xQzQ and zQzQ are, respectively, cross- and autocovariance estimates of one-bit measurements observed
through binary quantizers with zero thresholds and values
-1 and +1.

Remark 3.1 Note that the necessity and sufficiency of the result follows from the fact that (8) provides us a closed form relationship between the correlation coefficients of the unquantized signals and the covariance of their quantized versions. Thus, the correlation coefficients of zero-mean jointly Gaussian signals can be obtained. Furthermore, the rank of the causality matrix and of the matrix constructed by correlation coefficients are equal. Hence, Theorem 2.2 and Remark 2.6 imply the necessity and sufficiency on Granger causality between jointly zero-mean Gaussian, stationary random processes.

In the following Section, we investigate Granger causality through signals quantized by any quantizers and general sufficient conditions on Granger causality via quantized measurement are introduced. And then such conditions are explored for non-uniform (Subsection 4.3), uniform and high resolution quantizers (Section 5).

4 Inferring Granger causality using finite-level quantized data
In this Section, the impact of finite-level quantization on the inference of Granger causality between the jointly Gaussian, stationary processes x and z where z is partially Markov of order m is investigated. Using the second-order statistics of the quantized data, we construct a post-quantization matrix CxQzQ that mirrors the causality matrix CGxz (3) of the unquantized processes. We then show that if the difference between these two matrices is sufficiently small, then the fullrankness of CxQzQ implies that x Granger causes (GC) z. In the following Subsection, some preliminary results useful for inferring Granger causality through quantized signals are presented.
4.1 Preliminaries
Theorem 2.2 implies that if for some q  m the causality matrix CGxz(m, q) has full rank m + 1, then x GC z. The question here is whether we can infer this causal relationship from the covariances of the quantized data. To answer this question, we will use a classical result in linear algebra:

Theorem 4.1 (Eckart-Young-Mirsky Matrix Approximation [17, 36]) Let the matrix M  Rl×s have rank r and singular value decomposition M = ri=1 iuivTi , where ui, v j, 1  i, j  r are orthonormal vectors and 1  2  · · ·  r(> 0) are the singular values. If p < r, then

min
rank X=p

M-X

2=

M - Mp

2 = p+1,

(12)

and

 min
rank X=p

M-X

F=

M - Mp F =

i2,
i p+1

(13)

where Mp = ip=1 iuivTi .
Remark 4.1 Unless otherwise stated, · denotes either two-norm ( · 2) or Frobenius norm ( · F) in the following.

4.2 Granger causality inference under quantization

In this Subsection, the relationship between the causality matrix and its counterpart constructed by quantized signals is investigated. Using the results above, a condition to determine the rank of the causality matrix through the rank of its counterpart CxQzQ is derived.

4.2.1 Relationship between CGxz and CxQzQ
We begin with the relationship between the scalar covariances zx, zz of the unquantized signals, and zQxQ , zQzQ of their quantized versions, where superscript Q on a signal denotes the quantized version. For convenience, in the following we suppress time lags and use w1 and w2 to denote x and/or z. We have

wQ1 wQ2 = w1w2 + w12 + 1w2 + 12 ,

(14)

where 1 and 2 denote the quantization errors of w1 and w2 respectively (i := wQi - wi). Now define the matrix

CxQzQ (m, ) := zQ,x~Q zQ,z~Q zQ,zoQ CGxz(m, ) + (m, ),

(15)

where  (m, ) := [ij ] is defined as:

 (m, ) := z,x~ z,z~ z,zo + z ,x~ z ,z~ z ,zo +

z ,x~ z ,z~ z ,zo ,

(16)

which is the matrix version of the scalar relationship (14).

5

Theorem 4.2 Let x, z be jointly Gaussian, stationary, scalar random processes and z be partial Markov-m. Further as-
sume that there is no deterministic relationship between any of the components of xkk-m+1 zk at any time k  m. If there exists some q  [m, k] such that the matrix CxQzQ (m, q) in (15) involving the covariances of quantized data is full-rank with smallest singular value

min(CxQzQ (m, q)) >  (m, q) 2,

(17)

where  (m, q) is defined in (16), then the unquantized Gaussian signal x Granger causes the unquantized Gaussian sig-
nal z.

PROOF. Denote the full-rank matrix CxQzQ (m, q) by M.
If X is any other matrix of the same dimensions but lower
rank, then Eckart-Young-Mirsky matrix approximation Theorem (Theorem 4.1) states that X - M  min(M). Conversely, if X - M < min(M), then X remains full-rank. Setting X = Cxz(m, q)  CxQzQ (m, q) +  (m, q), we see that if min(CxQzQ (m, q)) >  (m, q) , then CGxz(m, q) is guaranteed to be full-rank. Theorem 2.2 then implies that x
GC z.

Remark 4.2 This result states that GC can be inferred from the statistics of the quantized data, provided that the quantization perturbation, as measured by (m, q) , is smaller than min(CxQzQ (m, q)), which can be taken as a measure of how far CxQzQ (m, q) is from losing full rank. However, both sides of inequality (17) depend on the quantization schemes. We can derive a condition that compares the size of the quantization perturbation to the statistics of the unquantized processes, as follows. We know that for two matrices A and B of the same dimension min(A) - max(B)  min(A + B), ([28], Problem 7.3.P16). Therefore, we have:
min(CGxz(m, q)) - max( (m, q))  min(CxQzQ (m, q)), (18)

Furthermore, we know that  (m, q) 2 = max( (m, q)). So if

 (m, q) 2 < min(CGxz(m, q)) - max( (m, q)), (19)

then by (18) it is guaranteed to be smaller than min(CxQzQ (m, q)) as required. Finally, note that since (m, q) 2 = max( (m, q)), the condition (19) may be equivalently writ-
ten as

2  (m, q) 2 < min(CGxz(m, q)),

(20)

which can be achieved with sufficiently fine quantization iff CGxz(m, q) is full-rank.

Remark 4.3 Note that Theorem 4.2 and Remark 4.2 hold not only for quantization but also for perturbations such as other nonlinearities or additive noise. In such cases,  (m, q) is the additive perturbation on the causality matrix and the smallest singular value in Theorem 4.2 corresponds to the matrix of covariances formed by the perturbed signals.
In the following, upper bounds (m, q) for non-uniform finite-level and uniform infinite-level quantization are obtained. Theorem 4.2 is then used to introduce sufficient conditions to infer the causality between jointly Gaussian signals through quantized measurements.

4.3 Granger causality under non-uniform quantization

In this Section we consider a general case of Granger causal inference with non-uniform, finite-level quantization. With more than two levels, we no longer have recourse to (8), which recovers the correlation coefficients of the unquantized signals from the covariances of their binary versions. Our approach here, as presented in Subsection 4.2, is to formally construct and analyze a matrix of the same form as the causality matrix (3), but in terms of the covariances of the quantized processes. As the quantized signals are not jointly Gaussian anymore, this matrix cannot be regarded as a causality matrix. We denote such a matrix by CxQzQ(m, ). Using the perturbations caused by quantization on the covariances of unquantized signals and the rank of the matrix CxQzQ (m, ), we then derive sufficient conditions to determine whether the causality matrix CGxz(m, ) is full rank. Then Theorem 2.2 implies that x Granger causes z (Proposition 4.1).
In the following Subsection, the relation between covariances of quantized and unquantized signals is first derived, using Price's theorem [38, 39]. Price's theorem has been previously used in the literature to study the determination of autocorrelations and variances from data passed through quantizers or other nonlinearities [12, 35].

4.3.1 Relation between covariance of quantized and unquantized signals

For convenience, in the following jointly Gaussian random
variables are denoted by w1 and w2 which can be x and/or z at different times and lags.

Let us consider two zero-mean jointly Gaussian random variables w1 and w2 with correlation coefficient  = ±1 that are quantized by non-uniform finite-level quantizers. Their quantized versions are denoted by wQ1 and wQ2 , respectively:

wQ1 = Q1 w1 wQ2 = Q2 w2

n1
 = liIci-1<w1ci , i=1

n2

 =

l

 j

Id

j-1 <w2 d

j

,

j=1

(21) (22)

6

where n1 and n2 are the number of quantization levels, ci's and d j's denote the thresholds of the quantizers and li and lj are levels of quantizers. Note that c0, d0, cn1 and dn2 can be infinite. Without loss of generality, suppose that the quan-
tizer levels are increasing (li+1 > li, i = 1, 2, . . . , n1 - 1 and lj+1 > lj, j = 1, 2, . . . , n2 - 1). Recall that the joint probability density function of w1 and w2 is as follows:

f

(w1, w2)

=

1 2 w1 w2

-1
e 2(1-2) 1-2

, w21
w2 1

-

2 w1 w2 w1 w2

+

w22 w2 2

(23)

where w1 and w2 are the standard deviations of the random variables w1 and w2, respectively.

First let us recall Price's theorem [38, 39].

Theorem 4.3 (Price's Theorem) Let w1 and w2 be jointly Gaussian random variables with joint probability density function f (w1, w2) and g(w1, w2) is some function such that |g(w1, w2)| < Ae|w1| +|w2| where A > 0,  < 2 then

 nE g w1, w2  wn1w2

=E

 2ng(w1, w2)  wn1 wn2

,

(24)

where w1w2 is covariance between w1 and w2.

Using Price's Theorem, the covariance between quantized signals wQ1 and wQ2 can be obtained as follows. See Appendix C for derivation of (25):

n1-1 n2-1

  wQ1 wQ2 = i=1

(li+1 - li)(lj+1 - lj)×
j=1


2
0

1

-1
e 2(1-y2)

c2i w2 1

-2y

cid j w1 w2

+

d2j w2 2

dy.

(25)

1 - y2

Note that after quantization we do not have access to the values of correlation coefficients  and of the standard deviations w1 and w2 (with one single subscript). In the following, we find upper bounds on |wQ1 wQ2 - w1w2 | and then we exploit such bounds to guarantee the causality matrix is full
rank. In other words, to apply Theorem 4.2 for non-uniform quantized signals, the norm of (m, ) = [ij ], whose components are the differences between covariances of unquan-
tized and quantized signals x and z as defined in (16), needs
to be bounded.

4.3.2 Granger causality investigation through nonuniformly quantized signals
Let zero-mean jointly Gaussian stationary random processes x and z be quantized by non-uniform quantizers of the form (21) and (22) with nx and nz levels, thresholds cxi , i = 1, ..., nx

and czj, j = 1, ..., nz, and quantization levels lix and lzj, respectively. For convenience, we assume that the quantizer lev-
els are increasing. We wish to determine whether x Granger causes z, using the statistics of the quantized data xQ and zQ. Our approach is to use upper bounds on |xQzQ () - xz()| and |zQzQ () - zz()| to find an upper bound on the norm of  (m, ). If this bound is less than min(CxQzQ (m, q)), then Theorem 4.2 implies x Granger causes z. Let

Sxz :=

max
| | xz

|xQzQ -  xz|,

 xxx

 z z  z

Szz :=

max
| | zz

|zQzQ -  z2|,

 zzz

Sz

:=

max
 zzz

|z2Q

-

z2|,

(26) (27) (28)

where xQzQ and zQzQ are functions of correlation coefficient  given by (25) and z2Q is given by -+(Q(z) - E{Q(z)})2 f (z)dz where f (z) is the univariate Gaussian den-

sity function. The parameters xz and zz are upper bounds

on max |xz()| and max=0 |zz()| respectively, z and

x are upper bounds on the standard deviations of z and x,

respectively,

and


z

and


x

are,

respectively,

lower

bounds

on the standard deviations of z and x.

An upper bound on (m, q) 2 in (17) is obtained as follows. We know that for A = [ai j]  Rs×t, we have A 2  A 1 A  where A 1 := max1 jt si=1 |ai j| and A  := max1is tj=1 |ai j|.
Note that the elements of the causality matrix CGxz(m, ) are the covariances between x and z and the variance of and the covariances between z. The maximum difference between the components of the causality matrix CGxz(m, ) and the matrix CxQzQ (m, ) can be obtained using (26)-(28) or through Appendix D. First upper bounds on  (m, q) 1 and  (m, q)  are derived as follows:
 (m, q) 1  No := max (m + 1)Sxz, (m + 1)Szz, mSzz + Sz , (29)

and

 (m, q)   NI :=mSxz + (q - 1)Szz + max Szz, Sz . (30)

Using (29), (30) and Theorem 4.2, the following can be presented:

Proposition 4.1 Let x, z be jointly zero-mean Gaussian, stationary, scalar random processes and z be partial Markov-
m. Further assume that there is no deterministic relationship between any of the components of xkk-m+1 zk at any time

7

k  m. Suppose there exists some q  [m, k] such that the matrix CxQzQ (m, q), involving covariances of the data obtained by non-uniform quantizers, is full-rank. Then x Granger causes
z, provided that the following condition is satisfied:

 NoNI

<

mQin ,

(31)

where No and NI are defined in (29) and (30), mQin is the smallest singular value of CxQzQ (m, q) and Sxz, Szz and Sz are defined in (26)-(28).

Remark 4.4 Note that we do not make assumption on the

identifiability of the second-order statistics of the unquan-

tized jointly Gaussian signals through quantized measure-

ments. The a priori information required to exploit Proposi-

tion 4.1 are the upper bounds on the cross-correlation co-

efficients between signal x and z (xz), the auto-correlation

coefficients of z at nonzero lags (zz), and the range of the

standard

deviation

of

z

(
z

and

 z ).

Remark 4.5 Sufficient condition (31) is useful for investi-

gating Granger causality between jointly Gaussian signals

through quantized observations. The RHS can be estimated

from the quantized data available and the LHS of the con-

dition depends on the quantizer specifications and on prior

knowledge of bounds on the underlying jointly Gaussian

statistics

( xz ,

 zz ,


z

and

 z ).

Remark 4.6 It can be shown that (31) is satisfied if:

 NoNI

<

min 2

,

(32)

where min is the smallest singular value of the causality matrix CGxz(m, q). Note that the RHS of (32) does not depend on the quantization scheme and can be lower-bounded in terms of the prior bounds on the underlying statistics, if the underlying system is indeed causal. The difference between the LHS and RHS can be thought of as a causality margin, which we would like to be as large as possible. This can be achieved by designing quantization parameters so that the LHS is as small as possible, subject to constraints on the number of levels available. This leads to designs that may be very different from minimum mean square error (MMSE) quantizers.

Remark 4.7 The approaches described in Sections 3 and 4 can be exploited for Granger causality inference of jointly Gaussian processes passing through other nonlinearities as well. Two cases can be considered. The first one is about nonlinearities whose relations between covariances or correlation coefficients of jointly Gaussian signals and secondorder moments of output signals from the nonlinearity are invertible. For such situations, necessary and sufficient conditions for inferring Granger causality similar to Section 3 using Price's Theorem and Theorem 2.2 can be introduced.

The second cases are nonlinearities where the identifiability is not guaranteed. Sufficient conditions to infer Granger causality between the jointly Gaussian signals using approaches similar to Section 4 can be derived. In fact, first the relationship between covariances of jointly Gaussian signals and output signals from nonlinearities through Price's Theorem can be obtained. Then bounds on the difference between covariances of jointly Gaussian signals and of the output signals from nonlinearities can be developed. And finally Theorem 4.2 to derive sufficient conditions can be used.

Remark 4.8 If the quantizers both have a threshold ci and d j at the origin, then an alternative approach is to lump

the positive and negative quantization intervals together

and then apply the binary quantizer results of section 3.

However, the multilevel quantization approach of this sec-

tion has the advantage that the empirical estimation of

the covariances of the quantized signals is expected to be-

come more accurate as the number of levels increases, see

e.g. [48] and references therein for discussion.

We further remark that by numerically inverting (25) and

similar equations, the unquantized variances and cor-

relation coefficients can be recovered exactly from the

(co)variances of the quantized signals. In principle, causal-

ity or non-causality could then be determined from the rank

of (6). However, when used on empirical estimates of the

quantized signal statistics, this technique can be unreliable

since it can be shown that

  wQ1 wQ2

becomes arbitrary large

if the underlying correlation coefficient  is sufficiently

strong. This is called the law of propagation of variance (or

of statistical error); see e.g. [15] for discussion. In contrast,

the approach taken in Proposition 4.1 does not require

inverting the functions and does not suffer from such issues.

In the following, we address Granger causality through infinite-level of quantization.

5 Granger causality using infinite-level uniformly quantized data
In this Section, Granger causality between jointly Gaussian signals is investigated when there is an infinite number of quantization levels. We also explore the high-resolution regime of such quantizers.
5.1 Finite resolution quantization
Infinite-level quantization has been studied in control systems [18], system identification [25, 42], communications [51], etc. Although practical quantizers have a finite number of levels, in the case of input signals with unbounded support they are difficult to analyse, because of the unbounded overload regions. In contrast, infinite-level quantizers have no overload regions to complicate the analysis. If the probability mass in the overload regions is sufficiently small, then an infinite-level quantizer is a reasonable approximation.

8

We develop an upper bound on the norm of perturbation matrix caused by quantization and use Theorem 4.2 to introduce a sufficient condition to infer Granger causality between jointly Gaussian signals x and z using uniform quantized signals. To do so, we first derive an upper bound on the difference between covariances of quantized and unquantized signals in Appendix E. Unlike the previous Section, we use results from [51] on the cross-covariances of bivariate Gaussian random vectors that are subjected to infinite-level uniform quantization.

Proposition 5.1 Let x, z be jointly Gaussian, stationary,
scalar random processes and z be partial Markov-m. Fur-
ther assume that there is no deterministic relationship between any of the components of xkk-m+1 zk at any time k  m. Suppose there exists some q  [m, k] such that the matrix CxQzQ (m, q), involving covariances of the uniformly quantized data with x < 2x and z < 2z, is full-rank. Then x Granger causes z, provided that the following condition
is satisfied:

 N

N1

<

mQin,

(33)

N :=1 (q - 1)+ 2 m + max{1, 3}, (34)

N1 := max 1 (m + 1), 2 (m + 1), 1 m+ 3 , (35)

where

1

:=

4 2 (1
z

-



zz

)



(szz szz

+ 1)2 eszz

+

4

zz

2 z



(2sz esz

)

,

(36)

2 := 2

 xz

xz



(2sz) esz

+

zx



(2sx) esx

+

2(1

-



xz

)

z



x



(sxz sxz

+ 1)2 esxz

,

(37)

3

:=

z2 12

+

z2  2esz

 (2sz

+ 2) +

4

2 z

2sz esz

 (2sz),

(38)

with  (.) the Riemann zeta-function, mQin is the smallest singular value of CxQzQ (m, q), and

2 2 2

sx = x2 x ,

(39)

2 2 2

sz = z2 z ,

(40)

sxz

42  (1 -

=

xz
xz

xz) ,

(41)

szz

=

4 2

2(1 -
z
z2

zz)

.

(42)

Remark 5.1 Similar to Proposition 4.1, the RHS can be estimated from the quantized measurements, while the LHS can be determined a priori from the quantization parameters and the assumed bounds on the underlying signal statistics.

Remark 5.2 Similar to Remark 4.6, we can derive the more stringent sufficient condition:

 N

N1

<

min , 2

(43)

where the RHS no longer depends on the quantized signals.

5.2 High-resolution quantization

Granger causality assessment between jointly Gaussian sig-
nals quantized by infinite-level quantizers is explored here in the high-resolution regime where z, x  0.

Recall that w1, w2 denote x and/or z at different time lags and 1, 2 are the corresponding quantization error terms. By

analysing the infinite sums representing covariances between

quantization errors and signals for sufficiently small ki :=

wi wi

,

i

=

1,

2,

it

is

shown

in

Appendices

G

and

H

that

the

last

three terms on the RHS of (14) can be expressed as follows:

|w12

|

<

4|w1

w2

-
|e

2 2 k22

,

|1

w2

|

<

4|w1

w2

-
|e

2 2 k12

,

12 =O

k1k2

-2
e

2

(1-|w1w2

|)(

1 k12

+

1 k22

)

,

where O(.) is Bachmann-Landau O-notation.

(44) (45) (46)

Furthermore, the variance of the quantized signal can be represented as follows as shown in Appendix I:

w2Q1

=

w21

+

k12w21 (= 12

w21 )

+O

-
e

2 2 k12

.

(47)

In Appendix J, we show that in the high resolution regime:

 (m, q)

2

<4(m

+

1)xze-

2 2 kx2

+ z2 + 12

4(m

+

1)

xz

-
e

2 2 kx2

+

z2 12

1
2O

max

-
e

2 kz2

,

(kxkz

)

1 2

-
e

2(1-

xz)(

1 kx2

+

1 kz2

)

,

kz

-
e

2 2 kz2

(1-

zz

)

,

(48)

PROOF. See Appendix F.

where xz is the upper bound on the cross-covariance between signals x and z. The following result then follows im-
mediately from Theorem 4.2:

9

Proposition 5.2 Let x, z be jointly Gaussian, stationary, scalar random processes and z be partial Markov-m. Fur-
ther assume that there is no deterministic relationship between any of the components of xkk-m+1 zk at any time k  m.

Suppose there exists some q  [m, k] such that the matrix CxQzQ (m, q) (15), involving covariances of the highresolution, uniformly quantized data, is full-rank. Then x
Granger causes z, provided that:

4(m + 1)xz exp

2 2 2

-

x
x2

+

z2 12

+ d(x, z)

<

mQin,

(49)

where mQin is the smallest singular value of CxQzQ (m, q) and

d(x, z) :=

4(m

+

1)

xz

-
e

2 2 kx2

+

z2 12

1
2O

max

-
e

2 kz2

,

(kx

kz)

1 2

-
e

2

(1- xz

)(

1 kx2

+

1 kz2

)
,

-
kze

2 2 kz2

(1- zz

)

.

(50)

Remark 5.3 This result gives an explicit formula in the high-resolution regime for deciding how finely quantized x and z should be in order to infer causality from CxQzQ (m, q), which can be constructed through quantized signals available. As in the quantization perturbation bound in Theorem 4.2, the RHS depends on the quantization scheme. Using (20) and (48), we can establish a quantizer-independent one hand side as follows:

8(m + 1)xz exp

2 2 2

-

x
x2

+

z2 6

+ 2d(x, z)

<

min,

(51)

where min := min(CGxz(m, q)). Note the LHS suggests that z plays a more critical role than x. This is related to the fact that in the causality matrix CGxz(m, q), x appears only in cross-covariances with z, whereas z also appears in auto-covariances with lag zero.

Even if x is coarse, the inequality can be satisfied by choosing a sufficiently fine z. Conversely, if x is small, z would
still have to be small to satisfy (51). Indeed it can be shown

that for the LHS to be as small as possible, we require

1/z  exp



2

2 x

x2

, i.e. the resolution in z should be expo-

nentially finer than the resolution in x. Furthermore, to im-

prove the causality margin, the LHS can be minimized with

respect to x and z under appropriate constraints on quan-

tization

resolution,

e.g.

log

1 x

+ log

1 z



R

(which

can

be

interpreted as the total expected bit rate in a variable-rate

code for quantized x and z).

6 Estimation of second-order statistics of quantized data
In this section, we establish that the auto- and crosscovariances between the quantized data can be consistently estimated, enabling the matrix CxQzQ (m, q) to be constructed. To do so, the ergodic theorem is exploited.
First, we state the following theorem.
Theorem 6.1 Let (yk)k1 be a stationary, ergodic l-variate vector process and f be a measurable function f : Rl×  Rs. Let k = f (yk, yk+1, ...) define an s-variate vector process (k)k1. Then (k)k1 is stationary ergodic.
PROOF. See Appendix K. It follows the same line of [8] with modifications to allow vector processes.

Theorem 6.1 implies that jointly Gaussian stationary, er-
godic signals xk and zk remain stationary and ergodic after quantization (xQk and zQk ). Furthermore, the Theorem implies that the products xQk xQk+ , zQk zQk+ , xQk zQk+ are also stationary and ergodic, for instance, by defining yk = [xk, zk], k = f (yk, yk+1, ...) = xQk zQk+ . This enables us to exploit the Ergodic Theorem to estimate the covariance functions of the
quantized signals.

Theorem 6.2 (Ergodic Theorem [8]) Suppose (k)k1 is a strict-sense stationary and ergodic univariate process with E|1| < , then almost surely and in the first mean

 lim
n

1 n

n k=1

k

=

E {1 }.

(52)

In the following, wQ1,k and wQ2,k denote xQk and/or zQk .
Remark 6.1 The standing assumption in this Section is that the unquantized processes are ergodic. For multivariate continuous-time Gaussian processes [1] and univariate discrete-time Gaussian processes [13], it is known that ergodicity follows under stationarity if the auto- and crosscovariances vanish as the lag approaches infinity. In Appendix L, this result is extended to stationary multivariate discrete-time Gaussian processes.
6.1 Auto- and cross-covariance estimators of quantized signals with zero mean
In order to estimate the auto- and cross-covariance between quantized signals with mean zero, Theorem 6.2 can be used. For instance, the cross-covariance between quantized signals

10

with mean zero wQ1,k and wQ2,k at lag  can be estimated as follows. Define k := wQ1,kwQ2,k+ and note that as discussed after Theorem 6.1, k is stationary and ergodic. Furthermore, it is clear that E|1| is finite. The following almost surely
and in the first mean convergent estimator can be presented:

 1
n

n-
wQ1,k wQ2,k+
k=1



wQ1 wQ2 ( )

:=

E{wQ1,kwQ2,k+ }.

(53)

6.2 Auto- and cross-covariance estimators of quantized signals with non-zero mean

For non-zero mean quantized signals, we can still use the theorem as follows. We know that:

wQ1 wQ2 ( ) = E wQ1,kwQ2,k+ - E wQ1,k E wQ2,k

(54)

Each terms on the right hand side of (54) can be estimated through Theorem 6.2. Let us first mention the following theorem:

Theorem 6.3 [14] Let h : Rr  R be a Borel function, continuous at a. If n  a almost surely, then h(n)  h(a)
almost surely.

Using Theorem 6.3, with r = 3 and h(1,n, 2,n, 3,n) = 1,n - 2,n3,n, the following almost surely convergent estimator can be obtained:

   1
n

n- k=1

wQ1,k wQ2,k+

-

1 n2

n
wQ1,k
k=1

n k=1

wQ2,k

 wQ1 wQ2 ( ). (55)

7 Simulation
In this Section, numerical examples are presented to illustrate the proposed methods. Throughout this Section, we consider an example from [16]. Its block diagram is shown in Fig. 1 where r1 and r2 are zero mean white Gaussian noise with unit variances and v is a zero mean white Gaussian noise with variance 0.1 [16].
Similar to [16], sufficiently long time is awaited to let the transient settle down. We collect n = 10, 000 samples from quantized versions of x and z to determine if x Granger causes z. The unquantized signals z and x are depicted in Fig. 2 to show how these signals change with each other over the period of samples [200, 400]. Note that we do not have access to such signals to investigate Granger causality. It can be shown that this example does not strictly satisfy the partial Markov-m assumption with finite m. Nonetheless, we take m = 7.

r1

0.7

q-1 1+0.5q-1

+

z

S

q-1 1-0.5q-1

q-1

1-0.3q-1

v

r2

+

1-0.5q-1 1-q-1

-

q-1 1-0.1q-1

+

x

Fig. 1. Block diagram of the system.
Note that empirical estimate of covariances at lag  becomes less reliable when the lag approaches to the number n of samples available. As a rule of thumb, it is suggested to estimate the covariances associated with at most a quarter of the number of samples (n/4) [50].

10 z

8

x

6

4

2

0

-2

-4

-6

200

300

400

Samples

Fig. 2. Unquantized signals z and x.

7.1 Binary quantization

In this Subsection, the signals xk and zk are quantized by binary quantizers with the thresholds zero and the quantization levels {-1, 1}, with the switch S closed. Using the estimators of the previous section, we estimate the smallest singular value in Theorem 3.1 to be 0.1590. The value is significantly different from zero. Thus, it shows that the causality matrix is full rank and therefore, x Granger causes z. The true smallest singular value equals 0.1652, which is close to the above-mentioned estimated value.
Now, we open the switch S in Fig. 1. In this situation, x does not Granger cause z. To investigate noncausality through binary quantized observations, data are collected after binary

11

5

4

zQ xQ

3

2

1

0

-1

-2

-3

-4

-5

200

300

400

Samples

Fig. 3. Quantized signals zQ and xQ with three-bit quantization.

quantization and rank of the causality matrix is determined to apply Theorem 3.1. The smallest singular value is 0.0023 which is near zero. We would therefore conclude that x does not Granger cause z.

7.2 Non-uniform finite-level quantization

Saturated quantizers with granular regions [-5, 5] and equal

quantization intervals in the granular regions are considered.

The value of quantizers in each granular cell is mapped to

the lower boundary point of the corresponding cell; i.e. in

the cell (ci, ci+1] the value of quantizer is ci. If the quantizer input falls outside [-5, 5], the quantizer takes the value of

the nearest quantizer point as described above. Let us con-

sider that the variances are within intervals with widths 0.2

and upper bounds on auto-correlation coefficient of signal

z and cross-correlation between signal x and z are, 0.5 and

0.9, respectively.

The corresponding quantized versions of signals z and x

with three-bit quantization depicted in Fig. 2 are shown

in Fig. 3. The matrix constructed by estimated covariances

ofNqouNaIn-tizemdQinsi=gn-al0s.0sa8t8is7fifeosr

the sufficient q = 17 while

condition (31), the switch S is

closed. From Proposition 4.1, we conclude that x Granger

causes z. If the true covariances of quantized signals through

(25) are used to determine the causality, the difference be-

tween the right and left hand sides of the sufficient condition

of Proposition 4.1 is -0.1021 which is less than zero and it

can be concluded that x Granger causes z.

Now let us open switch S and calculate the sufficient condi-

tion (31). When to construct the

mthaetreisxtiCmxaQtedzQa,ntdhetrvuaeluceosvaorfianNceosNaIr-e

used mQin

are, respectively, greater than +0.2546 and +0.2675 for dif-

ferent values of q. The values are significantly bigger than

zero and we cannot conclude that x Granger causes z.

Now we investigate how the numbers of bits denoted by bx

Fig. 4. Values of sufficient condition (31) for different numbers of quantization bits.
Fig. 5. Values of sufficient condition (32) for different numbers of quantization bits. and bz associated with quantizers of signals x and z impact on determining Granger causality using the approach given
iin.e.SubNsoeNctIio-n4mQ.3in.,IannFdig(3s.24),-5i.,et.he NsuofNfiIc-ientm2cino,nadrietidoenpsi(c3te1d),, respectively. As the numbers of bits increase, the perturbation reduces and the differences between LHS and RHS of (31) and (32) become more significant. 8 Conclusions In this paper, first we showed a rank-based representation for the equality of two conditional jointly Gaussian random vectors. Then, Granger causality inference for jointly stationary Gaussian, partially Markov-m signals using quantized data was investigated. A rank criterion to assess the Granger causality between Gaussian signals was proposed.

12

Next a necessary and sufficient condition was proposed for assessing Granger causality under binary quantization. Furthermore, conditions under which causality can be inferred by using just the statistical properties of the quantized signals instead of estimating the statistics or model of the underlying Gaussian signals were introduced. Such conditions have been proposed for a range of quantizers including non-uniform, uniform and high resolution quantizers. Note that such an approach can be extended to other nonlinearities as well. Future work will focus on relaxing the Gaussian assumption. In this case, it is likely that information-theoretic approaches based on directed information (see e.g. [5]) will prove useful.
References
[1] R.J. Adler. The Geometry of Random Fields. Wiley, London, 1981.
[2] S. Ahmadi and G.N. Nair. Granger causality of Gaussian signals from binary or non-uniformly quantized measurements, to appear in IFAC-PapersOnLine.
[3] S. Ahmadi, G.N. Nair, and E. Weyer. Granger causality of Gaussian signals from quantized measurements. In 58th IEEE Conference on Decision and Control, pages 3587­3592. IEEE, 2019.
[4] S. Ahmadi, G.N. Nair, and E. Weyer. Granger causality of Gaussian signals from noisy or filtered measurements. In IFAC-PapersOnLine, pages 506­511. IFAC, 2020.
[5] P.O. Amblard and O.J.J. Michel. The relation between Granger causality and directed information theory: a review. Entropy, 15:113­ 143, 2013.
[6] B.D.O. Anderson, M. Deistler, and J.-M. Dufour. On the sensitivity of Granger causality to errors-in-variables, linear transformations and subsampling. J. Time Ser. Anal., 40:102­123, 2019.
[7] L.V. Benkevitch, A.E.E. Rogers, C.J. Lonsdale, R.J. Cappallo, D. Oberoi, P.J. Erickson, and K.A.V. Baker. Van Vleck correction generalization for complex correlators with multilevel quantization. arXiv:1608.04367, 2016.
[8] L. Breiman. Probability. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, 1992.
[9] T.J.I'A. Bromwich. An Introduction to the Theory of Infinite Series. Macmillan & Co., London, 1908.
[10] P. Caines. Weak and strong feedback free processes. IEEE Trans. Autom. Contr., 21:737­739, 1976.
[11] P. Caines and C. Chan. Feedback between stationary stochastic processes. IEEE Trans. Autom. Contr., 20:498­508, 1975.
[12] S. Cambanis and E. Masry. On the reconstruction of the covariance of stationary Gaussian processes observed through zero-memory nonlinearities. IEEE Trans. Inform. Theory, 24:485­494, 1978.
[13] H. Crame´r and M.R. Leadbetter. Stationary and Related Stochastic Processes: Sample Function Properties and Their Applications. Wiley, New York, 1967.
[14] J. Davidson. Stochastic Limit Theory. Oxford University Press, New York, 1994.
[15] E.W. Deming. Statistical Adjustment of Data. John Wiley and Sons Interscience, New York, 1948.
[16] P. Duan, F. Yang, T. Chen, and S.L. Shah. Direct causality detection via the transfer entropy approach. IEEE Trans. Control Syst. Technol., 21:2052­2066, 2013.
[17] C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika, 1:211­218, 1936.

[18] N. Elia and S.K. Mitter. Stabilization of linear systems with limited information. IEEE Trans. Autom. Contr., 46:1384­1400, 2001.
[19] J.P. Florens and M. Mouchart. A note on noncausality. Econometrica, 50:583­591, 1982.
[20] E. Florin, J. Gross, J. Pfeifer, G.R. Fink, and L. Timmermann. The effect of filtering on Granger causality based multivariate causality measures. Neuroimage, 50:577­588, 2010.
[21] J.F. Geweke. Measures of conditional linear dependence and feedback between time series. J. Am. Stat. Assoc., 79:907­915, 1984.
[22] C.W.J. Granger. Economic process involving feedback. Inf. Control, 6:28­48, 1963.
[23] C.W.J. Granger. Testing for causality: a personal viewpoint. J. Econ. Dyn. Control, 2:329­352, 1980.
[24] C.W.J. Granger. Some recent development in a concept of causality. J. Econom., 39:199­211, 1988.
[25] F. Gustafsson and R. Karlsson. Statistical results for system identification based on quantized observations. Automatica, 45:2794­ 2801, 2009.
[26] L. Guttman. Enlargement methods for computing the inverse matrix. Ann. Math. Statist., 17:336­343, 1946.
[27] J.B. Hagen and D.T. Farley. Digital correlation techniques in radio science. Radio Sci., 8:775­784, 1973.
[28] R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 2013.
[29] M. Jo´zsa, M. Petreczky, and M.K. Camlibel. Relationship between Granger noncausality and network graph of state-space representations. IEEE Trans. Autom. Contr., 64:912­927, 2019.
[30] T. Kailath, A.H. Sayed, and B. Hassibi. Linear Estimation. PrenticeHall, New Jersey, 2000.
[31] I. Kontoyiannis and M. Skoularidou. Estimating the directed information and testing for causality. IEEE Trans. Inform. Theory, 62:6053­6067, 2016.
[32] M. Laghate and D. Cabric. Learning wireless networks' topologies using asymmetric Granger causality. IEEE J. Sel. Topics Signal Process., 12:233­247, 2018.
[33] G. Lindgren. Stationary Stochastic Processes: Theory and Applications. Chapman and Hall/CRC, Florida, 2012.
[34] M. Lundquist and W. Barrett. Rank inequalities for positive semidefinite matrices. Linear Algebra Appl., 248:91­100, 1996.
[35] E. Masry and S. Cambanis. On the reconstruction of the covariance of stationary Gaussian processes observed through zero-memory nonlinearities­part II. IEEE Trans. Inform. Theory, 26:503­507, 1980.
[36] L. Mirsky. Symmetric gauge functions and unitarily invariant norms. Quart. J. Math. Oxford, 11:50­59, 1960.
[37] H. Nalatore, M. Ding, and G. Rangarajan. Mitigating the effects of measurement noise on Granger causality. Physical Review E, 75:031123, 2007.
[38] A. Papoulis. Comments on `An extension of Price's theorem' by McMahon, E. L. IEEE Trans. Inform. Theory, 11:154­154, 1965.
[39] R. Price. A useful theorem for nonlinear devices having Gaussian inputs. IEEE Trans. Inform. Theory, 4:69­72, 1958.
[40] C.J. Quinn, T.P. Coleman, N. Kiyavash, and N.G. Hatsopoulos. Estimating the directed information to infer causal relationships in ensemble neural spike train recordings. J. Comput. Neurosci., 30:17­ 44, 2011.
[41] C.J. Quinn, N. Kiyavash, and T.P. Coleman. Directed information graphs. IEEE Trans. Inform. Theory, 61:6887­6909, 2015.
[42] R.S. Risuleo, G. Bottegal, and H. Hjalmarsson. Identification of linear models from quantized data: a midpoint-projection approach. IEEE Trans. Autom. Contr., 65:2801­2813, 2020.

13

[43] C.A. Sims. Money, income and causality. Am. Econ. Rev., 62:540­ 552, 1972.
[44] V. Solo. On causality I: sampling and noise. In 46th IEEE Conference on Decision and Control, pages 3634­3639. IEEE, 2007.
[45] V. Solo. State-space analysis of Granger-Geweke causality measures with application to fMRI. Neural Comput., 26:914­949, 2016.
[46] S. Sullivant. Algebraic geometry of Gaussian bayesian networks. Adv. in Appl. Math., 40:482­513, 2008.
[47] S. Sullivant. Algebraic Statistics. American Mathematical Society, Providence, 2018.
[48] A.R. Thompson, J.M. Moran, and G.W. Swenson. Interferometry and Synthesis in Radio Astronomy. Springer, Berlin, 2017.
[49] J.H. Van Vleck and D. Middleton. The spectrum of clipped noise. Proc. IEEE, 54:2­19, 1966.
[50] W.W.S. Wei. Time Series Analysis: Univariate and Multivariate Methods. Pearson, New Jersey, 2005.
[51] B. Widrow and I. Kollar. Quantization Noise: Roundoff Error in Digital Computation, Signal Processing, Control, and Communications. Cambridge University Press, Cambridge, 2008.
[52] N. Wiener. The Theory of Prediction. McGraw-Hill, New York, 1956.

A Proof of Theorem 2.1

We begin by showing that two first two items in the Theorem are equivalent.

The conditional Gaussian random vectors A|C, B and A|C, D have distributions as follows:

A|C, B  N (µcAo|nCd,B,cAon|Cd,B), A|C, D  N (µcAo|nCd,D,cAon|Cd,D),

where µcAo|nCd,B and µcAo|nCd,D are conditional means and cAon|Cd,B and cAon|Cd,D are conditional covariances of the random vectors A|C, B and A|C, D, respectively. It can be shown, e.g. [30], that

µcAo|nCd,B :=µA + A,CC-,C1(C - µC) + (A,B - A,CC-,C1C,B)K1×

B - µB - B,CC-,C1(C - µc) ,

(A.1)

µcAo|nCd,D :=µA + A,CC-,C1(C - µC) + (A,D - A,CC-,C1C,D)K2×

D - µD - D,CC-,C1(C - µc) ,

(A.2)

where

K1 := B,B - B,CC-,C1C,B -1, K2 := D,D - D,CC-,C1C,D -1.

(A.3) (A.4)

In order for two Gaussian distributions to be identical, it is necessary and sufficient that they should have the same
mean vector and covariance matrix. Let us begin with the equality of mean vectors (µcAo|nCd,B = µcAo|nCd,D). Equating (A.1) and (A.2) and noting the positive definiteness of [BCD],[BCD]

imply that the coefficients of linear relationship between B - µB, C - µC and D - µD appearing in µcAo|nCd,B = µcAo|nCd,D must be zero. The coefficients are zero if and only if

A,B - A,CC-,C1C,B = 0, A,D - A,CC-,C1C,D = 0.

(A.5) (A.6)

Thus, µcAo|nCd,B = µcAo|nCd,D if and only if (A.5) and (A.6) hold. It can be shown that if (A.5) and (A.6) are satisfied, the equality of the conditional covariance matrices cAon|Cd,B and cAon|Cd,D also holds. In other words, the conditional Gaussian probability density functions are equal (P(A|C, B) = P(A|C, D)) if and
only if (A.5) and (A.6) are satisfied.

We know that a matrix A is zero if and only if rank A = 0. Using Guttman rank additivity formula [26] for (A.5) and (A.6), we have:

rank A,B A,C = #C, C,B C,C
rank A,D A,C = #C. C,D C,C

(A.7) (A.8)

There are two partitioned matrices whose the second column is the same in the equations above. To deal with the ranks of these two matrices, let

 := A,B A,C A,D . C,B C,C C,D
Using Lemma 6 of [34], it follows:

(A.9)

rank  rank A,B A,C + rank A,C A,D

C,B C,C

C,C C,D

- rank A,C C,C
=#C,

(A.10)

We also have:

rank   max rank A,C , rank A,B A,D

C,C

C,B C,D

 #C.

(A.11)

And it follows that:

rank  = #C.

(A.12)

We can conclude that if the conditional Gaussian probability density functions are equal (P(A|C, B) = P(A|C, D)), (A.5)

14

and (A.6) are satisfied and in turn show that the rank criterion (A.12) holds. Therefore, one direction is proved.
In order to prove the converse, we should show that (A.7) and (A.8) are satisfied if (A.12) holds. Note that since C,C is positive definite, (A.12) implies that both (A.7) and (A.8) hold.
Proof of item 3: Here we show that items 1 and 3 are equivalent. If item 1 in the Theorem holds, we know that (A.7) and (A.8) (and therefore (A.5) and (A.6)) are satisfied. Thus, (A.1) and (A.2) are both equal to µcAo|nCd,B = µcAo|nCd,D = µA + A,CC-,C1(C - µC), where µA + A,CC-,C1(C - µC) is the conditional mean of A given C, µcAo|nCd. It follows that cAon|Cd,B = cAon|Cd,D = cAon|Cd, where cAon|Cd is the conditional covariance of A given C. This completes the proof.

B Proof of Theorem 2.2

We can apply Theorem 2.1 to (1) with A := zk+1, B := xkk-m+1, C := zkk-m+1 and D := zk-m, to prove the first two parts of the Theorem. To prove the last part, let us also define H := xk-m. We have:

E(A|C, D) =E E(A|B,C, D, H) C, D =E E(A|B,C) C, D =E E(A|C) C, D =E (A|C),

(B.1)

where the first equality follows from the law of iterated expectations, the second equality follows from Assumption
2.1 and the third one is due to the assumption of the last part of the Theorem. The conditional variance of A|C, D equals the conditional variance of A|C as well. This completes the proof.

C Derivation of equation (25)

Following [7, 27], we derive here the relationship between covariances of jointly Gaussian random variables quantized by two different non-uniform quantizers and correlation coefficients of the underlying jointly Gaussian random variables.

Let us consider g(w1, w2) = Q1(w1)Q2(w2) and n = 1 in Price's Theorem. We have:

  Q1(w1)
 w1

=

n1-1
(li+1 - li) (w1
i=1

- ci),

  Q2(w2)
 w2

=

n2-1

(l

 j+1

-

lj

)

(w2

j=1

- d j),

(C.1) (C.2)

where  (.) is Dirac delta function. The RHS of (24) can be written as follows:

E



Q1(w1  w1

)

.



Q2(w2  w2

)

+ +

=
- -



Q1(w1  w1

)



Q2(w2)  w2

f

(w1

,

w2)d

w1dw2

n1-1 n2-1

  =

(li+1

-

li)(l

 j+1

-

lj)×

i=1 j=1

+ +

 (w1 - ci) (w2 - d j) f (w1, w2)dw1dw2

- -

n1-1 n2-1

  =

(li+1

-

li)(l

 j+1

-

lj)

f

(ci,

d

j

).

i=1 j=1

(C.3)

Price's Theorem implies that:

 E wQ1 wQ2  w1w2

n1-1 n2-1

  =

(li+1 - li)(lj+1 - lj) f (ci, d j).

i=1 j=1

We have from the fundamental theorem of calculus:

E wQ1 wQ2 - E wQ1 wQ2 win1iw2

n1-1 n2-1

w1 w2

  =

(li+1 - li)(lj+1 - lj)

f (ci, d j)dw1w2 .

i=1 j=1

win1i w2

(C.4)

Let us set win1iw2 = 0 which implies that:

E wQ1 wQ2 win1iw2 =0 = E{wQ1 }E{wQ2 }.

(C.5)

Therefore, we have:

E wQ1 wQ2 - E{wQ1 }E{wQ2 }

n1-1 n2-1

w1 w2

  =

(li+1 - li)(lj+1 - lj)

f (ci, d j)dw1w2 .

i=1 j=1

0

(C.6)

Thus, the covariance of quantized signals can be obtained

15

as follows:

wQ1 wQ2 :=E wQ1 wQ2 - E{wQ1 }E{wQ2 }

n1-1 n2-1

  =

(li+1 - li)(lj+1 - lj)×

i=1 j=1

e w1 w2

-1 2(1- 2 )

c2i w2 1

-2

cid j w1 w2

+

d2j w2 2

0

2 w1w2 1 -  2

dw1w2

n1-1 n2-1

  =

(li+1 - li)(lj+1 - lj)×

i=1 j=1


2
0

1

-1
e 2(1-y2)

1 - y2

c2i w2 1

-2y

cid j w1 w2

+

d

2 j

w2 2

dy, (C.7)

which is (25).

D Bounds on perturbation of covariances (Bounds on Sxz and Szz)

To facilitate dealing with the integral (25) appearing in (26) and (27), an upper bound on the one-dimensional integrals can be obtained and then upper bounds on Sxz and on Szz can be derived. In the following, we first find upper bounds on the function in integrand associated with (25) and then the integral and the maximum of the resulted upper bound are obtained to have upper bounds on Sxz and on Szz. For ease of notation, let us first use w1 and w2 to denote x and/or z in the following. The final bounds are presented for signal x and z at the end of this Section for convenience as well. The exponential function in (25) can be bounded as follows:

e -1 2(1-y2 )

c2i w2 1

-2y

cid j w1 w2

+

d2j w2 2



-
e

1 2

c2i w2 1

+

d2j w2 2

, +

 1- 2

|

ci d j w1 w2

|

(D.1)

-1
e 2(1-2)

c2i w2 1

+

d2j w2 2

+2|

ci d j w1 w2

|

-1
 e 2(1-y2)

. c2i
w2 1

-2y

ci w1

dj w2

+

d

2 j

w2 2

(D.2)

Let us define:

  KL

:=

n1-1 i=1

n2-1 j=1

li

lj

e

-1 2(1- 2

)

c2i



2 1

+

d2j



2 2

+2

|cid j | 12

,

(D.3)

  KU

:=



i

max
wi 

i

n1-1 i=1

n2-1 j=1

li

l

 j

-
e

1 2

i=1,2

c2i w2 1

+

d2j w2 2

, +

 1-

2

|cid j | w1 w2

(D.4)

where li := li+1 - li and lj := lj+1 - lj. Applying (D.1) - (D.4) to (25), the bounds on wQ1 wQ2 can be

found as follows:

1 2

KL

arcsin( )



wQ1 wQ2



1 2

KU

arcsin( ),

for

  0,

(D.5)

1 2

KU

arcsin( )



wQ1 wQ2



1 2

KL

arcsin( ),

for

  0.

(D.6)

Thus the bound on wQ1 wQ2 - w1w2 is as follows: For   0:

KL 2

arcsin() - w1w2

wQ1 wQ2

- w1w2



KU 2

arcsin() - w1w2 ,

(D.7)

for   0:

KU 2

arcsin() - w1w2

wQ1 wQ2

- w1w2



KL 2

arcsin() - w1w2 .

(D.8)

In order to find the supremum of |wQ1 wQ2 - w1w2 |, we have to find the maximum value of its upper bounds and minimum
value of its lower bounds. Note that the upper bound and the lower bound are odd functions in . Therefore, we just examine the maximum of |KU arcsin() - w1 w2 | and the minimum of |KL arcsin() - w1w2 | for positive values of  in the following. The supremum of |wQ1 wQ2 - w1w2 | is as follows:

Sw1w2 := sup |wQ1 wQ2 - w1w2 | = max Uw1w2 , Lw1w2 ,

(D.9)

where

Uw1w2 :=

max
0 

KU 2

arcsin() - 

,




1

2





1

2

Lw1w2 :=

min
0 

KL 2

arcsin() -



.




1

2





1

2

(D.10) (D.11)

For the signals x and z and their quantized versions, the derived bounds above can be expressed as follows. Note that

|xQzQ () - xz()|  Sxz, |zQzQ () - zz()|  Szz,  = 0,
where

(D.12) (D.13)

Swz := max{Uwz, Lwz},

(D.14)

16

w can be x or z,

Uwz := max
0  wz

KwUz 2

arcsin() - 

,

 w  z    w  z

(D.15)

Lwz := min
0  wz

KwLz 2

arcsin() -



,

 w  z    w  z

(D.16)

  KwUz

:=

max
 w w  w

nw-1 nz-1
wiz
i=1 j=1

j

-
e

1 2

 zzz

cwi 2 w2

+

czj 2 z2

, +

 wz 1- wz

2

|cwi czj | w z

(D.17)

nw-1 nz-1

-1

  KwLz :=

wiz j e 2(1-wz2)

i=1 j=1

cwi 2  2w

+

czj 2



2 z

+2

wz

|cwi czj | wz

, (D.18)

wi

:=

liw+1 - liw

and

z j

:=

l

z j+1

-

l zj .

E Bounds on quantization perturbation

First let us obtain an upper bound on the difference between
covariances of quantized and of unquantized signals. Sup-
pose that zero-mean, jointly Gaussian scalar signals w1 and w2 are passed through uniform quantizers with quantization intervals of length w1 and w2 , respectively, with infinite number of quantization intervals. The cross-covariances of
uniformly quantized bivariate Gaussian vectors are derived
in [51], in the form of infinite sums. In Appendices E.1 and E.2, it is shown that |wQ1 wQ2 -w1w2 |  |w12 | + |1w2 | + |12 | where

|w12 |



|w1

w2 |w1 2s-1

w2 k22s 2ses

ss



(2s),

|1w2 |



|w1w2 |w1 w2 k12   2-1 2 e

 (2),

|12 |



(k1k2) +1w1 w2   22  2 +2e (1 - |w1w2 |)

 (

+ 1)2,

(E.1) (E.2) (E.3)

ki

:=

wi wi

,

i

=

1,

2,

wi

(with

a

single

subscript)

denotes

the

standard deviation of wi,  (.) is the Riemann zeta-function,

1 2

<

s



2 2 k22

,

1 2

<





2 2 k12

,

and

0

<





4

2 (1-|w1 k1 k2

w2

|)

.

Since

a-priori

known

ranges

of

standard

deviations

(
wi



wi



inwtie,riv=als1,o2f)sa,nd

of correlation and  should

coefficients be valid for

are the

available, the entire ranges

of the standard deviations and the correlation coefficients.

Therefore, such variables for the upper bounds derived in

(E.1)-(E.3) should be in the following intervals:

1 2

<

s



2 2 K12

,

1 2

<





2 2 K22

,

0 <   42(1 - ||) , K1 K2

(E.4) (E.5) (E.6)

where

Ki

:=

wi 

,i

=

1, 2.

wi

In addition to cross-covariances between x and z and auto-

covariances of z, the variance of z is also present in the

causality matrix. Therefore, to exploit Theorem 4.2, the difference between the variances of z and zQ should also be

derived as presented in Appendix E.3:

|z2Q

-

z2|



z2 12

+

z2  2e

 (2

+

2)

+

4 2z 2 e

 (2 ),

(E.7)

where



:=

2

2



2 z

z2

.

E.1 Derivation of equations (E.1) and (E.2)

First let us introduce an inequality for the exponential functions appearing in covariances of quantized signals as follows:

Lemma

E.1

For

real

0<x

1 s

,

s  R>0:

e-

1 x



xsss es

.

(E.8)

PROOF. Using the monotonicity of the exponential func-

tion

and

the

fact

that

g(x)

:=

-

1 x

-

s

ln

x

+

s

-

s

ln

s,

0

<

x



1 s

is always non-positive, the inequality (E.8) follows.

Moreover the following series is absolutely convergent:

 

(s

+

1)

=

+ i=1

1 is+1

,

s  R>0,

where  (.) is the Riemann zeta-function.

(E.9)

Using (E.8), (E.9) and comparison test theorem for series,
-2 2 n22
it can be shown that +n2=1 e k22 is convergent and also -2 2 n22
+n2=1(-1)n2 e k22 converges in the region where (E.8)

17

holds. Exploiting the infinite series representing w12 derived in [51], Chapter 11, the following bound can be ob-
tained using (E.8) and (E.9):

+

-2 2 n22

 |w12 | = 2w1w2 w1 w2

(-1)n2 e k22

n2=1



|w1w2 |w1 2s-1

w2 k22s 2ses

ss



(2s)

(E.10)

where

0

<

s



2 2 k22

.

Furthermore,

due

to

region

of

conver-

gence of the Riemann zeta-function which is 2s > 1 in

(E.10), we have:

1 2

<

s



2 2 k22

.

(E.11)

And the region of quantization intervals should be as follows:

k2 < 2, (2 < 2w2).

(E.12)

to have (E.11). Equation (E.2) can be derived in a similar way.

E.2 Derivation of equation (E.3)

Throughout the paper, when we deal with double infinite series, convergence in Pringsheim sense [9] is considered.

Because of the absolute convergence of the Riemann zetafunction and Cauchy's multiplication theorem, the following holds:

  n1 n2 1

lim
n1 ,n2 

i=1

j=1

i +1

1 j +1

=  2(

+ 1),

(E.13)

which

implies

that

+n1=1 +n2=1

(k1k2)   22 2 e (1-w1w2 )

1 n1 +1n2 +1

converges. Furthermore:

e  e -42

1 2

(

n1 k1

)2+

1 2

(

n2 k2

)2 -w1 w2

n1 n2 k1 k2

-42(1-w1 w2 )n1n2 k1 k2



22



(k1k2)   2 e (1 - w1w2

)

1 n1 n2

.

(E.14)

where the first inequality follows from Arithmetic Mean-

Geometric Mean inequality and the second inequality is

obtained by (E.8). Thus, using (E.14) and Comparison

Test Theorem for double series [9], it can be shown that

+n1=1

+n2=1

1 n1n2

e-4 2

1 2

(

n1 k1

)2+

1 2

(

n2 k2

)2 -w1 w2

n1 n2 k1 k2

converges

in

the

region

0<

k1k2



4

2 (1-|w1 

w2

|)

,



> 0.

It

can

be

shown that the following series is also convergent:

  +

+ (-1)n1+n2 e-42

1 2

(

n1 k1

)2+

1 2

(

n2 k2

)2 -w1 w2

n1 n2 k1 k2

.

n1=1 n2=1 n1n2

(E.15)

Using the order limit theorem for doubly index sequences, an upper bound can be derived as follows:

  +

+ (-1)n1+n2 e-42

1 2

(

n1 k1

)2+

1 2

(

n2 k2

)2-

w1w2 n1n2 k1 k2

n1=1 n2=1 n1n2



  (k1k2)  
22  2 e (1 - w1w2 )

+ + n1=1 n2=1

n1

1 +1n2

+1

.

(E.16)

The covariance between quantization error terms 1 and 2 is expressed as follows in [51], Chapter 11:

  12

=

k1

k2w1 2 2

w2

+ + n1=1 n2=1

(-1)n1+n2 n1n2

×

e-4 2

1 2

(

n1 k1

)2+

1 2

(

n2 k2

)2-

w1w2 n1n2 k1 k2

1 - e-42

2w1 w2 n1n2 k1 k2

.

(E.17)

For 0 < w1w2 < 1, we have:

  |12 |



k1k2w1 w2 2 2

+ + n1=1 n2=1

1× n1n2

e-4 2

1 2

(

n1 k1

)2+

1 2

(

n2 k2

)2 -w1 w2

n1 n2 k1 k2

1 - e-42

2w1w2 n1n2 k1 k2

-22 n21k22+n22k12-2w1 w2 n1n2k1k2

  

k1k2w1 w2 2 2

+ + n1=1 n2=1

2e

k12 k22
n1n2



(k1k2) +1w1 w2   22  2 +2e (1 - w1w2 )

 (

+ 1)2

(E.18)

where the last inequality follows from (E.13) and (E.14). In fact, it can be shown in a similar way to (E.18) that:

|12 |



(k1k2) +1w1 w2   22  2 +2e (1 - |w1w2 |)

 (

+ 1)2,

|w1w2 | < 1.

(E.19)

Note that  should be chosen such that the following inequality holds for all positive integers n1 and n2:

k1k2 4 2(1 - |w1w2 |)n1n2



1 

(E.20)

18

Hence,  is as follows:

0 <   42(1 - |w1w2 |) . k1k2

(E.21)

Note that the region of  for convergence of the Riemann zeta-function is satisfied for any  > 0 in (E.19).

E.3 Derivation of equation (E.7)

The variance of quantized signal zQ can be represented as follows [51], Chapter 11:

  z2Q

=

z2 +

z2 12

+

z2 2

+ n=1

(-1)n n2

-
e

2 2 n2 kz2

+

4z2

+

(-1)n

-
e

2 2 n2 kz2

n=1

.

(E.22)

are the cross-covariances between x and z and the autocovariances of signal z. The infinity norm of  (m, q) is as follows:

 (m, q)  = CxQzQ (m, q) - CGxz(m, q) 

m+q

 = max 1im+1

j=1

|i j |

m+q

m+q

  =max

j=1

|1

j

|,

max
2im+1

j=1

|i j |

m

sup


|xQ

zQ

(

)

-

xz(

)|+

(q - 1) sup |zQzQ () - zz()|+
 =0

max

sup
 =0

|zQzQ

(

)

-

zz(

)|,

|z2Q

-

z2|

.

(F.1)

Using (E.8), we have:

An upper bound on  (m, q)  can be derived as follows:

  |z2Q

- z2|



z2 12

+

z2 2

+ n=1

1 n2

-
e

2 2 n2 kz2

+

4z2

+

-
e

2 2 n2 kz2

n=1



z2 12

+

z2kz2   2  2( +1)e

 (2

+ 2) +

4z2kz2   2  2 e

 (2 )



z2 12

+

z2kz2   2  2( +1)e

 (2

+ 2) +

42z kz2   2  2 e

 (2 )

(E.23)

where

1 2

<



2 2 kz2

due

to

Lemma

E.1

and

the

convergence

region of the Riemann zeta-function. Furthermore, since

kz

=

z z

and

a-priori

known

range

of

z,

i.e.


z

 z

 z,

is

known,



should

be

in

the

region

of

1 2

<



2 2  z2

2 z

.

Note

that the value of  in (E.23) should be chosen such that the

upper bound on |z2Q - z2| is minimized. It can be shown

that the second and third terms in (E.23) are monotonically

decreasing with respect to  . Thus,  should be chosen to

equal

2

2



2 z

z2

in

order

to

have

a

tighter

bound.

Substituting



=

2

2

2 z

z2

in

(E.23), it

can

be

shown

that:

|z2Q

- z2|



z2 12

+

z2  2e

 (2

+ 2) +

4

2 z

2 e

 (2 ),

(E.24)

which is (E.7).

F Proof of Proposition 5.1

In order to develop a bound on the norm of quantization per-

turbation ( (m, q) ) to exploit Theorem 4.2 for the infinite-

level quantized signals, the difference between elements of

causality matrix CGxz(m, q) obtained. The elements of

and CxQzQ (m, q) should be first the causality matrix CGxz(m, q)

 (m, q)  mIxz + (q - 1)Izz + max Izz, |z2Q - z2| (F.2)

where Ixz := sup |xQzQ () - xz()| and Izz := sup=0 |zQzQ () - zz()| are obtained as follows by (E.1)-(E.3) and by replacing the standard deviations and correlation coefficients
with their bounds:

Ixz

=

f1

(

xz



z,



,
x

x,

sx

)

+

f1(

xz



x

,



,
z

z,

sz)+

f2(zx(1 - xz), xz, sxz),

(F.3)

Izz

:=

2

f1(

zz

-1 z

,



z

,

z,

sz

)

+

f2



2(1
z

-



zz

),

z2

,

szz

,

(F.4)

f1(a, b, c, d)

:=

ac2d d d 2d-1 2d edb2d-1



(2d),

f2(a, b, c)

:=

bc+1cc 22c 2c+2ecac



(c

+

1)2,

(F.5) (F.6)

and

 xz

:=

max


|xz ( )|

,

 zz

:=

max
 =0

|zz ( )|

.

(F.7) (F.8)

A bound on the norm 1 can be also obtained as follows:

 (m, q) 1 := CxQzQ (m, q) - CGxz(m, q) 1

 max (m + 1) sup|xQzQ () - xz()|,

(m + 1) sup |zQzQ () - zz()|,
 =0

m

sup
 =0

|zQzQ

(

)

-

zz(

)|

+

|z2Q

-

z2|

 max (m + 1)Ixz, (m + 1)Izz, mIzz + |z2Q - z2| . (F.9)

19

The term |z2Q - z2| appearing in (F.2) and (F.9) should be replaced by its upper bound derived in (E.7). Moreover,
note that Ixz and Izz appearing in the norm one and infinity of  (m, q) depend on sx, sz, sxz and szz. Such variables should be chosen such that the upper bound on  (m, q) 2
(  (m, q) 2   (m, q) 1  (m, q) ) is minimized and be also in the intervals defined in (E.4)-(E.6). It can be shown
that the functions f1 and f2 are monotonically decreasing with respect to such variables. Thus, it is sufficient to evaluate the upper bound on (m, q) 2 just for the values of sx, sz, sxz and szz mentioned in the following:

sx

=

2 2 x2

2 x

,

2 2 2 sz = z2 z ,

sxz

42  (1 -

=

xz
xz

xz) ,

szz

=

4 2

2z (1 - z2

zz)

,

(F.10) (F.11) (F.12) (F.13)

where xz := max |xz()| and zz := max=0 |zz()| . Substituting such optimal values in (F.3) and (F.4) yields
(34) and (35) and Theorem 4.2 implies Proposition 5.1. Note that the value of  in (E.7) equals to sz in (F.11). Therefore, for ease of notation,  is replaced by sz in the Proposition.

G Derivations of equations (44) and (45)

Using the comparison test theorem for infinite series, it can

be

shown

that

+n2=1(-1)n2

-
e

2 2 n22 k22

is absolutely convergent.

The expression of w12 is introduced in [51], Chapter 11.

The series can be bounded as follows:

 |w12 | =

2w1w2 w1 w2

+

(-1)n2

-
e

2 2 n22 k22

n2=1

 < 2|w1w2 |w1 w2

+

e-

2 2 n22 k22

n2=1

 < 2|w1w2 |w1 w2

+

e-

2 2 n2 k22

n2=1

<

2|w1w2 |w1 w2

-
e

2 2 k22

1

-

-
e

2 2 k22

.

(G.1)

For sufficiently fine k2, it can be written as:

|w1

2

|

<

4|w1w2

|w1

w2

-
e

2 2 k22

=

4|w1w2

-
|e

2 2 k22

,

(G.2)

and (44) follows. The same method can be used to derive (45).

H Derivation of equation (46)

The covariance between quantization error terms 1 and 2 is as follows [51], Chapter 11:

  12

=

k1

k2w1 2 2

w2

+ + n1=1 n2=1

(-1)n1+n2 n1n2

×

e-4 2

1 2

(

n1 k1

)2+

1 2

(

n2 k2

)2-

w1w2 n1n2 k1 k2

1 - e-42

2w1 w2 n1n2 k1 k2

.

(H.1)

It can be shown that it is convergent and a bound as follows can be obtained:

  |12 |

<

k1k2w1 w2 2 2

+ + n1=1 n2=1

1 n1n2

1 - e-42

2|w1w2 |n1n2 k1 k2

×

e-4 2

1 2

(

n1 k1

)2+

1 2

(

n2 k2

)2

-|w1w2

|

n1 n2 k1 k2

  <

k1k2w1 w2 2

+ + e-22
n1=1 n2=1

(

n1 k1

)2

+(

n2 k2

)2-2|w1

w2

|

n1 k1

n2 k2

  <

k1k2w1 w2 2

+ + e-22(1-|w1w2 |)
n1=1 n2=1

(

n1 k1

)2

+(

n2 k2

)2

,

(H.2)

where the last inequality follows from Rayleigh quotient theorem. And

  |12 |

<

k1k2w1 w2 2

+

+

-2
e

2(1-|w1w2

|)

n1=1 n2=1

n1 k12

+

n2 k22

.

(H.3)

Note

that

since

+n=1

-
e

22(1-|w1w2 |) ki2

n
,i

=

1,

2

are

convergent

for |w1w2 | = 1, it can be shown that:

|12 | <

k1k2w1 w2 2

e-

2 2

(1-|w1 w2 k12

|)

1 - e-

2

2(1-|w1 k12

w2

|)

e-

2

2 (1-|w1 k22

w2

|)

1 - e . -

2 2 (1-|w1 k22

w2

|)

(H.4)

For sufficiently fine k1 and k2, we have:

12 = O

k1

-2
k2e

2(1-|w1w2

|)(

1 k12

+

1 k22

)

.

(H.5)

20

I Derivation of equation (47)

The relation between variance of quantized and unquantized signals is as follows [51], Chapter 11:

 w2Q1

=w21

+

k12 w2 1 12

+

w21 2

+ n=1

(-1)n n2

-
e

2 2 n2 k12

+

 4w21

+

(-1)ne-

2 2 n2 k12

.

n=1

(I.1)

Therefore, we have:

w2Q1

-

w21

-

k12w21 12

  <

w21 2

+ n=1

1 n2

-
e

2 2 n2 k12

+ 4w21

+

e-

2 2 n2 k12

n=1

 <(

w21 2

+

4w21

)

+

-
e

2 2 n2 k12

n=1

<(

w21 2

+

4w21

)

1

-
e

2 2 k12

-

-
e

2 2 k12

,

(I.2)

and for sufficiently small k1, (47) follows.

J Upper bound on (m, q) 2 in high-resolution regime

Norm infinity of the matrix  (m, q) can be written as follows in high resolution regime:

m+q

  (m, q)



:= max
1im+1

j=1

|i j |

m

sup


|xQ

zQ

(

)

-

xz(

)|

+

(q

-

1)×

sup |zQzQ () - zz()|+
 =0

max

sup
 =0

|zQzQ

(

)

-

zz(

)|,

|z2Q

-

z2|

,

(J.1)

where

sup


|xQ

zQ

(

)

-

xz(

)|

<

4

xz

-
e

2 2 kx2

+O

-
e

2 2 kz2

+

-2
kxkze

2(1-

xz)(

1 kx2

+

1 kz2

)

,

(J.2)

sup |zQzQ () - zz()| = O

-
e

2 2 kz2

+

kz2e-

4 2 kz2

(1-

zz

)

,

 =0

(J.3)

|z2Q

- z2|

=

kz2z2(= 12

z2 )

+

-
O(e

2 2 kz2

),

(J.4)

are obtained from (G.2) and (44)-(47) and xz is the upper bound on the cross-covariance between signals x and z.

For sufficiently fine kz,

kz2 z2 12



-
e

2 2 kz2

and

kz2 z2 12



kz2

-
e

4 2 kz2

(1-zmz ax ) .

Hence, the dominant term between terms appearing in

sup=0 |zQzQ ( ) - zz( )|

and

|z2Q - z2|

is

kz2 z2 12

.

Furthermore,

kz2 z2 12



kx

-2
kze

2(1-xmzax

)(

1 kx2

+

1 kz2

)

since

in

high-resolution regime

kz z2 12

2 2
e kz2

(1-xmzax)



kx

-
e

2 2 kx2

(1-xmz ax ) .

Therefore, the following can be stated:

 (m, q)





4m

xz

-
e

2 2 kx2

+ kz2z2 12

(J.5)

Based on what is mentioned above, the norm one  (m, q) 1 can be written as follows:

 (m, q) 1  max (m + 1) sup |xQzQ () - xz()|,


m

sup
 =0

|zQ zQ

(

)

-

zz(

)|

+

|z2Q

-

z2|

 max

4(m

+

1)

xz

-
e

2 2 kx2

,

kz2z2 12

(J.6)

Using (J.5) and (J.6):

 (m, q) 2 

4m

xz

-
e

2 2 kx2

+

z2 12

×

max

4(m

+

1)

xz

-
e

2 2 kx2

,

z2 12

1
2+

4(m

+

1)

xz

-
e

2 2 kx2

+

kz2z2 12

1
2×

O

-
e

2 kz2

+ max

(kxkz)

1 2

-
e

2(1-

xz )(

1 kx2

+

1 kz2

)

,

kz

-
e

2 2 kz2

(1-

zz

)

4(m

+

1)

xz

-
e

2 2 kx2

+

z2 + 12

4(m

+

1)

xz

-
e

2 2 kx2

+

z2 12

1
2×

O

-
e

2 kz2

+ max

(kxkz)

1 2

-
e

2(1-

xz )(

1 kx2

+

1 kz2

)

,

kz

-
e

2 2 kz2

(1-

zz

)

(J.7)

which is (48).
K Proof of Theorem 6.1
Let us first mention some definitions and make the definitions of their vector counterparts.

21

Definition K.1 (Stationary Process) [8] A process
(yk)k1 is said to be stationary if yk+1, yk+2, ... has the same distribution as y1, y2, ... for every k  1; that is, if for each k  1:

P (y1, y2, ...)  B = P (yk+1, yk+2, ...)  B

(K.1)

for every B  B, where the Borel field B is the smallest  -field of subsets of R containing all finite-dimensional rectangles, R denotes the space consisting of all infinite sequences (y1, y2, ...) of real numbers, and an n-dimensional rectangle in R is a set of the form

{y  R; y1  I1, ..., yn  In},

(K.2)

where I1, ..., In are finite or infinite intervals.

Definition K.2 (Invariant Set) [8] An event A is invariant if there exists B  B such that for every k  1:

A = {(yk, yk+1, ...)  B}.

(K.3)

Definition K.3 (Ergodic Process) [8] A stationary process (yk)k1 is ergodic if every invariant event has probability zero or one.

The following can be defined for vector processes:

Definition K.4 (Stationary Vector Process) An l-variate
vector process (yk)k1, where yk := [y1,k, ..., yl,k], is said to be stationary if yk+1, yk+2, ... has the same distribution as y1, y2, ... for every k  1; that is, if for each k  1:

P (y1, y2, ...)  B = P (yk+1, yk+2, ...)  B

(K.4)

for every B  B, where B is the corresponding Borel field.

Definition K.5 (Invariant Set for Vector Processes) An
event A is invariant if there exists B  B such that for every k  1:

A = {(yk, yk+1, ...)  B}.

(K.5)

Definition K.6 (Ergodic Vector Processes) A stationary
vector process (yk)k1 is ergodic if every invariant event has probability zero or one.

Now let us return to the proof of Theorem 6.1. According to Definition K.4, we need to show that for every B  B and k  1, the following holds:

P  : (1(), 2(), ...)  B = P  : (k+1(), k+2(), ...)  B .

(K.6)

We know that
P  : (1(), 2(), ...)  B = P  : f y1(), y2(), ... , f y2(), y3(), ... , ...  B .
(K.7)

Since f is a measurable function, for any B there exists By such that:

 : f y1(), y2(), ... , f y2(), y3(), ... , ...  B

=  : y1(), y2(), ...  By

(K.8)

and

 : f yk+1(), yk+2(), ... , f yk+2(), yk+3(), ... ,
...  B =  : yk+1(), yk+2(), ...  By . (K.9)

Since (yk)k1 is stationary due to the assumption of the Theorem, the probability of the RHS's of (K.8) and (K.9) is
equal. Thus, the probability of the LHS's of (K.8) and (K.9) is equal for any B  B and k  1. It implies that (k)k1 is a stationary vector process.
For ergodicity part of the Theorem, let us assume that A is an invariant set for (k)k1. Therefore, there exists Be  B such that for every k  1:

A =  : (k(), k+1(), ...)  Be .

(K.10)

Since f is a measurable function, there exists Bey for Be such that (K.10) can be written as follows:

 : f yk(), yk+1(), ... , f yk+1(), yk+2(), ... ,
...  Be =  : yk(), yk+1(), ...  Bey . (K.11)

Since A is an invariant set for (k)k1, the set  :
y1(), y2(), ...  Bey is also an invariant set for (yk)k1. Since (yk)k1 is ergodic and using (K.11), P(A) is zero or one which implies that the process (k)k1 is ergodic.

L Sufficient Condition for Ergodicity of Stationary Discrete-time Gaussian Vector Processes
Theorem L.1 Suppose (yk)k1 is a stationary Gaussian vector process. If the auto- and cross-covariances vanish as the lag  approaches infinity, then (yk)k1 is ergodic.

22

PROOF. The proof follows the same line of [13] and references therein with modifications to allow stationary discretetime Gaussian vector processes. Let us first mention a theorem useful for the proof.

Theorem L.2 [8] Suppose ( , F , P) is a probability space and A is a field of subsets of  such that the sigma-field containing A is F . For any  > 0 and every C  F there is a set C~  A such that

P(CC~)  ,

(L.1)

where  is symmetric difference and defined as CC~ := (C - C~)  (C~ - C).

The events in the sigma-field B in R can be approximated in probability by finite dimensional sets. If (R, B, P) is a probability space and C  B, then for any  > 0, there is a finite n and an event Cn  Bn, where Bn is the  -field in Rn, such that

P(CC~n)  ,

(L.2)

where C~n = {x  R : (x1, ..., xn)  Cn} [33].

Now we turn to the proof. Let us assume that A := { : (yk(), yk+1(), ...)  B} is an invariant set for (yk)k1. Define:

A := { : (yk+ (), yk++1(), ...)  B},   0. (L.3)
It follows from stationarity that P(A) = P(A ) and since A is an invariant set, the following holds:

P(A) = P(A ) = P(A  A ).

(L.4)

Theorem L.2 implies that for any  > 0, there exist a finite n and A~n such that:

P(AA~)  ,

(L.5)

where A~ = { : (yk(), yk+1(), ..., yk+n())  A~n}. Using the fact that for any two random events E1 and E2, |P(E1) - P(E2)|  P(E1E2) yields that:

|P(A) - P(A~)|  .

(L.6)

Now let us define

A~ = { : (yk+ (), yk++1(), ..., yk++n())  A~n}. (L.7)
By stationarity, the following holds:

P(AA~ )  .

(L.8)

Using (L.5) and (L.8):

|P(A) - P(A~  A~ )|  P A(A~  A~ )  P(AA~) + P(AA~)  2,

(L.9)

where the second inequality follows from the fact that for any three random events E1, E2 and E3, E1 E2  E2)  (E1E2)  (E1E3). Since the random vector process is jointly Gaussian and
based on the assumption, the auto- and cross-covariances go to zero when  approaches infinity, it follows that:

lim
 

P(A~



A~ 

)

=

P(A~)P(A~

).

(L.10)

By stationarity of the random process (yk)k1 (P(A~) = P(A~ )), it results that:

lim
 

P(A~



A~ 

)

=

P(A~)2.

(L.11)

Equations (L.8), (L.9) and (L.11) and stationarity imply that:

P(A~)

=

lim
 

P(A~ 

)

=

P(A),

(L.12)

and

P(A~)2

=

lim
 

P(A~



A~ 

)

=

P(A).

(L.13)

Using (L.12) and (L.13), it follows that:

P(A)2 = P(A),

(L.14)

which means that P(A) is zero or one. Therefore, the stationary sequence (yk)k1 is ergodic.

23

