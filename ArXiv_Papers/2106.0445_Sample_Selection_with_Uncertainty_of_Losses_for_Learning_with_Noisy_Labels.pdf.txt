arXiv:2106.00445v1 [cs.LG] 1 Jun 2021

Sample Selection with Uncertainty of Losses for Learning with Noisy Labels
Xiaobo Xia1, Tongliang Liu1, Bo Han2, Mingming Gong3, Jun Yu4, Gang Niu5, Masashi Sugiyama5,6
1The University of Sydney; 2Hong Kong Baptist University; 3The University of Melbourne; 4University of Science and Technology of China;
5RIKEN; 6The University of Tokyo
Abstract In learning with noisy labels, the sample selection approach is very popular, which regards small-loss data as correctly labeled during training. However, losses are generated on-the- y based on the model being trained with noisy labels, and thus large-loss data are likely but not certainly to be incorrect. There are actually two possibilities of a large-loss data point: (a) it is mislabeled, and then its loss decreases slower than other data, since deep neural networks "learn patterns rst"; (b) it belongs to an underrepresented group of data and has not been selected yet. In this paper, we incorporate the uncertainty of losses by adopting interval estimation instead of point estimation of losses, where lower bounds of the con dence intervals of losses derived from distribution-free concentration inequalities, but not losses themselves, are used for sample selection. In this way, we also give large-loss but less selected data a try; then, we can better distinguish between the cases (a) and (b) by seeing if the losses e ectively decrease with the uncertainty after the try. As a result, we can better explore underrepresented data that are correctly labeled but seem to be mislabeled at rst glance. Experiments demonstrate that the proposed method is superior to baselines and robust to a broad range of label noise types.
1

1 Introduction
Learning with noisy labels is one of the most challenging problems in weakly-supervised learning, since noisy labels are ubiquitous in the real world [40, 74, 45, 1, 70]. For instance, both crowdsourcing and web crawling yield large numbers of noisy labels everyday [15]. Noisy labels can severely impair the performance of deep neural networks with strong memorization capacities [76, 78, 49, 34].
To reduce the in uence of noisy labels, a lot of approaches have been recently proposed [43, 33, 35, 77, 80, 63, 64, 54, 37, 28, 38, 55, 69, 57, 22, 20, 17]. They can be generally divided into two main categories. The rst one is to estimate the noise transition matrix [47, 52, 18, 14], which denotes the probabilities that clean labels ip into noisy labels. However, the noise transition matrix is hard to be estimated accurately, especially when the number of classes is large [74]. The second approach is sample selection, which is our focus in this paper. This approach is based on selecting possibly clean examples from a mini-batch for training [15, 71, 58, 74, 26, 58, 59]. Intuitively, if we can exploit less noisy data for network parameter updates, the network will be more robust.
A major question in sample selection is what criteria can be used to select possibly clean examples. At the present stage, the selection based on the small-loss criteria is the most common method, and has been veri ed to be e ective in many circumstances [15, 19, 74, 60, 71]. Speci cally, since deep networks learn patterns rst [2], they would rst memorize training data of clean labels and then those of noisy labels with the assumption that clean labels are of the majority in a noisy class. Small-loss examples can thus be regarded as clean examples with high probability. Therefore, in each iteration, prior methods [15, 60] select the small-loss examples based on the predictions of the current network for robust training.
However, such a selection procedure is debatable, since it arguably does not consider uncertainty in selection. The uncertainty comes from two aspects. First, this procedure has uncertainty about small-loss examples. Speci cally, the procedure uses limited time intervals and only exploits the losses provided by the current predictions. For this reason, the estimation for the noisy class posterior is unstable [72], which causes the network predictions to be equally unstable. It thus takes huge risks to only use losses provided by the current predictions (Figure 1, left). Once wrong selection is made, the inferiority of accumulated errors will arise [74]. Second, this procedure has uncertainty about large-loss examples. To be speci c, deep networks learn easy examples at the beginning of training, but ignore some clean examples with large losses. Nevertheless, such examples are always critical for generalization. For instance, when learning with imbalanced data, distinguishing the examples with non-dominant labels are more pivotal during training [39]. Deep networks often give large losses to such examples (Figure 1, right). Therefore, when learning under the realistic scenes, e.g., learning with noisy imbalanced data, prior sample selection methods cannot address such an issue well.
To relieve the above issues, we study the uncertainty of losses in the sample selection procedure to combat noisy labels. To reduce the uncertainty of small-loss examples, we extend time intervals and utilize the mean of training losses at di erent training iterations. In consideration of the bad in uence of mislabeled data on training losses, we build two robust mean estimators from the perspectives of soft truncation and hard truncation w.r.t. the truncation level, respectively. Soft truncation makes the mean estimation more robust by holistically changing the behavior of losses. Hard truncation makes the mean estimation more robust by locally removing outliers from losses. To reduce the uncertainty of large-loss examples, we encourage networks to pick the sample that
2

Loss Average Loss

1.75 1.50 1.25 1.00 0.75 0.50 0.25
1

Clean Mislabeled Clean (Mean) Mislabeled (Mean)

2

Ep3och

4

5

1.2 1.0 0.8 0.6 0.4 0.2 0.0 Clean

Mislabeled Clean Balanced Clean Imbalanced

Figure 1: Illustrations of uncertainty of losses. Experiments are conducted on the imbalanced noisy MNIST dataset. Left: uncertainty of small-loss examples. At the beginning of training (Epochs 1 and 2), due to the instability of the current prediction, the network gives a larger loss to the clean example and does not select it for updates. If we consider the mean of training losses at di erent epochs, the clean example can be equipped with a smaller loss and then selected for updates. Right: uncertainty of large-loss examples. Since the deep network learns easy examples at the beginning of training, it gives a large loss to clean imbalanced data with non-dominant labels, which causes such data unable to be selected and severely in uence generalization.

has not been selected in a conservative way. Furthermore, to address the two issues simultaneously, we derive concentration inequalities [5] for robust mean estimation and further employ statistical con dence bounds [3] to consider the number of times an example was selected during training.
The study of uncertainty of losses in learning with noisy labels can be justi ed as follows. In statistical learning, it is known that uncertainty is related to the quality of data [56]. Philosophically, we need variety decrease for selected data and variety search for unselected data, which share a common objective, i.e., reduce the uncertainty of data to improve generalization [42]. This is our original intention, since noisy labels could bring more uncertainty because of the low quality of noisy data. Nevertheless, due to the harm of noisy labels for generalization, we need to strike a good balance between variety decrease and search. Technically, our method is specially designed for handling noisy labels, which robustly uses network predictions and conservatively seeks less selected examples meanwhile to reduce the uncertainty of losses and then generalize well.
Before delving into details, we clearly emphasize our contributions in two folds. First, we reveal prior sample selection criteria in learning with noisy labels have some potential weaknesses and discuss them in detail. The new selection criteria are then proposed with detailed theoretical analyses. Second, we experimentally validate the proposed method on both synthetic noisy balanced/imbalanced datasets and real-world noisy datasets, on which it achieves superior robustness compared with the state-of-the-art methods in learning with noisy labels. The rest of the paper is organized as follows. In Section 2, we propose our robust learning paradigm step by step. Experimental results are discussed in Section 3. The conclusion is given in Section 4.
2 Method
In this section, we rst introduce the problem setting and some background (Section 2.1). Then we discuss how to exploit training losses at di erent iterations (Section 2.2). Finally, we introduce the proposed method, which exploits training losses at di erent iterations more robustly and encourages networks to pick the sample that is less selected but could be correctly labeled (Section 2.3).

3

2.1 Preliminaries
Let X and Y be the input and output spaces. Consider a k-class classi cation problem, i.e., Y = [k], where [k] = {1, . . . , k}. In learning with noisy labels, the training data are all sampled from a corrupted distribution on X × Y. We are given a sample with noisy labels, i.e., S~ = {(x, y~)}, where y~ is the noisy label. The aim is to learn a robust classi er that could assign clean labels to test data by only exploiting a training sample with noisy labels.
Let f : X  Rk be the classi er with learnable parameters w. At the i-th iteration during training, the parameters of the classi er f can be denoted as wi. Let : Rk × Y  R be a
surrogate loss function for k-class classi cation. We exploit the softmax cross entropy loss in this paper. Given an arbitrary training example (x, y~), at the i-th iteration, we can obtain a loss i, i.e., i = (f (wi; x), y~). Hence, until the t-th iteration, we can obtain a training loss set Lt about the example (x, y~), i.e., Lt = { 1, . . . , t}.
In this paper, we assume that the training losses in Lt conform to a Markov process, which is to represent a changing system under the assumption that future states only depend on the current state (the Markov property) [51]. More speci cally, at the i-th iteration, if we exploit an optimization algorithm for parameter updates (e.g., the stochastic gradient descent algorithm [4]) and omit other dependencies (e.g., S~), we will have P (wi|wi-1, . . . , w0) = P (wi|wi-1), which means that the future state of the classi er f only depends on the current state. Furthermore, given a training example and the parameters of the classi er f , we can determine the loss of the training example as discussed. Therefore, the training losses in Lt will also conform to a Markov process.

2.2 Extended Time Intervals

As limited time interval cannot address the instability issue of the estimation for the noisy class posterior well [49], we extend time intervals and exploit the training losses at di erent training iterations for sample selection. One straightforward idea is to use the mean of training losses at di erent training iterations. Hence, the selection criterion could be

1t

µ~ = t

i.

(1)

i=1

It is intuitive and reasonable to use such a selection criterion for sample selection, since the operation of averaging can mitigate the risks caused by the unstable estimation for the noisy class posterior, following better generalization. Nevertheless, such a method could arguably achieve suboptimal classi cation performance for learning with noisy labels. The main reason is that, due to the great harm of mislabeled data, part of training losses are with too large uncertainty and could be seen as outliers. Therefore, it could be biased to use the mean of training losses consisting of such outliers [12], which further in uences sample selection. More evaluations for our claims are provided in Section 3.

2.3 Robust Mean Estimation and Conservative Search
We extend time intervals and meanwhile exploit the training losses at di erent training iterations more robustly. Speci cally, we build two robust mean estimators from the perspectives of soft

4

truncation and hard truncation [7]. Note that for speci c tasks, it is feasible to decide the types of robust mean estimation with statistical tests based on some assumptions [8]. We leave the analysis as future work. Two distribution-free robust mean estimators are introduced as follows.
Soft truncation. We extend a classical M-estimator from [7] and exploit the widest possible choice of the in uence function. More speci cally, give a random variable X, let us consider a
non-decreasing in uence function  : R  R such that

(X) = log(1 + X + X2/2), X  0.

(2)

The choice of  is inspired by the Taylor expansion of the exponential function, which can make the estimation results more robust by reducing the side e ect of extremum holistically. The illustration for this in uence function is provided in Appendix A.1. For our task, given the observations on training losses, i.e., Lt = { 1, . . . , t}, we estimate the mean robustly as follows:

1t

µ~s = t ( i).

(3)

i=1

We term the above robust mean estimator (3) the soft estimator.

Hard truncation. We propose a new robust mean estimator based on hard truncation. Specif-

ically, given the observations on training losses Lt, we rst exploit the K-nearest neighbor (KNN)

algorithm [30] to remove some underlying outliers in Lt. The number of outliers is denoted by

to(to < t), which can be adaptively determined as discussed in [79]. Note that we can also employ

other algorithms, e.g., principal component analysis [53] and the local outlier factor [6], to iden-

tify underlying outliers in Lt. The main reason we employ KNN is because of its relatively low

computation costs [79].

The truncated loss observations on training losses are denoted by Lt-to. We then utilize Lt-to for the mean estimation. As the potential outliers are removed with high probability, the robust-

ness of the estimation results will be enhanced. We denote such an estimated mean as µ~h. We

have

1

µ~h = t - to iLt-to i.

(4)

The corresponding estimator (4) is termed the hard estimator. We derive concentration inequalities for the soft and hard estimators respectively. The search
strategy for less selected examples and overall selection criterion are then provided. Note that we

do not need to explicitly quantify the mean of training losses. We only need to sort the training examples based on the proposed selection criterion and then use the selected examples for robust training.

Theorem 1. Let Zn = {z1, · · · , zn} be an observation set with mean µz and variance 2. By exploiting the non-decreasing in uence function (z) = log(1 + z + z2/2). For any > 0, we have

1 n

n

(zi) - µz



2(n

+

2

log( n2

n - 2

-1) ) ,

(5)

i=1

with probability at least 1 - 2 .

5

Proof can be found in Appendix A.1.
Theorem 2. Let Zn = {z1, . . . , zn} be a (not necessarily time homogeneous) Markov chain with mean µz, taking values in a Polish state space 1 × . . . × n, and with a minimal mixing time min. The truncated set with hard truncation is denoted by Zno, with no < n. If |zi| is upper bounded by Z. For any 1 > 0 and 2 > 0, we have

1

1

n - no ziZn\Zno -µz

 n - no

2Z

2min log

2
1

+

2Z no n

2n 2min log
2

,

(6)

with probability at least 1 - 1 - 2.

Proof can be found in Appendix A.2. For our task, let the training loss be upper-bounded by

L. The value of L can be determined easily by training networks on noisy datasets and observing

the loss distribution [1].

Conservative search and selection criteria. In this paper, we will use the concentration

inequalities (5) and (6) to present conservative search and the overall sample selection criterion.

Speci cally, we exploit their lower bounds and consider the selected number of examples during

training. The selection of the examples that are less selected is encouraged.

Denote the number of times one example was selected by nt(nt  t). Let

=

1 2t

.

For the

circumstance with soft truncation, the selection criterion is

s

=

µ~s

-

2(t + nt

2
-

log(2t) t2
2

)

.

(7)

Let

1=

2

=

1 2t

,

for

the

situation

with

hard

truncation,

by

rewriting

(6),

the

selection

criterion

is





2 h = µ~h -

2minL(t

+ 

2to)

(t - to) t

log(4t) .
nt

(8)

Note that we directly replace t with nt. If an example is rarely selected during training, nt will be far less than n, which causes the lower bounds to change drastically. Hence, we do not use the mean of all training losses, but use the mean of training losses in xed-length time intervals. More details about this can be checked in Section 3.
For the selection criteria (7) and (8), we can see that they consist of two terms and have one term with a minus sign. The rst term in Eq. (7) (or Eq. (8)) is to reduce the uncertainty of smallloss examples, where we use robust mean estimation on training losses. The second term, i.e., the statistical con dence bound, is to encourage the network to choose the less selected examples (with a small nt). The two terms are constraining and balanced with 2 or min. To avoid introducing strong assumptions on the underlying distribution of losses [8], we tune  and min with a noisy validation set. For the mislabeled data, although the model has high uncertainties on them (i.e., a small nt) and tends to pick them, the over tting to the mislabeled data is harmful. Also, the mislabeled data and clean data are rather hard to distinguish in some cases as discussed. Thus, we should search underlying clean data in a conservative way. In this paper, we initialize  and min with small values. This way can reduce the adverse e ects of mislabeled data and meanwhile select the clean examples with large losses, which helps generalize. More evaluations will be presented in Section 3.

6

Algorithm 1 CNLCU Algorithm.

1: Input 1 and 2, learning rate , xed  , epoch Tk and Tmax, iteration tmax;
for T = 1, 2, . . . , Tmax do 2: Shu le training dataset S~;

for t = 1, . . . , tmax do 3: Fetch mini-batch S¯ from S~; 4: Obtain S¯1 = arg minS :|S |R(T )|S¯| 5: Obtain S¯2 = arg minS :|S |R(T )|S¯| 6: Update 1 = 1 -  (1, S¯2); 7: Update 2 = 2 -  (2, S¯1);
end

(1, S ); (2, S );

// calculated with Eq. (7) or Eq. (8) // calculated with Eq. (7) or Eq. (8)

8: Update R(T ) = 1 - min

T Tk



,



;

end

9: Output 1 and 2.

The overall procedure of the proposed method, which combats noisy labels by concerning uncertainty (CNLCU), is provided in Algorithm 1. CNLCU works in a mini-batch manner since all deep learning training methods are based on stochastic gradient descent. Following [15], we exploit two networks with parameters 1 and 2 respectively to teach each other. Speci cally, when a mini-batch S¯ is formed (Step 3), we let two networks select a small proportion of examples in this mini-batch with Eq. (7) or (8) (Step 4 and Step 5). The number of instances is controlled by the function R(T ), and two networks only select R(T ) percentage of examples out of the minibatch. The value of R(T ) should be larger at the beginning of training, and be smaller when the number of epochs goes large, which can make better use of memorization e ects of deep networks [15] for sample selection. Then, the selected instances are fed into its peer network for parameter updates (Step 6 and Step 7).
3 Experiments
In this section, we evaluate the robustness of our proposed method to noisy labels with comprehensive experiments on the synthetic balanced noisy datasets (Section 3.1), synthetic imbalanced noisy datasets (Section 3.2), and real-world noisy dataset (Section 3.3).
3.1 Experiments on Synthetic Balanced Noisy Datasets
Datasets. We verify the e ectiveness of our method on the manually corrupted version of the following datasets: MNIST [25], F-MNIST [66], CIFAR-10 [24], and CIFAR-100 [24], because these datasets are popularly used for the evaluation of learning with noisy labels in the literature [15, 74, 62, 26]. The four datasets are class-balanced. The important statistics of the used synthetic datasets are summarized in Appendix B.1.
Generating noisy labels. We consider broad types of label noise: (1). Symmetric noise (abbreviated as Sym.) [61, 35, 29]. (2) Asymmetric noise (abbreviated as Asym.) [36, 65, 60]. (3) Pair ip noise (abbreviated as Pair.) [15, 74, 80]. (4). Tridiagonal noise (abbreviated as Trid.) [77]. (5). Instance noise (abbreviated as Ins.) [10, 64]. The noise rate is set to 20% and 40% to ensure clean

7

labels are diagonally dominant [36]. More details about above noise are provided in Appendix B.1.

We leave out 10% of noisy training examples as a validation set.

Baselines. We compare the proposed method (Algorithm 1) with following methods which

focus on sample selection, and implement all methods with default parameters by PyTorch, and

conduct all the experiments on NVIDIA Titan Xp GPUs. (1). S2E [71], which properly controls

the sample selection process so that deep networks can better bene t from the memorization ef-

fects. (2). MentorNet [19], which learns a curriculum to lter out noisy data. We use self-paced

MentorNet in this paper. (3). Co-teaching [15], which trains two networks simultaneously and

cross-updates parameters of peer networks. (4). SIGUA [16], which exploits stochastic integrated

gradient underweighted ascent to handle noisy labels. We use self-teaching SIGUA in this paper.

(5). JoCor [60], which reduces the diversity of networks to improve robustness. Other types of

baselines such as adding regularization are provided in Appendix B.2. Note that we do not com-

pare the proposed method with some state-of-the-art methods, e.g., SELF [44] and DivideMix [27].

It is because their proposed methods are aggregations of multiple techniques. We mainly focus on

sample selectionin in learning with noisy labels. Therefore, the comparison is not fair. Here, we

term our methods with soft truncation and hard truncation as CNLCU-S and CNLCU-H respec-

tively.

Network structure and optimizer. For MNIST, F-MNIST, and CIFAR-10, we use a 9-layer

CNN structure from [15]. Due to the limited space, the experimental details on CIFAR-100 are

provided in Appendix B.3. All network structures we used here are standard test beds for weakly-

supervised learning. For all experiments, the Adam optimizer [23] (momentum=0.9) is used with

an initial learning rate of 0.001, and the batch size is set to 128 and we run 200 epochs. We linearly

decay learning rate to zero from 80 to 200 epochs as did in [15]. We take two networks with

the same architecture but di erent initializations as two classi ers as did in [15, 74, 60], since even

with the same network and optimization method, di erent initializations can lead to di erent local

optimal [15]. The details of network structures can be checked in Appendix C. For the hyper-parameters 2 and min, we determine them in the range {10-1, 10-2, 10-3, 10-4}

with a noisy validation set. Here, we assume the noise level  is known and set R(T ) = 1 -

min{

T Tk



,



}

with

Tk=10.

If



is

not

known

in

advanced,

it

can

be

inferred

using

validation

sets

[33, 75]. As for performance measurement, we use test accuracy, i.e., test accuracy = (# of cor-

rect prediction) / (# of testing). All experiments are repeated ve times. We report the mean and

standard deviation of experimental results.

Experimental results. The experimental results about test accuracy are provided in Table

1, 2, and 3. Speci cally, for MNIST, as can be seen, our proposed methods, i.e., CNLCU-S and

CNLCU-H, produce the best results in the vast majority of cases. In some cases such as asymmetric

noise, the baseline S2E outperforms ours, which bene ts the accurate estimation for the number

of selected small-loss examples. For F-MNIST, the training data becomes complicated. S2E cannot

achieve the accurate estimation in such situation and thus has no great performance like it got on

MNIST. Our methods achieve varying degrees of lead over baselines. For CIFAR-10, our methods

once again outperforms all the baseline methods. Although some baseline, e.g., Co-teaching, can

work well in some cases, experimental results show that it cannot handle various noise types. In

contrast, the proposed methods achieve superior robustness against broad noise types. The results

mean that our methods can be better applied to actual scenarios, where the noise is diversiform.

Ablation study. We rst conduct the ablation study to analyze the sensitivity of the length

of time intervals. In order to avoid too dense gures, we exploit MNIST and F-MNIST with the

8

Noise type Method/Noise ratio S2E MentorNet Co-teaching SIGUA JoCor CNLCU-S CNLCU-H

Sym. 20% 40% 98.46 95.62 ±0.06 ±0.91 95.04 92.08 ±0.03 ±0.42 97.53 95.62 ±0.12 ±0.30 92.31 91.88 ±1.10 ±0.92 98.42 98.04 ±0.14 ±0.07 98.82 98.31 ±0.03 ±0.05 98.70 98.24 ±0.06 ±0.06

Asym. 20% 40% 99.05 98.45 ±0.02 ±0.26 96.32 90.86 ±0.17 ±0.97 98.25 95.08 ±0.08 ±0.43 93.96 62.59 ±0.82 ±0.15 98.05 94.55 ±0.37 ±1.08 98.93 97.67 ±0.06 ±0.22 99.01 98.01 ±0.04 ±0.03

Pair. 20% 40% 98.56 94.22 ±0.32 ±0.79 93.19 90.93 ±0.17 ±1.54 96.05 94.16 ±0.96 ±1.37 93.77 86.22 ±1.40 ±1.75 98.01 96.85 ±0.19 ±0.43 98.86 97.71 ±0.06 ±0.64 98.44 97.37 ±0.19 ±0.32

Trid. 20% 40% 99.02 97.23 ±0.09 ±1.26 96.42 93.28 ±0.09 ±1.37 98.05 96.18 ±0.06 ±0.85 94.92 83.46 ±0.83 ±2.98 98.45 96.98 ±0.17 ±0.25 99.09 98.02 ±0.04 ±0.17 98.89 97.92 ±0.15 ±0.05

Ins. 20% 40% 97.93 94.02 ±1.26 ±2.39 94.65 90.11 ±0.73 ±1.26 97.96 95.02 ±0.09 ±0.39 92.90 86.34 ±1.82 ±3.51 98.62 96.07 ±0.06 ±0.31 98.77 97.78 ±0.08 ±0.25 98.74 97.42 ±0.16 ±0.39

Table 1: Test accuracy (%) on MNIST over the last ten epochs. The best two results are in bold.

Noise type Method/Noise ratio S2E MentorNet Co-teaching SIGUA JoCor CNLCU-S CNLCU-H

Sym. 20% 40% 89.99 75.32 ±2.07 ±5.84 90.37 86.53 ±0.17 ±0.65 91.48 88.80 ±0.10 ±0.29 87.64 87.23 ±1.29 ±0.72 91.97 89.96 ±0.13 ±0.19 92.37 91.45 ±0.15 ±0.28 92.42 91.60 ±0.21 ±0.19

Asym. 20% 40% 89.00 81.03 ±0.95 ±1.93 89.69 67.21 ±0.19 ±2.94 91.03 68.07 ±0.14 ±4.58 76.97 45.96 ±2.59 ±3.40 90.95 79.79 ±0.21 ±2.39 92.57 83.14 ±0.15 ±1.77 92.60 82.69 ±0.18 ±0.43

Pair. 20% 40% 88.66 67.09 ±1.32 ±4.03 87.92 83.70 ±1.08 ±0.49 90.77 86.91 ±0.23 ±0.71 69.59 68.93 ±5.75 ±2.80 91.52 87.40 ±0.24 ±0.58 92.04 88.20 ±0.26 ±0.44 91.70 87.70 ±0.18 ±0.69

Trid. 20% 40% 89.53 77.29 ±2.63 ±3.97 88.74 85.63 ±0.33 ±0.59 91.24 89.18 ±0.11 ±0.36 79.97 76.14 ±3.23 ±4.24 92.01 89.42 ±0.17 ±0.33 92.24 90.08 ±0.17 ±0.34 92.33 90.22 ±0.26 ±0.71

Ins. 20% 40% 88.65 79.35 ±2.12 ±3.04 87.52 83.27 ±0.15 ±1.42 90.60 87.90 ±0.12 ±0.45 76.92 74.89 ±5.09 ±4.84 91.43 87.59 ±0.71 ±0.94 91.69 89.02 ±0.10 ±1.02 91.50 88.79 ±0.21 ±1.22

Table 2: Test accuracy on F-MNIST over the last ten epochs. The best two results are in bold.

mentioned noise settings as representative examples. For CNLCU-S, the length of time intervals is chosen in the range from 3 to 8. For CNLCU-H, the length of time intervals is chosen in the range from 10 to 15. Note that the reason for their di erent lengths is that their di erent mechanisms. Speci cally, CNLCU-S holistically changes the behavior of losses, but does not remove any loss from the loss set. We thus do not need too long length of time intervals. As a comparison, CNLCUH needs to remove some outliers from the loss set as discussed. The length should be longer to guarantee the number of examples available for robust mean estimation. The experimental results are provided in Appendix B.4, which show the proposed CNLCU-S and CNLCU-H are robust to the choices of the length of time intervals. Such robustness to hyperparameters means our methods can be applied in practice and does not need too much e ect to tune the hyperparameters.
Furthermore, since our methods concern uncertainty from two aspects, i.e., the uncertainty

9

from both small-loss and large-loss examples, we conduct experiments to analyze each part of our methods. Also, as mentioned, we compare robust mean estimation with non-robust mean estimation when learning with noisy labels. More details are provided in Appendix B.4.

Noise type Method/Noise ratio S2E MentorNet Co-teaching SIGUA JoCor CNLCU-S CNLCU-H

Sym. 20% 40% 80.78 69.72 ±0.88 ±3.94 80.92 74.67 ±0.48 ±1.17 82.35 77.96 ±0.16 ±0.39 78.19 77.67 ±0.22 ±0.41 80.96 76.65 ±0.25 ±0.43 83.03 78.25 ±0.21 ±0.70 83.03 78.33 ±0.47 ±0.50

Asym. 20% 40% 84.03 75.04 ±1.01 ±1.24 80.37 71.69 ±0.26 ±1.06 83.87 73.43 ±0.24 ±0.62 75.14 52.76 ±0.36 ±0.68 81.39 69.92 ±0.74 ±1.63 85.06 75.34 ±0.17 ±0.32 84.95 75.29 ±0.27 ±0.80

Pair. 20% 40% 81.72 61.50 ±0.93 ±4.63 77.98 69.39 ±0.31 ±1.73 80.94 72.81 ±0.46 ±0.92 74.41 61.91 ±0.81 ±5.27 80.33 71.62 ±0.20 ±1.05 83.16 73.19 ±0.25 ±1.25 83.39 73.40 ±0.68 ±1.53

Trid. 20% 40% 81.44 64.39 ±0.59 ±2.82 78.02 71.56 ±0.29 ±0.93 81.17 74.37 ±0.60 ±0.64 75.75 74.05 ±0.53 ±0.41 79.03 74.33 ±0.13 ±1.09 82.77 74.37 ±0.32 ±1.37 82.52 74.79 ±0.71 ±1.13

Ins. 20% 40% 79.89 62.42 ±0.26 ±3.11 77.02 68.17 ±0.71 ±2.52 79.92 73.29 ±0.57 ±1.62 74.34 67.98 ±0.39 ±1.34 78.21 71.46 ±0.34 ±1.27 82.03 73.67 ±0.37 ±1.09 81.93 73.58 ±0.25 ±1.39

Table 3: Test accuracy (%) on CIFAR-10 over the last ten epochs. The best two results are in bold.

3.2 Experiments on Synthetic Imbalanced Noisy Datasets
Experimental setup. We exploit MNIST and F-MNIST. For these two datasets, we reduce the number of training examples along with the labels from "0" to "4" to 1% of previous numbers. We term such synthetic imbalanced noisy datasets as IM-MNIST and IM-F-MNIST respectively. This setting aims to simulate the extremely imbalanced circumstance, which is common in practice. Moreover, we exploit asymmetric noise, since these types of noise can produce more imbalanced case [47, 36]. Other settings such as the network structure and optimizer are the same as those in experiments on synthetic balanced noisy datasets.
As for performance measurements, we use test accuracy. In addition, we exploit the selected ratio of training examples with the imbalanced classes, i.e., selected ratio=(# of selected imbalanced labels / # of all selected labels). Intuitively, a higher selected ratio means the proposed method can make better use of training examples with the imbalanced classes, following better generalization [21].
Experimental results. The test accuracy achieved on IM-MNIST and IM-F-MNIST is presented in Figure 2. Recall the experimental results in Table 1 and 2, we can see that the imbalanced issue is catastrophic to the sample selection approach when learning with noisy labels. For IMMNIST, as can be seen, all the baselines have serious over tting in the early stages of training. The curves of test accuracy drop dramatically. As a comparison, the proposed CNLCU-S and CNLCU-H can give a try to large-loss but less selected data which are possible to be clean but equipped with imbalanced labels. Therefore, our methods always outperform baselines clearly. In the case of Asym. 10%, our methods achieve nearly 30% lead over baselines. For IM-F-MNIST, we can also see that our methods perform well and always achieve about 5% lead over all the baselines. Note that due to the huge challenge of this task, some baseline, e.g., S2E, has a large error bar. In addition,
10

S2E

MentorNet

Co-teaching

SIGUA

JoCor

CNLCU-S

CNLCU-H

Test Accuracy

IM-MNIST ­ ­ Test Accuracy

­ Asym. 10% ­
100 80 60 40

Test Accuracy

­ Asym. 20% ­
100 80 60 40

­ Asym. 30% ­
100 80 60 40

Test Accuracy

­ Asym. 40% ­
80 60 40

20

20

20

20

0

50

Ep10o0ch

150

200

0

50

Ep10o0ch

150

200

0

50

Ep10o0ch

150

200

0

50

Ep10o0ch

150

200

IM-F-MNIST ­ ­ Test Accuracy

14

1670
60 50

Time Interval

18 70 60 50

20 60
50

22

60

24

50

Test Accuracy

Test Accuracy

Test Accuracy

40

40

40

40

30

30

30

30

20

20

20

20

10 0

50

Ep10o0ch

150

10

200

0

50

Ep10o0ch

150

10

200

0

10

50

Ep10o0ch

150

200

0

50

Ep10o0ch

150

200

Figure 2: Test accuracy vs. number of epochs on IM-MNIST and IM-F-MNIST. The error bar for standard deviation in each gure has been shaded.
the baseline SIGUA performs badly. It is because SIGUA exploits stochastic integrated gradient underweighted ascent on large-loss examples, which makes the examples with imbalanced classes more di cult to be selected than them in other sample selection methods.
The selected ratio achieved on IM-MNIST and IM-F-MNIST is presented in Table 4. The results explain well why our methods perform better on synthetic imbalanced noisy datasets, i.e., our methods can make better use of training examples with the imbalanced classes. Note that since we give a try to large-loss but less selected data in a conservative way, the selected ratio is still far away from the class prior probability on the test set, i.e., 10%. However, a little improvement of the selection ratio can bring a considerable improvement of test accuracy. These results tell us that, in the sample selection approach when learning with noisy labels, improving the selected ratio of training examples with the imbalanced classes is challenging but promising for generalization. This practical problem deserves to be studied in depth.

Dataset Method/Noise ratio S2E MentorNet Co-teaching SIGUA JoCor CNLCU-S CNLCU-H

10% 0.13 ±0.12 0.10 ±0.02 0.09 ±0.03 0.04 ±0.00 0.11 ±0.04 0.60 ±0.11 0.57 ±0.13

IM-MNIST 20% 30% 0.11 0.09 ±0.05 ±0.02 0.15 0.12 ±0.02 ±0.03 0.07 0.05 ±0.02 ±0.01 0.04 0.01 ±0.00 ±0.00 0.08 0.07 ±0.01 ±0.03 0.37 0.39 ±0.09 ±0.04 0.32 0.37 ±0.01 ±0.07

40% 0.05 ±0.01 0.13 ±0.02 0.12 ±0.01 0.02 ±0.00 0.06 ±0.02 0.38 ±0.06 0.32 ±0.05

10% 0.13 ±0.04 0.12 ±0.01 0.17 ±0.05 0.03 ±0.00 0.05 ±0.01 0.35 ±0.03 0.34 ±0.02

IM-F-MNIST 20% 30% 0.17 0.16 ±0.03 ±0.02 0.15 0.09 ±0.03 ±0.01 0.04 0.13 ±0.00 ±0.04 0.02 0.04 ±0.00 ±0.00 0.13 0.13 ±0.04 ±0.03 0.39 0.36 ±0.04 ±0.03 0.35 0.32 ±0.06 ±0.04

40% 0.12 ±0.04 0.14 ±0.02 0.07 ±0.01 0.00 ±0.00 0.07 ±0.02 0.30 ±0.02 0.28 ±0.03

Table 4: Selected ratio (%) on IM-MNIST and IM-F-MNIST. The best two results are in bold.

11

3.3 Experiments on Real-world Noisy Datasets
Experimental setup. To verify the e cacy of our methods in the real-world scenario, we conduct experiments on the noisy dataset Clothing1M [67]. Speci cally, for experiments on Clothing1M, we use the 1M images with noisy labels for training and 10k clean data for test respectively. Note that we do not use the 50k clean training data in all the experiments. For preprocessing, we resize the image to 256×256, crop the middle 224×224 as input, and perform normalization. The experiments on Clothing1M are performed once due to the huge computational cost. We leave 10% noisy training data as a validation set for model selection. Note that we do not exploit the resampling trick during training [27]. Here, Best denotes the test accuracy of the epoch where the validation accuracy was optimal. Last denotes test accuracy of the last epoch. For the experiments on Clothing1M, we use a ResNet-18 pretrained on ImageNet as did in [60]. We also use the Adam optimizer and set the batch size to 64. During the training stage, we run 15 epochs in total and set the learning rate 8 × 10-4, 5 × 10-4, and 5 × 10-5 for 5 epochs each.
Experimental results. The results on Clothing1M are provided in Table 5. Speci cally, the proposed methods get better results than state-of-the-art methods on Best, which achieve an improvement of +1.28% and +0.99% over the best baseline JoCor. Likewise, the proposed methods outperform all the baselines on Last. We achieve an improvement of +1.01% and +0.54% over JoCor. Note that the results are a bit lower than some state-of-art methods, e.g., [73] and [54], because of the following reasons. (1). We follow [60] and use ResNet-18 as a backbone. The state-of-art methods [73, 54] use ResNet-50 as a backbone. Our aim is to make the experimental results directly comparable with previous papers [60] in the same area. (2). We only focus on the sample selection approach and do not employ other advanced techniques, e.g., introducing the prior distribution [54] and combining semi-supervised learning [27, 44, 32].

Methods S2E MentorNet Co-teaching SIGUA JoCor CNLCU-S CNLCU-H

Best 67.34 68.36

69.37

62.89 70.09 71.37

71.08

Last 65.90 67.42

68.62

58.73 69.75 70.76

70.29

Table 5: Test accuracy (%) on Clothing1M. The best two results are in bold.

4 Conclusion
In this paper, we focus on promoting the prior sample selection in learning with noisy labels, which starts from concerning the uncertainty of losses during training. We robustly use the training losses at di erent iterations to reduce the uncertainty of small-loss examples, and adopt con dence interval estimation to reduce the uncertainty of large-loss examples. Experiments are conducted on benchmark datasets, demonstrating the e ectiveness of our method. We believe that this paper opens up new possibilities in the topics of using sample selection to handle noisy labels, especially in improving the robustness of models on imbalanced noisy datasets.

12

Acknowledgement
TL was supported by Australian Research Council Project DE-190101473. BH was supported by the RGC Early Career Scheme No. 22200720 and NSFC Young Scientists Fund No. 62006202. JY was supported by USTC Research Funds of the Double First-Class Initiative (YD2350002001). GN and MS were supported by JST AIP Acceleration Research Grant Number JPMJCR20U3, Japan. MS was also supported by the Institute for AI and Beyond, UTokyo.

A Proof of Theoretical Results

A.1 Proof of Theorem 1

For the circumstance with soft truncation,

µ~z

=

1 n

n i=1

(zi).

As

suggested

in

[7],

we

can

exploit

µ~-z and µ~+z such that

µ~-z  µ~z  µ~+z ,

(9)

to derive a bound for µ~z. For some positive real parameter , we de ne

n

r(µ~z) =  [(zi - µ~z)] = 0.

(10)

i=1

Let us introduce the quantity

1n

r() = n

 [(zi - )] .

(11)

i=1

With the exponential moment inequality [13] and the Cr inequality [41], we have

exp{nr()}  1 + (µz - ) + 2[2 + (µz - )2] n  exp{n(µz - ) + n2[2 + (µz - )2]}.

(12)

In the same way,

exp{-nr()}  exp{-n(µz - ) + n2[2 + (µz - )2]}.

(13)

If we de ne for any µs  R the bounds

B-()

=

µz

-



-

[2

+

(µz

-

)2]

-

log( -1) n

(14)

and

B+()

=

µz

-



+

[2

+

(µz

-

)2]

+

log( -1) .
n

(15)

From [9] (Lemma 2.2), we obtain that

P (r() > B-())  1 - and P (r() < B+())  1 - .

(16)

13

Let µ~-z be the largest solution of the quadratic equation B-() and µ~+z be the smallest solution

of the quadratic equation B+(). Also, to guarantee the solution of the quadratic equation, we

assume

422 + 4 log( -1)  1.

(17)

n

From [9] (Theorem 2.6), we then have

µ~-z



µz

-

2

+

log( -1) n

-1

,

(18)

and

µ~+z



µz

+

2

+

log( -1) n

-1

.

(19)

With

probability

at

least

1-2

,

we

have

µ~-z



µ~z



µ~+z .

We

can

choose



=

n 2

.

Then

we

have

|µ~z

-

µz |



2(n

+

2

log( n2

n - 2

-1) ) ,

(20)

which holds with probability at least 1-2 .

We exploit the lower bound and let

=

1 2t

.

Then

we

have

s

=

µ~s

-

2(t + nt

2
-

log(2t) t2
2

)

,

(21)

where nt denotes the number of times that the example was selected in the time intervals.

y

10

y=x

y = log(1 + x + x2/2)

8

6

4

2

0

0

2

4x 6

8

10

Figure 3: The illustration of the in uence function for the soft estimator.

Here, we provide the graph of the used in uence function for the soft estimator, which explains the mechanism of the function y = log(1 + x + x2/2) more clearly. The illustration is presented in Figure 3. As can be seen, when x is large and may be an outlier, the in uence function can reduce its negative impact for mean estimation. Therefore, we exploit such an in uence function for robust mean estimation, which brings better classi cation performance.

14

A.2 Proof of Theorem 2

Lemma 1 ([48]). Let Zn = {z1, . . . , zn} be a (not necessarily time homogeneous) Markov chain

with mean µz, taking values in a Polish state space 1 × . . . × n, with a mixing time  () (for

0    1). Let

2- 2

min = inf  () ·
0<1

1-

.

(22)

For some   R+, suppose that f :   R satis es the following inequality:

n

f (a) - f (b)  1[ai = bi],

(23)

i=1

for every a, b  . Then for any  0, we have

P (|f (Zn) - Ef (Zn)|  )  2 exp

-2 2 2min

.

(24)

The detailed de nition of the mixing time for the Markov chain can be found in [48, 50]. Let f be the mean function. Following the prior work on mean estimation [31, 12, 11, 46], without loss of generality, we assume µz = 0 for the underlying true distribution, and |zi| is upper bounded by Z. Then we can set  to 4Z/n for Eq. (23). Combining the above analyses, we can revise Eq. (24) as follows:

1n

2Z

2

P

n zi  n
i=1

2min log

 1,

1

(25)

and

2Z

2n

P

max |zi| 
i[n]

n

2min log
2

 2,

(26)

for 1 > 0 and 2 > 0. If we remove the potential outliers Zno from Zn. Therefore, we have

1

1

n - no ziZn\Zno -µz

= n - no

-
ziZn ziZno





1





+



n - no ziZn

zi Zno

(27)





1





n - no ziZn

+ no max |zi|
i[n]

1 
n - no

2Z

2min log

2
1

+

2Z no n

2n 2min log
2

,

which holds with probability at least 1 - 1 - 2.

For our task, we exploit the concentration inequality. Let

1=

2

=

1 2t

,

and

the

losses

be

bounded by L. Next we can obtain

15

2L

|µ~h

-

µ|



t- 

to

2min

log(4t)

+

to t

4min log(4t)



(28)

2 =

2minL(t +

2to)

log(4t)

(t - to)t

with

the

probability

at

least

1

-

1 t

.

In

practice,

it

is

easy

to

identify

the

value

of

L.

For

example,

we

can training deep networks on noisy datasets to observe the loss distributions. Then, we exploit

the lower bound such that





2 h = µ~h -

2minL(t

+ 

2to)

(t - to) t

log(4t) nt

(29)

for sample selection.

B Complementary Experimental Analyses

MNIST F-MNIST CIFAR-10 CIFAR-100

# of training 60,000 60,000 50,000 50,000

# of testing 10,000 10,000 10,000 10,000

# of class 10 10 10 100

size 28×28×1 28×28×1 32×32×3 32×32×3

Table 6: Summary of synthetic datasets used in the experiments.

B.1 The Details of Datasets and Generating Noisy Labels
For the details of datasets, the important statistics of the used datasets are summarized in Table 6. For the details of generating noisy labels, we exploit both class-dependent and instance-dependent
label noise which include ve types of synthetic label noise to verify the e ectiveness of the proposed method. Here, we describe the details of the noise setting as follows:
(1). Class-dependent label noise: · Symmetric noise: this kind of label noise is generated by ipping labels in each class uniformly to incorrect labels of other classes. · Asymmetic noise : this kind of label noise is generated by ipping labels within a set of similar classes. In this paper, for MNIST, ipping 27, 38, 56. For F-MNIST, ipping TSHIRTSHIRT, PULLOVERCOAT, SANDALSSNEAKER. For CIFAR-10, ipping TRUCKAUTOMOBILE, BIRD AIRPLANE, DEERHORSE, CATDOG. For CIFAR-100, the 100 classes are grouped into 20 super-classes, and each has 5 sub-classes. Each class is then ipped into the next within the same super-class. · Pair ip noise: the noise ips each class to its adjacent class. · Tridiagonal noise: the noise corresponds to a spectral of classes where adjacent classes are easier to be mutually mislabeled, unlike the unidirectional pair ipping. It can be implemented by two consecutive pair ipping transformations in the opposite direction.
16

9876543210 9876543210 9876543210 9876543210

Symmetric 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0123456789
(a)

Asymmetric 1000000000 0100000000 0 0 0.8 0 0 0 0 0.2 0 0 0 0 0 0.8 0 0 0 0 0.2 0 0000100000 0 0 0 0 0 0.8 0.2 0 0 0 0 0 0 0 0 0.2 0.8 0 0 0 0000000100 0000000010 0000000001 0123456789
(b)

Pairflip 0.8 0.2 0 0 0 0 0 0 0 0 0 0.8 0.2 0 0 0 0 0 0 0 0 0 0.8 0.2 0 0 0 0 0 0 0 0 0 0.8 0.2 0 0 0 0 0 0 0 0 0 0.8 0.2 0 0 0 0 0 0 0 0 0 0.8 0.2 0 0 0 0 0 0 0 0 0 0.8 0.2 0 0 0 0 0 0 0 0 0 0.8 0.2 0 0 0 0 0 0 0 0 0 0.8 0.2 0.2 0 0 0 0 0 0 0 0 0.8 0123456789
(c)

Tridiagonal 0.8 0.1 0 0 0 0 0 0 0 0.1 0.1 0.8 0.1 0 0 0 0 0 0 0 0 0.1 0.8 0.1 0 0 0 0 0 0 0 0 0.1 0.8 0.1 0 0 0 0 0 0 0 0 0.1 0.8 0.1 0 0 0 0 0 0 0 0 0.1 0.8 0.1 0 0 0 0 0 0 0 0 0.1 0.8 0.1 0 0 0 0 0 0 0 0 0.1 0.8 0.1 0 0 0 0 0 0 0 0 0.1 0.8 0.1 0.1 0 0 0 0 0 0 0 0.1 0.8 0123456789
(d)

Figure 4: Synthetic class-dependent transition matrices used in our experiments on MNIST. The noise rate is set to 20%.

Noise type Method/Noise ratio APL
CDR

Sym. 20% 40% 98.76 94.92 ±0.06 ±0.31 94.77 92.16 ±0.17 ±0.73

Asym. 20% 40% 98.63 88.65 ±0.05 ±1.72 96.73 91.05 ±0.19 ±0.76

Pair. 20% 40% 98.66 68.44 ±0.10 ±2.95 93.25 71.02 ±0.90 ±3.89

Trid. 20% 40% 98.93 76.44 ±0.04 ±3.04 94.06 70.28 ±0.92 ±4.01

Ins. 20% 40% 97.63 87.90 ±0.73 ±1.94 93.17 77.45 ±0.96 ±3.04

Table 7: Test accuracy (%) on MNIST over the last ten epochs.

Noise type Method/Noise ratio APL
CDR

Sym. 20% 40% 91.73 89.06 ±0.20 ±0.41 85.62 71.83 ±0.96 ±1.37

Asym. 20% 40% 90.13 80.34 ±0.17 ±0.63 89.78 79.05 ±0.41 ±1.39

Pair. 20% 40% 90.22 78.54 ±0.80 ±4.33 85.72 69.07 ±0.65 ±2.31

Trid. 20% 40% 90.84 86.53 ±0.22 ±0.76 86.75 73.63 ±1.19 ±2.82

Ins. 20% 40% 90.96 85.55 ±0.77 ±2.86 85.92 73.14 ±1.43 ±3.12

Table 8: Test accuracy on F-MNIST over the last ten epochs.

(2). Instance-dependent label noise: · Instance noise: the noise is quite realistic, where the probability that an instance is mislabeled depends on its features. We generate this type of label noise to validate the e ectiveness of the proposed method as did in [64]. We use synthetic noisy MNIST as an example and plot the noise transition matrices in Figure 4. The noise rate is set to 20%.
B.2 Comparison with Other Types of Baselines
As we focus on the sample selection approach in learning with noisy labels, in the main paper (Section 3.1), we fairly compare our methods with the baselines which also focus on sample selection. Here, we evaluate other types of baselines. We exploit APL [36] and CDR [65], which add implicit regularization from di erent perspectives. The experiments are conducted on MNIST and F-MNIST. Other experimental settings are the same as those in the main paper. The experimental results are provided in Table 7 and 8, which show that the proposed methods can outperform them with respect to classi cation performance.

17

B.3 Experiments on Synthetic CIFAR-100
For CIFAR-100, we use a 7-layer CNN structure from [74, 71]. Other experimental settings are the same as those in the experiments on MNIST, F-MNIST, and CIFAR-10. The results are provided in Table 9. We can see the proposed method outperforms all the baselines.

Noise type Method/Noise ratio S2E MentorNet Co-teaching SIGUA JoCor CNLCU-S CNLCU-H

Sym. 20% 40% 44.59 25.78 ±0.32 ±5.44 43.15 37.62 ±0.42 ±0.89 45.17 40.95 ±0.25 ±0.52 42.03 40.53 ±0.33 ±0.49 45.93 41.56 ±0.21 ±0.57 46.09 42.11 ±0.29 ±0.70 46.27 42.05 ±0.38 ±0.87

Asym. 20% 40% 42.18 26.81 ±1.73 ±2.25 41.03 28.27 ±0.22 ±0.41 42.76 30.27 ±0.34 ±0.33 36.67 26.71 ±0.25 ±0.42 42.89 29.19 ±0.37 ±1.42 43.06 30.47 ±0.28 ±0.37 43.21 30.55 ±0.93 ±0.72

Pair. 20% 40% 42.99 26.96 ±1.54 ±2.48 40.06 27.17 ±0.37 ±0.92 42.50 30.07 ±0.39 ±0.17 36.48 26.73 ±0.37 ±0.33 42.12 30.12 ±0.35 ±0.65 43.08 30.33 ±0.92 ±0.74 43.25 30.79 ±0.75 ±0.86

Trid. 20% 40% 43.16 27.72 ±0.93 ±3.56 42.20 31.74 ±0.30 ±0.88 44.41 34.96 ±0.41 ±0.35 39.21 32.69 ±0.40 ±0.36 44.98 34.23 ±0.27 ±1.13 45.19 35.49 ±0.90 ±1.30 45.02 35.24 ±1.06 ±0.93

Ins. 20% 40% 43.13 27.12 ±0.67 ±3.86 40.54 33.09 ±0.69 ±1.53 42.23 35.87 ±0.52 ±1.47 39.19 33.51 ±0.32 ±0.43 44.28 35.60 ±0.59 ±0.99 44.80 36.23 ±0.70 ±0.49 45.02 36.17 ±1.07 ±1.54

Table 9: Test accuracy (%) on CIFAR-100 over the last ten epochs. The best two results are in bold.

B.4 Experiments for Ablation Study

We conduct the ablation study to analyze the sensitivity of the length of time intervals. The results are shown in Figure. 5 and 6. As we can seen, the proposed method, i.e., CNLCU-S and CNLCU-H are robust to the choices of hyperparameters.

­ MNIST ­ Test Accuracy

­ Sym. ­

­ Asym. ­

­ Pair. ­

­ Trid. ­

­ Ins. ­

99.00

20% noise 40% noise

99.00

99.00

99.20 99.00

98.75

Test Accuracy

Test Accuracy

Test Accuracy

Test Accuracy

98.75

98.50

98.80

98.50

98.80

98.50

98.60

98.25

98.60

98.25 98.00

98.00

98.40 98.20

98.00

98.40

97.75

97.50

98.00

97.75

97.50

97.80

97.50

98.20

97.25

97.00

97.60

97.25

3

4

T5ime Interva6l

7

8

3

4

T5ime Interva6l

7

8

3

4

T5ime Interva6l

7

8

3

4

T5ime Interva6l

7

8

3

4

T5ime Interva6l

7

8

93.00

20% noise 40% noise

92.00

93.00

92.50

92.00

92.50

90.00

92.00 91.00

92.00 91.50

91.00

Test Accuracy

Test Accuracy

Test Accuracy

Test Accuracy

92.00

88.00 86.00

90.00

91.00 90.50

90.00

91.50

84.00

89.00

90.00

89.00

91.00

82.00

88.00

89.50 89.00

88.00

3

4

T5ime Interva6l

7

8

3

4

T5ime Interva6l

7

8

87.00 3

4

T5ime Interva6l

7

8

3

4

T5ime Interva6l

7

8

3

4

T5ime Interva6l

7

8

­ F-MNIST ­ Test Accuracy

Figure 5: Illustrations of the hyperparameter sensitivity for the proposed CNLCU-S. The error bar for standard deviation in each gure has been shaded.

Note that in this paper, we concern uncertainty from two aspects, i.e., the uncertainty about small-loss examples and the uncertainty about large-loss examples. Here, we conduct ablation study to show the e ect of removing di erent components to provide insights into what makes

18

­ MNIST ­ Test Accuracy

­ Sym. ­

­ Asym. ­

­ Pair. ­

­ Trid. ­

­ Ins. ­

98.80

20% noise 40% noise

99.00

98.80

98.50

99.20 99.00

99.00

98.60

98.60

98.00

98.80

98.50

Test Accuracy

Test Accuracy

Test Accuracy

Test Accuracy

98.40

98.40 98.20

97.50

98.60 98.40

98.00

98.20

98.00 97.80

97.00

98.20 98.00

97.50

98.00

97.60

96.50

97.80

97.00

10

11

1T2ime Interv1a3l

14

15

10

11

1T2ime Interv1a3l

14

15

10

11

1T2ime Interv1a3l

14

15

10

11

1T2ime Interv1a3l

14

15

10

11

1T2ime Interv1a3l

14

15

92.75 92.50

20% noise 40% noise

92.00

Test Accuracy

92.25

90.00

92.00

91.75

88.00

91.50

86.00

91.25

84.00

Test Accuracy

92.00 91.00 90.00 89.00 88.00

Test Accuracy

92.50 92.00 91.50 91.00 90.50 90.00 89.50

Test Accuracy

91.00 90.00 89.00 88.00

91.00 90.75

82.00

87.00

89.00

87.00

10

11

1T2ime Interv1a3l

14

15

10

11

1T2ime Interv1a3l

14

15

10

11

1T2ime Interv1a3l

14

15

10

11

1T2ime Interv1a3l

14

15

10

11

1T2ime Interv1a3l

14

15

­ F-MNIST ­ Test Accuracy

Figure 6: Illustrations of the hyperparameter sensitivity for the proposed CNLCU-H. The error bar for standard deviation in each gure has been shaded.

the proposed methods successful. The experiments are conducted on MNIST and F-MNIST. Other experimental settings are the same as those in the main paper (Section 3.1). Note that we employ two networks to teach each other following [15]. Therefore, when we do not consider uncertainty in sample selection, the proposed methods will reduce to the baseline Co-teaching [15].
To study the e ect of concerning uncertainty about small-loss examples, we remove the concerns about large-loss examples, i.e., the network is not encouraged to choose the less selected examples for updates. We express such a setting as "without concerning about large-loss examples" (abbreviated as w/o cl). To study the e ect of concerning uncertainty about large-loss examples, we remove the concerns about small-loss examples, i.e., we only exploit the predictions of the current network. We express such a setting as "without concerning about small-loss examples" (abbreviated as w/o cs). Besides, we express the setting which directly uses non-robust mean as Co-teaching-M.
The experimental results of ablation study are provided in Table 10 and 11. As can be seen, both aspects of uncertainty concerns can improve the robustness of models. Therefore, combining two uncertainty concerns, we can better combat noisy labels. In addition, robust mean estimation is superior to the non-robust mean in learning with noisy labels.
C Complementary Explanation for Network Structures
Table 12 describes the 9-layer CNN [15] used on MNIST, F-MNIST, and CIFAR-10. Table 13 describes the 9-layer CNN [74] used on CIFAR-100. Here, LReLU stands for Leaky ReLU [68]. The slopes of all LReLU functions in the networks are set to 0.01. Note that that the 7/9-layer CNN is a standard and common practice in weakly supervised learning. We decided to use these CNNs, since then the experimental results are directly comparable with previous approaches in the same area, i.e., learning with noisy labels.

19

Noise type Method/Noise ratio CNLCU-S CNLCU-S w/o cl CNLCU-S w/o cs CNLCU-H CNLCU-H w/o cl CNLCU-H w/o cs Co-teaching-M Co-teaching

Sym. 20% 40% 98.82 98.31 ±0.03 ±0.05 98.02 96.83 ±0.08 ±0.29 98.15 97.12 ±0.20 ±0.22 98.70 98.24 ±0.06 ±0.06 98.06 96.92 ±0.13 ±0.23 98.19 97.05 ±0.22 ±0.49 97.72 97.78 ±0.08 ±0.32 97.53 95.62 ±0.12 ±0.30

Asym. 20% 40% 98.93 97.67 ±0.06 ±0.22 98.50 96.25 ±0.04 ±0.13 98.36 96.39 ±0.07 ±0.48 99.01 98.01 ±0.04 ±0.03 98.39 96.51 ±0.04 ±0.57 98.76 97.17 ±0.59 ±0.60 98.27 95.42 ±0.03 ±0.42 98.25 95.08 ±0.08 ±0.43

Pair. 20% 40% 98.86 97.71 ±0.06 ±0.64 98.22 96.08 ±0.13 ±0.75 98.04 96.12 ±0.24 ±0.68 98.44 97.37 ±0.19 ±0.32 97.04 95.62 ±0.87 ±0.93 97.26 96.31 ±1.19 ±0.25 96.22 95.01 ±0.10 ±0.65 96.05 94.16 ±0.96 ±1.37

Trid. 20% 40% 99.09 98.02 ±0.04 ±0.17 98.64 97.25 ±0.31 ±0.24 98.74 97.30 ±0.05 ±0.52 98.89 97.92 ±0.15 ±0.05 98.33 97.41 ±0.47 ±0.92 98.29 97.65 ±0.17 ±0.92 97.92 96.64 ±0.14 ±0.77 98.05 96.18 ±0.06 ±0.85

Ins. 20% 40% 98.77 97.78 ±0.08 ±0.25 98.17 97.13 ±0.20 ±0.40 98.11 97.32 ±0.15 ±0.43 98.74 97.42 ±0.16 ±0.39 98.01 96.15 ±0.20 ±0.28 98.34 96.49 ±0.36 ±0.48 98.02 96.03 ±0.04 ±0.57 97.96 95.02 ±0.09 ±0.39

Table 10: Test accuracy (%) on MNIST over last ten epochs.

Noise type Method/Noise ratio CNLCU-S CNLCU-S w/o cl CNLCU-S w/o cs CNLCU-H CNLCU-H w/o cl CNLCU-H w/o cs Co-teaching-M Co-teaching

Sym. 20% 40% 92.37 91.45 ±0.15 ±0.28 91.77 89.40 ±0.35 ±0.26 91.85 90.76 ±0.33 ±0.28 92.42 91.60 ±0.21 ±0.19 91.70 90.05 ±0.04 ±0.31 91.82 90.92 ±0.13 ±0.42 91.33 89.05 ±0.18 ±0.73 91.48 88.80 ±0.10 ±0.29

Asym. 20% 40% 92.57 83.14 ±0.15 ±1.77 91.25 72.93 ±0.30 ±2.63 91.94 80.99 ±0.09 ±2.74 92.60 82.69 ±0.18 ±0.43 91.08 71.35 ±0.06 ±2.30 92.45 80.73 ±0.25 ±1.63 91.14 71.03 ±0.90 ±3.73 91.03 68.07 ±0.14 ±4.58

Pair. 20% 40% 92.04 88.20 ±0.26 ±0.44 91.53 87.31 ±0.17 ±0.59 91.28 87.31 ±0.20 ±0.72 91.70 87.70 ±0.18 ±0.69 91.03 87.22 ±0.29 ±0.72 91.21 87.49 ±0.17 ±0.32 90.85 86.95 ±0.61 ±0.19 90.77 86.91 ±0.23 ±0.71

Trid. 20% 40% 92.24 90.08 ±0.17 ±0.34 91.31 89.50 ±0.52 ±0.32 91.39 89.29 ±0.07 ±0.51 92.33 90.22 ±0.26 ±0.71 91.59 90.01 ±0.07 ±0.24 92.08 89.72 ±0.13 ±0.24 91.50 89.18 ±0.46 ±0.44 91.24 89.18 ±0.11 ±0.36

Ins. 20% 40% 91.69 89.02 ±0.10 ±1.02 91.09 88.45 ±0.13 ±0.57 90.98 88.73 ±0.43 ±0.62 91.50 88.79 ±0.21 ±1.22 90.80 88.31 ±0.27 ±1.09 91.21 88.62 ±0.38 ±0.73 90.74 88.25 ±1.06 ±0.92 90.60 87.90 ±0.12 ±0.45

Table 11: Test accuracy (%) on F-MNIST over last ten epochs.

20

Table 12: CNN on MNIST, F-MNIST, and CIFAR-10.

CNN on MNIST CNN on F-MNIST CNN on CIFAR-10

28×28 Gray Image 28×28 Gray Image 32×32 RGB Image

3×3 conv, 128 LReLU

3×3 conv, 128 LReLU

3×3 conv, 128 LReLU

2×2 max-pool

dropout, p = 0.25

3×3 conv, 256 LReLU

3×3 conv, 256 LReLU

3×3 conv, 256 LReLU

2×2 max-pool

dropout, p = 0.25

3×3 conv, 512 LReLU

3×3 conv, 256 LReLU

3×3 conv, 128 LReLU

avg-pool

dense 12810

dense 12810

dense 12810

Table 13: CNN on CIFAR-100.
CNN on CIFAR-100 32×32 RGB Image 3×3 conv, 64 ReLU 3×3 conv, 64 ReLU
2×2 max-pool 3×3 conv, 128 ReLU 3×3 conv, 128 ReLU
2×2 max-pool 3×3 conv, 196 ReLU 3×3 conv, 196 ReLU
2×2 max-pool dense 256100

References
[1] Eric Arazo, Diego Ortego, Paul Albert, Noel O'Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. In ICML, pages 312­321, 2019.
[2] Devansh Arpit, Stanislaw Jastrzbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In ICML, pages 233­242, 2017.
[3] Peter Auer. Using con dence bounds for exploitation-exploration trade-o s. Journal of Machine Learning Research, 3(Nov):397­422, 2002.
[4] Léon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pages 421­436. Springer, 2012.
[5] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.
[6] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. Lof: identifying density-based local outliers. In SIGMOD, pages 93­104, 2000.
[7] Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study. In Annales de l'IHP Probabilités et statistiques, volume 48, pages 1148­1185, 2012.
[8] Arijit Chakrabarty and Gennady Samorodnitsky. Understanding heavy tails in a bounded world or, is a truncated heavy tail heavy or not? Stochastic models, 28(1):109­143, 2012.
[9] Peng Chen, Xinghu Jin, Xiang Li, and Lihu Xu. A generalized catoni's m-estimator under nite -th moment assumption with   (1, 2). arXiv preprint arXiv:2010.05008, 2020.

21

[10] Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded instance-and label-dependent label noise. In ICML, 2020.
[11] Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high-dimensional robust statistics. arXiv preprint arXiv:1911.05911, 2019.
[12] Ilias Diakonikolas, Daniel M Kane, and Ankit Pensia. Outlier robust mean estimation with subgaussian rates via stability. arXiv preprint arXiv:2007.15618, 2020.
[13] Evarist Giné, Rafal Latala, and Joel Zinn. Exponential and moment inequalities for u-statistics. In High Dimensional Probability II, pages 13­38. Springer, 2000.
[14] Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and Masashi Sugiyama. Masking: A new perspective of noisy supervision. In NeurIPS, pages 5836­5846, 2018.
[15] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NeurIPS, pages 8527­8537, 2018.
[16] Bo Han, Gang Niu, Xingrui Yu, Quanming Yao, Miao Xu, Ivor Tsang, and Masashi Sugiyama. Sigua: Forgetting may make learning with noisy labels more robust. In ICML, pages 4006­ 4016, 2020.
[17] Hrayr Harutyunyan, Kyle Reing, Greg Ver Steeg, and Aram Galstyan. Improving generalization by controlling label-noise information in neural network weights. In ICML, pages 4071­4081, 2020.
[18] Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. In NeurIPS, 2018.
[19] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, pages 2309­2318, 2018.
[20] Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In ICML, pages 4804­4815, 2020.
[21] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classi er for long-tailed recognition. In ICLR, 2020.
[22] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In ICCV, pages 101­110, 2019.
[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[24] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
22

[25] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/.
[26] Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo Li, and Jinwoo Shin. Robust inference via generative classi ers for handling noisy labels. In ICML, pages 3763­3772, 2019.
[27] Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In ICLR, 2020.
[28] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In AISTATS, 2020.
[29] Xuefeng Li, Tongliang Liu, Bo Han, Gang Niu, and Masashi Sugiyama. Provably end-to-end label-noise learning without anchor points. 2021.
[30] Yihua Liao and V Rao Vemuri. Use of k-nearest neighbor classi er for intrusion detection. Computers & security, 21(5):439­448, 2002.
[31] Liu Liu, Tianyang Li, and Constantine Caramanis. High dimensional robust m-estimation: Arbitrary corruption and heavy tails. arXiv preprint arXiv:1901.08237, 2019.
[32] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Earlylearning regularization prevents memorization of noisy labels. In NeurIPS, 2020.
[33] Tongliang Liu and Dacheng Tao. Classi cation with noisy labels by importance reweighting. IEEE Transactions on pattern analysis and machine intelligence, 38(3):447­461, 2016.
[34] Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, and Sanjiv Kumar. Does label smoothing mitigate label noise? In ICML, pages 6448­6458, 2020.
[35] Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In ICML, pages 3361­3370, 2018.
[36] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In ICML, pages 6543­6553, 2020.
[37] Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". In NeurIPS, pages 960­970, 2017.
[38] Aditya Krishna Menon, Brendan Van Rooyen, and Nagarajan Natarajan. Learning from binary labels with instance-dependent noise. Machine Learning, 107(8-10):1561­1595, 2018.
[39] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.
[40] Baharan Mirzasoleiman, Kaidi Cao, and Jure Leskovec. Coresets for robust training of neural networks against noisy labels. In NeurIPS, 2020.
23

[41] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. MIT Press, 2018.
[42] David S Moore. Uncertainty. On the shoulders of giants: New approaches to numeracy, pages 95­137, 1990.
[43] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In NeurIPS, pages 1196­1204, 2013.
[44] Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. Self: Learning to lter noisy labels with selfensembling. In ICLR, 2020.
[45] Kento Nishi, Yi Ding, Alex Rich, and Tobias Höllerer. Augmentation strategies for learning with noisy labels. arXiv preprint arXiv:2103.02130, 2021.
[46] Laura Niss and Ambuj Tewari. What you see may not be what you get: Ucb bandit algorithms robust to -contamination. In UAI, pages 450­459, 2020.
[47] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, pages 1944­1952, 2017.
[48] Daniel Paulin et al. Concentration inequalities for markov chains by marton couplings and spectral methods. Electronic Journal of Probability, 20, 2015.
[49] Geo Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying mislabeled data using the area under the margin ranking. In NeurIPS, 2020.
[50] Gareth O Roberts, Je rey S Rosenthal, et al. General state space markov chains and mcmc algorithms. Probability surveys, 1:20­71, 2004.
[51] Je rey S Rosenthal. Faithful couplings of markov chains: now equals forever. Advances in Applied Mathematics, 18(3):372­381, 1997.
[52] Jun Shu, Qian Zhao, Zengben Xu, and Deyu Meng. Meta transition adaptation for robust deep learning with noisy labels. arXiv preprint arXiv:2006.05697, 2020.
[53] Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classi er. Technical report, 2003.
[54] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. In CVPR, 2018.
[55] Kiran K Thekumparampil, Ashish Khetan, Zinan Lin, and Sewoong Oh. Robustness of conditional gans to noisy labels. In NeurIPS, pages 10271­10282, 2018.
[56] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
24

[57] Qizhou Wang, Jiangchao Yao, Chen Gong, Tongliang Liu, Mingming Gong, Hongxia Yang, and Bo Han. Learning with group noise. In AAAI, 2021.
[58] Xiaobo Wang, Shuo Wang, Jun Wang, Hailin Shi, and Tao Mei. Co-mining: Deep face recognition with noisy labels. In ICCV, pages 9358­9367, 2019.
[59] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia. Iterative learning with open-set noisy labels. In CVPR, pages 8688­8696, 2018.
[60] Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint training method with co-regularization. In CVPR, pages 13726­13735, 2020.
[61] Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen. A topological lter for learning with label noise. In NeurIPS, 2020.
[62] Songhua Wu, Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Nannan Wang, Haifeng Liu, and Gang Niu. Class2simi: A noise reduction perspective on learning with noisy labels. In ICML, 2021.
[63] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? In NeurIPS, pages 6835­6846, 2019.
[64] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instancedependent label noise. In NeurIPS, 2020.
[65] Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi Chang. Robust early-learning: Hindering the memorization of noisy labels. In ICLR, 2021.
[66] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
[67] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classi cation. In CVPR, pages 2691­2699, 2015.
[68] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of recti ed activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.
[69] Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise. In NeurIPS, pages 6222­6233, 2019.
[70] Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In ICLR, 2021.
[71] Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James Tin-Yau Kwok. Searching to exploit memorization e ect in learning with noisy labels. In ICML, pages 10789­10798, 2020.
[72] Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi Sugiyama. Dual t: Reducing estimation error for transition matrix in label-noise learning. In NeurIPS, 2020.
25

[73] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In CVPR, pages 7017­7025, 2019.
[74] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does disagreement bene t co-teaching? In ICML, 2019.
[75] Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, and Dacheng Tao. An e cient and provable approach for mixture proportion estimation using linear independence assumption. In CVPR, pages 4480­4489, 2018.
[76] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017.
[77] Yivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only noisy labels via total variation regularization. In ICML, 2021.
[78] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NeurIPS, pages 8778­8788, 2018.
[79] Yue Zhao, Zain Nasrullah, and Zheng Li. Pyod: A python toolbox for scalable outlier detection. Journal of Machine Learning Research, 20(96):1­7, 2019.
[80] Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, and Chao Chen. Error-bounded correction of noisy labels. In ICML, pages 11447­11457, 2020.
26

