arXiv:2106.00577v1 [stat.AP] 1 Jun 2021

Efficient adaptive MCMC implementation for Pseudo-Bayesian quantum tomography
The Tien Mai
Oslo Centre for Biostatistics and Epidemiology, Department of Biostatistics, University of Oslo, Norway. Email: t.t.mai@medisin.uio.no
Abstract
We revisit the Pseudo-Bayesian approach to the problem of estimating density matrix in quantum state tomography in this paper. Pseudo-Bayesian inference has been shown to offer a powerful paradign for quantum tomography with attractive theoretical and empirical results. However, the computation of (Pseudo-)Bayesian estimators, due to sampling from complex and high-dimensional distribution, pose significant challenges that hampers their usages in practical settings. To overcome this problem, we present an efficient adaptive MCMC sampling method for the Pseudo-Bayesian estimator. We show in simulations that our approach is substantially faster than the previous implementation by at least two orders of magnitude which is significant for practical quantum tomography.
Keywords: quantum tomography; Bayesian analysis; MCMC, low-rank matrix.
1 Introduction
Quantum state tomography is a fundamental important step in quantum information processing [Nielsen and Chuang, 2000, Paris and R eh´acek, 2004]. In general, it aims at finding the underlying density matrix which describing the given state of a physical quantum system. This task is done by utilizing the results of measurements performed on repeated state preparations [Nielsen and Chuang, 2000].
Bayesian methods have been recognized as a powerful paradigm for quantum state tomography [Blume-Kohout, 2010], that deal with uncertainty in meaningful and informative ways and are the most accurate approach with respect to the expected error (operational divergence) even with finite samples. Several studies have been conducted: for example, the papers [Buzek et al., 1998, Baier et al., 2007] performed numerical comparisons between Bayesian estimations with other methods on simulated data; algorithms for computing Bayesian estimators have been discussed in [Kravtsov et al., 2013, Ferrie, 2014, Kueng and Ferrie, 2015, Schmied, 2016, Lukens et al., 2020].
Pseudo-Bayesian method for quantum tomography, introduced in [Mai and Alquier, 2017], propose a novel approach for this problem with several attractive features. Importantly, a novel prior distribution for quantum density matrix is introduced based on spectral decomposition parameterization (inspired by the priors used for low-rank matrix estimation, e.g. [Mai and Alquier, 2015, Cottet and Alquier, 2016]). This prior can be easily used in any dimension and is found to be significantly more efficient to sample from and evaluate than the Cholesky approach in [Struchalin et al., 2016, Z yczkowski et al., 2011, Seah et al., 2015], see [Lukens et al., 2020] for more detals. By replacing the likelihood with a loss function between a proposed density matrix and experimental data, the paper [Mai and Alquier, 2017] presents two different estimators: the prob-estimator and the dens-estimator.
1

However, the reference [Mai and Alquier, 2017] propose simply to approximate these two Pseudo-Bayesian estimators by naive Metropolis-Hastings algorithms which is very slow for high-dimensional systems. Recently, a faster and more efficient sampling method has been proposed for the dens-estimator, see [Lukens et al., 2020]. However, we would like to note that the prob-estimator is shown in [Mai and Alquier, 2017] to reach the best known up-to-date rate of convergence [Butucea et al., 2015] while the theoretical guarantee for the dens-estimator is far less satisfactory. Moreover, it is also shown in simulations that the prob-estimator yields better results compare to the den-estimator.
In this paper, we present a novel efficient adaptive Metropolis-Hastings implementation for the prob-estimator. This adaptive implementation base on considering the whole density matrix as a parameter need to sample at a time. Moreover, an adaptive proposal is explored based on the 'preconditioned Crank-Nicolson' [Cotter et al., 2013] sampling procedure that can elimiate the 'curse of dimensionality', which is the case for quantum state tomography where the dimension increases exponentially. We show in the simulations that our implementation is significantly faster than the algorithm in [Mai and Alquier, 2017].
The rest of the paper is organized as follow. In Section 2, we provide the necessary background and the statistical model for the problem of quantum state tomography. In Section 3, we recall the Pseudo-Bayesian approach and the prior distribution. Section 4 presents our novel adaptive MCMC implementation for the Pseudo-Bayesian estimator. Simulations studies are presented in Section 5. Conclusions are given in Section 6.
2 Background
2.1 The quantum state tomography problem
Hereafter, we only provide the necessary background on quantum state tomography required for the paper. We would like to remind that a very nice introduction to this problem, from a statistic perspective, can be found in [Artiles et al., 2005]. Here, we have opted for the notations used in reference [Mai and Alquier, 2017].
Mathematically speaking, a two-level quantum system of n-qubits is characterized by a 2n × 2n density matrix  whose its entries is complex, i.e.   C2n×2n. For the sake of simplicity, put d = 2n, so  is a d × d matrix. This density matrix must satisfy that it is
· Hermitian:  =  (i.e. self-adjoint),
· positive semi-definite:  0,
· normalized: Trace() = 1.
In addition, physicists are especially interested in pure states and that a pure state  can be defined by rank() = 1. In practice, it often makes sense to assume that the rank of  is small [Gross et al., 2010, Gross, 2011, Butucea et al., 2015].
The goal of quantum tomography is to estimate the underlying density matrix  using measurement outcomes of many independent and identically systems prepared in the state  by the same experimental devices.
For a qubit, it is a standard procedure to measure one of the three Pauli observables x, y, z. The outcome for each will be 1 or -1, randomly (the corresponding probability is given in (1) below). As a consequence, with a n-qubits system, there are 3n possible experimental observables. The set of all possible performed observables is
{a = a1  . . .  an ; a = (a1, . . . , an)  En := {x, y, z}n},
where vector a identifies the experiment. The outcome for each fixed observable setting will be a random vector s = (s1, . . . , sn)  {-1, 1}n, thus there are 2n outcomes in total.
2

Denote Ra a random vector that is the outcome of an experiment indexed by a. From the Born's rule [Nielsen and Chuang, 2000], its probability distribution is given by

s  {-1, 1}n, pa,s := P(Ra = s) = Trace ( · Psa) ,

(1)

where Psa := eigenvalues si

Psa11  · · ·  {±1} in

 Psann and Psaii is the diagonalization

the orthogonal of ai;,ai{x,y,z}

projection associated to the ­ that is ai = 1P+ai1 - 1P-ai1.

Statistically, for each experiment a  En, the experimenter repeats m times the ex-

periment corresponding to a and thus collects m independent random copies of Ra, say

R1a, . . . , Rma . As there are 3n possible experiment settings a, we define the quantum sample size as N := m · 3n. We will refer to (Ria)i{1,...,m},aEn as D (for data). Therefore, quantum state tomography is aiming at estimating the density matrix  based on the data

D.

2.2 Popular estimation methods
Here, we briefly recall three classical major approaches have been adopted to estimate  which are: linear inversion, maximum likelihood and Bayesian inference.

Linear inversion

The first and simplest method considered in quantum information processing is the 'tomographic' method also known as linear/direct inversion [Vogel and Risken, 1989, R eh´acek et al., 2010]. It is actually the analogous of the least-square estimator in the quantum setting. This method relies on the fact that measurement outcome probabilities are linear functions of the density matrix.
More specifically, let us consider the empirical frequencies

p^a,s

=

1 m

m

1{Rai =s}.

i=1

It is noted that p^a,s is an unbiased estimator of the underlying probability pa,s in (1). Therefore, the inversion method is based on solving the linear system of equations

p^a,s = Trace (^ · Psa) , a  En, s  {-1, 1}n.

(2)

As mentioned above, the computation of ^ is quite clear and explicit formulas are classical that can be found for example in e.g. [Alquier et al., 2013]. While straightforward and providing unbiased estimate [Schwemmer et al., 2015], it tends to generate a non-physical density matrix as an output [Shang et al., 2014]: positive semi-definiteness cannot easily be satisfied and enforced.

Maximum likelihood
A popular approach in QST in recent years is the maximum likelihood estimation (MLE). MLE aims at finding the density matrix which is most likely to have produced the observed data D:
MLE = arg max L(; D)
where L(; D) is likelihood, the probability of observing the outcomes given state , as defined by some model [Hradil et al., 2004, James et al., 2001, Gonc¸alves et al., 2018]. However, it has some critical problems, detailed in [Blume-Kohout, 2010], including a huge computational cost. Moreover, it is a point estimate which does not account the level of uncertainty in the result.
Furthermore, these two methods (Linear inversion and MLE) can not take advantage of a prior knowledge where a system is in a state  for which some additional information is available. More particularly, it is noted that physicists usually focus on so-called pure states, for which rank() = 1.

3

Bayesian inference
Starting receiving attention in recent years, Bayesian QST had been shown as a promissing method in this problem [Blume-Kohout, 2010, Buzek et al., 1998, Baier et al., 2007, Lukens et al., 2020]. Through Bayes' theorem, experimental uncertainty is explicitly accounted in Bayesian estimation. More specifically, suppose a density matrix  is parameterized by (x) for some x, Bayesian inference is carried out via the posterior distribution
((x)|D)  L((x); D)(x),
where L((x); D) is the likelihood (as in MLE) and (x) is the prior distribution. Using the posterior distribution ((x)|D), the expectation value of any function of  can be inferred, e.g. the Bayesian mean estimator as (x)((x)|D)dx.
Although recognized as a powerful approach, the numerical challenge of sampling from a high-dimensional probabilty distribution prevents widespread use of Bayesian methods in the physical problem.
Other approaches
Several other methods have also recently introduced and studied. The reference [Cai et al., 2016] proposed a method based on the expansion of the density matrix  in the Pauli basic. Some rank-penalized approaches were studied in [Gu¸ta et al., 2012, Alquier et al., 2013]. A thresholding method is introduced in [Butucea et al., 2015].

3 Pseudo-Bayesian quantum state tomography

3.1 Pseudo-Bayesian estimation

Let us consider the pseudo-posterior, studied in [Mai and Alquier, 2017], defined by

~(d)  exp [-(, D)] (d),

where exp [-(, D)] is the pseudo-likelihood that acting the role of the empirical evidence

to give more weight to the density  when it fits the data well; (d) is the prior given in

Section 3.2; and  > 0 is a tunning parameter that balances between evidence from the

data and prior information.

Taking

(, D) := prob(, D) =

[Tr(Psa) - p^a,s]2 ,

aEn sRn

the "prob-estimator" in [Mai and Alquier, 2017] is defined as the mean estimator of the

pseudo-posterior :

~prob =  exp -prob(, D) (d),

(3)

this estimator also refered to, in statistical machine learning, as Gibbs estimator, PAC-

Bayesian estimator or EWA, for exponentially weighted aggregate [Catoni, 2007, Dalalyan and Tsybakov, 200

For the sake of simplicity, we use the shortened notation p := [Tr(Psa)]a,s and p^ :=

[p^a,s]a,s then

prob(, D) =

p

- p^

2 F

( · F is the Frobenius norm). Clearly, we can see that this distance measures the difference
between the probabilities and the empirical frequencies in the sample. We remind that the matrix [p^a,s]a,s is of dimension 3n × 2n.

Remark 1. This kind of pseudo-posterior is an increasingly popular approach in Bayesian statistics and machine learning, see for example [Bissiri et al., 2016, Mai, 2021c, Gru¨nwald et al., 2017, Mai, 2021a, Catoni, 2007, Alquier et al., 2015, Mai, 2021b, B´egin et al., 2016], for models with intractable likelihood or for misspecification models.

4

3.2 Prior distribution for quantum density matrix

The pior distribution employed in [Mai and Alquier, 2017] is as follow: the d × d density
matrix  can be parameterized by d non-negative real numbers yi and d complex column vectors of length d, zi. Put x = {y1, . . . , yd, z1, . . . , zd}, then the density matrix is

d
(x) =
i=1

yi zizi  y zi 2

(4)

the prior distribution for x as

d

(x) 

yi-1

e-yi

e-

1 2

zi

zi

(5)

i=1

where the weights are being treated as Gamma-distributed random variables Yi i.i.d. (, 1), and the vectors zi are standard-normal complex Gaussian distributed Zi i.i.d. CN (0, Id).
The tunning parameter  in (5) allows the user to favor low-rank or high-rank density
matrices which are corresponding to pure or mixed states, respectively. More particularly, the normalized random variables Yi/( Yj) with Yi i.i.d. (, 1) follows a Dirichlet distribution Dir() which ensures both normalization and non-negativity. An  < 1 pro-
motes sparse draws and thus purer states, while  = 1 returns a fully uniform prior on all
physically realizable states.

Remark 2. It is noted that this parameterization satisfies all physicallity conditions for the density matrix. The details can be found in [Mai and Alquier, 2017]. Moreover, this parameterization have been shown to be significantly more efficient to sample from and to evaluate than the Cholesky approach in references [Struchalin et al., 2016, Z yczkowski et al., 2011, Seah et al., 2015], see [Lukens et al., 2020] for details.

Remark 3. The theoretical guarantees for the "prob-estimator" in (3) are validated only

for 0 <   1. More specifically, the prob-estimator satisfies (up to a multiplicative

logarithmic factor) that

~prob - 0

2 F



c3nrank(0)/N

which

is

the

best

known

up-to-

date rate in the problem of quantun state estimation [Butucea et al., 2015], where c is a

numerical constant and  = m/2.

4 A novel efficient adaptive MCMC Implementation

Appropriately, the prob-estimator requires an evaluation of the integral (3) which is numerically challenging due to its sophisticated features and high dimentionality. A first attempt has been done in [Mai and Alquier, 2017] is to use a naive Metropolis-Hastings (MH) algorithm where the authors iterate between a random walk MH for Yi and an independent MH for zi. Typically, the approach is designed to obtain T samples x(1), . . . , x(T ) as a consequence the integral (3) can be approximated as

^MH



1 T

T

(x(t)).

t=1

However, as also noted in the reference [Mai and Alquier, 2017], their proposed algorithm can run into slow convergence and can be arbitrarily slow as the system dimensionality increases. For sake of self-containedness, the implementation of reference [Mai and Alquier, 2017] is given in the Appendix.
Borrowing motivation from the recent work [Lukens et al., 2020] that proposes an efficient 'preconditioned Crank-Nicolson' [Cotter et al., 2013] sampling procedure for Bayesian

5

quantum state estimation (which improve the computation of "dens-estimator" in [Mai and Alquier, 2017]
only), we introduce an efficient adaptive Metropolis-Hastings implementation for the prob-
estimator in [Mai and Alquier, 2017]. We remind that the prob-estimator shows better
performance than the dens-estimator both in theory and simulations.
Specifically, we propose to use a modification of random-walk MH by scaling the previous step before adding a random move and generating the proposal z. Following
[Cotter et al., 2013] who introduced an efficient MCMC approach elimiating the 'curse of
dimensionality', termed as 'preconditioned Crank-Nicolson', we use the proposal for zj as zj = 1 - z2zj(k) + zj, j i.i.d. CN (0, Id) where z  (0, 1) is a tunning parameter. The proposal is a scaled, by the factor 1 - z2, random walk that results in a slightly simpler acceptance probability. Unlike the independent proposal in [Mai and Alquier, 2017] (with
z = 1) where the acceptance probability can vary substantially, this kind of adaptive proposal allows one to control the acceptance rate efficiently.
The acceptance ratio min{1, A(x|x(k))} are followed from the standard form for MH [Robert and Casella, 2013]. Let p(x|x(k)) denote the proposal density, we have

A(x|x(k))

=

~((x)) ~((x(k)))

p(x(k)|x) p(x|x(k))

.

The details of the adaptive MH is given in Algorithm 1.

Algorithm 1 Adaptive MH for Pseudo-Bayesian Quantum state estimation
Input: The tunning parameters y, z  (0, 1). Parameters: Positive real numbers   (0, 1], T .
Onput: The density matrix ^. Initialize: x(0) drawn from the prior (5) , ^ = 0d×d. for k = 1 to T do
Simulate a new point x = {y1 , ..., yd ; z1 , ..., zd }, according to

yj = yj(j)eyj , zj = 1 - z2zj(k) + zj ,

j  {1, ..., d},

where j i.i.d. U inf orm(-0.5, 0.5) and independently j i.i.d. CN (0, Id). Set x(k+1) = x with probability min 1, A(x, x(k)) , where

d

log A(x, x(k)) = log LD(x) - log LD(x(k)) +

 log yj - yj -  log yj(k) + yj(k) .

j=1

Otherwise set x(k+1) = x(k). ^  ^ + (x(k))/T
end for

5 Numerical studies

Simulations setups and details

To access the performance of our new proposed algorithm, a series of experiments were conducted with simulated tomographic data. More particularly, we consider the following settiing for choosing the true density matrix, with n = 2, 3, 4, (d = 4, 8, 16):

· Setting 1: we consider the ideal entangled state which characterized by a rank-2

density matrix that

rank-2

=

1 2

1

1

+

1 2

22

6

with 1 = u/ u and u = (u1, . . . , ud/2, 0, . . . , 0), u1 = . . . = ud/2 = 1; 2 = v/ v and v = (0, . . . , 0, vd/2+1, . . . , vd), vd/2 = . . . = vd = 1.
· Setting 2: a maximal mixed state (rank-d) that is

mixed =

d

1 d

i

i,

i=1

with i are normalized vectors and independently simulated from CN (0, Id).

The experiments are done following Section 2 for m = 1000. The prob-estimator is employed with  = m/2 and a prior with  = 1. We compare our adaptive MH implementation, denoted by "a-MH", against the (random-walk) in [Mai and Alquier, 2017], denoted by "r-MH". We run 100 independent samplers for each experiment, and compute the mean of the square error (MSE),

MSE :=

^ - 

2 F

/d2

for each method, together with their standard deviations. We also measure the mean absolute error of eigen values (MAEE) by

MAEE

:=

1 d

d

|i(^) - i()|,

i=1

where i(A) are the eigen values of the matrix A.

Significantly speeding up

r-MH a-MH

time (s) in log-scale -2 0 2 4 6 8 10

n=2

n=4

n=6

n=7

Figure 1: Plot to compare the running times (s) in log-scale for 10 steps of two algorithms in the setup of Setting 1, for the qubits n = 2, 4, 6, 7 (d = 4, 16, 64, 128).
From Figure 1, it is clear to see that our adaptive MH implementation is greatly faster than the previous implementation from [Mai and Alquier, 2017] by at least two orders of magnitude as the number of qubits increase. More specifically, the data are simulated as in Setting 1 for n = 2, 4, 6, 7 for which the dimensions of the density matrix are d = 4, 16, 64, 128 and of the empirical frequencies matrices [p^a,s] are 9×4, 81×16, 729× 64, 2187 × 128. We note that this improvement is quite significant for practical quantum tomography where computational time is a precious resource.

7

Tunning parameters via acceptance rate
The tunning parameters y, z are chosen such that the acceptance rate of Algorithm 1 is approximating 0.3, which follows the optimum acceptence probability for random-walk Metropolis-Hastings under various assumptions [Gelman et al., 1997]. For example, as in our experiments, for n = 2 qubits: y = 0.33, z = 0.2; for n = 3 qubits: y = 0.03, z = 0.03 and for n = 4 qubits: y = 0.03, z = 0.02 (all are run with  = 1,  = m/2). We note that as the number of qubits n increase, these tunning parameters tend to be smaller and smaller to asure that the 0.3 acceptance rate is obtained.
As an illustration, we conduct some simulations with n = 4 qubits in Setting 2. It can be seen from Figure 2 that the acceptance rate around 0.3 would be optimal, where as high acceptance rate like 0.7 could make the algorithm be trapped at local points, and very small acceptance rate as 0.1 could make the algorithm convergernce slower.

1e-03

MSE 6e-04

2e-04

0.1

0.3

0.5

0.7

acceptance rate

Figure 2: Boxplots to examine the effect of the acceptance rate to MSE. The simulations are run within Setting 2 for n = 4.

Similar accuracy with less variation

r-MH a-MH

MAEE 0.00 0.05 0.10 0.15 0.20 0.25

MSE 0.00 0.01 0.02 0.03 0.04 0.05 0.06

n=2

n=3

n=4

n=2

n=3

n=4

Figure 3: MSE
Results from Figure 3 return that both algorithm share similar accuracy in term of both considered errors. However, it shows a clear improvement that our proposed adaptive algorithm yields much stable results compare to the naive MH approach as expected.

8

6 Discussion and conclusion
We have introduced an efficient sampling algorithm for Pseudo-Bayesian quantum tomography, especially for the prob-estimator. Our approach is an adaptive Metropolis-Hasting implementation which shows a clear improvement in convergence and computational time comparing with a naive MH implementation. We would like to mention that such an improvement is significant important for practical quantum state tomography.
Last but not least, faster algorithms based on optimization, such as Variation inference, for Bayesian quantum tomography would be an interesting research problem. However, it should be noted that the analysis of the uncertainty quantification when using Variational inference is not known, while this matter is an important aspect in the problem of quantum state estimation.
Availability of data and code
The R codes and data used in the numerical experiments are available at: https://github.com/tienmt/bqst .
Acknowledgments
This research of T.T.M was supported by the European Research Council grant no. 742158.
Financial disclosure
None reported.
Conflict of interest
The authors declare no potential conflict of interests.
9

A Naive Metropolis-Hastings

Algorithm 2 MH implementation from [Mai and Alquier, 2017] For t from 1 to T , we iteratively update through the following steps:

updating for Yis: for i from 1 to d,

Sample Y~i  h(y|Yi(t-1)) where h is a proposal distribution given explicitely below.

Calculate ~i = Y~i/(

d i=1

Y~i).

Set

Yi(t) =

Y~i Yi(t-1)

with probability min 1, R(Y~ , Y (t-1)) , otherwise

where R(Y~ , Y (t-1)) is the acceptance ratio given below.

Put i(t) = Yi(t)/(

d j=1

Yj(t)),

i

=

1,

.

.

.

,

d.

updaStianmgpfleorV~iVfirso:mfotrhie

from 1 to d, uniform distribution

on

the

unit

sphere.

Set

Vi(t) =

V~i Vi(t-1)

with probability min{1, A(V (t-1), V~ )}, otherwise,

where A(V (t-1), V~ ) is the acceptance ratio given below.

In details, h(·|Yi(t-1)) is the probability distribution of U = Yi(t-1) exp(y) where y  U (-0.5, 0.5). Following [Robert and Casella, 2013] the acceptance ratios are then given by:

d

d

log(R(Y~ , Y (t-1))) = 

~iViVi, D - 

i(t-1)ViVi, D

i=1

i=1

d

d

+ (( - 1) log(Y~i) - Y~i) - (( - 1) log(Yi(t-1)) - Yi(t-1))

i=1

i=1

d

d

+

Y~i -

Yi(t-1)

i=1

i=1

where (·, D) stands for prob(·, D).

10

References
Alquier, P., Butucea, C., Hebiri, M., Meziani, K., and Morimae, T. (2013). Rank-penalized estimation of a quantum system. Physical Review A, 88(3):032113.
Alquier, P., Ridgway, J., and Chopin, N. (2015). On the properties of variational approximations of gibbs posteriors. arXiv preprint arXiv:1506.04091.
Artiles, L., Gill, R., and Gu¸ta, M. (2005). An invitation to quantum tomography. Journal of the Royal Statistical Society - series B, 67:109­134.
Baier, T., Petz, D., Hangos, K. M., and Magyar, A. (2007). Comparison of some methods of quantum state estimation. In Quantum probability and infinite dimensional analysis, volume 20 of QP­PQ: Quantum Probab. White Noise Anal., pages 64­78. World Sci. Publ., Hackensack, NJ.
B´egin, L., Germain, P., Laviolette, F., and Roy, J.-F. (2016). Pac-bayesian bounds based on the r´enyi divergence. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 435­444.
Bissiri, P. G., Holmes, C. C., and Walker, S. G. (2016). A general framework for updating belief distributions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), pages n/a­n/a.
Blume-Kohout, R. (2010). Optimal, reliable estimation of quantum states. New Journal of Physics, 12(4):043034.
Butucea, C., Gu¸ta, M., and Kypraios, T. (2015). Spectral thresholding quantum tomography for low rank states. New Journal of Physics, 17(11):113050.
Buzek, V., Derka, R., Adam, G., and Knight, P. (1998). Reconstruction of quantum states of spin systems: From quantum bayesian inference to quantum tomography. Annals of Physics, 266(2):454­496.
Cai, T., Kim, D., Wang, Y., Yuan, M., and Zhou, H. H. (2016). Optimal large-scale quantum state tomography with pauli measurements. Ann. Statist., 44(2):682­712.
Catoni, O. (2007). PAC-Bayesian supervised classification: the thermodynamics of statistical learning. IMS Lecture Notes--Monograph Series, 56. Institute of Mathematical Statistics, Beachwood, OH.
Cotter, S. L., Roberts, G. O., Stuart, A. M., and White, D. (2013). Mcmc methods for functions: modifying old algorithms to make them faster. Statistical Science, pages 424­446.
Cottet, V. and Alquier, P. (2016). 1-bit matrix completion: Pac-bayesian analysis of a variational approximation. arXiv preprint arXiv:1604.04191.
Dalalyan, A. and Tsybakov, A. B. (2008). Aggregation by exponential weighting, sharp pac-bayesian bounds and sparsity. Machine Learning, 72(1-2):39­61.
Ferrie, C. (2014). Quantum model averaging. New Journal of Physics, 16(9):093035.
Gelman, A., Gilks, W. R., and Roberts, G. O. (1997). Weak convergence and optimal scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110­120.
Gonc¸alves, D., Azevedo, C., Lavor, C., and Gomes-Ruggiero, M. (2018). Bayesian inference for quantum state tomography. Journal of Applied Statistics, 45(10):1846­1871.
11

Gross, D. (2011). Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on Information Theory, 57(3):1548­1566.
Gross, D., Liu, Y.-K., Flammia, S. T., Becker, S., and Eisert, J. (2010). Quantum state tomography via compressed sensing. Physical review letters, 105(15):150401.
Gru¨nwald, P., Van Ommen, T., et al. (2017). Inconsistency of bayesian inference for misspecified linear models, and a proposal for repairing it. Bayesian Analysis, 12(4):1069­ 1103.
Gu¸ta, M., Kypraios, T., and Dryden, I. (2012). Rank-based model selection for multiple ions quantum tomography. New Journal of Physics, 14(10):105002.
Hradil, Z., R eh´acek, J., Fiura´sek, J., and Jezek, M. (2004). 3 maximum-likelihood methodsin quantum mechanics. In Quantum state estimation, pages 59­112. Springer.
James, D. F. V., Kwiat, P. G., Munro, W. J., and White, A. G. (2001). Measurement of qubits. Phys. Rev. A, 64:052312.
Kravtsov, K., Straupe, S., Radchenko, I., Houlsby, N., Husz´ar, F., and Kulik, S. (2013). Experimental adaptive bayesian tomography. Physical Review A, 87(6):062122.
Kueng, R. and Ferrie, C. (2015). Near-optimal quantum tomography: estimators and bounds. New Journal of Physics, 17(12):123013.
Lukens, J. M., Law, K. J., Jasra, A., and Lougovski, P. (2020). A practical and efficient approach for bayesian quantum state estimation. New Journal of Physics, 22(6):063038.
Mai, T. T. (2021a). Bayesian matrix completion with a spectral scaled student prior: theoretical guarantee and efficient sampling. arXiv preprint arXiv:2104.08191.
Mai, T. T. (2021b). Efficient bayesian reduced rank regression using langevin monte carlo approach. arXiv preprint arXiv:2102.07579.
Mai, T. T. (2021c). Numerical comparisons between bayesian and frequentist low-rank matrix completion: estimation accuracy and uncertainty quantification. arXiv preprint arXiv:2103.11749.
Mai, T. T. and Alquier, P. (2015). A bayesian approach for noisy matrix completion: Optimal rate under general sampling distribution. Electronic Journal of Statistics, vol.9:823­841.
Mai, T. T. and Alquier, P. (2017). Pseudo-bayesian quantum tomography with rankadaptation. Journal of Statistical Planning and Inference, 184:62­76.
Nielsen, M. A. and Chuang, I. L. (2000). Quantum Computation and Quantum Information. Cambridge University Press.
Paris, M. and R eh´acek, J., editors (2004). Quantum state estimation, volume 649 of Lecture Notes in Physics. Springer-Verlag, Berlin.
R eh´acek, J., Mogilevtsev, D., and Hradil, Z. (2010). Operational tomography: fitting of data patterns. Physical review letters, 105(1):010402.
Robert, C. and Casella, G. (2013). Monte Carlo statistical methods. Springer Science & Business Media.
Schmied, R. (2016). Quantum state tomography of a single qubit: comparison of methods. Journal of Modern Optics, 1142018:1­15.
12

Schwemmer, C., Knips, L., Richart, D., Weinfurter, H., Moroder, T., Kleinmann, M., and Gu¨hne, O. (2015). Systematic errors in current quantum state tomography tools. Phys. Rev. Lett., 114:080403.
Seah, Y.-L., Shang, J., Ng, H. K., Nott, D. J., and Englert, B.-G. (2015). Monte carlo sampling from the quantum state space. ii. New Journal of Physics, 17(4):043018.
Shang, J., Ng, H. K., and Englert, B.-G. (2014). Quantum state tomography: Mean squared error matters, bias does not. arXiv preprint arXiv:1405.5350.
Struchalin, G., Pogorelov, I., Straupe, S., Kravtsov, K., Radchenko, I., and Kulik, S. (2016). Experimental adaptive quantum tomography of two-qubit states. Physical Review A, 93(1):012103.
Vogel, K. and Risken, H. (1989). Determination of quasiprobability distributions in terms of probability distributions for the rotated quadrature phase. Physical Review A, 40(5):2847.
Z yczkowski, K., Penson, K., Nechita, I., and Collins, B. (2011). Generating random density matrices. Journal of Mathematical Physics, 52(6):062201.
13

