Reinforce Security: A Model-Free Approach Towards Secure Wiretap Coding

Rick Fritschek, Rafael F. Schaefer, and Gerhard Wunder

 Heisenberg Communications and Information Theory Group Freie Universität Berlin
{rick.fritschek, g.wunder}@fu-berlin.de

 Lehrstuhl für Nachrichtentechnik/Kryptographie und Sicherheit Universität Siegen
rafael.schaefer@uni-siegen.de

arXiv:2106.00343v1 [cs.IT] 1 Jun 2021

Abstract--The use of deep learning-based techniques for approximating secure encoding functions has attracted considerable interest in wireless communications due to impressive results obtained for general coding and decoding tasks for wireless communication systems. Of particular importance is the development of model-free techniques that work without knowledge about the underlying channel. Such techniques utilize for example generative adversarial networks to estimate and model the conditional channel distribution, mutual information estimation as a reward function, or reinforcement learning. In this paper, the approach of reinforcement learning is studied and, in particular, the policy gradient method for a model-free approach of neural network-based secure encoding is investigated. Previously developed techniques for enforcing a certain coset structure on the encoding process can be combined with recent reinforcement learning approaches. This new approach is evaluated by extensive simulations, and it is demonstrated that the resulting decoding performance of an eavesdropper is capped at a certain error level.
I. INTRODUCTION
A recent breakthrough in wireless communication is the deep learning-based approximation of encoding and decoding functions. These deep learning approaches are based on neural network (NN) representations of these functions, where the weights are optimized to yield encoder-decoder pairs for reliable communication over noisy channels. In particular, one of the first approaches looked at end-to-end learning of these communication systems, by utilizing the so-called autoencoder approach [1]. This approach demonstrated that the resulting NN-based encoder and decoder can perform close to classical baseline techniques [2]. These approaches usually utilize an optimization over a minimum squared error term or a crossentropy loss term via variants of gradient descent. There, the loss function is linked to the decoder, and it is not possible to train the encoder without it. Another line of work optimized the encoder without a corresponding decoder by optimizing a mutual information approximation over samples of the channel input and output [3], [4]. Furthermore, a series of recent works has adapted these NN-based encoder-decoder pairs for reliable and secure communication, i.e., to learn secure encoding functions by introducing a secrecy constraint into the optimization. In wireless communication, and in particular information theory, secrecy means to bound the information
This work was supported by the German Research Foundation (DFG) under Grants FR 4209/1-1 and SCHA 1944/7-1.

leakage, i.e., information about a confidential message that is leaked to unintended receivers (eavesdroppers). In general, it is hard to compute or even approximate the leakage if one has only access to samples, as the the leakage is defined by mutual information. This makes it hard to straightforwardly optimize an NN encoder-decoder pair with a secrecy constraint. Recent examples that try to tackle this problem are: In [5] the leakage is approximated by tracking the NN, which has the drawback that it cannot use stochastic gradient descent. Another example is [6], where NNs were utilized to learn appropriate precoding for a MIMO Gaussian wiretap scenario. In [7], a secrecy constraint was introduced by altering the one-hot representation of the input of a structure enforcing decoder, where the resulting secrecy enabling loss function is based on the standard cross-entropy loss. This cross-entropy loss will impose a clustering in the transmit constellations and, accordingly, will imitate the classical co-set coding approach for security.
A third branch within this recent deep learning-assisted wireless communication field is to make these methods channel model independent. Some of the recent works include the use of i) Generative adversarial networks (GANs): GANs were introduced in [8] and are composed of a generative NN and a discriminative NN. The generative NN gets a noise input and has the goal to generate a certain wanted distribution. The discriminator, on the other hand, has as two inputs, samples from the generated distribution and samples from the real distribution, with the goal to distinguish between fake and real samples. Both NNs are now alternatingly optimized, and the resulting generative NN can be used as an approximation of the real distribution. In the context of wireless communication, one can approximate the channel distribution by samples and use the generator as a piece within the NN encoderdecoder chain [9], [10]. This was recently utilized to enable a form of secure communication in [11]. ii) Mutual information estimators: A recent breakthrough has shown that mutual information can be approximated through sampling of the random variables with the help of NNs [12]. This was utilized in [3] to estimate the mutual information between channel input and output samples and use this as a metric to train the NN encoder to maximize mutual information. This approach has the advantage that it tackles the communication problem from the information-theoretic foundations. However, these

mutual information approximations are lower bounds and cannot be used as approximations for the leakage as an upper bound would be needed. A possible workaround for certain channel is given by [13] which shows how a conditional mutual information can be estimated. iii) Reinforcement learning (RL): The third line of work in this branch is to utilize policy gradient methods. Considering the messages and codewords of the communication system as states and actions, and integrating the channel and decoding function into the reward function evaluation, one can formulate a corresponding policy gradient problem where only the reward function evaluations are used, not its derivatives. The idea to utilize RL to learn optimized NN encoder-decoder pairs was introduced in [14], and subsequently extended to noisy feedback links in [15]. The disadvantage of using model-free reinforcement learning is that the approach is necessarily less effective than utilizing more structure, i.e, gradients of the channel function [16]. However, its generality and the ease of implementation makes it an option worth to be further explored.
Our contribution is now that we combine the previously mentioned RL-based learning approach that uses the policy gradient theorem with the secure encoding approach that uses the altered one-hot input distribution for model-aware training. The combination of them is particularly promising, because the one-hot security approach utilizes a cross-entropy based metric. This makes it possible to construct a novel per sample loss, which conserves the structure enforcing properties of the secure encoding approach. With this, we show how to build a secrecy enabling per sample loss for encoding and demonstrate that the resulting method can learn to cluster the codewords into co-sets and therefore enable secrecy with appropriate encoding without model knowledge.
Notation: We stick to the convention of upper case random variables X and lower case realizations x, i.e., X  p(x), where p(x) is the probability mass or density function of X. Moreover, p(x) is the probability mass or density function of the random vector X. We also use |X | to denote the cardinality of a set X . The expectation is denoted by E[·].

II. WIRETAP CHANNEL

In this paper, we consider the communication scenario

with a transmitter Alice, a legitimate receiver Bob, and an

eavesdropper Eve as shown in Fig. 1. Alice wants to transmit a confidential message m  M = {1, 2, . . . , 2nR} with rate R by

using an encoding function f that encodes the message m into a codeword x(m)  Cn and transmit it over the noisy channel

to Bob who needs to decode the message. At the same time,

Eve needs to be kept ignorant about the message. This model

is called the wiretap channel and provides the basic scenario

to investigate at what rate messages can be reliably sent to

a legitimate receiver (Bob) while providing secrecy against a

wiretapper (Eve).

To this end, for every message m  M, we assume

an

average

power

constraint

1 n

n i=1

|xi(m)|2



P

on

the

corresponding codewords x(m). The channel from Alice to

Bob is given by the transition probability density pY|X(y|x)

Feedback

Noise
M Encoder f xn
Alice

yn Bob

M^

Channel B

zn
Channel E

Eve M~

constraint for security

Fig. 1. The wiretap channel. The encoder f of Alice is trained to enable secure communication to Bob. Alice's signal will be perturbed, to enable exploration for the policy gradient method. Moreover, a security constraint is included by invoking an exemplary eavesdropper Eve. After that the channel to Bob estimates a per sample loss, which is designed to allow secure encoding and will be fed back to the encoder. The encoder can now use the policy gradient theorem to train on the security enabling per sample loss, without channel knowledge.

for input and output sequences x and y. If the channel is

further memoryless, one has pY|X(y|x) =

n i=1

p(yi|xi),

i.e.,

the output at time instant i depends only on the corresponding

input at time instant i and is independent of the previous inputs.

The channel from Alice to Eve can be defined accordingly. The

receiver Bob uses a decoder g to estimate a message g(y) = m^

which should recover the original message. Moreover, the block

error rate Pe is defined as the average probability of error over

all messages

Pe

=

1 |M|

Pr(M^ = m|M = m).

(1)

mM

Without any secrecy constraint, the maximal transmission rate R such that the error Pe vanishes for sufficiently large n is called the capacity C of the channel and is known to be C = maxp(x) I(X; Y ) for discrete memoryless channels, cf. for example [17].
Since Eve is eavesdropping upon the legitimate communication, we impose a secrecy constraint to keep the transmitted message confidential. Usually, information theoretic principles are invoked and the information leakage to the eavesdropper is required to vanish. There are multiple definitions including weak [18] or strong secrecy [19]. For strong secrecy, this criterion is defined as

lim I(M ; Z) = 0

(2)

n

with Z = (Z1, Z2, ..., Zn) the channel output at Eve, cf. for example [20].
Now, the secrecy capacity characterizes the maximum transmission rate R at which Bob can reliably decode the transmitted message while, simultaneously, Eve is kept in the dark, i.e., the secrecy criterion (2) is satisfied. As we pointed out above, it is unfortunately, still a major challenge to optimize NNs according to such a constraint, as it is challenging to estimate an upper bound for the mutual information from samples. Instead, as in [7], we opt for a secrecy criterion based on the cross-entropy metric. This goes well with the overall approach of using NNs to approximate optimal encoder-decoder

pairs, as well as the reinforcement learning-based technique of policy gradient, because it can be implemented on a per sample basis.

III. REINFORCEMENT LEARNING FOR WIRELESS
COMMUNICATIONS
The goal of reinforcement learning is to optimize a reward r(si, ai) based on states si of an environment and actions ai taken in this environment. The actions ai are done by a policy (ai|si), parameterized by , based on the state si. One can then write down the probability of the whole trajectory  of actions and states as p(s1, a1, . . .) = p( ) = p(s1) i (ai|si)p(si+1|si, ai) and state the optimization problem as

max J()


=

max


Ep()[r( )].

Since we want to maximize J(), we can use gradient ascent. The gradient can be written as

J () = Ep

r(si, ai) log (ai|si) ,

i

where we see that we do not need the derivative of p(si+1|si, ai), but only the derivative of the policy (ai|si) is needed. A complete derivation of this result can be found for example in [21]. The optimization step is therefore free of any model knowledge. This is a very convenient form because the gradient now involves only the reward function evaluations and not its derivatives. Note that this only works for stochastic policies, otherwise system model knowledge is required.
For the particular case of wireless communications, one can identify messages as states, and the sent codewords as actions. In standard wireless encoding situations, one has deterministic encoders and therefore a deterministic policy. However, [14] showed that one can add noise on the codeword x, arriving at a perturbed version xp = x + w, where w can be zero mean Gaussian noise. The policy therefore takes in a message m, and outputs a perturbed codeword xp. Denoting the decoder as g, which has a softmax output, the channel function as h, and assuming one-hot input, one can define a per sample reward as ri := log g(h(xp,i)), which gives the cross-entropy over the sent messages with the estimated messages. One can therefore optimize the encoder, via , without having access to the derivatives of the channel or the decoder. However, note that the decoder is an integral part of the per sample loss, which is why encoder and decoder need to be trained alternatingly.

IV. ENCODING-DECODING PROCEDURE AND IMPLEMENTATION

The encoder of Alice is modelled as a NN with weights

, one fully-connected hidden layer with an elu activation

function, and a linear output layer. The encoder gets one-hot

encoded

messages,

i.e.,

binary

vectors

moh



|M|
F2

of

the

form (0, ..., 0, 1, 0, ..., 0) which have a one at the i-th position,

representing the i-th message of M = {1, ..., |M|}. The output

of the network is then normalized to have unit power and is

shaped from 2n real values to n complex values or codewords x(m), which are sent over the legitimate channel.
The decoder g of Bob, which receives the noisy channel output y, is also modelled as a NN with weights , with one
fully-connected hidden layer with elu activation. Moreover, it
has a softmax output layer, which gives an estimate of the probability distribution of the sent message. Let   R|M| be
the output of the last dense layer in the decoder network. The softmax function takes  and returns a vector of probabilities for the message set, i.e., p  (0, 1)|M|, where the entries pm are calculated by

pm = f ()m :=

exp(m) i exp(i

)

.

The decoder then declares the estimated message to be

m^ = arg maxm pm. Furthermore, it outputs the estimated probabilities pm of the received message index and feeds it into a cross-entropy function together with the true index m

H(M, M^ ) = -

p(m) log pdecoder(m)

mM

= -Ep(m)[log pdecoder(m)],

which is estimated by averaging over the sample size k, which yields the cross-entropy cost function to optimize the decoder weights 

L()

=

-

1 k

k

log pmi ,

(3)

i=1

where mi represents the index of the message of the i-th

sample. The per sample loss is therefore defined as

li = - log pmi .

(4)

In Section IV-B, we will present a novel re-formulation of this per sample loss, which will enable the learning of the security enforcing structure.

A. Policy Gradient Method for Wireless Communications
Since the encoder is deterministic conditioned on a specific message m, one needs to introduce a perturbation on the
codeword. This is usually done by an additive Gaussian noise w  N (0, I2 ). Moreover, the codeword gets scaled such that the perturbed codeword xp still obeys the power normalization. Therefore, we have that xp = 1 - 2 x + w. With this definition, the policy (xp|m) is given as (see also [14])

 (xp,i |mi )

=

1 (2 )n

exp

-

xp,i -

1 - 2 x 2

(5)

where xp,i is the evaluation of the function xp for the i-th sample of the message mi. We therefore have

log

 (xp,i |mi )

=

-

1 2

xp,i -

1 - 2 x + c.

(6)

For the training of the encoder weights , one can now feed

J ()



1 k

k

li log (xp,i|mi)

(7)

i

to an optimizer like Nadam [22] and train the NN.

B. Enforcing Structure and Security Constraints on the Encoder

As argued above in Section II, due to the major challenges of

incorporating a secrecy criterion based on information theoretic

metrics, we opt for a secrecy constraint based on the cross-

entropy metric. We therefore take a similar approach as in [7].

A cross-entropy based metric is also dependent on a decoder,

which is why we need to introduce a second decoder, which

enforces a particular structure. From now on we refer to this

as the structure enforcing (SE) decoder. This decoder needs

to share the noise parameter with Eve, to apply the secrecy

methods from [7] to our case. With this decoder, we can enforce

a co-set-like structure on the resulting constellation. There,

the data-carrying messages label the co-set and the particular

codeword inside the co-set/cluster are chosen at random. This

technique therefore mimics classical co-set coding methods,

which go back to the seminal work of Wyner [18]. We further

refer the reader to [23, Appendix A] for further discussions on

how co-sets can enable secrecy. Intuitively, the eavesdropper

can only distinguish between clusters of codewords, but not

between the codewords inside each cluster. The legitimate

receiver Bob however, has a better channel and can use his

advantage to also distinguish between the codewords inside

the cluster. Our objective is therefore to produce a clustered

constellation from the cross-entropy loss. This constellation

can then be used for secure encoding afterwards. To enable

this cluster structure in our NN encoding, we introduce a cross-

entropy loss constraint for our SE decoder which is fed with

a modified input distribution. This approach follows previous

work in [7], and for convenience and completeness we will

repeat the basic construction here.

The goal of the modification is to obtain clusters of

codewords (calculated with the k-means method) that have the

same input probability. Normally, due to the one-hot encoding

approach, a certain symbol has probability one if it was sent

in the sample in the batch. Consider for example the training

vector batch m = (1, 2, 3, 4), resulting in the one-hot data

matrix

1 0 0 0

S

=

0 0

1 0

0 1

0 0

0001

where the rows are the samples of the batch and the columns

indicate the symbol. We now modify the true data matrix towards an equalized matrix S¯:

0.5 0.5 0 0 

S¯

=

SE

=

0.5

 

0

0.5 0

0 0.5

0 0.5

0 0 0.5 0.5

where, for example, the first sample has an equal probability to be symbol 1 and symbol 2. The matrix E can be calculated with the k-means algorithm in conjunction with Algorithm 1 from [7]. The SE decoder's cross-entropy can now be written as

H(pdata(M )), pSE(M )) = -

s¯m log s~m,

mM

where the vectors ~s and ¯s can be interpreted as the decoded distribution and as the equalized input symbol distribution, respectively, as both are normalized to one. Averaging over k samples yields the loss

LSE

=

-

1 k

k

s¯m log s~m,i,

(8)

i=1 mM

which has a different form than (3). This is due to the fact that in (3), we used one-hot encoded messages, which pick the corresponding log pm term from the sum, and set the others to zero. However, in our secure encoding scenario, we have the equalized vector s¯, which picks all log s~m terms, that are uniformly distributed with p > 0 in their cluster. Therefore, strictly speaking, the sum in (8) is now only over clusters/cosets.
Now, in this paper we want to enable secrecy by applying RL via the policy gradient method. This means that we need a per sample loss. Interestingly, (8) is in a form such that we can extract a per samples loss as follows:

lSE,i := -

s¯m log s~m,i.

mM

This loss can be seen as a per sample secrecy constraint which takes into account the whole cluster around a specific sample. Therefore, in the exploration step, the whole cluster influences the decision process. Together with the previous encoder per sample loss function, we can define a new security enabled per sample loss function

lsec,i = (1 - )li - lSE,i,

(9)

where   [0, 1] controls the influence of the secrecy structure enforcing constraint. Therefore, the parameter  controls the trade-off between security and communication rate on the legitimate channel. We can now re-formulate the RL-objective, i.e., the gradient of J in (7), such that it is a security-enabled gradient update as

 Jsec ()



1 k

k

lsec,i log (xp,i|mi).

(10)

i

The new security structure enforcing training algorithm, which uses the policy gradient method, is shown in Algorithm 1.

C. Training of the Encoder-Decoder Network
1) Encoder-decoder training without secrecy structure: To train the encoder, we use a signal-to-noise ratio (SNR) per bit of Eb/N0 = 7 dB. This specifies the noise variance of the direct intended channel in relation to our normalized codeword power. When we add the perturbation, the codewords are scaled such that they are still normalized. Moreover, we assume an SNR per bit of Eb/N0 = 6 dB for Eves channel, which corresponds approximately to an Eb/N0 = 12 dB additional noise factor on top of Bob's channel. The training of the encoder-decoder pair of Alice and Bob, before adjusting for security, is done similar to [14]. This means we use an alternating optimization, where we start with optimizing the decoder weights  of Bob with

Algorithm 1: This algorithm trains the decoder and

encoder alternatingly. The decoder is trained with a

standard procedure, while the encoder is trained via a

policy gradient method. Moreover, we enable secure

encoder training with a modified per sample loss.

Require: SE Decoder, Equalization operator E  R|M|×|M|
while stopping criterion not met do Train Decoder:

Require: encoder with randomly initialized
weights  Initialize: sample batch from source moh X(Moh)  encoder Y  channel P (Y |X) M^ (Moh)  decoder with weights  L(M^ , Moh)  cross entropy loss   Nadam optimzer on L()

Train Encoder: Require: encoder with weights 

Require: structure enforcing (SE) decoder

Require: decoder with learned weights 

Initialize: sample batch with size k from

one-hot source moh

Initialize: Equalization operator E

X(Moh)  encoder with weights 

Xp  policy: 1 - pX + W

Y  channel P (Y |Xp)

Z  channel P (Z|Xp)

M^ (Moh)  decoder with weights  M^ SE(Moh)  structure enforcing decoder

li(m^ , moh)  per sample loss

Meq  EMoh

li,SE(m^ SE, meq)  new per sample loss

lsec,i  (1 - )li + lSE,i 

log

 (xp,i |mi )



-

||xp,i -x p2

1-p ||2

J ()



1 k

i lsec,i log (xp,i|mi)

  Nadam optimzer on J()

end

randomly initialized encoder weights  of Alice and the usual cross-entropy loss metric, together with the Nadam optimizer. This does not require channel knowledge or policy gradient methods, since we only need the gradient of the decoder. After that we train for the encoder weights , with the policy gradient method, to optimize  with Nadam, providing the gradient in (7) for the update, without secrecy constraint. This is done iterative for 2 epochs with 400 steps, where each step draws a new batch of messages.
2) Encoder-decoder training with secrecy structure: For the security constraint, we need the SE decoder, which is implemented with a standard NN decoder with a hidden layer with elu activation and an output layer with softmax activation. The decoder will be pre-trained with a batch size of 200 and

400 iterations per epoch, for 4 epochs with a learning rate of 0.005 with the Nadam optimizer. Afterwards, we initialize the matrix E and train the Alice-Bob encoder-decoder pair, which will be optimized for secure encoding with Alg. 1 and the new per sample loss (10) over 2 epochs, a batch size of 500, with 400 iterations, a learning rate of 0.005 and  = 0.7. The simulation code is available at [24], implemented with TensorFlow 2 [25].
V. EVALUATION
For the evaluation we use NN decoders with the standard cross-entropy loss function. Moreover, Bob and Eve use the same decoder structure to have a fair comparison. Both decoders use one hidden layer with elu activation which yields better results compared to Relu in our simulations. After the alternating training of Alice and Bob, and the training of the SE decoder, and the secrecy-enabled alternating training of Alice and Bob, we have an encoding system for secure communication. We now train the decoder Bob one more time and train another decoder as a representative for Eve. Both are trained with the same parameters for 400 iterations, and a batch size of 200. We test the system once before, and once after training for secure encoding, with 106 samples for each Eb/N0 data point. We model Eve's channel with an additive fixed noise of Eb/N0 = 12 dB which is additionally to Bob's noise and helps to unify both results in one figure. Note that after secure training of the encoder, we utilize the resulting co-set structure and use a message set with 4 messages, then we randomly choose the satellite codeword inside the cluster, mapping the 4 symbols code to a 16 symbols code. The evaluation results in Fig. 2 show that Bob's and Eve's performance is similar good with a relatively low symbol error rate per batch. In these results, the error rate for Eve's signal is worse than Bob's due to higher baseline noise of Eve's received signal. After we have trained the system for secure encoding, both error rates are elevated. Bob's error rate is higher due to the trade-off between security and communication rate, but still declines with a higher Eb/N0 ratio. On the other hand, Eve's error rate is elevated but stays flat above a certain threshold, even for high Eb/N0 values. This shows that security can be achieved in this scenario.
Furthermore, we trained the NN models for two signal transmission dimensions, to visualize the constellations. We note that one could use t-SNE on the higher dimensional signals from above, however, this output would be highly dependent on the used parameters. The resulting constellations before and after our secure encoding process can be seen in Fig. 3. We note that the training iterations were lowered and we used early stopping after 260 iterations in the second epoch. Moreover, we tuned the security trade-off parameter to  = 0.5. All other parameters are the same as in the 32 dimensional case. Here, it can be seen that the system forms clusters and that the structure enforcing method works as intended.

Batch Symbol Error Rate

100

Bob

10 1

Eve Secure encoding Bob

Secure encoding Eve

10 2

10 3

10 4

10 50.0

2.5

5.0

7.5EbN100.0[dB]12.5 15.0 17.5 20.0

Fig. 2. Evaluation of the proposed method for a 32-dimensional codeword constellation. Bob and Eve show the error rate for transmission of the codewords before secure encoding and, accordingly, Secure encoding Bob and Secure encoding Eve refer to after secure encoding.

2.0

1.5

1.0

x2

0.5 0.0

0.5

1.0

1.5

2.0 2.0 1.5 1.0 0.5 x0.10 0.5 1.0 1.5 2.0

(a) Standard constellation

2.0

1.5

1.0

x2

0.5 0.0

0.5

1.0

1.5

2.0 2.0 1.5 1.0 0.5 x0.10 0.5 1.0 1.5 2.0

(b) Secure constellation

Fig. 3. Constellations before and after secure encoding for two dimensions and 16 constellation points.

VI. CONCLUSIONS
We have shown that a recently proposed model-free approach for training NN encoder-decoder pairs for reliable communication, using policy gradient methods, can be extended to produce a secrecy enforcing modulation structure. The challenge was to define a meaningful per sample loss that obeys constraints for secrecy, as well as works for policy gradient methods. For that we re-visited another recent approach which enabled co-set structures for NN encoders with a modified input function for regular model-aware training. This approach was based on one-hot encoded messages in conjunction with the regular cross-entropy loss. We showed how to extract a per sample loss from this method, which conserves its structure enforcing properties. This makes it possible to use in conjunction with the policy gradient method. In our simulations, we showed that the security enhanced policy gradient exploration, with our novel per sample loss, can indeed enable an advantage in terms of error for Bob, and therefore secure communication. Moreover, we showed that the learned modulation indeed produces cluster structures, which enable the secure communication.
REFERENCES
[1] T. O'Shea and J. Hoydis, "An introduction to deep learning for the physical layer," IEEE Trans. Cogn. Commun. Netw., vol. 3, no. 4, pp. 563­575, Dec. 2017.

[2] S. Dörner, S. Cammerer, J. Hoydis, and S. ten Brink, "Deep learning based communication over the air," IEEE J. Sel. Topics Signal Process., vol. 12, no. 1, pp. 132­143, Feb. 2018.
[3] R. Fritschek, R. F. Schaefer, and G. Wunder, "Deep learning for channel coding via neural mutual information estimation," in Proc. 20th Int. Workshop Signal Process. Adv. Wireless Commun., Cannes, France, July 2019, pp. 1­5.
[4] R. Fritschek, R. F. Schaefer, and G. Wunder, "Neural mutual information estimation for channel coding: State-of-the-art estimators, analysis, and performance comparison," in Proc. 21th Int. Workshop Signal Process. Adv. Wireless Commun., Atlanta, GA, USA, May 2020, pp. 1­5.
[5] K. Besser, C. R. Janda, P.-H. Lin, and E. A. Jorswieck, "Flexible design of finite blocklength wiretap codes by autoencoders," in Proc. IEEE Int. Conf. Acoustics, Speech, Signal Process., Brighton, UK, May 2019, pp. 2512­2516.
[6] X. Zhang and M. Vaezi, "Deep learning based precoding for the MIMO Gaussian wiretap channel," in Proc. IEEE Global Commun. Conf. Workshops, Waikoloa, HI, USA, Dec. 2019, pp. 1­6.
[7] R. Fritschek, R. F. Schaefer, and G. Wunder, "Deep learning for the Gaussian wiretap channel," in Proc. IEEE Int. Conf. Commun., Shanghai, China, May 2019, pp. 1­6.
[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," in Proc. Adv. Neural Inf. Process. Syst., Montréal, Canada, Dec. 2014, pp. 2672­2680.
[9] H. Ye, G. Y. Li, B. F. Juang, and K. Sivanesan, "Channel agnostic endto-end learning based communication systems with conditional GAN," in Proc. IEEE Global Commun. Conf. Workshops, Abu Dhabi, UAE, Dec. 2018, pp. 1­5.
[10] T. J. O'Shea, T. Roy, N. West, and B. C. Hilburn, "Physical layer communications system design over-the-air using adversarial networks," in Proc. 26th European Signal Process. Conf., Rome, Italy, Sept. 2018, pp. 529­532.
[11] T. Marchioro, N. Laurenti, and D. Gündüz, "Adversarial networks for secure wireless communications," in Proc. IEEE Int. Conf. Acoustics, Speech, Signal Process., Barcelona, Spain, May 2020, pp. 8748­8752.
[12] I. Belghazi, S. Rajeswar, A. Baratin, R. D. Hjelm, and A. Courville, "MINE: Mutual information neural estimation," arXiv preprint arXiv:1801.04062, June 2018.
[13] Sina Molavipour, Germán Bassi, and Mikael Skoglund, "Conditional mutual information neural estimator," in Proc. IEEE Int. Conf. Acoustics, Speech, Signal Process., Barcelona, Spain, May 2020, pp. 5025­5029.
[14] F. A. Aoudia and J. Hoydis, "End-to-end learning of communications systems without a channel model," in Proc. 52nd Asilomar Conf. Signals, Systems, and Computers, Pacific Grove, CA, Oct. 2018, pp. 298­303.
[15] M. Goutay, F. A. Aoudia, and J. Hoydis, "Deep reinforcement learning autoencoder with noisy feedback," arXiv preprint arXiv:1810.05419, Oct. 2018.
[16] Benjamin Recht, "A tour of reinforcement learning: The view from continuous control," Annual Review of Control, Robotics, and Autonomous Systems, vol. 2, pp. 253­279, 2019.
[17] Thomas M. Cover and Joy A. Thomas, Elements of Information Theory, Wiley & Sons, 2 edition, 2006.
[18] A. D. Wyner, "The Wire-Tap Channel," Bell Syst. Tech. J., vol. 54, pp. 1355­1387, Oct. 1975.
[19] Ueli M. Maurer and Stefan Wolf, "Information-theoretic key agreement: From weak to strong secrecy for free," in EUROCRYPT 2000, Lecture Notes in Computer Science. May 2000, vol. 1807, pp. 351­368, SpringerVerlag.
[20] M. Bloch and J. Barros, Physical-Layer Security: From Information Theory to Security Engineering, Cambridge University Press, Cambridge, UK, 2011.
[21] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, MIT press, 2018.
[22] T. Dozat, "Incorporating Nesterov momentum into Adam," in Proc. 4th Int. Conf. Learning Representations Workshops, San Juan, Puerto Rico, May 2016.
[23] F. Oggier, P. Solé, and J. C. Belfiore, "Lattice codes for the wiretap Gaussian channel: Construction and analysis," IEEE Trans. Inf. Theory, vol. 62, no. 10, pp. 5690­5708, Oct. 2016.
[24] R. Fritschek, "Simulations ICC 2021," https://github.com/Fritschek, 2021. [25] J. Dean, R. Monga, et al., "TensorFlow: Large-scale machine learning on
heterogeneous systems," 2015, Software available from tensorflow.org.

