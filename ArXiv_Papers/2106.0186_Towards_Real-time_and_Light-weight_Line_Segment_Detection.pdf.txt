Towards Light-weight and Real-time Line Segment Detection
Geonmo Gu*, Byungsoo Ko*, SeoungHyun Go, Sung-Hyun Lee, Jingeun Lee, Minchul Shin NAVER/LINE Vision
github.com/navervision/mlsd

arXiv:2106.00186v1 [cs.CV] 1 Jun 2021

Abstract
Previous deep learning-based line segment detection (LSD) suffer from the immense model size and high computational cost for line prediction. This constrains them from real-time inference on computationally restricted environments. In this paper, we propose a real-time and lightweight line segment detector for resource-constrained environments named Mobile LSD (M-LSD). We design an extremely efficient LSD architecture by minimizing the backbone network and removing the typical multi-module process for line prediction in previous methods. To maintain competitive performance with such a light-weight network, we present novel training schemes: Segments of Line segment (SoL) augmentation and geometric learning scheme. SoL augmentation splits a line segment into multiple subparts, which are used to provide auxiliary line data during the training process. Moreover, the geometric learning scheme allows a model to capture additional geometric cues from matching loss, junction and line segmentation, length and degree regression. Compared with TP-LSD-Lite, previously the best real-time LSD method, our model (M-LSDtiny) achieves competitive performance with 2.5% of model size and an increase of 130.5% in inference speed on GPU when evaluated with Wireframe and YorkUrban datasets. Furthermore, our model runs at 56.8 FPS and 48.6 FPS on Android and iPhone mobile devices, respectively. To the best of our knowledge, this is the first real-time deep LSD method available on mobile devices.
1. Introduction
Line segments and junctions are crucial visual features in low-level vision, which provide fundamental information to the higher level vision tasks, such as pose estimation [20, 29, 19], structure from motion [3, 18], 3D reconstruction [5, 6], image matching [32], wireframe to image translation [33] and image rectification [34]. Moreover, the growing demand for performing such vision tasks on resource constraint platforms, like mobile or embedded de-
*Authors contributed equally.

Figure 1: Comparison of M-LSD and existing LSD methods on Wireframe dataset. Inference speed (FPS) is computed on Tesla V100 GPU. Size and value of circles indicate the number of model parameters (Millions). M-LSD achieves competitive performance with the lightest model size and the fastest inference speed. Details are in Table 3.
vices, have made real-time line segment detection (LSD) an essential but challenging task. The difficulty arises from the limited computational power and model size while finding the best accuracy and resource-efficiency trade-offs to achieve real-time inference.
With the advent of deep neural networks, deep learningbased LSD architectures [30, 36, 31, 35, 12] have adopted models to learn various geometric cues of line segments and have proved to show improvements in performance. As described in Figure 2, we have summarized multiple strategies that use deep learning models for LSD. The top-down strategy [30] first detects regions of line segment with attraction field maps and then predicts line segments by squeezing regions into line segments. In contrast, the bottomup strategy first detects junctions, then arranges them into line segments, and lastly verifies the line segments by using an extra classifier [36, 31, 35] or a merging algorithm [10, 11]. Recently, [12] proposes Tri-Points (TP) representation for a simpler process of line prediction without the time-consuming steps of line proposal and verification.
Although previous efforts of using deep learning models have made remarkable achievements, real-time inference for LSD on resource-constraint platforms still remains

Figure 2: Different strategies for LSD. Previous LSD methods exploit the multi-module processing for line segment prediction. In contrast, our method directly predicts line segments from feature maps with a single module.

Strategy

Method

Input

Inference speed (FPS) Backbone Prediction Total

TD AFM

320

L-CNN

512

BU L-CNN-P

512

HAWP

512

TP-LSD-Lite 320

TP

TP-LSD-Res34 TP-LSD-Res34

320 512

TP-LSD-HG

512

M-LSD-tiny

320

Ours

M-LSD-tiny M-LSD

512 320

M-LSD

512

77.1 55.2 55.2 55.0 138.4 129.0 128.8 64.7 241.1 201.6 156.3 132.8

17.3 23.8 0.4 82.2 234.6 71.0 23.7 200.5 1202.8 881.9 1194.7 883.4

14.1 16.6 0.4 32.9 87.1 45.8 20.0 48.9 200.8 164.1 138.2 115.4

Table 1: Inference speed of backbone, prediction modules, and total on GPU. Strategies are from Figure 2. Our method shows superior speed on backbone and line prediction by employing a lightweight network with a single module of line prediction.

limited. There have been attempts to present real-time LSD [12, 17, 31], but they have been limited to server-class GPUs. This is mainly because the models that are used exploit heavy backbone networks, such as dilated ResNet50based FPN [35], stacked hourglass network [11, 17, 12], and atrous residual U-net [30], which require large memory and high computational power. In addition, as shown in Figure 2, the line prediction process consists of multiple modules, which include line proposal [30, 35, 36, 31], line verification networks [35, 36, 31] and mixture of convolution module [12, 11]. As the size of the model and the number of modules for line prediction increase, the overall inference speed of LSD can become slower, as shown in Table 1, while demanding higher computation. Thus, increases in computational cost make it difficult to deploy LSD on resource-constraint platforms.
In this paper, we propose a real-time and light-weight line segment detector for resource-constrained environments, named Mobile LSD (M-LSD). For the network, we design a significantly efficient architecture with a single module to predict line segments. By minimizing the network size and removing the multi-module process from previous methods, M-LSD is extremely light and fast. To maintain competitive performance even with a light-weight network, we present novel training schemes: SoL augmentation and geometric learning scheme. SoL augmentation divides a line segment into subparts, which are further used to provide augmented line data during the training phase. Geometric learning schemes train a model with additional geometric information, including matching loss, junction and line segmentation, length and degree regression. As a result, our model is able to capture extra geometric information during training to make more accurate line predictions.
As shown in Figure 1, our methods achieve competitive performance and faster inference speed with an extremely

smaller model size. M-LSD outperforms the previous best real-time method, TP-LSD-Lite [12], with only 6.3% of the model size but gaining an increase of 32.5% in inference speed. Moreover, M-LSD-tiny runs in real-time at 56.8 FPS and 48.6 FPS on Android and iPhone mobile devices, respectively. To the best of our knowledge, this is the first real-time LSD method available on mobile devices.
2. Related Works
Deep Line Segment Detection. There have been active studies on deep learning-based LSD. In junction-based methods, DWP [11] includes two parallel branches to predict line and junction heatmaps, followed by a merging process. PPGNet [35] and L-CNN [36] utilize junctionbased line segment representations with an extra classifier to verify whether a pair of points belongs to the same line segment. Another approach exploits dense prediction. AFM [30] predicts attraction field maps that contain 2-D projection vectors representing associated line segments, followed by a squeeze module to recover line segments. HAWP [31] is presented as a hybrid model of AFM and L-CNN. Recently, [12] devises the TP line representation to remove the use of extra classifiers or heuristic postprocessing from previous methods and proposes TP-LSD network with two branches: TP extraction and line segmentation branches. However, previous multi-module processing for line prediction, such as line verification network, squeeze module, and multi-branch network can limit for real-time inference on resource-constrained environments.
Real-time Object Detectors. Real-time object detection has been an important task for deep learning-based object detection. Object detectors proposed in the early days, such as RCNN-series [8, 7, 24] consist of two-stage architecture: generating proposals in the first stage, then classi-

Figure 3: The overall architecture of M-LSD-tiny. In the feature extractor, block 1  11 are parts of MobileNetV2, and block 12  16 are designed as a top-down architecture. The final feature maps are simply generated by upscale. The predicted line segments are generated by merging center points and displacement vectors from the TP maps.

fying the proposals in the second stage. These two-stage detectors typically suffer from slow inference speed and hard optimization difficulty. To handle this problem, one-stage detectors, such as YOLO-series [21, 22, 23] and SSD [15] are proposed to achieve GPU real-time inference by reducing backbone size and simplifying the two-stage process into a one-stage process. This one-stage architecture has been further studied and improved to run in real-time on mobile devices [9, 25, 28, 14]. Motivated by the transition from two-stage to one-stage architecture in object detection, we argue that the complicated multi-module processing in previous LSD can be disregarded. We simplify the line prediction process with a single module for faster inference speed and enhance the performance by the efficient training strategies; SoL augmentation and geometric learning scheme.
3. M-LSD for Line Segment Detection
In this section, we present the details of M-LSD. Our design mainly focuses on efficiency while retaining competitive performance. Firstly, we design a light-weight backbone and reduce the modules involved in processing line predictions for better efficiency. Next, we apply additional training schemes, including SoL augmentation and geometric learning schemes, to capture extra geometric cues. As a result, M-LSD is able to balance the trade-off between accuracy and efficiency to be well suited for mobile devices. 3.1. Light-weight Backbone
We design light (M-LSD) and lighter (M-LSD-tiny) models as popular encoder-decoder architectures. In efforts

to build a light-weight LSD model, our encoder networks are based on MobileNetV2 [25] which is well-known to run in real-time on mobile environments. The encoder network uses parts of MobileNetV2 (block 1  11 of the feature extractor in Figure 3) to make it even lighter, which includes an input to 64-channel of bottleneck blocks. The number of parameters in the encoder network is 0.25M (7.4% of MobileNetV2) while the total parameters of MobileNetV2 are 3.4M. For M-LSD, a slightly bigger yet more performant model, the encoder network also uses parts of MobileNetV2 including an input to 96-channel of bottleneck blocks which results to a number of 0.56M parameters (16.5% of MobileNetV2). The decoder network is designed using a combination of block types A, B, and C. Block type A concatenates feature maps from skip connection and upscale. Block type B performs two 3 × 3 convolutions with a residual connection in-between. Similarly, block type C performs two 3 × 3 convolutions followed by a 1 × 1 convolution, where the first is a dilated convolution. The final feature maps in M-LSD-tiny are generated by upscaling with H/2 × W/2 × 16 tensors when the input image is H × W × 3. On the other hand, M-LSD uses the feature map from block type C as a final feature map with the same size of H/2 × W/2 × 16.
Each feature map channel serves its own purpose: 1) TP maps have seven feature maps, including one length map, one degree map, one center map, and four displacement maps. 2) SoL maps have seven feature maps with the same configuration as TP maps. 3) Segmentation maps have two feature maps, including junction and line maps. Please refer to the supplementary material for further details on the architectures of M-LSD-tiny and M-LSD.

(a) TP representation

(b) SoL augmentation

Figure 4: Tri-Points (TP) representation and Segments of Line segment (SoL) augmentation. ls, lc, and le denote start, center, and end points, respectively. ds and de are displacement vectors to start and end points. l0  l2 indicates internally dividing points of the line segment lsle.

3.2. Line Segment Representation Line segment representation determines how line seg-
ment predictions are generated and ultimately affects the efficiency of LSD. Hence, we employ the TP representation [12] which has been introduced to have a simple line generation process and shown to perform real-time LSD using GPUs. TP representation uses three key-points to depict a line segment: start, center, and end points. As illustrated in Figure 4a, the start ls and end le points are represented by using two displacement vectors (ds, de) with respect to the center lc point. The line generation process, which is to convert center point and displacement vectors to a vectorized line segment, is performed as:

(xls , yls ) = (xlc , ylc ) + ds(xlc , ylc ),

(xle , yle ) = (xlc , ylc ) + de(xlc , ylc ),

(1)

where (x, y) denotes the  point. ds(xlc , ylc ) and de(xlc , ylc ) indicate 2D displacements from the center point lc to the corresponding start ls and end le points. The center point and displacement vectors are trained with one center map and four displacement maps (one for each x and y value of the displacement vectors ds and de). For the ground truth (GT) of the center map, positions of the center point are marked on a zero map, which is then scaled using a Gaussian kernel truncated by a 3 × 3 window. In the line generation process, we extract the exact center point position by non-maximum suppression on the center map. Next, we generate line segments with the extracted center points and the corresponding displacement vectors using a simple arithmetic operation as expressed in Equation 1; thus, making inference efficient and fast. For a direct comparison with [12], we perform the line generation immediately from the final feature maps in a single module process, while [12] performs multi-module processing as illustrated in Figure 2.
For a loss function to train the center map, the weighted binary cross-entropy (WBCE) loss is used in [12]. How-

ever, we observe that the number of positive (foreground) pixels is much less than that of negative (background) pixels, and such foreground-background class imbalance degrades the performance of the WBCE loss. This is because the majority of pixels are easy negatives that contribute no useful learning signals. Thus, we separate positive and negative terms of the binary cross-entropy loss to have the same scale, and reformulate a separated binary classification loss as follows:

pos(F ) =

-1 p I(p)

p W (p) · log(F (p)), (2)

neg(F ) =

-1 p 1-I(p)

p(1 - I(p)) · log(1 - (F (p))), (3)

cls(F ) = pos · pos(F ) + neg · neg(F ), (4)

where I(p) outputs 1 if the pixel p of the GT map is nonzero, otherwise 0,  denotes a sigmoid function, and W (p) and F (p) are pixel values in the GT and feature map, respectively. We use the center loss as Lcenter = cls(C), where C denotes center map and set the weights (pos, neg) as (1,30). For the displacement loss Ldisp, we use smooth L1 loss for regression learning as [12].

3.3. SoL Augmentation

We propose Segments of Line segment (SoL) augmentation that increases the number of line segments with wider varieties of length for training. Learning line segments with center points and displacement vectors can be insufficient in certain circumstances where a line segment may be too long to manage within the receptive field size or the center points of two distinct line segments are too close to each other. To address these issues and provide auxiliary information to the TP representation, SoL explicitly splits line segments into multiple subparts with overlapping portions with each other. An overlap between each split is enforced to preserve connectivity among the subparts. As described in Figure 4b, we compute k internally dividing points (l0, l1, l2) and separate the line segment lsle into three subparts (lsl1, l0l2, l1le). Expressed in TP representation, each subpart is trained as if it is a typical line segment. The number of internally dividing points k is determined by the length of the line segment as k = r(l)/(µ/2) - 1, where r(l) denotes the length of line segment l, and µ is the base length of subparts. Note that when k  1, we do not split the line segment. The resulting length of each subpart can be shorter or longer than µ, and we use µ = input size × 0.125. Loss functions Lcenter and Ldisp for SoL maps are the same as Equation 4 and smooth L1 loss, respectively, when the ground truth is from subparts. Note that the line generation process is only done in TP maps, not in SoL maps.

3.4. Learning with Geometric Information

To boost the quality of predictions, we incorporate various geometric information about line segments which helps

(a) Matching loss

(b) Geometric losses

Figure 5: Matching and geometric losses. (a) Given a matched pair of a predicted line ^l and a GT line l, matching loss (Lmatch) optimizes the predicted start, end, and center points. (b) Given a line segment, M-LSD learns various geometric cues: junction (Ljunc) and line (Lline) segmentation, length (Llength) and degree (Ldegree) regression.

the overall learning process. In this section, we present learning LSD with matching loss, junction and line segmentation, and length and degree regression for additional geometric information.

3.4.1 Matching Loss

Line segments under the TP representation are decoupled into center points and displacement vectors, which are optimized with center and displacement loss separately. They become line segments by the line generation process as formulated in Equation 1. However, this coupled information of the generated line segment is under-utilized in loss functions. Thus, we present a matching loss, which also leverages information of the coupled information w.r.t the ground truth. Note that matching loss is used for both TP and SoL maps.
As illustrated in Figure 5a, matching loss guides the generated line segments to be similar to the matched GT. We first take the endpoints of each prediction, which can be calculated via the line generation process, and measure the Euclidean distance d(·) to the endpoints of the GT. Next, these distances are used to match predicted line segments ^l with GT line segments l that are under a threshold :

d(ls, ^ls) <  and d(le, ^le) < ,

(5)

where ls and le are the start and end points of the line l, and  is set to 5 pixels. Then, we obtain a set M of matched line segments (l, ^l) that satisfies this condition. Finally, the L1 loss is used for the matching loss, which aims to minimize the geometric distance of the matched line segments w.r.t the start, end, and center points as follows:

Lmatch

=

1 | M | (l,^l)M

ls - ^ls 1 +

le - ^le 1

+ C~(^l) - (ls + le)/2 1,

(6)

where C~(^l) is center point of a line ^l from the center map.

3.4.2 Junction and Line Segmentation Center point and displacement vectors are highly related to pixel-wise junctions and line segments in the segmentation maps of Figure 3. For example, end points, derived from the center point and displacement vectors, should be the junction points. Also, center points must be localized on the pixel-wise line segment. Thus, learning the segmentation maps of junctions and line segments works as a spatial attention cue for LSD. As illustrated in Figure 3, M-LSD contains segmentation maps, including a junction map and a line map. We construct the junction GT map by scaling with Gaussian kernel as the center map, while using a binary map for line GT map. The separated binary classification loss is used for the junction loss Ljunc = cls(J ) and line loss Lline = cls(E), where J and E denote junction and line maps, respectively. The total segmentation loss is defined as Lseg = Ljunc + Lline, where we set the weights (pos, neg) as (1, 30) for Ljunc and (1, 1) for Lline.

3.4.3 Length and Degree Regression As displacement vectors can be derived from the length and degree of line segments, they can be additional geometric cues to support the displacement maps. We compute the length and degree from the ground truth and mark the values on the center of line segments in each GT map. Then, these values are extrapolated to a 3 × 3 window so that all neighboring pixels of a given pixel contain the same value. As shown in Figure 3, we maintain predicted length and degree maps for both TP and SoL maps, where TP uses the original line segment and SoL uses augmented subparts. As the ranges of length and degree are wide, we divide each length by the diagonal length of the input image for normalization. For degree, we divide each degree by 2 and add 0.5. Finally, the length and degree maps are used for smooth L1 loss (Llength and Ldegree) based regression learning. 3.5. Final Loss Functions
The loss function for TP maps LT P is defined as the sum of center and displacement loss, length and degree regression loss, and a matching loss:

LT P = Lcenter + Ldisp + Llength + Ldegree + Lmatch. (7)

The loss function for SoL maps LSoL follows the same formulation as Equation 7 but with SoL augmented GT. Finally, we obtain the final loss function Ltotal as follows:

Ltotal = LT P + LSoL + Lseg.

(8)

As illustrated in Figure 3, LT P and LSoL each optimizes the line representation maps and Lseg optimizes the segmentation maps.

(a) w/o matching loss (M2) (b) w/ matching loss (M3)

(a) Junction segmentation map (b) Line segmentation map

(c) w/o SoL augmentation (M7) (d) w/ SoL augmentation (M8) Figure 6: Saliency maps generated from TP center map. Model numbers (M28) are from Table 2.
4. Experiments 4.1. Experimental Setting
Dataset and Evaluation Metrics. We evaluate our model with two famous LSD datasets: Wireframe [11] and YorkUrban [5]. The Wireframe dataset consists of 5,000 training and 462 test images of indoor and outdoor scenes, while the YorkUrban dataset has 102 test images. Following the typical training and test protocol [12, 35, 17, 30, 36], we train our model with the training set from the Wireframe dataset and test with both Wireframe and YorkUrban datasets. We evaluate our models using prevalent metrics for LSD [12, 35, 17, 30, 36] that include: heatmapbased metric F H , structural average precision (sAP), and line matching average precision (LAP).
Optimization. We train our model on Tesla V100 GPU. We use the TensorFlow [2] framework for model training and TFLite [1] for porting models to mobile devices. Input images are resized to 320 × 320 or 512 × 512 in both training and testing, which are specified in each experiment. The input augmentation consists of horizontal and vertical flips, shearing, rotation, and scaling. We use ImageNet [4] pre-trained weights on the parts of MobileNetV2 [25] in MLSD and M-LSD-tiny. Our model is trained using the Adam optimizer [13] with a learning rate of 0.01. We use linear learning rate warm-up for 5 epochs and cosine learning rate decay [16] from 70 epoch to 150 epoch. We train the model for a total of 150 epochs with a batch size of 64.
4.2. Ablation Study and Interpretability We conduct a series of ablation experiments to analyze
our proposed method. M-LSD-tiny is trained and tested on the Wireframe dataset with an input size of 512 × 512. As shown in Table 2, all the proposed schemes contribute to a significant performance improvement. In addition, we include saliency map visualizations by using GradCam [26],

(c) TP length regression map (d) TP degree regression map Figure 7: Saliency maps generated from each feature map. M-LSD-tiny (M8 in Table 2) model is used for generation.

M Schemes

F H sAP 10 LAP

1 Baseline

73.2 47.6 46.8

2 + Input augmentation 74.3 48.9 48.1

3 + Matching loss

75.4 52.2 52.5

4 + Line segmentation

75.4 52.9 53.7

5 + Junction segmentation 76.2 53.7 54.6

6 + Length regression

76.1 54.5 54.8

7 + Degree regression

76.2 55.1 55.3

8 + SoL augmentation

77.2 58.0 57.9

Table 2: Ablation study of M-LSD-tiny on Wireframe. The baseline is trained with M-LSD-tiny backbone including only TP representation. M denotes model number.

which is generated from each feature map to analyze the network learned from each training scheme. The saliency map interprets important regions and importance levels on the input image by computing the gradients from each feature map. We include a comparison of models in Figure 6 and feature maps in Figure 7.
Baseline and Augmentation. The baseline model is trained with M-LSD-tiny backbone, including only the TP representation and no other proposed schemes. We observe that adding horizontal and vertical flips, shearing, rotation, and scaling input augmentation on the baseline model shows further performance improvement.
Matching Loss. Integrating matching loss shows slight improvements on pixel localization accuracy by 1.1 in F H while significant enhancements on line prediction quality by 3.3 in sAP 10 and 4.4 in LAP . We observe from saliency maps that learning w/o matching loss shows weak attention on center points, as shown in Figure 6a, while w/ matching loss amplifies the attention on center points in Figure 6b. This demonstrates that training with coupled information of center points and displacement vectors allows the model to learn with more line-awareness features.
Line and Junction Segmentation. Adding line and junction segmentation gives performance boosts in the fol-

Methods

Input FH

Wireframe sAP5 sAP10 LAP

FH

YorkUrban sAP5 sAP10

LAP

Params(M)

FPS

LSD [27]

320 64.1 6.7 8.8 18.7 60.6 7.5 9.2 16.1

-

100.0

DWP [11]

512 72.7 -

5.1 6.6 65.2 -

2.6 3.1

33.0

2.2

AFM [30]

320 77.3 18.3 23.9 36.7 66.3 7.0 9.1 17.5

43.0

14.1

L-CNN [36]

512 77.5 58.9 62.8 59.8 64.6 25.9 28.2 32.0

9.8

16.6

L-CNN-P [36]

512 81.7 52.4 57.3 57.9 67.5 20.9 23.1 26.8

9.8

0.4

LGNN [17]

512 -

-

62.3

-

-

-

-

-

-

15.8

LGNN-lite [17]

512 -

-

57.6

-

-

-

-

-

-

34.0

HAWP [31]

512 80.3 62.5 66.5 62.9 64.8 26.1 28.5 30.4

10.4

32.9

TP-LSD-Lite [12] 320 80.4 56.4 59.7 59.7 68.1 24.8 26.8 31.2

23.9

87.1

TP-LSD-Res34 [12] 320 81.6 57.5 60.6 60.6 67.4 25.3 27.4 31.1

23.9

45.8

TP-LSD-Res34 [12] 512 80.6 57.6 57.2 61.3 67.2 27.6 27.7 34.3

23.9

20.0

TP-LSD-HG [12]

512 82.0 50.9 57.0 55.1 67.3 18.9 22.0 24.6

7.4

48.9

M-LSD-tiny M-LSD-tiny M-LSD M-LSD

320 76.8 43.0 51.3 50.1 61.9 17.4 21.3 23.7

0.6

200.8

512 77.2 52.3 58.0 57.9 62.4 22.1 25.0 28.3

0.6

164.1

320 78.7 48.2 55.5 55.7 63.4 20.2 23.9 27.7

1.5

138.2

512 80.0 56.4 62.1 61.5 64.2 24.6 27.3 30.7

1.5

115.4

Table 3: Quantitative comparisons with existing LSD methods. FPS is evaluated in Tesla V100 GPU, where  denotes CPU FPS and  denotes the value from the corresponding paper because of no published implementation. The best score from previous methods and our models are colored in blue and red, respectively.

lowing metrics: 0.8 in F H , 1.5 in sAP 10 and 2.1 in LAP . Moreover, the junction and line attention on saliency maps of Figure 7a and 7b are precise, which shows that junction and line segmentations work as spatial attention cues for LSD.
Length and Degree Regression. The line prediction quality improves 1.4 in sAP 10 and 0.7 in LAP by adding length and degree regression, while the pixel localization accuracy F H remains the same. The length saliency map in Figure 7c contains highlights on the entire line, and the degree saliency map in Figure 7d has highlights on the center points. We speculate that computing length needs the entire line information whereas computing the degree only needs part of the line. Overall, learning with additional geometric information of line segments, such as length and degree, further increases the performance.
SoL Augmentation. Integrating SoL augmentation shows significant performance boost by 1.0 in F H , 2.9 in sAP 10 and 2.6 in LAP . In the saliency maps of Figure 6c and 6d, w/o SoL augmentation shows strong but vague attention on center points with disconnected line attention for the long line segments, when the entire line information is essential to compute the center point. However, w/ SoL augmentation shows more precise center point attention as well as clearly connected line attention. It demonstrates that augmenting line segments based on the number and length guides the model to be more robust in pixel-based and line matching-based qualities. 4.3. Comparison with Other Methods
We compare our method with previous LSD methods, including LSD [27], DWP [11], AFM [30], L-CNN [36] with post-processing (L-CNN-P), LGNN [17], HAWP [31], and

TP-LSD [12]. Table 3 shows that our method achieves competitive performance and the fastest inference speed even with a limited model size. In comparison with the previous fastest model, TP-LSD-Lite, M-LSD with input size of 512 shows higher performance and an increase of 32.5% in inference speed with only 6.3% of the model size. Our fastest model, M-LSD-tiny with 320 input size, has a slightly lower performance than that of TP-LSD-Lite, but achieves an increase of 130.5% in inference speed with only 2.5% of the model size. Compared to the previous lightest model TP-LSD-HG, M-LSD with 512 input size outperforms on sAP 5, sAP 10 and LAP with an increase of 136.0% in inference speed with 20.3% of the model size. Our lightest model, M-LSD-tiny with 320 input size, shows an increase of 310.6% in the inference speed with 8.1% of the model size compared to TP-LSD-HG. Previous methods can be deployed as real-time line segment detectors on server-class GPUs, but not on resource-constrained environments either because the model size is too large or the inference speed is too slow. Although M-LSD does not achieve state-ofthe-art performance, it shows competitive performance and the fastest inference speed with the smallest model size, offering the potential to be used in real-time applications on resource-constrained environments, such as mobile devices. 4.4. Visualization
We visualize outputs of M-LSD and M-LSD-tiny in Figure 8. Junctions and line segments are colored with cyan blue and orange, respectively. Compared to the GT, both models are capable of identifying junctions and line segments with high precision even in complicated low contrast environments such as (a) and (c). Although M-LSDtiny contains a few missing small line segments and incor-

Figure 8: Qualitative evaluation of M-LSD-tiny and M-LSD on WireFrame dataset.
rectly connected junctions, the fundamental line segments to aware of the environmental structure are accurate. In addition, there are straight patterns on the floor in (b) and the wall in (d), that are missing in GT taken from the WireFrame [11] dataset which was annotated by humans. However, our proposed methods are capable of detecting even the minute details in patterns and confirms the robustness of our models against complicated scenes. 4.5. Deployment on Mobile Devices
We deploy M-LSD on mobile devices and evaluate the memory usage and inference speed. We use iPhone 12 Pro with A14 bionic chipset and Galaxy S20 Ultra with Snapdragon 865 ARM chipset. As shown in Table 4, M-LSDtiny and M-LSD are small enough to be deployed on mobile devices where memory requirements range between 78MB and 508MB. The inference speed of M-LSD-tiny is fast enough to be real-time on mobile devices where it ranges from a minimum of 17.9 FPS to a maximum of 56.8 FPS. M-LSD still can be real-time with 320 input size, however, with 512 input size, FP16 may be required for a faster FPS over 10. Overall, as all our models have small memory requirements and fast inference speed on mobile devices, the exceptional efficiency allows M-LSD variants to be used in real-world applications. To the best of our knowledge, this is the first and the fastest real-time line segment detector on mobile devices ever reported. 4.6. Applications
As line segments are fundamental low-level visual features, there are various real-world applications that use LSD. We show an example with real-time box detection on a mobile device as described in Figure 9. We implement a box detector on a mobile device by using the M-LSD-

Model

Input Device FP Latency (ms) FPS Memory (MB)

M-LSD-tiny

320 512

iPhone

32 16

Android

32 16

iPhone

32 16

Android

32 16

30.6 20.6 31.0 17.6 51.6 36.8 55.8 25.4

32.7 48.6 32.3 56.8 19.4 27.1 17.9 39.4

169 111 103 78 203 176 195 129

M-LSD

320

iPhone

32 16

Android

32 16

74.5 46.4 82.4 38.4

13.4 21.6 12.1 26.0

512

iPhone

32 16

Android

32 16

121.6 90.7 177.3 79.0

8.2 11.0 5.6 12.7

241 188 236 152 327 261 508 289

Table 4: Inference speed and memory usage on iPhone (A14 Bionic chipset) and Android phone (Snapdragon 865 chipset). FP denotes floating point.

Figure 9: Real-time box detection using M-LSD-tiny on a mobile device. Given an image as input to the mobile device as (a), line segments are detected using M-LSD-tiny as (b). Then, box candidates are computed from post-processing as (c), and finally we obtain box detection by a ranking process as (d).
tiny model. Since the application consists of line detection and post-processing, a model for the line detection has to be light and fast enough for real-time usage, when M-LSDtiny is playing a sufficient role. The potential of real-time LSD on a mobile device can further be extended to other real-world applications like a book scanner, wireframe to image translation, and SLAM.
5. Conclusion
We introduce M-LSD, a light-weight and real-time line segment detector for resource-constrained environments. Our model is designed with a significantly efficient network architecture and a single module process to predict line segments. To maintain competitive performance even with a light-weight network, we present novel training schemes: SoL augmentation and geometric learning. As a result, our proposed method achieves competitive performance and the fastest inference speed with the lightest model size. Moreover, we show that M-LSD is deployable on mobile devices in real-time, which demonstrates the potential to be used in real-time mobile applications.

References
[1] Tensorflow lite. https://www.tensorflow.org/lite. 6
[2] Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. 6
[3] Adrien Bartoli and Peter Sturm. Structure-from-motion using lines: Representation, triangulation, and bundle adjustment. Computer vision and image understanding, 100(3):416­441, 2005. 1
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248­255. Ieee, 2009. 6
[5] Patrick Denis, James H Elder, and Francisco J Estrada. Efficient edge-based methods for estimating manhattan frames in urban imagery. In European conference on computer vision, pages 197­210. Springer, 2008. 1, 6
[6] Olivier D Faugeras, Rachid Deriche, Herve´ Mathieu, Nicholas Ayache, and Gregory Randall. The depth and motion analysis machine. In Parallel Image Processing, pages 143­175. World Scientific, 1992. 1
[7] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 1440­1448, 2015. 2
[8] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580­587, 2014. 2
[9] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 3
[10] Kun Huang and Shenghua Gao. Wireframe parsing with guidance of distance map. IEEE Access, 7:141036­141044, 2019. 1
[11] Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding, Shenghua Gao, and Yi Ma. Learning to parse wireframes in images of man-made environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 626­635, 2018. 1, 2, 6, 7, 8
[12] Siyu Huang, Fangbo Qin, Pengfei Xiong, Ning Ding, Yijia He, and Xiao Liu. Tp-lsd: Tri-points based line segment detector. arXiv preprint arXiv:2009.05505, 2020. 1, 2, 4, 6, 7
[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6
[14] Yuxi Li, Jiuwei Li, Weiyao Lin, and Jianguo Li. Tiny-dsod: Lightweight object detection for resource-restricted usages. arXiv preprint arXiv:1807.11013, 2018. 3
[15] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C

Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21­37. Springer, 2016. 3
[16] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 6
[17] Quan Meng, Jiakai Zhang, Qiang Hu, Xuming He, and Jingyi Yu. Lgnn: A context-aware line segment detector. In Proceedings of the 28th ACM International Conference on Multimedia, pages 4364­4372, 2020. 2, 6, 7
[18] Branislav Micusik and Horst Wildenauer. Structure from motion with line segments under relaxed endpoint constraints. International Journal of Computer Vision, 124(1):65­79, 2017. 1
[19] Bronislav Pribyl, Pavel Zemc´ik, and Martin C ad´ik. Camera pose estimation from lines using pl\" ucker coordinates. arXiv preprint arXiv:1608.02824, 2016. 1
[20] Bronislav Pribyl, Pavel Zemc´ik, and Martin C ad´ik. Absolute pose estimation from line correspondences using direct linear transformation. Computer Vision and Image Understanding, 161:130­144, 2017. 1
[21] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779­788, 2016. 3
[22] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7263­7271, 2017. 3
[23] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. 3
[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015. 2
[25] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510­4520, 2018. 3, 6
[26] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618­626, 2017. 6
[27] Rafael Grompone Von Gioi, Jeremie Jakubowicz, JeanMichel Morel, and Gregory Randall. Lsd: A fast line segment detector with a false detection control. IEEE transactions on pattern analysis and machine intelligence, 32(4):722­732, 2008. 7
[28] Robert J Wang, Xiang Li, and Charles X Ling. Pelee: A real-time object detection system on mobile devices. arXiv preprint arXiv:1804.06882, 2018. 3
[29] Chi Xu, Lilian Zhang, Li Cheng, and Reinhard Koch. Pose estimation from line correspondences: A complete analysis and a series of solutions. IEEE transactions on pattern analysis and machine intelligence, 39(6):1209­1222, 2016. 1

[30] Nan Xue, Song Bai, Fudong Wang, Gui-Song Xia, Tianfu Wu, and Liangpei Zhang. Learning attraction field representation for robust line segment detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1595­1603, 2019. 1, 2, 6, 7
[31] Nan Xue, Tianfu Wu, Song Bai, Fudong Wang, Gui-Song Xia, Liangpei Zhang, and Philip HS Torr. Holisticallyattracted wireframe parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2788­2797, 2020. 1, 2, 7
[32] Nan Xue, Gui-Song Xia, Xiang Bai, Liangpei Zhang, and Weiming Shen. Anisotropic-scale junction detection and matching for indoor images. IEEE Transactions on Image Processing, 27(1):78­91, 2017. 1
[33] Yuan Xue, Zihan Zhou, and Xiaolei Huang. Neural wireframe renderer: Learning wireframe to image translations. arXiv preprint arXiv:1912.03840, 2019. 1
[34] Zhucun Xue, Nan Xue, Gui-Song Xia, and Weiming Shen. Learning to calibrate straight lines for fisheye image rectification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1643­ 1651, 2019. 1
[35] Ziheng Zhang, Zhengxin Li, Ning Bi, Jia Zheng, Jinlei Wang, Kun Huang, Weixin Luo, Yanyu Xu, and Shenghua Gao. Ppgnet: Learning point-pair graph for line segment detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7105­ 7114, 2019. 1, 2, 6
[36] Yichao Zhou, Haozhi Qi, and Yi Ma. End-to-end wireframe parsing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 962­971, 2019. 1, 2, 6, 7

Towards Light-weight and Real-time Line Segment Detection Supplementary Material

A. Details of M-LSD
Architecture. The detailed architecture of M-LSD-tiny and M-LSD is described in Table A. M-LSD-tiny includes an encoder structure from MobileNetV2 [4] in block 111 and a custom decoder structure in block 12final. MLSD also includes an encoder structure from MobileNetV2 in block 114 and a designed decoder structure in block 15final, which is illustrated in Figure A. For the upscale operation, we use bilinear interpolation.

Feature Maps and Losses. For the displacement maps, we compute displacement vectors from the ground truth (GT) and mark those values on the center of line segment in the GT map. Next, these values are extrapolated to a 3×3 window (center blob) so that all neighboring pixels of a given pixel contain the same value. For the displacement, length, and degree maps, we use the smooth L1 loss for regression learning. The regression loss can be formulated as follows:

reg(F ) =

1 p H(p)

p

H(p) · Ls1mooth(F (p), F^(p)),

(i)

where F (p) and F^(p) denote values of pixel p in the feature map F and the GT map F^, and H(p) outputs 1 if the pixel p of the GT map is on the center blob (extrapolated 3×3 window). We use the displacement loss Ldisp = reg(D), where D denotes the displacement map. The length and degree losses are Llength = reg((L)) and Ldegree = reg((G)), where (L) and (G) are sigmoid function  applied to length and degree maps. In the line generation process, the center map is applied with a sigmoid function to output a probability value, while the displacement map uses the original values. Then, we extract the exact center point position by non-maximum suppression [1, 6, 2] on the center map to remove duplicates around correct predictions.

Final Feature Maps. In the training phase, M-LSD-tiny and M-LSD output final feature maps of 16 channels, which include 7 channels for TP maps, 7 channels for SoL maps, and 2 channels for segmentation maps as illustrated in Figure Ba. However, as the line generation process only requires the center and displacement maps of TP maps, operations for the other auxiliary maps are unnecessary in the

Block

Input

SC input

Operator

cn

1

H×W×3

-

conv2d

32 1

2

H/2×W/2×32

-

bottleneck 16 1

34 H/2×W/2×16

-

bottleneck 24 2

57 H/4×W/4×24

-

bottleneck 32 3

811 H/8×W/8×32

-

bottleneck 64 4

12 H/16×W/16×64 H/8×W/8×32 block type A 128 1

13 H/8×W/8×128

-

block type B 64 1

14 H/8×W/8×64 H/4×W/4×24 block type A 64 1

15 H/4×W/4×64

-

block type B 64 1

16 H/4×W/4×64

-

block type C 16 1

-

H/4×W/4×16

-

upscale

16 1

Final H/2×W/2×16

-

-

--

(a) M-LSD-tiny

Block

Input

SC input

Operator

cn

1

H×W×3

-

2

H/2×W/2×32

-

34

H/2×W/2×16

-

57

H/4×W/4×24

-

811 H/8×W/8×32

-

1214 H/16×W/16×64

-

conv2d bottleneck bottleneck bottleneck bottleneck bottleneck

32 1 16 1 24 2 32 3 64 4 96 3

15 16 17 18 19 20 21 22 23 Final

H/16×W/16×96 H/16×W/16×64 block type A 128 1

H/16×W/16×128

-

block type B 64 1

H/16×W/16×64 H/8×W/8×32 block type A 128 1

H/8×W/8×128

-

block type B 64 1

H/8×W/8×64

H/4×W/4×24 block type A 128 1

H/4×W/4×128

-

block type B 64 1

H/4×W/4×64

H/2×W/2×16 block type A 128 1

H/2×W/2×128

-

block type B 64 1

H/2×W/2×64

-

block type C 16 1

H/2×W/2×16

-

-

--

(b) M-LSD

Table A. Architecture details of M-LSD-tiny and M-LSD. Each line describes a sequence of 1 or repeating n identical layers where each layer in the same sequence has the same c output channels. Block numbers (`Block') and block type AC in `Operator' are from Figure 3 and Figure A. `SC input' denotes a skip connection input and the bottleneck operation is from MobileNetV2 [4].

inference phase. Thus, we disregard these operations and output only 5 channels of TP maps in the inference phase, including 1 center map and 4 displacement maps, as shown in Figure Bb. As a result, we can minimize computational cost and maximize the inference speed.

Figure A. The overall architecture of M-LSD. In the feature extractor, block 1  14 are parts of MobileNetV2, and block 15  23 are designed as a top-down architecture. The final feature maps are simply generated with upscale. The predicted line segments are generated by merging center points and displacement vectors from the TP maps.

(a) Final feature maps in the training phase
(b) Final feature maps in the inference phase Figure B. Final feature maps in the training and inference phase. (a) In the training phase, the final feature maps include TP, SoL, and segmentation maps with a total of 16 channels. (b) For better efficiency in the inference phase, we disregard unnecessary convolutions and maintain only the center and displacement maps in the TP maps with a total of 5 channels.
B. Extended Experiments B.1. Ablation Study of Architecture
We run a series of ablation experiments to investigate various encoder and decoder architectures. As shown in

Table Ba, we vary the parts used from the MobileNetV2 on the encoder architecture. As the encoder size increases, we add block types A and B to the decoder structure by following the structural format in Table Ab. Model 1  3 exploit bigger and deeper encoder architectures, which result in larger model parameters and slower inference speed. The performance turns out to be slightly higher than that of M-LSD. However, we choose `Input  96-channel' of MobileNetV2 as the encoder for M-LSD because increasing the encoder size causes larger amounts of model parameters to be used and decreases the inference speed with a negligible performance boost. Moreover, we observe that `Input  96-channel' is the largest model that can work on a mobile device in real-time. In contrast, when performing real-time LSD on GPUs, model 1  3 are good candidates as they outperform TP-LSD-Lite [2], previously the best real-time LSD, with faster inference speed and lighter model size.
In Table Bb, we vary the block types used in the decoder architecture. Model 4 changes every 1 × 1 convolution to a 3 × 3 convolution in block type A, while model 5 changes the residual connection from being in between the convolutions (`pre-residual') to the end of the convolutions (`post-residual') for block type B. These changes result in an increase in model size and a decrease in inference speed because `post-residual' requires twice the number of output channels than that of `pre-residual'. However, the performance remains similar to that of M-LSD-tiny. For models 6 and 7, the dilated rate of the first convolution in block type C is changed to 1 and 3, respectively. Here we observe that by decreasing the dilated rate can improve the inference

Model M-LSD-tiny M-LSD 1 2 3

Parts of MNV2 in encoder Input  64-channel Input  96-channel Input  160-channel Input  320-channel Input  1280-channel

Params (M) Encoder (% of MNV2) Decoder

0.3 (7.4)

0.3

0.6 (16.5)

0.9

1.0 (30.6)

1.3

1.8 (54.1)

1.5

2.3 (66.5)

1.7

Total 0.6 1.5 2.3 3.3 4.0

Inference speed (FPS) Backbone Prediction Total

201.6 132.8 124.7 117.9 107.6

881.9 883.4 885.1 885.7 883.4

164.1 115.4 109.3 104.0 95.9

Performance F H sAP 10 LAP 77.2 58.0 57.9 80.0 62.1 61.5 79.9 62.8 62.4 79.7 62.5 62.6 80.2 62.8 62.1

(a) Ablation study by varying the parts used from the MobileNetV2 (MNV2) for the encoder architecture. Performance is reported on Wireframe dataset. `% of MNV2' indicates the percentage of parameters used in each type of encoder compared to the total parameters used in MobileNetV2.

Model M-LSD-tiny 4 5 6 7

Setup Block type A: 1 × 1 conv / B: pre-residual / C: dilated rate 5
Block type A: 1 × 1 conv  3 × 3 conv Block type B: pre-residual  post-residual
Block type C: dilated rate 5  1 Block type C: dilated rate 5  3

Params (M) 0.6 0.7 0.7 0.6 0.6

Inference speed (FPS) Backbone Prediction Total

201.6 199.2 200.5 215.2 203.5

881.9 881.9 881.9 881.9 881.9

164.1 162.5 163.4 173.0 165.3

Performance F H sAP 10 LAP 77.2 58.0 57.9 76.7 58.1 57.9 76.9 58.1 58.0 75.9 56.1 56.0 76.7 57.6 57.4

(b) Ablation study by varying block types for the decoder architecture. Performance is reported on Wireframe dataset with M-LSD-tiny as the baseline. Block type A  B are from Figure 3 and Figure A.

Table B. Ablation study on encoder and decoder architectures.

speed but conversely decrease the performance. This is because the dilated convolution can effectively manage long line segments, which require large receptive fields. Thus, we choose to use 1 × 1 convolution in block type A, `preresidual' in block type B, and the dilated rate of 5 in block type C. B.2. Needs of Offset Maps
In some of the previous LSD methods [3, 6, 5], offset maps are used to estimate offsets between the predicted map and input image because the predicted map has a smaller resolution than the input image. We perform experiments and evaluate the effectiveness of offset maps with M-LSDtiny. When we apply offset maps to M-LSD-tiny, we need two offset maps for the center point (one for each coordinate). As shown in Table C, w/ offset maps increase in model parameters and decrease in inference speed, while the performance does not change. This demonstrates that offset maps are unnecessary for M-LSD-tiny because the resolution of the input image is two times the size of the resolution of predicted maps, which is minor. Thus, we disregard offset maps in M-LSD architectures. B.3. Impact of SoL Augmentation
In SoL augmentation, the number of internally dividing points k is based on the length of the line segment and computed as k = r(l)/(µ/2) - 1, where r(l) denotes the length of line segment l, and µ is the base length of the subparts. Note that when k  1, we do not split the line segment. When dividing the line segment, the base length

Setup w/o offset w/ offset

Params 629253 629383

Inference speed (FPS) Backbone Prediction Total

201.6 201.6

881.9 811.4

164.1 161.5

Performance F H sAP 10 LAP 77.2 58.0 57.9 77.2 57.9 57.9

Table C. Experiments of w/o and w/ offset maps in M-LSDtiny on Wireframe dataset.

of subparts µ is determined by µ = input size × . We conduct an experiment to investigate the impact of ratio in Table D. Small ratio will split line segments with a shorter length while producing a greater number of subparts, and vice versa when using a large ratio . As shown in Table D, although a small ratio produces a large number of augmented line segments, performance improvement is small. This is because the center and end points of small subparts are too close to each other to be distinguished, and thus become distractions for the model. Using a large ratio
also shows small performance improvement because not only does the amount of augmented line segments decrease, but also these line segments result to resemble the original line segment. We observe the proper ratio is 0.125, which produces enough number of augmented line segments with different lengths and location from the originals.
When applying SoL augmentation, we split line segments into multiple subparts with overlapping portions with each other. To see the impact of retaining such overlap in SoL augmentation, we conduct an experiment as shown in Table E. W/o overlap shows a smaller performance boost than that of w/ overlap. Hence we conclude that using a

µ # origin # aug # total F H sAP 10 LAP

0.000 - 374884

0

374884 76.2 55.1 55.3

0.050 25.6 374884 851555 1226439 76.2 56.2 56.3

0.100 51.2 374884 251952 626836 76.4 57.2 57.3

0.125 64.0 374884 151804 526688 77.2 58.0 57.9

0.150 76.8 374884 102719 477603 77.0 57.5 57.9

0.200 102.4 374884 47500 422384 76.6 56.8 56.5

0.300 153.6 374884 12123 387007 76.6 56.1 56.7

0.400 204.8 374884 3250 378134 76.4 55.5 56.1

0.500 256.0 374884 170 375054 76.2 55.0 55.7

Table D. Impact of ratio in SoL augmentation with MLSD-tiny on Wireframe dataset. = 0.0 is the baseline with no SoL augmentation applied. The base length of subpart µ is computed by µ = input size × . `# origin', `# aug', and `# total' denote the number of original, augmented, and total line segments.

# origin # aug # total F H sAP 10 LAP

baseline

374884

-

374884 76.2 55.1 55.3

w/ overlap 374884 151804 526688 77.2 58.0 57.9

w/o overlap 374884 41101 415985 76.4 56.7 56.7

Table E. Impact of overlapping in SoL augmentation with M-LSD-tiny on Wireframe dataset. The baseline is not trained with SoL augmentation. `# origin', `# aug', and `# total' denote the number of original, augmented, and total line segments.



Input size 320

Input size 512

F H sAP 10 LAP F H sAP 10 LAP

0.0 75.9 47.1 44.9 76.1 55.1 54.8 2.5 76.2 50.4 48.9 76.5 57.2 57.2 5.0 76.8 51.3 50.1 77.2 58.0 57.9 7.5 76.0 49.0 48.5 76.8 58.5 57.2 10.0 75.0 45.1 45.0 76.8 57.8 56.7 12.5 74.1 43.1 43.2 76.2 56.7 55.8 15.0 74.2 42.7 42.8 75.7 54.0 53.2 20.0 73.6 41.4 42.1 75.1 51.0 50.6

Table F. Impact of matching loss threshold  with M-LSDtiny on Wireframe dataset.  = 0.0 is the baseline with no matching loss applied.

larger number of augmented lines and preserving connectivity among subparts with overlaps can yield higher performance than without overlaps. B.4. Threshold of Matching Loss
In the matching loss, the threshold  decides whether to match the predicted and GT line segments. When  is small, the matching condition becomes strict, where the predicted line would be matched only with a highly similar GT line. When  is large, the matching condition becomes lenient,

where the predicted line would be easily matched with the GT line even if it is not similar. We conduct an experiment to see the impact of the threshold  in matching loss. As shown in Table F, when the threshold is high (  10.0), the matching condition is too broad, and poses a higher chance of predicted lines matching with non-similar GT lines. This becomes a distraction and shows performance degradation. On the other hand, when the threshold is too low ( = 2.5), the matching condition is strict and consequently restrains the effect of the matching loss to be minor due to the small number of matched lines. We observe that a value around 5.0 is the proper threshold , which provides optimal balance.
B.5. Precision and Recall Curve
We include Precision-Recall (PR) curves of sAP 10 for L-CNN [6], HAWP [5], TP-LSD [2], and M-LSD (ours). Figure C shows comparisons of PR curves on Wireframe and YorkUrban datasets.
References
[1] Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding, Shenghua Gao, and Yi Ma. Learning to parse wireframes in images of man-made environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 626­635, 2018. i
[2] Siyu Huang, Fangbo Qin, Pengfei Xiong, Ning Ding, Yijia He, and Xiao Liu. Tp-lsd: Tri-points based line segment detector. arXiv preprint arXiv:2009.05505, 2020. i, ii, iv
[3] Quan Meng, Jiakai Zhang, Qiang Hu, Xuming He, and Jingyi Yu. Lgnn: A context-aware line segment detector. In Proceedings of the 28th ACM International Conference on Multimedia, pages 4364­4372, 2020. iii
[4] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510­4520, 2018. i
[5] Nan Xue, Tianfu Wu, Song Bai, Fudong Wang, Gui-Song Xia, Liangpei Zhang, and Philip HS Torr. Holistically-attracted wireframe parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2788­2797, 2020. iii, iv
[6] Yichao Zhou, Haozhi Qi, and Yi Ma. End-to-end wireframe parsing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 962­971, 2019. i, iii, iv

Precision Precision

0.9

0.8

0.7

f=0.8

0.6

f=0.7

0.5

L-CNN(512)

f=0.6

0.4

HAWP(512) TP-Lite(320)

0.3

TP-Res34(320) TP-Res34(512)

0.2

M-LSD-tiny(320) M-LSD-tiny(512)

0.1

M-LSD(320) M-LSD(512)

f=0.5
f=0.5 f=0.4 f=0.3 f=0.2

0.00.0 0.1 0.2 0.3 0.4 R0ec.5all 0.6 0.7 0.8 0.9

L-CNN(512)

0.9

HAWP(512)

0.8 0.7

TP-Lite(320) TTPP--RReess3344((35f21=020)).8 M-LSD-tiny(320)

0.6

M-LSD-tiny(f=5102.7) M-LSD(320)

0.5

M-LSD(512f)=0.6

0.4

f=0.5

0.3

f=0.5 f=0.4

0.2

f=0.3

0.1

f=0.2

0.00.0 0.1 0.2 0.3 0.4 R0ec.5all 0.6 0.7 0.8 0.9

(a) Wireframe dataset

(b) YorkUrban dataset

Figure C. Precision-Recall (PR) curves of sAP 10 on Wireframe and YorkUrban datasets. (320) and (512) denote input image size.

