Statistical tests based on R´enyi entropy estimation
Mehmet Siddik Cadirci1, Dafydd Evans1, Nikolai Leonenko1 and Oleg Seleznjev2
1 School of Mathematics, Cardiff University, Cardiff, Wales, UK. 2 Department of Mathematics and Mathematical Statistics, Ume°a University, Ume°a,
Sweden.
June 2, 2021

arXiv:2106.00453v1 [math.ST] 1 Jun 2021

Abstract
Entropy and its various generalizations are important in many fields, including mathematical statistics, communication theory, physics and computer science, for characterizing the amount of information associated with a probability distribution. In this paper we propose goodness-offit statistics for the multivariate Student and multivariate Pearson type II distributions, based on the maximum entropy principle and a class of estimators for R´enyi entropy based on nearest neighbour distances. We prove the L2-consistency of these statistics using results on the subadditivity of Euclidean functionals on nearest neighbour graphs, and investigate their rate of convergence and asymptotic distribution using Monte Carlo methods. In addition we present a novel iterative method for estimating the shape parameter of the multivariate Student and multivariate Pearson type II distributions.

1 Introduction

Entropy is a measure of randomness that emerged from information theory, and its estimation plays an important role in many fields including mathematical statistics, cryptography, machine learning and indeed almost every branch of science and engineering. There are many possible definitions of entropy, for example, the differential entropy of a multivariate density function f : Rm  R is defined by

H1(f ) = - f (x) log f (x) dx.

(1)

Rm

In this paper, we propose statistical tests for a class of multivariate Student and Pearson type II distribtuions, based on estimation of their R´enyi entropy

1 Hq(f ) = 1 - q log

f q(x) dx,
Rm

q = 1.

(2)

1

Estimation of R´enyi entropy for absolutely continuous multivariate distributions has been considered by many authors, including [15], [10], [7], [17], [16], [21], [5], [9], [2], and [1]. The quadratic R´enyi entropy was investigated by [18]. An entropy-based goodness-of-fit test for generalized Gaussian distributions is presented by [3]. A recent application to image processing can be found in [6].
The remainder of this paper is organized as follows. In Section 2, we present maximum entropy principles for R´enyi entropy. In Section 3, we provide nearestneighbour estimators for R´enyi entropy. In Section 4, we propose statistical tests for the multivariate Student and Pearson II distributions. In Section 5, we report the results of numerical experiments.

2 Maximum entropy principles

Let X  Rm be a random vector that has a density function f (x) with respect to Lebesgue measure on Rm, and let S = {x  Rm : f (x) > 0} be the support of
the distribution. The R´enyi entropy of order q  (0, 1)(1, ) of the distribution

is

1 Hq(f ) = 1 - q log

f q(x) dx,
S

(3)

which is continuous and non-increasing in q. If the support has finite Lebesgue

measure |S|, then

lim Hq(f ) = log |S|,
q0

otherwise Hq(f )   as q  0. Note also that

lim Hq(f ) = H1(f ) = - f (x) log f (x) dx.

q1

S

Let a  Rm and let  be a symmetric positive definite m × m matrix. The multivariate Gaussian distribution Nm(a, ) on Rm has density function

faG,(x) = (2)-m/2||-1/2 exp

1 - (x - a)

-1(x - a)

2

.

For X  Nm(a, ), we have a = E(X) and  = Cov(X), where Cov(X) = E[(X - a)(X - a) ] is the covariance matrix of the distribution.

For  > 0, the multivariate Student distribution Tm(a, , ) on Rm has density function

faS,, (x) = cS ||-1/2

1

+

1 (x

-

a)

-1(x

-

a)

-

 +m 2



[( + m)/2]

where cS(m, ) = ()m/2(/2) .

(4)

For X  Tm(a, , ) we have a = E(X) when  > 1 and  = (1 - 2/)Cov(X) when  > 2, see [13]. It is known that faS,, (x)  faG,(x) as   .

2

For  > 0, the multivariate Pearson Type II distribution Pm(a, , ) on Rm, also known as the Barenblatt distribution, has density function

faP,, (x) = cP ||-1/2

1 - (x

- a)

-1(x

- a)

 +

(m/2 +  + 1)

where t+ = max{t, 0} and cP (m, ) = m/2( + 1) .

(5)

For X  Pm(a, , ) we have a = E(X) and  = (m + 2 + 2)Cov(X). It is known that faP,, (x)  faG,(x) as   .

Remark 1. If the covariance matrix C is diagonal, the Pearson Type II distribution belongs to the class of time-dependent distributions

u(x, t) = c(, )t-m

1-

x ct



+

with c > 0, supp{u(x, t)} = {x  Rm : x  ct} and

c(, ) = 

m

/

2cm



m 2

B

m , +1

,

2



which are known as Barenblatt solutions of the source-type non-linear diffusion equations ut = (uq), where q > 1,  is the Laplacian and  = 1/(q - 1). For details, see [8], [24] and [11].

2.1 R´enyi entropy

The R´enyi entropy of the multivariate Gaussian distribution Nm(a, ) is

Hq(faG,) = log

(2)m/2||1/2

m

-

log q

2(1 - q)

=

H1(faG,)

-

m 2

log q 1+
1-q

where H1(faG,) = log (2e)m/2||1/2 is the differential entropy of Nm(a, ).

From [26], the R´enyi entropy of the multivariate Student distribution Tm(a, , )

is

Hq faS,,

1 = 2 log || + cS(m, q, )

(6)

where

1 cS(m, q, ) = 1 - q log

B

q

+m 2

-

m 2

,

m 2

B

 2

,

m 2

q

m

m

+ log() - log 

.

2

2

Likewise the R´enyi entropy of the multivariate Pearson Type II distribution

Pm(a, , ) is

Hq faP,,

1 = 2 log || + cP (m, q, ),

(7)

3

where

1 cP (m, q, ) = 1 - q log

B

q

+

1,

m 2

B



+

1,

m 2

q

m

m

+ log() - log 

.

2

2

2.2 Maximum entropy principle
Definition 2. Let K be the class of density functions supported on Rm and subject to the constraints

xf (x) dx = a and

(x - a)(x - a) f (x) dx = C

Rm

Rm

where a  Rm and C is a symmetric and positive definite m × m matrix.

It is well-known that the differential entropy H1 is uniquely maximized by the multivariate normal distribution Nm(a, ), that is
H1(f )  H1(faG,) = log (2e)m/2||1/2
with equality if and only if f = faG, almost everywhere. The following result is discussed by [14], [19], [12], and [13] among others.
Theorem 3 (Maximum R´enyi entropy). (1) For m/(m + 2) < q < 1, Hq(f ) is uniquely maximized over K by the multivariate Student distribution Tm(a, , ) with  = 1/(1 - q) - m and  = (1 - 2/)C.
(2) For q > 1, Hq(f ) is uniquely maximized over K by the multivariate Pearson Type II distribution Pm(a, , ) with  = 1/(q - 1) and  = (2 + m + 2)C.

Applying (6) and (7) yields the following.

Corollary 4. (1) For m/(m + 2) < q < 1 the maximum value of Hq is

Hqmax

=

1 2

log

||

+

cS(m, q, )

with  = 1/(1 - q) - m and  = (1 - 2/)C. (2) For q > 1 the maximum value of Hq is

Hqmax

=

1 2

log ||

+ cB(m, q, )

with  = 1/(q - 1) and  = (2 + m + 2) C.

4

3 Statistical estimation of R´enyi entropy

We state some known results on the statistical estimation of R´enyi entropy due
to [17], and [21]. Extensions of these results can be found in [20], [1], [5], [2], and [9]. Let X  Rm be a random vector with density function f , and let Gq(f ) denote the expected value of f q-1(X),

Gq(f ) = E f q-1(X) = f q(x) dx
Rm

so

that

Hq(f )

=

1 1-q

log Gq(f ).

Let X1, X2, . . . , XN be independent random vectors from the distribution of X, and for k  N with k < N , let i,k,N denote the k-nearest neighbour distance of Xi among the points X1, X2, . . . , XN , defined to be the kth order statistic of the N - 1 distances Xi - Xj with j = i,

i,1,N  i,2,N  · · ·  i,N-1,N .

We estimate the expectation Gq(f ) = E(f q-1) by the sample mean

G^ k,N ,q

=

1 N

N

(i,k,N )1-q ,

i=1

where

i,k,N = (N - 1)CkVmm i,k,N

1

(k)

1-q

with Ck = (k + 1 - q)

m

and

Vm

=

2

(

m 2

+1)

is

the

volume

of

the

unit

ball

in

Rm.

Definition 5. For r > 0, the r-moment of a density function f is

Mr(f ) = E( X r) =

x rf (x) dx,

Rm

and the critical moment of f is

rc(f ) = sup{r > 0 : Mr(f ) < }

so that Mr(f ) <  if and only if r < rc(f ).

The following result was stated without proof in [16]: here we present the proof. Theorem 6. Let 0 < q < 1 and k  1 be fixed.

1. If Gq(f ) <  and

m(1 - q)

rc(f ) >

, q

(8)

then E G^k,N,q  Gq(f ) as N  .

(9)

5

2.

If

Gq(f ) < ,

q

>

1 2

and

2m(1 - q)

rc(f ) >

, 2q - 1

(10)

2

then E G^k,N,q - Gq(f )  0 as N  .

(11)

Remark 7. If Gq(f ) <  for q 

1,

k+1 2

then by [17],

E G^k,N,q

 Gq(f ) and E

G^k,N,q - Gq(f )

2
 0 as N  .

Remark 8. If Gq(f ) <  for q  (0, 1) and f (x) = O( x -) as x   for some  > m, then rc(f ) =  - m and condition (8) is automatically satisfied: see [21] for a discussion, and counterexamples showing that conditions (8) and (10) cannot be omitted in general.

Proof of Theorem 6. Let us write

G^ k,N ,q

=

1 N

N

(1-q)m

(N - 1)1/m(CkVk)1/mi,k,N

.

i=1

We show that the method proposed by [22] for k = 1 in fact works for any fixed k  1. By Theorem 2.1 of [22], the uniform integrability condition

sup E ((N - 1)(CkVk)m i,k,N-1 (1-q)p < 
N

(12)

for some p > 1 (statement 1) or some p > 2 (statement 2) ensures the Lp convergence of G^k,N,q to Iq as N  . Because we only need to obtain a bound on left-hand side of (12), we can use results on the subadditivity of
Euclidean functionals defined on the nearest-neighbors graph [25]. We use the
following result (Lemma 3.3) from [21], see also [25, p.85].

Lemma

9.

Let

0 < s < m.

If rc(f ) >

ms m-s

,

then



2js

[P

(Aj

)]

m-s m

<

where

P (Aj) =

f (x) dx

j=1

Aj

and Aj = B(0, 2j+1) \ B(0, 2j) for j = 1, 2, . . .

with B(0, R) = {x  Rm : x  R} and A0 = B(0, 2).

We continue the proof of Theorem 6. Let b = (1 - q)mp, and note that we can always choose p to ensure that 0 < 1 - b/m < 1. By exchangeability,

b
E (N - 1)1/m(CkVm)1/mi,k,N-1

=

E

1N N

b
(N - 1)1/m(CkVm)1/mi,k,N-1

i=1

=

(N

- 1)b/m N

(Ck Vm )b/m E

N
bi,k,N -1

i=1

 (CkVm)b/m(N - 1)b/m-1E(Lbk(XN )),

6

where XN = {X1, X2, . . . , XN }, and for any finite point set X  Rm and b > 0 we write
Lbk(X ) = Dkb (x, X ),
xX

where Dkb (x, X ) denotes the Euclidean distance from x to its k-nearest neighbour in the point set X \ {x} when card(X )  k; set Dkb (x, X ) = 0 if card(X )  k. The function X  Lbk(X ) satisfies the subadditivity relation

Lbk(X  Y)  Lbk(X ) + Lbk(Y) + Uktb

(13)

for all t > 0 and finite X and Y contained in [0, t]m, where Uk = 2kmb/2, b > 0. Indeed, if X has more than k elements, the k-nearest neighbour distances of
points in X can only become smaller when we add some other set Y. Hence,
(13) holds with Uk = 0 if X and Y have more than k elements. If X has k elements or fewer, then Lbk(X ) is zero, but when we add the set Y, we gain at most k new edges from points in X in the nearest neighbours graph, and each of these is of length most t m (for more details, see [25, pp 101-103]).

Let s(N ) be the largest j  N such that the set XN = {X1, X2, . . . , XN }  Aj is not empty. Using ideas from [25, p.87] we have that

s(N)  s(N)

XN   Aj = (XN  Aj),

j=0

j=0

and by the subadditivity property,

Lbk(XN )  Lbk{XN  As(N)}




s(N )-1







+Lbk XN 

Aj  + Uk2(s(N)+1)b.

 j=0 

Applying subadditivity in the same way to the second term on the right yields




s(N )-1







Lbk XN 

Aj   Lbk(XN  As(N)-1)

 j=0 




s(N )-2







b

+Lbk XN 

Aj  + Uk 2s(N) .

 j=0 

Repeatedly applying subadditivity, we arrive at

s(N )

Lbk (X1, . . . , XN ) 

Lbk (XN



Aj )

+

2b+bs(N ) 1

Uk - 2-b

j=0

s(N )



Lbk(XN  Aj ) + 2bs(N)Mk

j=0

s(N )



Lbk (XN



Aj )

+

Mk

max
1iN

Xi b

j=0

(14)

7

for some constant Mk depending on m, k and b. From (13) and (14), we get

b
E (N - 1)1/m(CkVm)1/mi,k,N-1





s(N )

 (CkVm)b/m(N - 1)b/m-1E  Lbk(XN  Aj )

j=0

+ Wk E (N - 1)b/m-1 max Xi b
1iN

(15)

for some constant Wk depending on m, k and b. Using Lemma 3.3 of [25] we

have

Lbk(X )  L0(diamX )b(cardX )1-b/m

(16)

for some constant L0 > 0. Following [21], by Jensen's inequality and the fact that diam(Aj) = 2j, we obtain from (15) and (16) that

s(N )



(N - 1)b/m-1E  Lbk(XN  Aj )

j=0

s(N )

 L1

2jb [P(X1  Aj )]1-b/m

j=0

(17)

where L1 > 0 is a constant.
Recall our assumptions that 0 <  < m/ where  {1, 2} and  = (1 - q)m, and also that rc(f ) > ( m)/(m - ). Setting s = b in Lemma 9, we see that the left hand side of (17) is finite, so the first term on the right hand side of (15) is bounded by a constant which is independent of N . For a non-negative random variable Z > 0, we know that


E(Z) = P(Z > z) dz,
0

so the second term in (15) is bounded by



Wk

P max Xi b > u · N 1-b/m du

0

1iN



1-b/m

 Wk 1 + N

P X1 b > um/(m-b)N

du

1

(18)

By

the

Markov

inequality

P(Z

>

a)



1 a

E|Z

|

for

a

>

0,

we

get

for

u



1

that

1-b/m
P X1 b > um/(m-b)N

= P X1 mb/(m-b) > um/(m-b)N



E

X1

mb/(m-b)

1

.

um/(m-b)N

(19)

8

From (18) and (19), we see that the second term in (15) is bounded by

Wk

1+


E
1

X1

mb/(m-b)

1

um/(m-b)

du

which is independent of N , because we can choose p to ensure that 0 < 1-b/m < 1, and

mp(1-q)

mp(1 - q)

E

X1

< , 1-p(1-q)

or

equivalently

rc(f )

>

1-

, p(1 - q)

which is consistent with conditions of Theorem 2.1. Note that the function

h(p, q)

=

mp(1-q) 1-p(1-q)

is

such

that

h(1, q)

gives

the

right-hand

side

of

(8)

and

h(2, q) gives the right-hand side of (10). Moreover, if rc(f ) > h(1, q) for some

q < 1 (resp. rc(f ) > h(2, q) for some q satisfying 1/2 < q < 1), we also have

rc(f ) > h(p, q) for some p > 1 (resp. rc(f ) > h(p, q) for p > 2).

4 Hypothesis tests

We now restrict the class K to only those distributions which satisfy the following conditions: for any fixed k  1 and q > 1/2,
E(H^N,k,q)  Hq as N  , and H^N,k,q  Hq in probability as N  .
By Theorem 6, we know that K contains Tm(a, , ) for all  > 2 and Pm(a, , ) for all  > 0.

Let X1, X2, . . . , XN be independent and identically distributed random vectors with common density f  K, and let C^N be the sample covariance matrix,

C^N

=

N

1 -1

N
(Xi - X¯ )(Xi - X¯ ) .

i=1

1. To test the hypothesis X  Tm(a, , 0) where 0 > 2, we define the test

statistic

WNS,k(m, ) = Hqmax - H^N,k(m, q),

(20)

where

Hqmax

=

1 2

log |^ N | + cS(m, q, )

with

q

=

1 - 1/(

+ m)

and

^ N

=

(1 - 2/)C^N .

2. To test the hypothesis X  Pm(a, , 0) where 0 > 0, we define the test

statistic

WNP,k(m, ) = Hqmax - H^N,k(m, q),

(21)

where

Hqmax

=

1 2

log

|^ N |

+

cP

(m,

q,

)

with

q

=

1 + 1/

and

^ N

=

(2 + m + 2)C^N .

9

By the law of a large numbers, C^N  C in probability as N  , so by Slutsky's theorem, for any fixed k  1, we have that

lim
N 

WNS ,k (m,

)

-P

0 c>0

if X  Tm(a, , ), otherwise,

and

lim
N 

WNP,k (m,

)

-P

0 c>0

if X  Pm(a, , ), otherwise,

where "-P" denotes convergence in probability and c is a constant that depends
on the distribution of X.
The distributions of WNP,k(m, ) when X  Tm(a, , ) and WNP,k(m, ) when X  Pm(a, , ) are unknown. An analytical derivation of these distributions seems difficult, because the random variables H^N,k and C^N are not independent and their covariance appears to be intractable, despite the fact that the asymptotic distribution of H^N,k can be revealed by applying the results of [4], [21], [5] and [1], and that of C^N by the delta method. In the next section, we investigate these null distributions using Monte Carlo methods.

5 Numerical experiments
5.1 Random samples
Random samples from Tm(a, , ) and Pm(a, , ) can be generated according to the stochastic representation
X = RBU + a,
where R represents the radial distance (X - a) -1(X - a) 1/2, B is an m × m matrix with BT B =  and U is uniformly distributed on the unit m-sphere Sm-1. In particular,
R2  InvGamma(m/2, m/2) yields X  Tm(a, , ), and R2  Beta(m/2,  + 1) yields X  Pm(a, , ).
Let Im be the m × m identity matrix. We investigate the distributions
Tm() = Tm 0, Im,  for  > 2 and Pm() = Pm 0, Im,  for  > 1.

5.2 Consistency
To investigate the consistency of WNS,k(m, ) for various values of m and , we generate M = 100 random samples of size N from the Tm() distribution, with N increasing from N = 500 to N = 5000 in steps of 500, and record the value
10

(a) Scatter plots for T2()

(b) Scatter plots for P2()

Figure 1: Scatter plots for the bivariate Student and Pearson II distributions.

of WNS,k(m, ) for k = 1, 2, 3 at each step. The mean values of the statistics for k = 1 are shown in Figure 2, where the lengths of the error bars are equal to the standard deviations of the statistics around their mean values. The mean statistics for k = 1, 2, 3 are shown in Figure 3, where it is evident that the rate of convergence increases with the parameter  and decreases with the dimension m.
The experiment is repeated for WNP,k(m, ) but this time with samples increasing in size from N = 50 to N = 500 in steps of 50. The mean values of the statistics for k = 2 are shown in Figure 4, where lengths of the error bars are equal to the standard deviations of the statistics around their mean values. The mean statistics for k = 1, 2, 3 are shown in Figure 5: note that these are only defined for k > 1/. The convergence of WNP,k(m, ) is evidently much faster than that of WNS,k(m, ), perhaps because the support of Pm() is bounded for any finite  > 0 while the support of Tm() is unbounded.

11

5.2.1 Rates of convergence
In Figure 6, we plot the convergence of WNS,k(m, ) as N   with m = 2, k = 1 and  = 5, together with the corresponding plot of log WNS,k(m, ) against log N . THe latter suggests an empirical convergence rate of approximately O(N -1/2) as N  . The experiment is repeated for WNP,k(m, ) with m = 2, k = 2 and  = 2. The results are shown in Figure 7, which in this case suggest an empirical convergence rate of approximately O(N -2/3) as N  . Analytic rates of convergence for WNS,k(m, ) and WNP,k(m, ) are currently under investigation by the authors.
5.3 Empirical distribution of the test statistics
For different values of (N, k) and (m, ), we generate n = 100 random samples of size N from the Tm() distribution and record the value of WNS,k(m, ) each time. We then apply the Shapiro-Wilk test for normality [23] to this random sample and record the probability value computed by the test. This process is repeated M = 1000 times. Figure 8 illustrates how the mean probability value behaves as N increases for various values of m, k and , where it appears that normal approximation improves as the distribution parameter  increases, but deteriorates as k increases. The experiment is repeated for the null distribution of WNP,k(m, ) with the results shown in Figure 9, where it appears that normal approximation again improves as the distribution parameter  increases, but in this case appears to also improve as k increases.
5.4 Point estimation
Point estimates for  and  can be computed according to
^ = argmin>2WNS,k(m, ), and ^ = argmin>1WNP,k(m, ) respectively.
A random sample of size N = 1000 was generated from the T3() distribution with  = 4, and the value of W1S000,1(3, ) was then computed for different values of  in the range [2.5, 10]. The results are shown in Figure 10, where we see that the statistic reaches a minimum value at approximately  = 4. Note that because we take  = Im, the estimated determinant |^ | is approximately equal to 1 when  = 4.
The experiment is repeated for a random sample from the P3() distribution with  = 3, and the value of W1P000,1(3, ) computed for different values of  in the range [1, 6]. The results are shown in Figure 11, where we see that the statistic reaches a minimum value at approximately  = 3.
The theoretical properties of these estimators are currently under investigation by the authors.
12

References
[1] T. B. Berrett, R. J. Samworth, and M. Yuan. Efficient multivariate entropy estimation via k-nearest neighbour distances. Annals of Statistics, 47(1):288­318, 2019.
[2] A. Bulinski and D. Dimitrov. Statistical estimation of the Shannon entropy. Acta Mathematica Sinica, English Series, 35(1):17­46, 2019.
[3] M. S. Cadirci, D. Evans, N. N. Leonenko, and V. Makogin. Entropy-based test for generalized gaussian distributions. arXiv:2010.06284, 2020.
[4] S. Chatterjee. A new method of normal approximation. Annals of Probability, 36(4):1584­1610, 2008.
[5] S. Delattre and N. Fournier. On the Kozachenko­Leonenko entropy estimator. Journal of Statistical Planning and Inference, 185:69­93, 2017.
[6] D. Dresvyanskiy, T. Karaseva, V. Makogin, S. Mitrofanov, C. Redenbach, and E. Spodarev. Detecting anomalies in fibre systems using 3-dimensional image data. Statistics and Computing, 30(4):817---837, 2020.
[7] D Evans. A computationally efficient estimator for mutual information. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 464(2093):1203­1215, 2008.
[8] T. D. Frank. Nonlinear Fokker-Planck Equations: Fundamentals and Applications. Springer Science & Business Media, 2005.
[9] W. Gao, S. Oh, and P. Viswanath. Demystifying fixed k-nearest neighbor information estimators. IEEE Transactions on Information Theory, 64(8):5629­5661, 2018.
[10] M. N. Goria, N. N. Leonenko, V. V. Mergel, P. L. Luigi, N. Inverardi, and P. Luigi. A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3):277­297, 2005.
[11] A. De Gregorio and R. Garra. Alternative probabilistic representations of barenblatt-type solutions. In Modern Stochastics: Theory and Applications, pages 1­16. VTeX, 2020.
[12] C. C. Heyde and N. N. Leonenko. Student processes. Advances in Applied Probability, 37(2):342­365, 2005.
[13] O. Johnson and C. Vignat. Some results concerning maximum R´enyi entropy distributions. Annales de l'IHP Probabilit´es et Statistiques, 43:339­ 351, 2007.
[14] S. Kotz and S. Nadarajah. Multivariate t-distributions and their applications. Cambridge University Press, 2004.
[15] L. F. Kozachenko and N. N. Leonenko. A statistical estimate for the entropy of a random vector. Problems of Information Transmission, 23(2):9­16, 1987.
13

[16] N. N. Leonenko and L. Pronzato. Correction: A class of r´enyi information estimators for multidimensional densities. Annals of Statistics, 38(6):3837­ 3838, 2010.
[17] N. N. Leonenko, L. Pronzato, and V. Savani. A class of R´enyi information estimators for multidimensional densities. Annals of Statistics, 36(5):2153­ 2182, 2008.
[18] N. N. Leonenko and O. Seleznjev. Statistical inference for the -entropy and the quadratic r´enyi entropy. Journal of Multivariate Analysis, 101(9):1981­ 1994, 2010.
[19] E. Lutwak, D. Yang, and G. Zhang. Moment-entropy inequalities. Annals of Probability, 32(1B):757­774, 2004.
[20] M. D. Penrose and J. E. Yukich. Weak laws of large numbers in geometric probability. Annals of Applied Probability, 13(1):277­303, 2003.
[21] M. D. Penrose and J. E. Yukich. Laws of large numbers and nearest neighbor distances. In Advances in Directional and Linear Statistics, pages 189­ 199. Springer, 2011.
[22] M. D. Penrose and J. E. Yukich. Limit theory for point processes in manifolds. The Annals of Applied Probability, 23(6):2161­2211, 2013.
[23] S. S. Shapiro and M. B. Wilk. An analysis of variance test for normality (complete samples). Biometrika, 52:591­­611, 1965.
[24] J. L. V´azquez. The Porous Medium Equation: Mathematical Theory. Oxford University Press, 2007.
[25] J. E. Yukich. Probability Theory of classical Euclidean optimization problems. Number 1675 in Lecture Notes in Mathematics. Springer, Berlin, 1998.
[26] K. Zografos and S. Nadarajah. Expressions for R´enyi and Shannon entropies for multivariate distributions. Statistics and Probability Letters, 71(1):71­84, 2005.
14

Figure 2: The asymptotic behaviour of WNS,k(m, ) as N   for k = 1. The rate of convergence appears to increase with  and decrease with m.
15

Figure 3: The asymptotic behaviour of WNS,k(m, ) as N  . The rate of convergence appears to increase with  and decrease with m.
16

Figure 4: Asymptotic behaviour of WNP,k(m, ) as N   for k = 2. 17

Figure 5: Asymptotic behaviour of WNP,k(m, ) as N  . Note that the statistic is defined only for k > 1/.
18

Figure 6: Asymptotic behaviour of WNS,k(m, ) with m = 2, k = 1 and  = 5.
Figure 7: Asymptotic behaviour of WNP,k(m, ) with m = 2, k = 1 and  = 2. 19

Figure 8: Average Shapiro-Wilk probability values for WNS,k(m, ) as N increases for different values of m, k and  (100 repetitions).
20

Figure 9: Average Shapiro-Wilk probability values for WNP,k(m, ) as N increases for different values of m, k and  (100 repetitions). Note that the statistic is only defined for k > 1/.
21

Figure 10: WNS,k(m, ) with N = 1000, k = 1, m = 3 and 0 = 4 (q0 = 0.86). The point estimate is ^ = 4.7.
Figure 11: WNP,k(m, ) with N = 1000, k = 1, m = 3 and 0 = 3 (q0 = 1.33). The point estimate is ^ = 2.8.
22

