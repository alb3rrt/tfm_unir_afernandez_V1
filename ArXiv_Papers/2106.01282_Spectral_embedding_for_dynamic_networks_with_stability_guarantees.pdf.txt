Spectral embedding for dynamic networks with stability guarantees
Ian Gallagher, Andrew Jones, and Patrick Rubin-Delanchy University of Bristol

arXiv:2106.01282v1 [stat.ML] 2 Jun 2021

Abstract
We consider the problem of embedding a dynamic network, to obtain time-evolving vector representations of each node, which can then be used to describe the changes in behaviour of a single node, one or more communities, or the entire graph. Given this open-ended remit, we wish to guarantee stability in the spatio-temporal positioning of the nodes: assigning the same position, up to noise, to nodes behaving similarly at a given time (cross-sectional stability) and a constant position, up to noise, to a single node behaving similarly across different times (longitudinal stability). These properties are defined formally within a generic dynamic latent position model. By showing how this model can be recast as a multilayer random dot product graph, we demonstrate that unfolded adjacency spectral embedding satisfies both stability conditions, allowing, for example, spatio-temporal clustering under the dynamic stochastic block model. We also show how alternative methods, such as omnibus, independent or time-averaged spectral embedding, lack one or the other form of stability.

1 Introduction

Consider a dynamic network in which nodes come and go, change abruptly or evolve, alone or
in communities, and form connections accordingly. Our goal is to find a vector representation, Y^ i(t)  Rd, for every node i and time t, which could be used for a diversity of downstream analyses, such as clustering, time series analysis, classification, model selection and more. This problem is
known as dynamic (or evolutionary) network (or graph) embedding, and a great number of scalable
and empirically successful techniques have been put forward, with recent surveys by [40, 44]. We
consider, among these, a subset about which it is reasonable to try to establish a certain statistical
guarantee.
The novelty of this paper is not to propose a new procedure. Instead, it is to demonstrate
that an existing procedure, unfolded adjacency spectral embedding (UASE) [14], has an important stability property. Given a sequence of symmetric adjacency matrices A(1), . . . , A(T )  {0, 1}n×n, where A(ijt) = 1 if nodes i and j form an edge at time t, UASE computes the rank d matrix factorisation of A := (A(1)| · · · |A(T )) to obtain A  X^ Y^ using the singular value decomposition (precise details later). The matrix Y^  RnT ×d contains, as rows, the desired representations Y^ 1(1), . . . , Y^ n(1), . . . , Y^ 1(T ), . . . , Y^ n(T ).
The original motivation for UASE was to analyse multilayer (or multiplex) graphs under an
appropriate extension of the random dot product graph model [14]. To evaluate UASE and other
procedures on the task of dynamic network embedding, we instead consider the dynamic latent
position model

A(ijt) ind Bernoulli f Z(it), Z(jt) ,

(1)

for 1  i < j  n, t  [T ], where Z(it)  Rk represents the unknown position of node i at time t, and f : Rk × Rk  [0, 1] is a symmetric function.
This model is well-established [34, 18, 12, 13, 28, 19, 7, 37, 9, 8], with recent reviews by [17] and [39], and inference usually proceeds on the basis of a parametric model for f (e.g. logistic in

1

the latent position distance [34]) and for the dynamics of Z(it) (e.g. a Markov process [34]). The model includes the dynamic standard, degree-corrected and mixed-membership stochastic block models as special cases, which were studied in [41, 45, 11, 24, 43, 42, 26, 2, 27, 16].
To make statistical sense of UASE under this latent position model, we must somehow connect its output Y^ i(t) to Z(it). To this end we construct a canonical representative of Z(it), denoted Yi(t), that UASE can be seen to estimate. Although we make some regularity assumptions on f and the Z(it), they are not modelled in an explicit, parametric way, and UASE can clearly be used in practice without having a specific model in mind. For example, we do not make a Markovian assumption on the evolution of Z(it) and UASE can be used to uncover periodic behaviours.
The key purpose of imposing a dynamic latent position model is to allow us to put down certain embedding stability requirements. Using this framework, we can define precisely what we mean by two nodes, i and j, behaving "similarly" at times s and t respectively. In such cases, ideally we would have Y^ i(s)  Y^ j(t). Two special cases, more easily seen to be desirable, are as follows: we assign the same position, up to noise, to nodes behaving similarly at a given time (cross-sectional stability) and a constant position, up to noise, to a single node behaving similarly across different times (longitudinal stability).
To achieve both cross-sectional and longitudinal stability is generally elusive. We show that two plausible alternatives, omnibus [21] and independent embedding of each A(t), alternately exhibit one form of stability and not the other. More generally, we find existing procedures [4, 22, 35, 6, 47, 5, 23, 3, 2, 27, 32, 16] tend to trade one type of stability off against the other, e.g. via user-specified cost functions. As a side-note, it could be observed that omnibus embedding is not being evaluated on a task for which it was designed, since in the theory of [21] the graphs are identically distributed. Because the technique is so different from the others, we still feel it makes an interesting addition.
Our central contribution is to prove that UASE asymptotically provides both longitudinal and cross-sectional stability: for two nodes, i and j, behaving similarly at times s and t respectively, we have Yi(s) = Yj(t) and, moreover, Y^ i(s) and Y^ j(t) have asymptotically equal error distribution. We emphasize that these properties hold without requiring any sort of global stability so, for example, if all nodes but one change behaviour, this last one will hold its position. In the asymptotic regime considered, we have n  , but T fixed so that, for example, our results are relevant to the case of two large graphs. The alternative regime where T   grows but n is fixed is not easily handled by existing theory and to provide constant updates to UASE (or omnibus embedding) in a streaming context presents significant computational challenges.
The remainder of this article is structured as follows. Section 2 gives a pedagogical example demonstrating the cross-sectional and longitudinal stability of UASE in a two-step dynamic stochastic block model, while highlighting the instability of omnibus and independent spectral embedding. In Section 3, we prove a central limit theorem for UASE under a dynamic latent position model, and demonstrate that the distribution satisfies both stability conditions. In Section 4, we review the stability of other dynamic network embedding procedures. Section 5 presents an example of UASE applied to a dynamic network of social interactions in a French primary school, with a dynamic community detection example given in the main text and a further classification example provided in the Appendix. Section 6 concludes.

2 Motivating example

Suppose it is of interest to uncover dynamic community structure, for example, communities splitting or merging, new communities being born or communities dying. The dynamic stochastic block model [45, 43] provides a simple explicit model for this, in which two nodes connect at a certain point in time with probability only dependent on their current community membership. Suppose that at times 1 and 2, we have the following inter-community link probability matrices,

0.08 0.02 0.18 0.10

0.16 0.16 0.04 0.10

B(1)

=

0.02 0.18

0.20 0.04

0.04 0.02

0.10 0.02

,

B(2)

=

0.16 0.04

0.16 0.04

0.04 0.09

0.10 0.02

.

0.10 0.10 0.02 0.06

0.10 0.10 0.02 0.06

2

At time 1 there are four communities present, for example, a node of community 1 connects with a node of community 3 with probability 0.18. At time 2, this matrix changes so that communities 1 and 2 have merged, community 3 has moved, whereas community 4 is unchanged.
We simulate a dynamic network from this model over these two time steps, on n = 1000 nodes, equally divided among the four communities and investigate the results of three embedding techniques: UASE, omnibus, and independent spectral embedding, displayed in Figure 1. Given the dynamics described above, we contend that the following properties would be desirable:

1. Cross-sectional stability: The embeddings for communities 1 and 2 at time 2 are close.

2. Longitudinal stability: The embeddings for community 4 at times 1 and 2 are close.

Figure 1 illustrates that UASE has both properties: the blue and orange point clouds merge at time 2, and the red point cloud has the same location and shape over the two time points. On the other hand, omnibus embedding only has longitudinal stability (the blue and orange point clouds don't merge at time 2), whereas independent spectral embedding only has cross-sectional stability (the red point cloud is at different locations over the two time points).

UASE Y(1)
0.4 0.2 0.0 0.2

Omnibus Y(1)
0.4 0.2 0.0 0.2

Independent Y(1)
0.4 0.2 0.0 0.2 0.4

0.4

UASE Y(2)
0.4 0.2 0.0 0.2

Omnibus Y(2)
0.4 0.2 0.0 0.2

Independent Y(2)
0.4 0.2 0.0 0.2 0.4

0.4 0.4 0.3 0.2 0.1

0.4 0.3 0.2

0.5 0.4 0.3 0.2 0.1

Community 1

Community 2

Community 3

Community 4

Figure 1: First two dimensions of the embeddings for the adjacency matrices A(1) and A(2) using three different techniques: UASE, omnibus and separate embedding. The points are coloured according to true community membership, with the black dots showing the fitted community centroid and the ellipses a fitted 95% level Gaussian contour.

3 Setup
We consider a sequence of random graphs G1, . . . , GT distributed according to a dynamic latent position model as described above, in which the latent position sequences Zi = (Z(i1)| · · · |Z(iT )) are independent of each other, and identically distributed according to a joint distribution F on ZT , for a bounded subset Z of Rk. We can associate to the function f a compact, self-adjoint operator A on L2(Rk) which has a--possibly infinite--sequence of non-zero eigenvalues 1  2  . . . with corresponding orthonormal eigenfunctions u1, u2, . . .  L2(Rk), such that Auj = juj (for a more detailed explanation of this construction, see for example [30]). We shall make the simplifying assumption that A has a finite number of non-zero eigenvalues, in which case f admits a canonical

3

eigendecomposition

D

f (x, y) = iui(x)ui(y),

(2)

i=1

assumed to hold everywhere, where after relabelling we may assume that |1|  . . .  |D|. The
individual graphs Gt defined as above can then be seen to be generated according to a GRDPG model [31]. Indeed, define a map  : Z  RD by setting the ith coordinate of (z) to be |i|1/2ui(z), and let X(it) = (Z(it)). Then f (Z(it), Z(jt)) = g(X(it), X(jt)), where g(x, y) = x Ip,qy and Ip,q = diag(sgn(1), . . . , sgn(D)); precisely the definition of a GRDPG model. The following
result shows that collectively the graphs are generated according to a multilayer RDPG model [14]:

Proposition 1. There exists a distribution G and matrices X, Y and  such that (A, X, Y)  MRDPG(G, ), and linear maps L : RT D  Rd and Lt : RD  Rdt (independent of n) such that Xi = L (X(i1)| · · · |X(iT )) and Yi(t) = Lt(X(it)), where Y = diag(Y(1)| · · · |Y(T )).
The dimensions dt and d are the asymptotic ranks of the Gram matrices P(t) = f (Z(it), Z(jt)) i,j[n] and their concatenation P = (P(1)| · · · |P(T )), bounded above by D and T D, respectively.
We can extend our model to incorporate a range of sparsity regimes by scaling the function f
by a sparsity factor n, which we assume is either constant and equal to 1, or else tends to zero as n grows, corresponding to dense and sparse regimes respectively. When this factor is present, the linear maps L and Lt (and consequently the points Xi and Yi(t)) in Proposition 1 are scaled by a factor of 1n/2, and the distribution G is replaced by a similarly scaled distribution G.
The UASE is constructed as follows. We form the concatenation A = (A(1)| · · · |A(T )), which
admits a singular value decomposition of the form

A = UAAVA + UA,A,VA,

(3)

where A contains the d largest singular values of A, and define the left and right embeddings

X^ = UA1A/2  Rn×d, Y^ = VA1A/2  RT n×d

(4)

and further divide Y^ into sub-embeddings Y^ (t)  Rn×d. Replacing A with P in this construction yields the noise-free embeddings X~ and Y~ (t), whose rows are known to be linear transformations of

the rows of X and Y(t) respectively (see Appendix for further details).

A desirable property of UASE is that since the matrices A(t) follow a multilayer RDPG model

there exist known asymptotic distributional results for the global embedding X^ and--of particular

interest to us--the graph-specific embeddings Y^ (t). In order to ensure the validity of these results,

we will make the assumption that the sparsity factor n satisfies n = 

logc (n) n1/2

for some universal

constant c > 1.

The first of our distributional results states that after applying an orthogonal transformation--

which leaves the structure of the resulting point cloud intact--the embedded points Y^ i(t) converge in the Euclidean norm to the noise-free embedded points Y~ i(t) as the graph size grows:

Theorem 2. There exists a sequence of orthogonal matrices W~  O(d) such that

max
i{1,...,n}

W~ Y^ i(t) - Y~ i(t)

= O log1/2(n)
n1/2 n1/2

(5)

almost surely for each t.
The second result (an extension of Theorem 2 in [14] incorporating ideas from [1]) states that after applying a second orthogonal transformation the above error converges in distribution to a fixed multivariate Gaussian distribution :

4

Theorem 3. For z  Z, define



 E f (z, t) 1 - f (z, t) · ()() if n = 1

t(z) =

(6)



E f (z, t) · ()()

if n  0

where  = (1| · · · |T )  F and () = L ((1)| · · · |(T )) for the linear map L appearing in Proposition 1. Then there exists a deterministic matrix R  Rd×d and a sequence of orthogonal matrices W  O(d) such that, given z  Z, for all y  Rd and for any fixed i,

P n1/2W(W~ Y^ i(t) - Y~ i(t))  y | Z(it) = z   y, Rt(z)R

(7)

for each t.

We can use these results to demonstrate one of the key advantages of using UASE over other embedding methods, namely that UASE exhibits both cross-sectional and longitudinal stability, in a sense that we shall now define. We say that two space-time positions (z, t) and (z , t ) are exchangeable if f (z, t) = f (z , t ) with probability one, where  = (1| · · · |T )  F , and that the positions are exchangeable up to degree if f (z, t) = f (z , t ) for some  > 0.
Definition 4. A generic method for embedding a dynamic network, whose output is denoted (Z^ (it))i[n];t[T ], is said to exhibit cross-sectional stability if, for exchangeable pairs (z, t) and (z , t), the points Z^ (it) and Z^ (jt) are asymptotically equal in distribution, conditional on Z(it) = z and Z(jt) = z . Similarly, the method exhibits longitudinal stability if Z^ (it) and Z^ (it ) are asymptotically equal in distribution, conditional on Z(it) = Zi(t ) = z, for exchangeable pairs (z, t) and (z, t ).
The following result--a consequence of Theorem 3--shows that UASE exhibits both types of stability:

Corollary 5. Conditional on Z(it) = z and Z(jt ) = z , the following properties hold:
1. If (z, t) and (z , t ) are exchangeable then Y~ i(t) = Y~ j(t ) and t(z) = t (z ), and so Y^ i(t) and Y^ j(t ) are asymptotically equal in distribution.

2. If (z, t) and (z , t ) are exchangeable up to degree then Y~ i(t) = Y~ j(t ), and, under a sparse regime, t(z) = t (z ). In the asymptotic limit Y^ i(t) and Y^ j(t ) follow distributions centred on a ray projecting from the origin in Rd, with proportional covariance matrices under a
sparse regime.

We can gain insight into our theoretical results by applying them in the context of common
statistical models. For our motivating dynamic stochastic block model example detailed in Section 2, f (Z(it), Z(jt)) = B(zti),zj , where B(t) denotes the inter-community link probability matrices and zi and zj denote the community membership of the ith and jth nodes respectively. A pair (z, t) and (z , t ) are exchangeable if and only if the corresponding rows of B(t) and B(t ) are identical. Theorem
3 then predicts that the point cloud obtained via UASE should decompose into a finite number
of Gaussian clusters, as we observe in Figure 1. Moreover, when combined with Corollary 5 the equality of the fourth rows of B(1) and B(2) implies that we should expect identical clusters (centre
and shape) corresponding to the fourth community at both time points (indicating longitudinal stability) and similarly the equality of the first and second rows of B(2) tells us that the communities
should be indistinguishable at the second time point (indicating cross-sectional stability), behaviours
we observe in Figure 1. For the dynamic degree-corrected stochastic block model [15, 24], f (Z(it), Z(jt)) = i(t)j(t)B(zti),zj ,
where the i(t)  (0, 1] are node-specific parameters which describe the activity level of each node at a given time point. In this setting, a pair (z, t) and (z , t ) are exchangeable up to degree if and only if the corresponding rows of B(t) and B(t ) are identical. Consequently, Corollary 5 indicates
that nodes belonging to the same community under such a model should have their corresponding
UASE estimates distributed along a ray projecting from the origin, a behaviour that is exhibited in
Figure 2.

5

4 Comparison

A method for embedding a dynamic network will generally admit one of the following descriptions:

1. A single, joint spectral decomposition, e.g. omnibus [21] or UASE [14];

2. A separate spectral decomposition obtained at each timepoint, from an individual or temporally smoothed graph [4, 35, 6, 2, 27, 32, 16];

3. A joint matrix factorisation, with smoothness constraints [22, 5, 47, 3, 23].

As mentioned in the introduction if, rather than obtaining a dynamic embedding, interest is solely on
recovering dynamic community structure, several other approaches based on the dynamic stochastic
block model are available. We also do not include in this comparison inference procedures which
are based on an explicit parametric dynamic latent position network model [34, 12, 13, 18, 28, 19,
7, 37, 9, 8], reviews of which can be found in [17] and [39], typically Bayesian and/or variational,
which are much harder to analyse and not usually regarded as embedding procedures per se. We define the spectral embedding of an arbitrary symmetric matrix M  RN×N to be XM =
U|S|1/2  RN×d (removing the subscript if clear from context), where S  Rd×d is a diagonal matrix containing the d largest eigenvalues of M, by magnitude, and U  RN×d is a matrix containing corresponding orthonormal eigenvectors.
The omnibus method computes the spectral embedding of the matrix A~  {0, 1}nT ×nT whose k, th n × n block is (A(k) + A( ))/2, giving a point cloud XA~  RnT ×d whose first n rows provide an embedding for the nodes at time 1, and so forth. In the second category, the sequence of spectral
embeddings of the time-averaged matrices

A¯ (t) =

wk A(t-k) ,

(8)

k

is computed, where wk are non-negative weights (e.g. sliding window average, exponential forgetting factors, and more), giving a point cloud X(A¯t)  Rn×d for each time point. Alternatively, timeaveraged versions of the normalised Laplacian or Katz matrix can be used. The description `matrix factorisation', in the third category, is used to refer to a low-rank decomposition of a matrix, possibly under constraints (such as positivity, or smoothness over time), which is not based on a straightforward spectral decomposition. An example is the sequence of spectral embeddings computed from the matrices [23]

T

T -1

argminA (1),...,A (T )

A(t) - A (t)

2 F

+

A (t) - A (t+1)

2 F

,

(9)

t=1

t=1

where  > 0 (a high value favouring a smooth solution), subject to a low rank constraint on A (1), . . . , A (T ), and again the normalised Laplacian (used by [23]) or Katz matrix could equally
well be used.

4.1 Stability of other embeddings
To show how those methods fail to provide both cross-sectional and longitudinal stability, we establish some elementary facts about the spectral embedding X of a matrix M. Although we consider the general case involving positive and negative eigenvalues, the issues we raise are very much present in the positive-definite case too. If M has signature (p, q) (p positive and q negative eigenvalues), its spectral embedding into d = p + q dimensions satisfies X = MX(X X)-1Ip,q by the Moore-Penrose inverse, so that Xi = MiX(X X)-1Ip,q where the subscript i picks out the ith row. Therefore,
(A1) Xi = Xj if and only if Mi = Mj.
Given a subset of indices S, we denote XS as the corresponding rows (X)iS and MS,S as the matrix of corresponding rows and columns (Mi,j)i,jS. Suppose we have two symmetric matrices M(1) and M(2) of signature (p, q), with (non-equal) spectral embeddings X(1) and X(2) respectively. If there exists a subset S such that the principal minors M(S1,)S and M(S2,)S are equal with signature (p, q),

6

(A2) X(S1) = X(S2)Q for some Q  O(p, q).

Furthermore, in general, X(S1) = X(S2), and the indefinite Procrustes matrix that best aligns X(1) and X(2),

Q^ = argminQO(p,q)

X(1)Q - X(2)

2 F

,

(10)

does not align the subsets of the spectral embeddings, X(S1) = X(S2)Q^ . It will be considered sufficient to establish whether an embedding is stable when applied to the
Gram matrices P(1), . . . , P(T ). A method found to be unstable in this noise-free condition is not expected to be stable when the matrices are replaced by their noisy observations A(1), . . . , A(T ). For the case of omnibus embedding, we therefore consider the matrix P~  [0, 1]nT ×nT , the noise-free counterpart of A~ . If (z, t) and (z, t ) are exchangeable, then, conditional on Z(it) = Z(it ) = z, the rows P~ n(t-1)+i and P~ n(t -1)+i are equal. But if (z, t) and (z , t) are exchangeable, then P~ n(t-1)+i = P~ n(t-1)+j, in general, when Z(it) = z and Z(jt) = z . By (A1), omnibus embedding provides longitudinal but not cross-sectional stability, a fact demonstrated in the central column of
Figure 1.
Methods in the second and third category generally trade-off longitudinal against cross-sectional
stability, seemingly never providing both. For example, if we pick w0 = 1, wk = 0 for k = 0 in (8) as an example of the second, or  = 0 in (9) as an example of the third, the methods reduce
to independent spectral embedding of the graphs, where cross-sectional stability is guaranteed.
However, the embeddings are subject to transformations Q  O(p, q) [31], which will not be the
same for all time steps, in general. By (A2) there exists a transformation which will realign the
stable subset of nodes, but it requires knowing or estimating that stable subset, the latter difficult
under noise. In this way, independent spectral embedding provides cross-sectional but not longitudinal
stability. At the other extreme, where wk = 1/T in (8), or    in (9), we evidently achieve longitudinal but not cross-sectional stability.

5 Real data
The Lyon primary school dataset shows the social interactions at a French primary school over two days in October 2009 [38]. The school consisted of 10 teachers and 241 students from five school years, each year divided into two classes. Face-to-face interactions were detected when radio-frequency identification devices worn by participants (10 teachers and 232 students gave consent to be included in the experiment) were in close proximity over an interval of 20 seconds and recorded as a pair of anonymous identifiers together with a timestamp. The data are available for download from the Network Repository website1 [29].
A time series of networks was created by binning the data into hour-long windows over the two days, from 08:00 to 18:00 each day. If at least one interaction was observed between two people in a particular time window, an edge was created to connect the two nodes in the corresponding network. This results in a time series of graphs A(1), . . . , A(20) each with n = 242 nodes. Where a node is not active in a given time window, it is still included in the graph as an isolated node. This is compatible with the theory and method, and the node is embedded to the zero vector at that time point.
Given the unfolded adjacency matrix A = (A(1)| · · · |A(20)), an estimated embedding dimension d^ = 10 was obtained using profile likelihood [48] and we construct the embeddings Y^ (1), . . . , Y^ (20)  Rn×10, taking approximately five seconds on a 2017 MacBook Pro. Figure 2 shows the first two dimensions of this embedding to visualise some of the structure in the data.
From this plot, we observe clustering of students in the same school class. For time windows corresponding to classroom time, for example, 09:00­10:00 and 15:00­16:00, the embedding forms rays of points in 10-dimensional space, with each ray broadly corresponding to a single school class. This is to be expected under a degree-corrected stochastic block model, and the distance along the ray is a measure of the node's activity level [20, 25, 33]. However, not all time windows exhibit this structure, for example, the different classes mix more during lunchtimes (time windows 12:00­13:00 and 13:00­14:00).
1https://networkrepository.com

7

Day 1, 08:00-09:00
0.4

0.2

0.0

0.2

0.4
Day 1, 13:00-14:00
0.4

0.2

0.0

0.2

0.4
Day 2, 08:00-09:00
0.4

0.2

0.0

0.2

0.4
0.4 Day 2, 13:00-14:00

0.2

0.0

0.2

0.4 0.0

0.2 0.4
Class 1A Class 1B

Day 1, 09:00-10:00 Day 1, 14:00-15:00 Day 2, 09:00-10:00 Day 2, 14:00-15:00
0.0 0.2 0.4
Class 2A Class 2B

Day 1, 10:00-11:00 Day 1, 11:00-12:00

Day 1, 15:00-16:00 Day 1, 16:00-17:00

Day 2, 10:00-11:00 Day 2, 11:00-12:00

Day 2, 15:00-16:00 Day 2, 16:00-17:00

0.0 0.2
Class 3A Class 3B

0.4 0.0
Class 4A Class 4B

0.2 0.4
Class 5A Class 5B

Day 1, 12:00-13:00 Day 1, 17:00-18:00 Day 2, 12:00-13:00 Day 2, 17:00-18:00
0.0 0.2 0.4
Teacher

Figure 2: First two dimensions of the embeddings Y^ (1), . . . , Y^ (20) of the unfolded adjacency matrix A = (A(1)| · · · |A(20)). The colours indicate different school years while the marker type distinguish
the two school classes within each year.

5.1 Clustering
Following recommendations regarding community detection under a degree-corrected stochastic block model [33], we analyse UASE using spherical coordinates (t)  [0, 2)n×9, for t  [T ]. Since UASE demonstrates cross-sectional and longitudinal stability, we can combine the embeddings into a single point cloud  = ((1) | · · · |(T ) )  RnT ×9 where each point represents a student or teacher in a particular time window. This allows us to detect people returning to a previous behaviour in the dynamic network. We fit a Gaussian mixture model with varying covariance matrices to the non-zero points in  with 20­50 clusters increasing in increments of 5, with 50 random initialisations, taking approximately five minutes on a 2017 MacBook Pro. Using the Bayesian Information Criterion, we select the best fitting model (30 clusters) and assign the maximum a posteriori Gaussian cluster membership to each student in each time window.
Figure 3 shows how students in the ten classes move between these clusters over time. Each class has one or two clusters unique to it, for example, the majority of students in class 1A spend their classroom time (as opposed to break time) assigned to cluster 1 or cluster 25. This highlights the importance of longitudinal stability in UASE, as we are detecting points in the embedding returning to some part of latent space.
There are also instances of multiple school classes being assigned the same cluster at the same time period, for example, on the morning of day 1, classes 5A and 5B are mainly in cluster 28

8

Cl5aBss Cl5aAss Cl4aBss Cl4aAss Cl3aBss Cl3aAss Cl2aBss Cl2aAss Cl1aBss Cl1aAss

Day 1

Day 2

1

1

25

25

29

29

25

25 1

25

1

1

1

25

1

25 25 25 1

1

17 6

6

24

17 6

29

29

24 6

6

6

6

6

6

17 17 6

2

24

6

6

24

6

11 11 22 26 29 29
10 26

11 11 11 11 11 22 11

29

29 10

10

10

10

10

26 10

26 10

10

3

11 22 11

3 10

10 10

5 5 13 5

5

13

5

20 20 20

5

27

3

5

5

13

7 13 7

7

18 18 7 13

7

20 20 20

7

7

7

13

7

12 15 15 12

2

2

12 12 12 12 12 15 12

14 14 19 14

14

14

19

14

14

14

19

14 9

28 23

28

28

23

8

8

23 23

4

23 23 23 23 23 21 21 21 23

4

23

30

28 28 28

88

16

16

30

30 30 30 30

2

21 30 30 30 30

08:00-090:090:00-101:000:00-111:010:00-121:020:00-131:030:00-141:040:00-151:050:00-161:060:00-171:070:00-180:080:00-090:090:00-101:000:00-111:010:00-121:020:00-131:030:00-141:040:00-151:050:00-161:060:00-171:070:00-18:00

Figure 3: Bar chart showing the Gaussian cluster assignment of school classes over time. The height of each coloured bar represents the proportion of students, in that class and at that time, assigned to the corresponding Gaussian cluster, the total available height representing 100%. If the coloured bars do not sum to the full available height, the difference represents the proportion of inactive students. For legibility, only bars representing over 35% of the class are labelled with the cluster number.

suggesting they are having a joint lesson, and we see this behaviour again on day 2 with classes 3A and 3B in cluster 20. In the lunchtime periods, particularly on day 1, the younger students (classes 1A­2B) mingle to form a larger cluster, as do the older students (classes 4A­5B), potentially explained by the cafeteria needing two sittings for lunch for space reasons [38]. This highlights the importance of cross-sectional stability in UASE, as it allows the grouping of nodes behaving similarly in a specific time window, irrespective of their potentially different past and future behaviours.

6 Conclusion
We prove that an existing procedure, UASE, allows dynamic network spectral embedding with longitudinal and cross-sectional stability guarantees. These properties make a range of subsequent spatio-temporal analyses possible using `off-the-shelf' techniques for clustering, time series analysis, classification and more.

9

References
[1] Agterberg, J., Tang, M., and Priebe, C. E. (2020). On two distinct sources of nonidentifiability in latent position random graph models. arXiv preprint arXiv:2003.14250.
[2] Bhattacharyya, S. and Chatterjee, S. (2018). Spectral clustering for multiple sparse networks: I. arXiv preprint arXiv:1805.10594.
[3] Chen, H. and Li, J. (2018). Exploiting structural and temporal evolution in dynamic link prediction. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 427­436.
[4] Chi, Y., Song, X., Zhou, D., Hino, K., and Tseng, B. L. (2007). Evolutionary spectral clustering by incorporating temporal smoothness. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 153­162.
[5] Deng, D., Shahabi, C., Demiryurek, U., Zhu, L., Yu, R., and Liu, Y. (2016). Latent space model for road networks to predict time-varying traffic. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1525­1534.
[6] Dunlavy, D. M., Kolda, T. G., and Acar, E. (2011). Temporal link prediction using matrix and tensor factorizations. ACM Transactions on Knowledge Discovery from Data (TKDD), 5(2):1­27.
[7] Durante, D. and Dunson, D. B. (2014). Nonparametric Bayes dynamic modelling of relational data. Biometrika, 101(4):883­898.
[8] Durante, D., Mukherjee, N., and Steorts, R. C. (2017). Bayesian learning of dynamic multilayer networks. The Journal of Machine Learning Research, 18(1):1414­1442.
[9] Friel, N., Rastelli, R., Wyse, J., and Raftery, A. E. (2016). Interlocking directorates in Irish companies using a latent space model for bipartite networks. Proceedings of the National Academy of Sciences, 113(24):6629­6634.
[10] Gao, S., Denoyer, L., and Gallinari, P. (2011). Temporal link prediction by integrating content and structure information. In Proceedings of the 20th ACM international conference on Information and knowledge management, pages 1169­1174.
[11] Ho, Q., Song, L., and Xing, E. (2011). Evolving cluster mixed-membership blockmodel for time-evolving networks. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 342­350. JMLR Workshop and Conference Proceedings.
[12] Hoff, P. D. (2011). Hierarchical multilinear models for multiway data. Computational Statistics & Data Analysis, 55(1):530­543.
[13] Hoff, P. D. et al. (2011). Separable covariance arrays via the Tucker product, with applications to multivariate relational data. Bayesian Analysis, 6(2):179­196.
[14] Jones, A. and Rubin-Delanchy, P. (2021). The multilayer random dot product graph. arXiv preprint arXiv:2007.10455.
[15] Karrer, B. and Newman, M. E. J. (2011). Stochastic blockmodels and community structure in networks. Phys. Rev. E (3), 83(1):016107, 10.
[16] Keriven, N. and Vaiter, S. (2020). Sparse and smooth: improved guarantees for spectral clustering in the dynamic stochastic block model. arXiv preprint arXiv:2002.02892.
[17] Kim, B., Lee, K. H., Xue, L., and Niu, X. (2018). A review of dynamic network models with latent variables. Statistics surveys, 12:105.
[18] Lee, N. H. and Priebe, C. E. (2011). A latent process model for time series of attributed random graphs. Statistical inference for stochastic processes, 14(3):231­253.
10

[19] Lee, N. H., Yoder, J., Tang, M., and Priebe, C. E. (2013). On latent position inference from doubly stochastic messaging activities. Multiscale Modeling & Simulation, 11(3):683­718.
[20] Lei, J. and Rinaldo, A. (2015). Consistency of spectral clustering in stochastic block models. Ann. Statist., 43(1):215­237.
[21] Levin, K., Athreya, A., Tang, M., Lyzinski, V., Park, Y., and Priebe, C. E. (2017). A central limit theorem for an omnibus embedding of multiple random graphs and implications for multiscale network inference. arXiv preprint arXiv:1705.09355.
[22] Lin, Y.-R., Chi, Y., Zhu, S., Sundaram, H., and Tseng, B. L. (2008). Facetnet: a framework for analyzing communities and their evolutions in dynamic networks. In Proceedings of the 17th international conference on World Wide Web, pages 685­694.
[23] Liu, F., Choi, D., Xie, L., and Roeder, K. (2018). Global spectral clustering in dynamic networks. Proceedings of the National Academy of Sciences, 115(5):927­932.
[24] Liu, S., Wang, S., and Krishnan, R. (2014). Persistent community detection in dynamic social networks. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 78­89. Springer.
[25] Lyzinski, V., Sussman, D. L., Tang, M., Athreya, A., and Priebe, C. E. (2014). Perfect clustering for stochastic blockmodel graphs via adjacency spectral embedding. Electron. J. Stat., 8(2):2905­2922.
[26] Matias, C. and Miele, V. (2015). Statistical clustering of temporal networks through a dynamic stochastic block model. arXiv preprint arXiv:1506.07464.
[27] Pensky, M., Zhang, T., et al. (2019). Spectral clustering in the dynamic stochastic block model. Electronic Journal of Statistics, 13(1):678­709.
[28] Robinson, L. F. and Priebe, C. E. (2012). Detecting time-dependent structure in network data via a new class of latent process models. arXiv preprint arXiv:1212.3587.
[29] Rossi, R. A. and Ahmed, N. K. (2015). The network data repository with interactive graph analytics and visualization. In AAAI.
[30] Rubin-Delanchy, P. (2020). Manifold structure in graph embeddings. In Proceedings of the Thirty-fourth Conference on Neural Information Processing Systems.
[31] Rubin-Delanchy, P., Cape, J., Priebe, C. E., and Tang, M. (2020). A statistical interpretation of spectral embedding: the generalised random dot product graph. arXiv preprint arXiv:1709.05506v3.
[32] Sanna Passino, F., Bertiger, A. S., Neil, J. C., and Heard, N. A. (2019). Link prediction in dynamic networks using random dot product graphs. arXiv preprint arXiv:1912.10419.
[33] Sanna Passino, F., Heard, N. A., and Rubin-Delanchy, P. (2020). Spectral clustering on spherical coordinates under the degree-corrected stochastic blockmodel. arXiv e-prints, pages arXiv­2011.
[34] Sarkar, P. and Moore, A. W. (2005). Dynamic social network analysis using latent space models. ACM SigKDD Explorations Newsletter, 7(2):31­40.
[35] Scheinerman, E. R. and Tucker, K. (2010). Modeling graphs using dot product representations. Computational statistics, 25(1):1­16.
[36] Seto, S., Zhang, W., and Zhou, Y. (2015). Multivariate time series classification using dynamic time warping template selection for human activity recognition. In 2015 IEEE Symposium Series on Computational Intelligence, pages 1399­1406. IEEE.
[37] Sewell, D. K. and Chen, Y. (2015). Latent space models for dynamic networks. Journal of the American Statistical Association, 110(512):1646­1657.
11

[38] Stehlé, J., Voirin, N., Barrat, A., Cattuto, C., Isella, L., Pinton, J.-F., Quaggiotto, M., Van den Broeck, W., Régis, C., Lina, B., et al. (2011). High-resolution measurements of face-to-face contact patterns in a primary school. PloS one, 6(8):e23176.
[39] Turnbull, K. (2020). Advancements in latent space network modelling. PhD thesis, Lancaster University.
[40] Xie, Y., Li, C., Yu, B., Zhang, C., and Tang, Z. (2020). A survey on dynamic network embedding. arXiv preprint arXiv:2006.08093.
[41] Xing, E. P., Fu, W., Song, L., et al. (2010). A state-space mixed membership blockmodel for dynamic network tomography. Annals of Applied Statistics, 4(2):535­566.
[42] Xu, K. (2015). Stochastic block transition models for dynamic networks. In Artificial Intelligence and Statistics, pages 1079­1087. PMLR.
[43] Xu, K. S. and Hero, A. O. (2014). Dynamic stochastic blockmodels for time-evolving social networks. IEEE Journal of Selected Topics in Signal Processing, 8(4):552­562.
[44] Xue, G., Zhong, M., Li, J., Chen, J., Zhai, C., and Kong, R. (2021). Dynamic network embedding survey. arXiv preprint arXiv:2103.15447.
[45] Yang, T., Chi, Y., Zhu, S., Gong, Y., and Jin, R. (2011). Detecting communities and their evolutions in dynamic social networks--a Bayesian approach. Machine learning, 82(2):157­189.
[46] Zhao, B., Lu, H., Chen, S., Liu, J., and Wu, D. (2017). Convolutional neural networks for time series classification. Journal of Systems Engineering and Electronics, 28(1):162­169.
[47] Zhu, L., Guo, D., Yin, J., Ver Steeg, G., and Galstyan, A. (2016). Scalable temporal latent space inference for link prediction in dynamic social networks. IEEE Transactions on Knowledge and Data Engineering, 28(10):2765­2777.
[48] Zhu, M. and Ghodsi, A. (2006). Automatic dimensionality selection from the scree plot via the use of profile likelihood. Computational Statistics & Data Analysis, 51(2):918­930.
12

A Appendix
Python notebooks producing the figures of this paper are available at https://github.com/ iggallagher/Dynamic-Network-Embedding

Proof of Proposition 1
Let F  be the joint distribution on RT D obtained by first assigning a random vector  = (1| · · · |T ) via F and then applying the map  to each of the components t, and let F1, . . . , FT denote the corresponding marginal distributions on RD. Given   F  and t  Ft, define the second moment matrices  = E[ ]  RT D×T D and t = E[tt ]  RD×D, and let r = rank() and rt = rank(t).
Let M  Rr×T D be a matrix whose rows form a basis of supp(F ), and similarly let Nt  Rrt×D be a matrix whose rows form a basis of supp(Ft). Let N = diag(N1, . . . , NT ), and define the matrix  := MDN  Rr×(r1+...+rT ), where D = diag(Ip,q, . . . , Ip,q). Construct the singular value decomposition  = UV , with U  O(r × d),   Rd×d and V  O((r1 + · · · + rT ) × d), and d = rank(). Writing V = (V1| · · · |VT ), where Vt  Rrt×d has rank dr, we can then construct the singular value decompositions Vt = UttWt , with Ut  O(rt × dt), t  Rdt×dt and Wt  O(d × dt), where dt = rank(Vt).
Define  = W  Rd×(d1+···+dT ), where W = (W11| · · · |WT T ), and let L : RT D  Rd be the linear map sending the ith row of M to the ith row of U, and similarly let Lt : RD  Rdt be
the linear map sending the ith row of Nt to the ith row of Ut. Then, setting G to be the joint distribution on Rd × Rd1 × · · · RdT obtained by first assigning a random vector  = (1| · · · |T ) via
F and then sending this to the tuple L ((1)| · · · |(T )) , L1 (1) , . . . , LT (T ) and letting
Xi = L (X(i1)| · · · |X(iT )) and Yi(t) = L X(it)), we find that (A, X, Y)  MRDPG(G, ).

Proof of Theorem 2

This follows directly from Theorem 1 in [14], which states that there exist sequences of matrices Rt = Rt(n)  Rdt×d and W~  O(d) such that

Y^ (t) - Y(t)RtW~ 2 = O

log1/2 (n) 1/2 n1/2

(11)

almost surely, where the matrices Rt satisfy Y~ (t) = Y(t)RY,t, and W  O(d) is the solution to the one-mode orthogonal Procrustes problem

W = arg min

UA - UPQ

2 F

+

VA - VPQ

2 F

,

QO(d)

(12)

where A and P admit the singular value decompositions A = UAAVA + UA,A,VA, and P = UPPVP respectively. Observing that the 2-to-infinity norm of a matrix is known to be equivalent to its maximum Euclidean row norm (and consequently is invariant under orthogonal
transformations) gives the desired result.

Proof of Theorem 3
From the proof of Theorem 1 in [14], we find that

n1/2(Y^ (t)W~ - Y~ (t)) = n1/2(A(t) - P(t))UP-P1/2 + n1/2E,

(13)

where the residual term E satisfies n1/2E 2  0. We can rewrite this as

n1/2(Y^ (t)W~ - Y~ (t)) = n1/2(A(t) - P(t))XL-P1 + n1/2E,

(14)

where L  GL(d) (the general linear group of invertible d × d matrices) satisfies X = X~ L, which is known to exist by Proposition 16 of [14].

13

We begin by showing that there exists a sequence of orthogonal matrices W  O(d) and a fixed matrix L~  Rd×d such that LW  L~, for which we adapt the arguments of Theorem 1 and
Corollary 2 of [1]. To begin with, note that the mapping v  X(X X)-1/2v sends the eigenvectors of the matrix
(X X)1/2Y Y (X X)1/2 to the left singular vectors of P, since if v is such an eigenvector
(with corresponding eigenvalue ) then

PP X(X X)-1/2v = XY Y (X X)1/2v

(15)

= X(X X)-1/2(X X)1/2Y Y (X X)1/2v

(16)

= X(X X)-1/2v

(17)

as required. Thus we may write UP = X(X X)-1/2V, where V is a matrix of eigenvectors of (X X)1/2Y Y (X X)1/2, and consequently observe that

L = (X X)-1X XP

= (X X)-1X X(X X)-1/2V1P/2

= (X X)-1/2V1P/2

-1/2

1/2

=

XX n

V

P n

.

(18) (19) (20) (21)

Let GX and GY,t denote the marginal distributions of G, and define the second moment matrices
X = E[ ] and Y,t = E[tt ], where   GX and t  GY,t, and let Y = diag(Y,1, . . . , Y,T ). Then the law of large numbers tells us that the first and last terms in (21) converge to (nX)-1/2 and (n~ )1/2 respectively, where ~ is the diagonal matrix whose entries are the square roots of the eigenvalues of 1X/2Y 1X/2 (see, for example, Proposition 7 of [14]). Note that V is also the matrix of eigenvectors of

1/2

XX

Y Y

n

n

1/2 XX
n

(22)

which converges to 1X/2Y 1X/2 by the law of large numbers. Consequently, for each distinct eigenvalue of 1X/2Y 1X/2 we may apply the Davis-Kahan theorem to find that the principal angles between the resulting eigenspace and the subspace spanned by the corresponding columns of V vanish, and thus V converges to V~ up to some block-orthogonal transformation W  O(d),
where V~ is a fixed matrix of eigenvectors of 1X/2Y 1X/2. Since W by definition commutes with ~ , we find that

LW  -X1/2V~ ~ 1/2

(23)

as required. Multiplying (14) by W, we find that

n1/2W(W~ Y^ i(t) - Y~ i(t))  nnW-P1L

1 n1/2n

(A(t)

-

P(t))X

i

(24)

and note that the term nnW-P1L converges to ~ -1L~ from our previous discussion. Moreover,

1 n1/2n

(A(t)

-

P(t))X

i

=

1 n1/2n

n
(A(ijt)
j=1

- P(ijt))Xj

=

1 (nn)1/2

n
(A(ijt)
j=1

- P(ijt))(Zj )

(25) (26)

where (Zj) = L ((Z(j1))| · · · |(Z(jT ))) . Conditional on Z(it) = z, we have P(ijt) = nf (z, Z(jt)), and so the sum in (26) is a scaled sum of n - 1 independent, identically distributed zero-mean

14

random variables, each with covariance matrix

E f (z, t) 1 - nf (z, t) · ()()

(27)

where   F , from which the result follows by setting R = ~ -1L~ and applying the multivariate versions of the central limit theorem and Slutsky's theorem.

Proof of Corollary 5
Note that for any t  [T ] the equality X~ Y~ (t) = P(t) holds, and so Y~ (t) = P(t)X~ (X~ X~ )-1 (since P(t) is symmetric). Consequently, for any i  [n] we find that Y~ i(t) = (X~ X~ )-1X~ P(it), where P(it) denotes the ith row of P(t) (again due to symmetry of P(t)). Thus for any exchangeable pair (z, t) and (z , t ), if Z(it) = z and Z(jt ) = z then the equality of the rows P(it) and P(jt ) implies the equality of Y~ i(t) and Y~ j(t ). Moreover, from the definition of exchangeability it is clear that the matrices t(z) and t (z ) present in the limiting distributions for Y^ i(t) and Y^ j(t ) in Theorem 3 are equal, and since the matrices W, W~ and R are independent of t and t equality of the full covariance matrices follows.
An analogous argument holds in the case that (z, t) and (z , t ) are exchangeable up to degree.

Lyon primary school data: classification
An alternative use of UASE is to analyse the trajectory of each node through time, in embedded space. Because of longitudinal stability of UASE, standard multivariate time series analysis techniques can be used to detect trends and seasonal behaviour. These, in turn, could enable latent position forecasting and, from this, link prediction. As background, in such analyses, the time series model is usually incorporated in the embedding process, for example, via a Markov model for the communities [3, 10, 47], or a seasonal autoregressive integrated process on the adjacency matrices [32].
In this section, we instead consider the task of time series classification. Given the trajectory for each student, the goal is to predict their school class. Given a arbitrary time series, this is often done using dynamic time warping [36] or convolutional neural networks [46] to allow for misalignment in time. However, in this simple example with fixed classroom times, we simply fit a random forest classifier with 100 trees to the concatenation of the spectral embeddings ((1)| · · · |(T ))  Rn×9T , each using five randomly selected features. The 10-fold cross-validation accuracy is 0.983 ± 0.035, taking approximately two seconds on a 2017 MacBook Pro.
When classifying a time series in this way, auto-correlation makes feature importance harder to measure. Nevertheless, certain features appear not to be important for classification, in particular, we find that the two lunchtime periods (time windows 12:00-13:00 and 13:00-14:00) are not useful, confirming what is shown by those spectral embeddings in Figure 2.
Using this classifier, we can predict the class of the 10 teachers. While we do not expect them to behave exactly as the students, we might hope to match a teacher to their class. Figure 4 shows the proportion of random forest trees classifying each teacher to each school class. Without truth data it is impossible to know if these labels are correct, but the most likely classification assigns exactly one teacher to each class.

15

Teacher

Teacher class probability 0.75 0.04 0.04 0.02 0.02 0.08 0.02 0.03

0.97 0.01 0.01

0.01

0.05 0.86 0.02 0.01 0.01 0.02 0.01 0.02

0.01 0.01 0.05 0.85 0.01 0.02 0.01 0.03 0.01

0.01 0.04 0.01 0.84 0.07 0.01 0.02

0.03 0.02 0.01 0.17 0.73 0.01 0.02 0.01

0.01

0.99

0.04 0.01 0.01 0.01 0.01 0.03 0.81 0.04 0.04

0.01

0.99

0.02 0.01 0.01

0.01 0.01 0.94

1A 1B 2A 2B 3AClas3sB 4A 4B 5A 5B

Figure 4: Heat map showing the proportion of random forest trees assigning the spectral embedding trajectory for each teacher to the ten school classes 1A­5B. The colour represents the school class, matching the colours used in Figure 2, where larger proportions are more opaque. Missing values represent a proportion of 0.

16

