arXiv:2106.00260v1 [physics.comp-ph] 1 Jun 2021

An adaptive scalable fully implicit algorithm based on stabilized finite element for reduced visco-resistive MHD
Qi Tanga,1,, Luis Chac´ona, Tzanio V. Kolevb, John N. Shadidc,d, Xian-Zhu Tanga
aLos Alamos National Laboratory, Los Alamos, New Mexico 87545. bLawrence Livermore National Laboratory, Livermore, California 94550.
cSandia National Laboratories, Albuquerque, New Mexico 87123. dDepartment of Mathematics and Statistics, University of New Mexico, Albuquerque, NM 87131.
Abstract
The magnetohydrodynamics (MHD) equations are continuum models used in the study of a wide range of plasma physics systems, including the evolution of complex plasma dynamics in tokamak disruptions. However, efficient numerical solution methods for MHD are extremely challenging due to disparate time and length scales, strong hyperbolic phenomena, and nonlinearity. Therefore the development of scalable, implicit MHD algorithms and high-resolution adaptive mesh refinement strategies is of considerable importance. In this work, we develop a high-order stabilized finite-element algorithm for the reduced visco-resistive MHD equations based on the MFEM finite element library (mfem.org). The scheme is fully implicit, solved with the Jacobian-free Newton-Krylov (JFNK) method with a physics-based preconditioning strategy. Our preconditioning strategy is a generalization of the physics-based preconditioning methods in [Chac´on, et al, JCP 2002] to adaptive, stabilized finite elements. Algebraic multigrid methods are used to invert sub-block operators to achieve scalability. A parallel adaptive mesh refinement scheme with dynamic load-balancing is implemented to efficiently resolve the multi-scale spatial features of the system. Our implementation uses the MFEM framework, which provides arbitrary-order polynomials and flexible adaptive conforming and nonconforming meshes capabilities. Results demonstrate the accuracy, efficiency, and scalability of the implicit scheme in the presence of large scale disparity. The potential of the AMR approach is demonstrated on an island coalescence problem in the high Lundquist-number regime ( 107) with the successful resolution of plasmoid instabilities and thin current sheets.
Keywords: visco-resistive implicit MHD; continuous finite element; streamline upwind Petrov-Galerkin; physics-based preconditioning; adaptive mesh refinement; plasmoid instability

Corresponding author Email addresses: qtang@lanl.gov (Qi Tang), chacon@lanl.gov (Luis Chac´on), tzanio@llnl.gov (Tzanio V. Kolev), jnshadi@sandia.gov (John N. Shadid), xtang@lanl.gov (Xian-Zhu Tang) 1This work was jointly supported by the U.S. Department of Energy through the Fusion Theory Program of the Office of Fusion Energy Sciences and the SciDAC partnership on Tokamak Disruption Simulation between the Office of Fusion Energy Sciences and the Office of Advanced Scientific Computing. Los Alamos National Laboratory is operated by Triad National Security, LLC, for the National Nuclear Security Administration of U.S. Department of Energy (Contract No. 89233218CNA000001). Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy's National Nuclear Security Administration under grant DE-NA-0003525. This work also performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. This paper describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government.

Preprint submitted to Elsevier

June 2, 2021

Contents

1 Introduction

3

2 Governing equations: visco-resistive MHD

4

3 Stabilized finite element formulation

6

3.1 2D visco-resistive MHD stabilized FE formulation . . . . . . . . . . . . . . . . . . . . . . . . . 6

3.2 Linearized system of stabilized FE formulation . . . . . . . . . . . . . . . . . . . . . . . . . . 8

3.3 Time stepping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

4 Physics-based preconditioning for stabilized finite element

9

4.1 Saddle-point problem and Schur complement . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

4.2 Approximate Schur-complement preconditioner . . . . . . . . . . . . . . . . . . . . . . . . . . 11

5 Preconditioned JFNK-based algorithm

13

6 Dynamic adaptive mesh refinement

14

6.1 Error estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

6.2 Refinement strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

7 Parallel implementation in MFEM

15

8 Numerical results

17

8.1 Alfv´en wave propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

8.2 MHD Rayleigh flow and Alfv´en wave propagation . . . . . . . . . . . . . . . . . . . . . . . . . 19

8.3 Tearing mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

8.3.1 Uniform-mesh results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

8.3.2 AMR results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

8.4 Island coalescence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

8.4.1 Static-mesh results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

8.4.2 AMR results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

9 Conclusions

37

2

1. Introduction
Magnetohydrodynamics (MHD) models the dynamics of a quasi-neutral plasma fluid in the presence of electromagnetic fields. It has a wide range of applications including space weather prediction, astrophysics, as well as in laboratory plasma applications such as Z-pinch, tokamak and stellarator devices [1]. The full MHD model comprises evolution equations for mass, moment and energy as well as an evolution equation for the magnetic field, while the electric field is determined by a (generalized) Ohm's law. The ultimate goal of this study is to develop an adaptive, scalable and high-order-accurate simulator for the fusion devices involving complex geometries. The resistive MHD model is commonly used due to its relative computational efficiency in contrast to kinetic descriptions, as well as the ability to efficiently capture many important macroscopic instabilities. In resistive MHD, the Lundquist number, SL, defined as the ratio between the resistive diffusion time scale for the magnetic field and the Alfv´en wave transit time scale, is commonly used to distinguish between different physics regimes. It is well known that the Lundquist number is very high in fusion devices, in the range of 106 to 109 [2], owing to the large plasma temperature and small densities. Therefore, in this work, we seek to develop a high-order-accurate and scalable resistive MHD solver that is suitable for a wide range of Lundquist numbers.
The resistive MHD system is a strongly coupled, highly nonlinear and very stiff system that involves multiple physical phenomena spanning a wide range of spatial and temporal scales [3, 4]. In the context of fusion-device modeling, it is of interest to evolve ideal and resistive instabilities (tearing mode, resistive kink mode, coalescence instability, etc.) excited from a given MHD equilibrium. However, hyperbolic dynamics in the MHD system is typically much faster than resistive instabilities (as suggested by the high Lundquist number), which results in numerical stiffness. It is therefore important to consider fully implicit algorithms to efficiently step over the stiff hyperbolic component in the system [2] along with scalable preconditioning strategies. In addition, thin current sheets develop in the high-Lundquist-number regime, along with a secondary plasmoid instability. The plasmoid length-scale is comparable to the current sheet width and very small compared to the size of a typical magnetic island (a ratio of approximately 10-3 to 10-5). Plasmoid emergence, transport, and merging is on a much faster time scale than the resistive diffusion time scale. Due to the large length scale disparity, the problem becomes truly multi-scale in space as well. Thus, adaptive implicit multiscale algorithms are needed to capture these dynamics in a self-consistent fashion.
Many of the earlier implicit studies for extended MHD have been summarized in Ref. [2]. Here we only highlight studies that focus on preconditioning strategies for the implicit MHD system. Ref. [3] is one of the earliest studies to propose a physics-based preconditioner for 2D resistive MHD in a finite-difference context. This work will serve as the starting point for our physics-based preconditioner in the stabilized finite-element formulation. The approach is later generalized to a 2D Hall MHD [5], a 3D compressible resistive MHD [6] and a 2D two-field low- extended MHD [7]. The significance of this series of studies is its nearly optimal performance under grid refinement, featuring O(N log N ) scaling, where N is the number of degrees of freedom (dofs). A drawback of these studies is their reliance on low-order finite differences, which complicates their generalization and extension to higher orders of accuracy. A stabilized finite element (FE) formulation is proposed in Ref. [4], which considers the lower-order continuous FE and its preconditioning strategy. The preconditioning is less optimal compared to Ref. [3] since it uses a fully coupled algebraic multilevel preconditioner, which does not parabolize the system as done in Ref. [3]. This approach is later extended to 3D resistive MHD [8]. A related approach, using approximate block factorization (ABF) and approximate Schur complements has been developed for the MHD system in [9­11]. More recently, a fully implicit hybridization discontinuous Galerkin approach (HDG) has been developed and analyzed in [12] and results for scaling of a block preconditioner based on an approximate Schur complement have been presented [13]. Note that the approaches developed in [4, 13] are also suitable for high-Lundquist-number cases. Another class of methods [14­16] rely on compatible finite elements and discrete exterior calculus for the resistive MHD system. This method can be divergence-free in u, B or both without introducing the potential formulation as in this study. For instance, the approach in Ref. [16] uses a constrained transport method similar to Ref. [17, 18] to obtain divergence-free B. However, in that context, an efficient preconditioner can be even more challenging, and as pointed out [16] it is unclear the advantage of such a method in practice compared to the potential-based approach proposed here.
Beyond an efficient preconditioner, high-Lundquist-number simulations require two more important ingredients. First, for advection-dominated problems, it is well known that some stabilization is necessary in
3

continuous Galerkin FE formulations to prevent spurious oscillations [19]. Here, we choose the streamline upwind Petrov­Galerkin (SUPG) approach to stabilize both the magnetic potential and vorticity equations. The SUPG approach is chosen because it is variationally consistent and has a simple form which separates the stabilization term from the advection term and thus eases the generalization of the physics-based preconditioner. The second necessary ingredient is the adaptive mesh refinement (AMR) algorithm to address vast length-scale disparity and capture resulting localized structures. It is known that when the Lundquist number is sufficiently high, the tearing mode instability breaks the thin current sheet and produces plasmoids [20]. This has been studied in detail in the context of magnetic reconnection, see [21, 22] for instance. However, these studies [21, 22] typically focus on the reconnection regions, and use very refined and locally stretched grids. Our goal is an algorithm to dynamically capture plasmoids in practical simulations as in [23, 24], which requires an AMR capability that self-consistently evolves as localized plasmoids form and merge. To the best of our knowledge, the only works that successfully develop an implicit AMR approach for resistive MHD are [25­27]. Among those, only [27] was able to capture some plasmoids in the reconnection study of [24]. The highest Lundquist number considered in Ref. [24] is 4×105 but the definition of Lundquist number there uses the local current sheet length, and therefore it is comparable to  106 in our study.
The present study complements the previous ones [3, 4, 8] by providing a robust, scalable, adaptive and high-order stabilized FE formulation for resistive MHD. Key contributions of the current work include (a) generalization of the physics-based preconditioning to the FE formulation; (b) generalization of the previous lower-order (finite-difference or finite-element) discretization to high-order continuous FE; (c) development of an AMR algorithm along with the FE discretization; and (d) optimization of the implicit AMR solver for high Lundquist numbers. The generalization of the physics-based preconditioners from finite differences to highorder FE is by no means trivial. While it is sufficient in finite differences to introduce numerical diffusion via flux limiting in the high Lunduiqst/Reynolds number cases, the FE formulation requires high-order stabilization, which can often introduce complicated coupling terms in the system. We find ignoring such stabilization terms in the preconditioner degrades the performance of the solver significantly. Furthermore, both AMR and careful optimization of the parameters in the full solver are found to be necessary to provide a robust and efficient algorithm for the high Lundquist-number runs.
Our algorithm is implemented with the scalable FE framework, MFEM (mfem.org), using many of its features. For instance, we use the equal-order continuous FE bases, supporting arbitrary order of accuracy and flexible meshes. We adopt the AMR algorithm from MFEM, but have developed our own custom error estimator and refinement strategies. As a result, the adaptive solver supports both conforming and nonconforming AMR, for both structured and unstructured meshes. Although the practical runs presented in the current work only utilize the octree-based AMR and quadrilateral elements, the flexible implementation provides an easy route to leverage the current solver to practical simulations of fusion devices in the future.
The remainder of this paper is organized as follows. Section 2 describes the governing equations of the visco-resistive MHD equation and discusses the specific formulation we choose in this work. Section 3 presents the stabilized FE formulation of the model, its linearized system and the choices of time integrators. Section 4 generalizes the physics-based preconditioning approach in Ref. [3] to the stabilized FE formulation. Section 5 outlines the Jacobian-free Newton-Krylov (JFNK) approach and Section 6 describes the AMR approach, including a description of the error estimator and some details on the refinement/coarsening strategy, followed by the implementation details of the full algorithm in Section 7. Section 8 presents the numerical results, including a refinement study, parallel scaling tests and simulation results of practical interest. The numerical experiments start from some simple smooth problems to verify the order of accuracy and efficiency of the scheme, followed by some harder problems (tearing mode) to examine both of the scheme and AMR approach. The final example, the high Lundquist-number island coalescence problem, is the most challenging, as plasmoid instabilities develop. Finally, Section 9 concludes and outlines possible future directions.
2. Governing equations: visco-resistive MHD
We consider the incompressible reduced visco-resistive MHD model in a two-dimensional space . The normalized model is described by a streamfunction-vorticity description (­) of the fluid quantities and a
4

magnetic flux , and is given by Ref. [28]

2 = ,

(1a)

t + v ·  - 2  = -E0,

(1b)

t + v ·  - 2  = B · J - S,

(1c)

with the velocity and magnetic field defined as

v = z ×  = [-y, x]T , B = z ×  = [-y, x]T ,

and the current density defined as

J = .

Other important quantities include the (constant) magnetic resistivity , (constant) kinematic viscosity , two source terms E0 and S. The source terms are used to balance visco-resistive dissipations of the initial equilibrium in the study.
The reduced MHD model is originally derived for the low- large-aspect-ratio tokamak from the full MHD model in Ref. [28]. The model is derived by ordering all the quantities based on a large aspect ratio and dropping all toroidal effects. The pressure term is also dropped due to the assumption of low . Here  is the ratio of the plasma pressure to the magnetic pressure. Other waves such as fast and slow magnetosonic waves are asymptotically removed and only the Alfv´en wave remains. The model (1) assumes infinite toroidal aspect ratio, which removes all toroidal terms. Despite being significantly simpler than the full MHD model, (1) still keeps many of the fundamental challenges in the original MHD model, particularly for the tokamak problems we are interested in, including the strongly hyperbolic nature (supporting a stiff Alfv´en wave), strong nonlinear coupling between the magnetic flux and fluid quantities, numerical challenges in the strong-convection regime, large disparity in time and length scales, etc. Note that, as pointed out in Ref. [28], the current system can be viewed as a model for a rectangular tokamak, and thus , J and  are the z-component of the magnetic potential, current density and vorticity vectors of a full 3D model, respectively. In this context, v and B are x- and y-components of the full 3D vectors.
We note that there is an alternative formulation for the reduced MHD model based on evolving the current density instead of . (Note that the current density in the system (1) can be viewed as an auxiliary variable, and thus one could choose evolve either J or .) The alternative model is given by

2 = , t + v ·  - 2 J = -E0 + g(, ), t + v ·  - 2  = B · J - S,

where

2 2 2

2 2 2

g(, ) :=2 xy

x2 - y2

- xy

x2 - y2

,

 =-1J.

The model avoids the challenging B · J term, which involves third-order derivatives for , and the system was first introduced in [29, 30] in order to improve the AMR performance. It is pointed out in the AMR study in Ref. [25] that such an implementation helps minimize the large error around the AMR coarse-fine grid interface. The above model has also been implemented recently in Ref. [27] for a continuous finiteelement implementation and the author pointed out that the challenge of B · J in Ref. (1) will always exist in AMR simulations except for spectral methods. We note, however, that the implementation in Ref. [25] is based on a second-order finite-different algorithm and Ref. [27] uses a polynomial order of 1 or 2. In our implementation, we observed a very similar behavior around coarse-fine interfaces in AMR examples when the polynomial order is lower than 3. However, when choosing the polynomial order greater than or equal to 3, coarse-fine interfaces no longer accumulate large errors. Employing the original model (1) has at

5

least two advantages: (i) the physics-based preconditioning strategy in [3] can be easily extended into the

high-order finite element discretization; (ii) for high-Lundquist-number simulations, the original model leads

to a much simpler SUPG terms in the preconditioner compared to the terms resulting from g(, ) in the

system evolving J. Therefore, we use the model in (1) for our implementation.

We handle the challenging B·J term by introducing an auxiliary variable J and inverting a mass matrix

with . For challenging cases with large Lundquist numbers (typically SL > 105 for which AMR is necessary to capture small scales) we also lump the mass matrix when evaluating J to minimize the oscillations in the

current density during local refinement or coarsening. Such a strategy appears to be sufficient to capture

small structures of reasonably high Lundquist number without introducing large oscillations. We note that

there is an alternative approach for the (A­v) formulation of MHD given in Ref. [4], in which the authors

reformulate the Lorentz force J × B =  · [BB -

B 2

2

I]

to

avoid

high-order

derivatives

in

their

finite

element

formulations. A straightforward extension to the current system leads to source term for  becoming the

third component of

B2

 × (J × B) =  ×  · [BB -

I]

2

=  ·  × [BB]

=  · ( × B)B - B × B .

It is clear that it does not help our implementation, since there are still third-order derivatives with respect to  in the source term. Overall, we find that the B · J term is always challenging to handle in both finite-element and finite-difference discretizations, and it will be interesting to explore other techniques to handle this operator, particularly with AMR.

3. Stabilized finite element formulation
We introduce the FE formulation for the continuous PDE system (1) in this section. The system (1) avoids the Ladyzhenskaya­Babuska­Brezzi (LBB) stability condition [19, 31] by using the potential formulations for both velocity and magnetic field. Therefore, it is natural to employ equal-order polynomial spaces for all the unknowns in the system. Given the small magnitude of resistivity and viscosity, it is all but assured that our simulations will be convection-dominated in most of the domain. In fact, we find that, for some high-Lundquist-number cases, the mesh Peclet number (the ratio between convective strength and diffusive conductance) can be higher than 100 even with a very aggressive AMR refinement. For such cases, relying on excessive AMR refinement is unrealistic due to the rapid increase of the total degrees of freedom and efficiency reduction of any preconditioning strategy. Therefore, it is necessary to use a stabilized FE approach for both  and  to achieve a stable solution locally.
Artificial diffusion is one of the most commonly used approaches to stabilize the convection term. For finite-element schemes, artificial diffusion can be introduced by the streamline upwinding stabilization. Such an approach, however, does not fit high-order schemes, since it inevitably introduces a low-order error whenever the stabilization term is turned on. To avoid this, we rely on a residual-based stabilization approach. These methods are variationally consistent in the sense that the exact solution satisfies the weak formulation, and it is more suitable for high-order approximations. We employ the standard Streamline-Upwind PetrovGalerkin (SUPG) approach with modifications to the stabilization parameter  for AMR and high-order schemes.
3.1. 2D visco-resistive MHD stabilized FE formulation A stabilized SUPG FE formulation is presented for the 2D reduced MHD equation. The formulation
is fully implicit, and the backward Euler method is used in the following presentation for simplicity and convenience. Unless otherwise noted, the numerical cases presented employ either Dirichlet or periodic boundary conditions. Therefore, as an example, we consider a Dirichlet boundary condition imposed on the Perfect Electrical Conductor (PEC) wall as
 = 0,  = 0,  = 0, for x  ,

6

in the discussion. Define the current J := 2 as an "auxiliary" variable and a FE space V as {U | U =
[, , , J ]T , , ,   H01(h), J  H1(h)}. Using the test functions,   H01(h) and   H1(h), a stabilized FE backward-Euler discretization gives

n+1 ·  dx + n+1 dx = 0,

h

h

n+1 - n  dx +

vn+1 · n+1 dx +

h

t

h

n+1 ·  dx
h

n+1 - n

 dx +

h

t

+ R(n+1)vn+1 ·  dx = -
h

vn+1 · n+1 dx + n+1 ·  dx

h

h

E0 dx,
h

- Bn+1 · J n+1 dx +  R(n+1)vn+1 ·  dx = - S dx,

h

h

h

(2a) (2b) (2c)

where the current density J is determined by (note that no boundary condition is needed since it is an

auxiliary variable)

J n+1 dx = - n+1 ·  dx +

n+1  ds,

(3)

h

h

h n

and R(·) in the formulation stands for the residual terms for each equation, which are defined as

R(n+1)

:=

n+1 - n t

+ vn+1

· n+1

- n+1

+ E0,

R (n+1 )

:=

n+1 - n t

+ vn+1

· n+1

- n+1

- Bn+1

· J n+1

+ S.

In the FE formulation, the vectors B and v are the strong derivatives computed from the potentials  and , respectively. Another advantage of the above system is that the vector fields B and v are divergence-free in a strong sense.
In this initial study, the stabilization parameters  in the formulation are taken as the standard term for a transient advection-diffusion equation [19], and are given by

 :=

 2 2v

+

t

h

2

4

2

-

1 2

+ 9 h2

.

Note that the standard formulation for a Crank-Nicholson approximation in time for SUPG has  = 2. In our case we have found this to work well for all cases but the most challenging AMR solution for high Lundquist number (i.e. S  106) island coalescence computations where we have used  = 10. This limited modification of  has the effect of reducing the diffusion where the flow speed is small while still providing sufficient numerical diffusion in the large-flow-speed regions. In the future, we will consider alternate forms for the stabilization parameters. Finally, in our implementation, the stabilization parameters  are computed element-wise, by taking a maximum of all the quadrature points in each element.
Since the current density can be determined from  through (3), the system (2) is a closed system for the three unknowns ,  and . The preconditioning strategy will therefore focus on these three components. Note that other than the nonlinear SUPG residual terms, the nonlinearity of the system comes from the terms related to the convection term v ·  and the Lorentz force term B · J, which will be the main focus when deriving the preconditioning strategy. Although the SUPG terms do not add extra stiffness to the system, some care is needed to achieve a scalable preconditioning strategy.
Comment 1. Regarding to the high-order derivative term B·J, there are three obvious choices to implement it: (B · J, )h , (-J, B · )h or (-BJ, )h . Here, the strong divergence-free condition of B is used. One would expect that the latter two choices are better since the operator is implemented weakly. However,

7

we found that the choices of the implementation help very little to minimize the oscillations in J in some high-Lundquist-number runs when the AMR refinement is very aggressive. Part of the reason is that the integral-by-parts does not fundamentally change the nature of the operator (unlike the reformulation in [4]). For instance, the second operator is simply the transpose of the first operator in our implementation.

3.2. Linearized system of stabilized FE formulation
Before presenting the preconditioning strategy, we first linearize the nonlinear system to simplify the discussion and identify the key terms. We begin by rewriting the system (2) in a block operator form. Let us define several operators to simplify the presentation:

M : mass matrix K : stiffness matrix K~ : fixed matrix that is a sum of stiffness and boundary operators as the RHS of (3) Nu : discrete operator for the continuous operator u ·  where u = [-yA, xA]T

The derivatives of the nonlinear terms corresponding to Nu[, ]T are implemented in a matrix-free fashion
in the FE formulation, i.e., Nu is assembled directly element-by-element without forming a global matrix.
The concept is referred to as partial assembly in MFEM. The current density is J^ = M -1K~ ^ based on (3). Substituting it into the system (2) leads to a nonlinear
equation for G(Un+1) = 0 (where U := [, , ]T ):

K G(Un+1) :=  0
0

0
A NBM -1K~

M 0

 

^ n+1 ^ n+1

+

0 R~(^ n+1)

+

 -

1 t

M

0 ^ n

+



E^ 0

 

,

A ^ n+1

 R~(^ n+1)

 -

1 t

M

^ n

+

S^



where R~(·) is the discrete form of the corresponding nonlinear residual operator R(·), and the two convectiondiffusion operators are defined as:

1

A

:=

M t

+ Nv

+ K,

1

A

:=

M t

+ Nv

+ K.

The Dirichlet boundary condition is enforced strongly in the FE formulation by eliminating the proper
boundary elements if needed. A Jacobian-free Newton-Krylov (JFNK) method is used to solve the nonlinear problem G(U^ n+1) = 0.
Although not explicitly formulated in our nonlinear solver, a reasonable approximation to the Jacobian system of G(U^ n+1) = 0 is

K

0

M  ^ (k+1)

G(U^ (k))

-NB0

A + S

0



^ (k+1)

 

=

-

G(U^ (k))

,

(4)

-P0 PJ0 + NB0 M -1K~ A +  T ^ (k+1)

G(U^ (k))

where the following continuous relations have been used to derive the Jacobian matrix approximation,

v · 0 = -B0 · , v · 0 = -(z × 0) · , B · J0 = -(z × J0) · .
PJ0 u^ stands for the discretization of the continuous operator (z × J0) · u, and P0 u^ stands for the discretization of the continuous operator (z × 0) · u. The readers are referred to [3] for the detailed derivations in a finite-difference context. Its extension to the FE formulation is rather straightforward and

8

thus not presented here. The major difference between [3] and the current work is the additional stabilization terms:
y S : discrete operator for h t + v0 · y - y v0 ·  dx,
y T : discrete operator for h t + v0 · y - y v0 ·  dx.
Note that even though we only keep part of the diagonal operators of the stabilization terms in the linearized form for the preconditioner development, the retained diagonal terms keep the most important term from SUPG: the streamline diffusion term, (v0 · y, v0 · )h , Those terms are found to be critical for the inversion of certain operators in our preconditioner. Other linearized terms in the stabilization terms are ignored in the preconditioner. Keeping all the terms is cumbersome and ultimately unimportant. This is a common approach in preconditioning SUPG for complicated systems, see [32] for instance.
Finally, the resulting system (4) is non-symmetric and indefinite. As pointed on in [3], the system is not diagonally dominant for certain waves, particularly when the time step is large. The stiff Alfv´en waves are retained in the linearized system. An efficient preconditioning strategy is necessary to achieve a scalable solver.

3.3. Time stepping
Using the backward Euler scheme as the building block, the time-stepping algorithm is generalized to high-order using diagonal implicit Runge-Kutta (DIRK) methods in the implementation. For instance, a two-stage second-order DIRK method based on the Butcher table (there are a few choices of x with different stability properties)

x

x0

1 - x 1 - 2x x ,

1

1

2

2

and a three-stage 3rd order DIRK method based on (x = 0.4358665215)

x

x

0

0

(1 + x)/2

(1 - x)/2

x

0

1

-3x2/2 + 4x - 1/4 3x2/2 - 5x + 5/4 x ,

-3x2/2 + 4x - 1/4 3x2/2 - 5x + 5/4 x

are fully supported, among others. The implementation details of DIRK can be found in the MFEM documentation, and many recent studies in numerical ODEs. As pointed out in [33], the DIRK methods provide proper damping for high-order modes, useful for AMR refinement or coarsening. Another advantage of DIRK methods is that they are single-stage, which makes the adaptive time-stepping easy to implement. The adaptive time-stepping is necessary for challenging simulations, for instance involving plasmoids, where the local dynamical time scale may become comparable to the Alfv´en wave speed propagation. When that happens, the time steps typically need to be reduced significantly to resolve the dynamics of interest.

4. Physics-based preconditioning for stabilized finite element
A successful physics-based preconditioning strategy for finite differences was proposed in [3]. Here, we modify and extend it to the stabilized FE formulation. The key steps in the extension include: replacing the finite-difference discretization with finite-element discretization, modifying certain pieces to suit the finiteelement representation better, and incorporating the treatments for the stabilization terms. The key idea is to first explicitly expose the stiffness in the system and then apply an approximate Schur complement based on certain physics understanding.

9

4.1. Saddle-point problem and Schur complement
The linearized system (4) is first converted to an approximate saddle-point system, suitable for Schur factorization. For ease of presentation, we ignore the diagonal stabilization terms and all the superscripts in (4). The first equation in (4) gives

^ = M -1(-K^ - G),

and substituting it into the third equation gives

-P0 ^ +

PJ0 + NB0 M -1K~ ^ +

1

M t

+

Nv0

+

K

M -1(-K^ - G) = -G,

which is simplified as

NB0 M -1K~ + PJ0 ^ -

1 t

K

+

Nv0 M -1K

+

P0

+

KM

-1K

^ = -G - A M -1(-G).

Note that the following approximations are convenient:

NB0 M -1K + PJ0  KM -1NB0 , Nv0 M -1K + P0  KM -1Nv0 ,

based on their continuous analogy,

(B0 · )2 - (z × J0) ·   2(B0 · ), (v0 · )2 - (z × 0) ·   2(v0 · ).

The above continuous approximations are used in [3] for finite-difference approximations. Here, we use their finite-element version. Then, the third equation becomes:

KM -1NB0 ^ - K

1 t

+

M -1Nv0

+

M -1K

^  -G - A M -1(-G),

which is simplified to

M t + Nv0 + K

^ - NB0 ^  -M K-1[-G - A M -1(-G)].

Recall that the second equation in (4) is

M t + Nv0 + K

^ = NB0 ^ - G.

When written as a system, it is a saddle-point system for  and ,

A -NB0 -NB0 A

^ ^

=

-M K-1[-G - A M -1(-G)] -G

,

(5)

with the Schur complement given by

S = A - NB0 A- 1NB0 .

Ideally, one would like to invert the Schur complement using scalable solvers. However, the Schur complement is a dense matrix due to the presence of A- 1, and it is not possible to assemble in practice, requiring further simplification. The study in [3] uses a diagonal approximation to A- 1, and a similar idea will be used here. Note there is a physical meaning in the operator NB0 A- 1NB0 . As, if we simply keep the mass matrix part of the operator A, it becomes tNB0 M -1NB0 , which is an approximation to the continuous Alfv´en wave propagator (B · )2. Since the Alfv´en wave is the stiffest wave, the most important step in the
preconditioner is actually inverting the Alfv´en wave propagator to a sufficiently good approximation. This

10

propagator is symmetric positive definite and therefore amenable to multigrid methods for suitable discrete approximations.
The above preconditioner works well in [3] for finite-difference schemes, and it also works extremely well for continuous finite elements for problems with relatively large resistivities and viscosities (typically  10-4). However, for sufficiently small resistivity and viscosity values, the stabilization terms are necessary in both of the residual and the preconditioner. Involving the stabilization term only in the residual will degrade the overall performance of the nonlinear solver for sufficiently large flows. When we add the diagonal stabilization term into the derivation of the preconditioner, it is easy to show that the modified saddle point problem becomes

A + M K-1( T )M -1K -NB0

-NB0

A + S

^ ^

=

-M K-1[-G - (A +  T )M -1(-G)] -G

.

(6)

The rest of the section will focus on finding a good approximate Schur-complement preconditioner for this system.

4.2. Approximate Schur-complement preconditioner
The approach proposed in [3] uses a diagonal approximation for A to make the Schur complement tractable. We use a similar idea to solve the saddle-point problem (6), and propose two approaches for the stabilized FE formulation.
The first approach inverts the following system with another level of Jacobi iteration (its iteration is denoted by m here)

D

-NB0

-NB0 A + S

^ (m+1) ^ (m+1)

=

-M K-1[-G - (A +  T )M -1(-G)] - O (m) -G

,

(7)

where the off-diagonal operator is defined as O = A + M K-1(T )M -1K - D and the diagonal operator D is defined as the diagonal portion for the operator of A. This is a direct extension of the approach in [3]. All the inversions of the mass and stiffness matrices are implemented through iterative methods.
In the second approach, we note the operator M K-1(T )M -1K composes the diagonal stabilization operator T with the stiffness and mass matrices and their inverses on both sides. If we simply assume all those operators commute with one another, we can simplify the operator to:

M K-1( T )M -1K   T.

(8)

Note that if T is replaced by a mass matrix or a stiffness matrix (or their weighted sum), the above approximation is exact. The operator T consists of the mass and stiffness matrices, but in the PetrovGalerkin sense. Therefore, the approximation (8) appears to be a reasonable assumption. This leads to the following Jacobi iteration for the saddle point system,

D~

-NB0

-NB0 A + S

^ (m+1) ^ (m+1)

=

-M K-1[-G - (A +  T )M -1(-G)] - O~ (m) -G

,

(9)

where O~ = A +T -D~ and the diagonal operator D~ is the diagonal portion of the operator A +T . We have implemented both approaches and have found that the second approach actually performs significantly better than the first one. They all indicate that the stabilization terms can play a significant role in the system and the preconditioner needs to treat those terms carefully.
With a Jacobi iteration of (7) or (9), the Schur complement becomes

S = A + S - NB0 D-1NB0 . In practice, the second operator is implemented weakly, i.e.,

-NB0 D-1NB0  NBT0 D-1NB0 ,

11

and thus

S = A + S + NBT0 D-1NB0 .

Note the last operator is symmetric and positive semi-definite. In the implementation, the operator S is

assembled as a matrix through a matrix-matrix multiplication and a matrix sum, so that we can directly

call available algebraic multigrid (AMG) methods to invert this matrix. A fixed number of Jacobi iterations

is used to make sure it converges to the original saddle-point problem. In Ref. [3], after ^ (k+1) and ^ (k+1) are obtained in the Jacobi iteration for the k-th Newton iteration,
the last component of the solution ^ (k+1) is directly determined by

M ^ (k+1) = -G - K^ (k+1).

Such an approach works well for the second-order finite-difference approximation. However, for high-order finite-element methods used in this work, it is found that it leads to oscillations in the solution of ^ (k+1)
resulting in preconditioner performance degradation. Instead, we propose to use the original system to solve ^ (k+1) again, i.e.,

(A +  T )^ (k+1) = -G + P0 ^ (k+1) - KM -1NB0 ^ (k+1), after the Jacobi iteration. Then we further update ^ (k+1) by

K^ (k+1) = -G - M ^ (k+1).

The above steps invert the convection-diffusion operator and a stiffness matrix, and therefore it will increase the smoothness in both ^ (k+1) and ^ (k+1). To achieve scalability, AMG solver is used through Hypre (BoomerAMG) to invert most of the linear operators in the preconditioning stage, except the mass matrix, which is typically inverted by conjugate gradient with a generic preconditioner. Finally, the steps of the full physics-based preconditioner are summarized in Algorithm 1.

Algorithm 1 Physics-based preconditioning
Compute b = M K-1[G - (A +  T )M -1G] Let (0) = 0
for m = 0 : Nja do Update b~ = b - (A + T - D)(m) Solve S ^ (m+1) = -G + NB0 D-1b~ Solve D ^ (m+1) = b~ + NB0 ^ (m+1)
end for Solve (A +  T ) ^ = -G + P0 ^ - KM -1NB0 ^ (Nja) Solve K^ = -G - M ^

We typically set the total number of Jacobi iterations Nja to 4. How tightly the sub-block matrices are inverted can play an important role in the performance of the preconditioner. Since the mass matrix and stiffness matrix are outside of the Jacobi iteration and they are essentially used to evaluate the right hand side or update the final solution, we invert them tightly with a relative tolerance of 10-6. On the other hand, the Schur complement matrix is inverted inside the Jacobi iteration, and since we do not need a very accurate solution from Jacobi iteration, we invert it relatively loosely with a relative tolerance of 10-4. In some cases, we find that solving the Schur complement more tightly can harm the overall performance of the preconditioner.
The overall cost in the preconditioner consists of inverting the Schur complement Nja times and inverting the stiffness matrix twice. All these inversions are performed with scalable AMG solver. Other costs in the preconditioner are negligible compared to this one. The cost of the preconditioner dominates the cost of the rest of the algorithm, including evaluation of the FE residual G(U) and the AMR overhead. Therefore, the

12

overall algorithm will be scalable in parallel as long as the total number of linear and nonlinear iterations do not grow when we increase the number of processors and, correspondingly, the mesh. This will be demonstrated later in the numerical section.
It is tempting to ignore the stabilization terms in the preconditioner since they are "small" compared to other operators. However, we find ignoring those terms can severely degrade preconditioner performance when the flow speed becomes sufficiently large. The reason is related to the Schur complement and the convection-diffusion operator A being inverted in the preconditioner. As demonstrated in [34], when the viscosity is around 10-5 or smaller, the AMG preconditioner can become very inefficient to invert the convection-diffusion operator if it is under-resolved even when a SUPG formulation is used. Without the SUPG term, it behaves much worse and the AMG preconditioner will fail to invert the convection-diffusion operator. The same in our context also applies to the Schur complement. Although the Schur complement involves the elliptic operator (B · )2, it is found that in certain regions the flow speed will dominate the magnetic field, resulting in AMG failure. Therefore, it is critical to keep the stabilization terms in both the residual and the preconditioner.
The current study only considers the essential boundary condition as the physical boundary condition. To impose it in the preconditioner, the diagonal terms are replaced with 1 and all the off-diagonal terms are replaced with 0 in A and ASL , while all the terms associated with the boundaries are replaced with 0 in NB0 . We have verified that there is no boundary layer introduced by the preconditioner in all the numerical examples. Finally, all the operators such as mass and stiffness matrices are discretized by the operator of the same polynomial spaces, and we have not explored the possibility of using the same operators from lower-order representations. As we document later, the above preconditioning strategy works well.
5. Preconditioned JFNK-based algorithm
We briefly summarize the high-level steps to solve the implicit system using JFNK combined with inexact Newton [35] and our physics-based preconditioner. For more details on JFNK and inexact Newton methods, see the review paper [36]. Assume at time tn there is a known solution of U^ n = [^ n, ^ n, ^ n]T , and we are seeking U^ n+1. JFNK consists of the following four important steps:
1. Define a nonlinear equation: G(U^ ) = 0 for U^ n+1 using the previous solution U^ n. 2. Perform a Newton iteration: the kth Newton iteration is defined by solving
JkU^ k = -G(U^ k),
where the matrix vector product is defined as Jkv := (G(U^ k + v) - G(U^ k))/ and stands for a small perturbation in the matrix-free product. 3. Solve the Jacobian system with an FGMRES Krylov solver : The FGMRES convergence tolerance is adjusted adaptively according to JkU^ k + G(U^ k) <  G(U^ k) , where  is the inexact Newton parameter. 4. Apply the preconditioner for FGMRES : a right preconditioner is used in each linear iteration:
(JkPk)(Pk-1U^ k) = -G(U^ k).
The product of JkPkvj in the jth Krylov iteration is provided by first finding y = Pkvj and then computing Jky by the definitions in Steps 1 and 2. Preconditioner Pk is the physics-based preconditioner described in Algorithm 1.
The JFNK solver provided in PETSc is used here. A fixed convergence criterion for the Newton iteration throughout all the numerical examples is used with a relative tolerance of 10-4 and no absolute tolerance. A fixed maximum number of 20 Newton iterations is used. If the number of Newton iterations exceed 20, we determine that Newton iteration has failed. In this work, a reasonable range for the matrix-free parameter
is found to be [10-4, 10-1], which is somewhat larger than values used in previous work [3]. This indicates the numerical residual are slightly more noisy, which is generally true since the current work uses high-order finite-element bases and targets much higher Lundquist numbers. Note that while the value of does not have an impact on the converged solutions, it may significantly affect the convergence of FGMRES.
13

The number of Newton iterations is also used as an indicator of the presence of fast dynamics during the time stepping. Since we have a very efficient physics-based preconditioner for the stiff Alfv´en wave, a general expectation is that the Newton iteration will converge very fast, typically only requiring 2 to 8 iterations in our implementation. If the Newton iteration fails to converge after 20 iterations, we surmise that there are fast dynamics present in the system, which cannot be stepped over by the implicit scheme. This happens, for instance, when plasmoids appear in the solution. When such a scenario occurs, we reduce the time step by a factor of two. When the Newton iteration starts to converge, we slowly increase the time step to improve the efficiency until the targeted time step is reached or the Newton iteration fails again. For simulations involving plasmoids, this strategy is necessary to run through the entire simulation.

6. Dynamic adaptive mesh refinement
AMR is an important component for reduced MHD, and a number of previous AMR implementations are available in [25, 27, 29, 30]. A key element in all AMR algorithms is the refinement/coarsening indicator, which are of two types, error-based and feature-based. For instance, in the finite-difference-based AMR solver in [25], the refinement criterion is based on the feature in the current J and vorticity , which is ad hoc, requiring parameter tuning and resulting in unnecessary refinement or coarsening. A common error-based approach for finite-difference is through Richardson extrapolation, which can be cumbersome to implement. On the other hand, an advantage of FE-based AMR algorithms is the availability of many a posteriori error estimators (see the textbook [37] for details). Unfortunately, for a complicated system like the reduced MHD equation, there is no a posteriori error estimator currently available. Ref. [27] instead uses a feature-based indicator for the FE formulation of the same MHD problem. In this work, we propose to use a combination of feature-based and a posteriori error estimators. Our approach is inspired by the AMR error estimator in the deal.II package [38]. In deal.II, a Kelly error estimator [39] is typically used for a complicated system if the system has an elliptic nature (or parabolic if it is time-dependent). Although the Kelly error estimator was originally proposed for the Poisson equation, the approach is used to estimate more complicated systems in deal.II. The estimator uses the gradient of the solution as a gradient recovery estimator. Even though the estimator does not feature theoretical bounds, in practice it typically gives a good hint for mesh refinement.

6.1. Error estimator

The error estimator is based on the superconvergent patch recovery (SPR) approach proposed in [40, 41],

commonly referred to as the Zienkiewicz-Zhu estimator. In a gradient recovery approach, the a posteriori

error estimator for a small domain K is simply defined as the energy norm of the difference between estimated

and numerical fluxes

K2 (uh) :=

Gh[uh] - uh 2 dx,

K

where Gh[uh] denotes an accurate approximation to the gradient of the true solution and uh is the numerical

gradient (the flux for the Poisson equation). The idea of SPR is to use a least-squares approximation to

recover a more accurate flux Gh[uh] locally, and the superconvergence of such a recovery can be proved at

certain points in the element. Note, however, that the above energy norm for the flux only rigiuously leads to

a good estimator for the Poisson equation -u = f . Thus, in principle, the Zienkiewicz-Zhu estimator can

only be used for  in the system. However, since the approach proposed in the current work is fully implicit,

one can argue that the above estimator is applicable to other components as well, since the dominant error

in the numerical solution is the component containing the highest-order derivative, which is a Laplacian

operator with constant coefficients. This is a common approach for practical problems, for instance, when

extending the Zienkiewicz-Zhu estimator from -u = f to a more general case -u + cu = f ; see the

discussions in [37] for details.

The estimated error for each element is then defined as a weighted sum of the Zienkiewicz-Zhu estimated

errors of certain components. Following [25], we choose one component from the fluid variables, , and one

component from the field-related variable, J. It is common in simulations that both variables contain more

interesting structures than other variables. The total error is then defined as a simple sum of two components

of the solutions, such as,

eK :=  K (J ) + K (),

(10)

14

where   (0, 0.1) is taken. Here,  is chosen as a small value because the current value can be significantly larger than the vorticity in our simulations. Note that our MFEM implementation can readily leverage to future error estimators, when they become available in the package.
6.2. Refinement strategy A successful AMR algorithm necessitates a good refinement and coarsening strategy. It is particularly
important to find balance among different pieces in the adaptive solver, including the refinement/coarsening frequency, the overhead due to the AMR algorithm, effective grid resolutions, smallest length scales and fastest time scales in the system, etc. For instance, interpolation or restriction grid operations may lead to small local oscillations in the solution near refinement boundary, which typically need a few time steps to damp away. If adaptive patches are changed too frequently, the small oscillations will accumulate and result into larger estimated errors later, which further leads to more unnecessary refinement. On the other hand, if adaptive grids are changed too infrequently, it leads to under-resolved structures. We have found under-refinement to be particularly problematic in simulations with current sheets of high aspect ratio in two ways: (i) it leads to oscillations in the current sheets, resulting in a lot more plasmoids generated (much more than well resolved runs, indicating triggered by oscillations); (ii) it typically results in the failure of the solver.
We find a refinement and coarsening frequency of once every four to ten time steps to be a good balance for the problems considered. The refinement and coarsening stages are called in the same time step. In order to mark elements for refinement/coarsening, we first compute the estimated errors eKi for each element Ki using (10) and then valuate the total error etotal though a L2-norm of the errors of all the elements. The L2 norm instead of L is chosen to avoid the total errors being dominated by a few elements. The element Ki is refined locally if
eKi > max(r etotal, elgoal) and etotal > egoal.
The element Ki is coarsened locally if
eKi < max(c etotal, c elgoal).
Here egoal and elgoal are the global and local error goals for the refinement and coarsening criteria, and they are used as safeguards to avoid refining or coarsening excessively. We typically set r = 0.1 and c = 0.001. Using the percentage of the total error is common in many finite-element AMR solvers based on error estimators, see [42, 43] for instance. Finally, after the mesh is updated, a load-balancing algorithm is called to guarantee the parallel performance.
7. Parallel implementation in MFEM
The described algorithm for the 2D visco-resistive MHD equation was implemented in the C++ finite element library MFEM [44]. Our implementation is complicated and below we comment on several specific points.
The fully implicit solvers, along with a simple explicit solver, are implemented in both serial and parallel in MFEM. The simple explicit solver is only implemented as a verification tool. All the results discussed in the current work are generated with the parallel fully implicit solver. The parallel algorithms are implemented using an MPI-based domain decomposition method. The vectors and small block matrices use the parallel distributed data structure in MFEM. PETSc [45] provides the nonlinear Newton solver as well as the linear solvers in the preconditioner, and Hypre [46] provides linear solvers in the FE residual and also provides the preconditioner for the sub-block matrices like the Schur complement and the convection-diffusion operator to improve scalability. The physics-based preconditioner is implemented through MFEM's PETSc interface. We take the full advantage of the pcshell interface provided by PETSc and linked by MFEM. In this shell interface, the input is a block vector x consisting of three component, ,  and , and its output is y = Px where P is the physics-based preconditioning described in Algorithm 1. Sub-block matrices assembled in MFEM are inverted using PETSc inside the shell interface. Note that the physics-based preconditioner is created once in every Newton iteration and it is reused until the FGMRES iteration converges.
Our implementation of the algorithms is general, supporting arbitrary order of accuracy and general meshes including triangular, quadrilateral and high-order curvilinear meshes, taking full advantage of the
15

Figure 1: Island coalescence test with AMR. Left: the current sheets and plasmoids around the center region before plasmoids become unstable. Right: the corresponding adaptive mesh and domain decomposition through dynamic load-balancing. Here,  =  = 10-7, p = 3 and 7 refinement levels are used. The AMR mesh captures both current sheet and plasmoids very well.
MFEM capabilities. However, in the numerical study, we focus on quadrilateral meshes and polynomial order less than or equal to four, particularly in the high-Lundquist-number runs. For high-order polynomials such as p = 8 or higher, the preconditioner performs considerably worse than the lower-order polynomials, which indicates the need for further tuning. The AMR implementation supports both conforming and nonconforming meshes with coarsening and refining as well as dynamic load-balancing. In practice, we focus on the nonconforming quadrilateral meshes due to their more flexible refinement. A load-balancing for quadrilateral meshes is based on a space-filling curve and it leads to a continuous decomposition of the meshes throughout the simulations, while other meshes typically cannot achieve that and the decompositions become discontinuous as time evolves. See Figure 1 for a sample AMR mesh. For more details on the nonconforming AMR algorithm and its MFEM implementation, see Ref. [47].
105 1
104 1
103

Time

102

100

101

102

103

# of CPUs

Figure 2: Strong scaling result. The test is island coalescence with a mesh size of 768 × 768 and p = 3.  =  = 10-3. Excellent nearly perfect linear scaling is observed up to 2048 CPUs.

Here

One significant advantage of the MFEM package is its scalability, which has been demonstrated up to hundreds of thousands of processors (as well as on GPU systems) [44]. In the current work, the majority of the cost of the algorithm comes from the preconditioner, which takes the full advantage of AMG solvers, so it is not too challenging to achieve good strong scaling. See Figure 2 for instance, which demonstrates a perfect linear scaling for the current solver up to 2048 CPUs on a relatively coarse mesh. (Due to the 2D nature of problems considered, the total number of processors typically does not exceed 4098.) The benchmark problem is an island coalescence with a relatively small Lundquist number. See the corresponding numerical section for more details of the problem setup. However, while achieving a good strong scaling is relatively easy in MFEM, it is very challenging to achieve a good weak scaling in implicit solvers, as it requires algorithmic

16

scalability. This will be one focus of the study in the numerical section.
Reproducibility. The implementation of the solvers described above, as well as most of the numerical examples presented in this work, is freely available as an MFEM branch of tds-mhd-dev distributed along with MFEM's master branch on GitHub2. The tds-mhd-dev branch currently contains both serial and parallel versions of the solver including a simple explicit scheme and fully implicit schemes along with physics-based preconditioning and AMR.
8. Numerical results
Several benchmark problems are considered in this section to demonstrate the accuracy and efficiency of the proposed full algorithm. The benchmark problems are organized in the order of increasing complexity. We begin in Section 8.1 by considering a linear Alfv´en wave propagation example. This benchmark problem provides a good first test of the accuracy of the scheme as well as the performance of the iterative solver. In Section 8.2, we consider a second benchmark problem propagating MHD Rayleigh flow and Alfv´en wave. The example is used to demonstrate the high-order spatial accuracy. Section 8.3 considers the tearing mode instability. Some physics scaling results are presented to validate the associated solver. The example is also used to perform a weak scaling test to demonstrate the algorithmic scalability of the proposed solver. This example is then used for a careful study of the performance of the AMR solver. Attention will be devoted to the accuracy improvement of the adaptive meshes, the computational cost savings, the performance of the iterative solver along with the AMR algorithm, etc. Some quantitative comparisons in those aspects will be presented. The last problem we consider in Section 8.4 is the island coalescence. The study of this example is split into two parts, steady meshes (Section 8.4.1) and adaptive meshes (Section 8.4.2). The steady mesh results are used for validation study through a physics scaling test relating the reconnection rate with the Lundquist number. A weak scaling demonstration up to 4096 CPUs is also presented on the uniform meshes using a moderate Lundquist number. The robustness and efficiency of the AMR solver is then demonstrated using moderately high to high Lundquist-number setups. In particular, results with Lundquist number of 105, 106 and 107 are presented. The solutions around the current sheets show a transition from no plasmoids (SL = 105), to isolated plasmoids and simple dynamics (SL = 106), and finally to many colliding plasmoids and complicated dynamics (SL = 107). Some interesting physics phenomena in the numerical solutions are observed. The computational savings of the adaptive meshes over a uniform refined mesh is also presented.
Throughout all the tests, a relative tolerance of 10-4 with no absolute tolerance of the Newton solver is used. A solution update tolerance of 10-6 is added to avoid over-solving, which only activates in the finest uniform resolution during the weak-scaling study. The Schur complement uses the same relative tolerance of 10-4 while the advection-diffusion operator is typically solved tighter, with a relative tolerance of 10-6. Other operators such as the mass matrix and stiffness matrix are also inverted tightly using a relative tolerance of 10-6. The parameters associated with JFNK and inexact Newton methods use the default values of PETSc. All the scaling tests are performed on the LANL Grizzly cluster, which consists of XEON E5-2695V4 18C 2.1GHz processors.
8.1. Alfv´en wave propagation We first consider a linear Alfv´en wave propagation test with zero dissipation ( =  = 0). The problem
is similar to the example given in [3], except a manufactured solution is introduced to examine the accuracy here. Consider the following system
2 = , (t + v · )  = -E0, (t + v · )  = B · J.
2See https://github.com/mfem/mfem/tree/tds-mhd-dev.
17

L2 Error L2 Error L2 Error

in the domain of [0, Lx] × [0, 1]. A manufactured standing-wave solution satisfies
 = - sin(y) sin(kx) sin(kt),
 = -y +  sin(y) cos (kx) cos(kt),  = (2 + k2) sin(y) sin(kx) sin(kt),
where the frequency is k = 2/Lx. To balance the equation, a time-dependent source electric field is needed E0 = 2k sin(kt) cos(kt) sin(y) cos(y). A periodic boundary condition is imposed in the x-direction and a homogeneous Dirichlet boundary condition is imposed in the y-direction. We let Lx = 3 and the perturbation magnitude  = 10-3. The problem is essentially driven by the background magnetic field of B0 = [1, 0]T which results into an Alfv´en wave with speed of 1. Note that the system develops a standing wave in the domain. The problem is used to examine both of the spatial and temporal accuracy of the scheme as well as the efficiency of the preconditioner. Due to the special linear-wave structure, the problem does not need a physical or numerical dissipation. Nevertheless, we still apply the fully implicit nonlinear solver to the system, as a way to examine overall performance of the algorithm.

10-3

Backward Euler
  

10-4



 

10-5

DIRK2

10-5



  10-6

DIRK3

10-4

1 1

10-1.8 10-1.6 10-1.4 10-1.2 10-1 t

10-6 10-7

2 1

10-1.8 10-1.6 10-1.4 10-1.2 10-1 t

10-7 10-8 10-9

3 1
10-1.8 10-1.6 10-1.4 10-1.2 10-1 t

Figure 3: Wave propagation. The refinement study for the temporal integrators are presented. A fixed computational grid of 192 × 192 is used for the first two tests, while a computational grid of 768 × 768 is used for the third-order integrator. The polynomial degree used in all the tests is p = 3. The expected temporal orders of accuracy are observed with all the integrators.

We have verified several time integrators implemented in the solver, including the backward Euler method, a second-order DIRK scheme and a third-order DIRK scheme. To avoid any potential superconvergence related to the sinusoidal waves, we compare the numerical solutions with the exact solutions at t = 1.8. A computational grid of 192 × 192 with polynomial order of 3 is used for the backward Euler and DIRK2 integrators while a grid of 768 × 768 is used for the DIRK3 integrator. The numerical errors are dominated by the temporal errors for these grid resolutions. The time step starts from t = 0.1 and refined by a factor of 2. The resulting numerical errors vs the time steps are presented in Figure 3. Here a L2-norm is used to evaluate the absolute errors. Note that the desired orders of accuracy are observed for all the time integrators.

Table 1: Wave propagation. Solver performance during the temporal refinement study (DIRK2) are presented. The averaged number of Newton iterations and Krylov iterations in each solve are presented. Note DIRK2 contains two backward Euler solves per time steps. The computational grid is fixed as 192 × 192 and p = 3. A fixed number of CPU=32 is used for all the runs.

Wave propagation (DIRK2) t Total steps Newton per solve Krylov per solve Total time (s) Time/t

0.1

18

0.05

36

0.025

72

0.0125

144

2.8333 2.7917 2.9167 2.4653

9.4167 8.0972 7.3542 5.2743

1369.53 1900.51 2176.62 2681.29

76.09 52.79 30.23 18.62

18

The problem is also used to examine the performance of the preconditioner. We use the second-order DIRK results for this study. Performance details are summarized in Table 1. The computational grid is fixed to 192 × 192, which corresponds to a grid spacing of x = 0.015625. The time step of t = 0.1 is therefore much larger than tCF L. Here tCF L follows the definition in Ref. [3], which is identical to x due to the normalized Alfv´en wave speed of 1 and CFL of 1, but for the high-order finite-element scheme, the stable explicit time step is typically smaller. We also note that the number of averaged Newton and Krylov iterations only increase slightly when the time step increases. As a result, the large time-step run is faster than smaller time-step runs. The good performance of the preconditioner is consistent with the nature of the problem, as it is dominated by the Alfv´en wave propagation. Note that the CPU time per time-step increases when the time steps increase. This is largely because the sub-block matrices, particularly the Schur complement, in the preconditioner become easier to invert for smaller time steps. For a given targeted accuracy, the overall CPU time suggests that it is preferable to take a large time step. This fact will be further emphasized in the next few examples.
Spatial convergence study

10-4



10-5

 

10-6

3 1

L2 Error

10-7

10-8

10-9

4

10-10

1

10-11 10-1.2 10-1 10-0.8 10-0.6 10-0.4 x
Figure 4: Wave propagation. Grid refinement study for p = 3. A fixed time step of t = 0.001 is used for all the simulations and the third-order DIRK integrator is used. Fourth-order accuracy is observed for both  and  while we find order-reduction for .

The problem is also used to verify the spatial accuracy of the solver. Verifying the spatial accuracy turns out to be challenging. To make sure the numerical error is dominated by the spatial accuracy, we use the third-order DIRK integrator with a much smaller time step t = 0.001. Due to the small time step used, we need to adjust the relative tolerance to 10-7 to fully expose the spatial accuracy. We focus on p = 3 in the convergence study. The refinement study starts from a 6 × 6 grid and refine it by a factor of 2. The resulting numerical errors vs. the grid spacing are presented in Figure 4. It is found that the expected accuracy (fourth-order) is observed for both  and  while  shows order reduction, with a convergence rate around 2.5. The reason for the poor performance of  is two-fold: the problem is diffusion-free, and the error in  is dominated by the high-order derivative B · J term. More importantly, note that physics components of interest in the reduced MHD system are  and , since they are used to construct the velocity and magnetic fields, respectively, while  acts like an auxiliary variable in the system. Therefore, for the purpose of practical simulations, it is sufficient to achieve optimal accuracy for  and .
We compare the results of p = 3 and p = 2 in Figure 5. The figure shows oscillatory behaviors in both of  and J for p = 2, while other solution components, such as , remain smooth. The assessment of the spatial accuracy for p = 2 indicates  and  are second-order while the numerical error in  decays slower than first order. However, as we shall see, the quality of the solution with p = 2 improves markedly in the presence of physical dissipation.

8.2. MHD Rayleigh flow and Alfv´en wave propagation
We consider the MHD Rayleigh flow and Alfv´en wave propagation with a known exact solution. An analytical solution to the MHD equation is given in [48] and tested in [4, 49]. The exact solution is given by

19



J





J



Figure 5: Wave propagation. The numerical solutions at t = 1.8 are presented for p = 3 (top row) and p = 2 (bottom row). Note that the current and vorticity become very oscillatory with p = 2, but other solution components remain smooth.

B

=

[Bx,

 B0/ µ0

]T

and

V

= [Vx, 0]T

where,

in

our

units:

Bx

=

U -
4

e-

A0 d

y

-1

+

e

A0 d

y

erfc

-A0t + y

+

e

A0 d

y

erfc

A0t + y

2 dt

2 dt

,

U Vx = 4 erfc

-A0t + y 2 dt

+ erfc

A0t + y 2 dt

+

e-

A0 d

y

erfc

-A0t + y 2 dt

+

e

A0 d

y

erfc

A0t + y 2 dt

,

and A0 here is the Alfv´en wave speed given by A0 = B0/µ0. Unlike the previous test, we consider here the full SUPG formulation.

The corresponding components in the resistive MHD system considered in the current work can be

worked out as

U =
4A0

tA02

+

de-

A0 d

y

- A0 y

+d

erf

A0t - y

+

tA02

+

de

A0 d

y

+ A0 y + d

erf

A0t + y

2 dt

2 dt



-

de

A0 d

y

+

de-

A0 y d

- 2 A0 y

+ U dt 2

e + e -

(A0 t-y)2 4dt

-

(A0 t+y)2 4dt

  = B0 x - U dt
µ0 2 

e - e -

(-A0 t+y)2 4dt

-

(A0 t+y)2 4dt

U -

tA20 + d

4A0

erf A0t - y 2 dt

- erf A0t + y 2 dt

U +
4A0

de-

A0 d

y

+ A0

y

erfc

-A0t + y

+

de A0 y d

- A0

y

erfc

A0t + y

2 dt

2 dt

 = U A0

2

e-

A0 d

y

-

e

A0 d

y

erfc

A0t + y

4d

2 dt

U +
2 dt

e + e -

(A0 t-y)2 4dt

-

(A0 t+y)2 4dt

-

e-

A0 d

y

erfc

A0t - y 2 dt

Note that due to the definition of the magnetic potential, there is a negative sign in  compared to the magnetic potential given in [4]. It is easy to verify that the above components satisfy the MHD system (1) with an external E field given by E0 = -U B0/(2 µ0) and d =  =  = 1. Therefore, the magnetic

20

Prandtl number P rm = / = 1 in this test. The rest of the parameters are taken from [4, 49] as U = 1.0,  = 4 × 10-5, B0 = 1.4494 × 10-4, and µ0 = 1.256636 × 10-6. The resulting Alfv´en speed is A0  20.44. The computational domain is [0, 5] × [0, 5] with a Dirichlet boundary condition applied to all the component of
the system using the exact solution. Note that unlike the case in [4, 49], the vorticity at t = 0 has a singular
boundary condition in our MHD system. To avoid such a singularity in the boundary condition, the exact
solution is initialized at t = 0.02 in this test.





2

t = 0.04

1

t = 0.06

t = 0.04 t = 0.06

t = 0.08

1.5

t = 0.08

0.8

t = 0.10

t = 0.10

0.6

1

0.4 0.5
0.2

0

0

012345
y


012345
y
Spatial convergence study (p = 3)

1

10-2



10-3

 

0.8

10-4

3

1

0.6

10-5

L2 Error

0.4

10-6

0.2

t = 0.04 t = 0.06

10-7

4

0

t = 0.08 t = 0.10

10-8

1

012345

10-2 10-1.8 10-1.6 10-1.4 10-1.2

y

h

Figure 6: MHD Rayleigh flow and Alfv´en wave propagation. The numerical solution at t = 0.08 at x = 0 are

presented as green triangles in the above plots. The exact solutions at different times are plotted as a reference. The

spatial refinement study for p = 3 is presented at the bottom right. The expected order of accuracy is observed in 

and  while the accuracy of  is shows order reduction.

We present the results of p = 3 in Figure 6. In particular, the numerical solution at t = 0.08 and x = 0 solved on a grid of 64 × 64 is presented as green triangles. The figures suggest the solution is smooth and matches well with the exact solution. A grid refinement study for p = 3 is performed at t = 0.08 and its result is also presented in Figure 6. It is observed that an expected order of accuracy is achieved in  and , while  again shows order reduction (although less than in the wave propagation case) with the finest resolutions showing an order of 2.8. Here the DIRK3 time integrator is used with a very small time step. The current test also suggests the SUPG terms do not diminish the optimal order of accuracy of the overall scheme.

21

8.3. Tearing mode

The next example we consider is the resistive tearing mode [3, 25, 50, 51]. The tearing mode is one of

the most magnetic

fundamental resistive instabilities. It is a spontaneous topologies [20]. The inverse time scale and length scale

roefcotnhne epcrtoiobnlemproscceaslsesoacscurrinagndin tshhuesartehde

problem becomes more challenging with increasing Lundquist numbers. This test is used to validate the

full solver through some physics scaling tests, and it is also used to verify the accuracy and efficiency of the

adaptive solver.

The same problem setup as described in [3] is adopted. The initial condition starts from an equilibrium

given by

1

1

0

=



ln[cosh (y

-

)], 2

0 = 0,

0 = 0,

balanced with a source term of E0 = 20. The initial magnetic field here is based on a Harris current sheet equilibrium. The problem is excited by a perturbation on 0 of

 = 10-3 sin(y) cos

2 x

.

Lx

The computational domain is [0, Lx] × [0, 1] and the boundary conditions are the PEC wall (Dirichlet) along the y boundaries and periodic along the x direction. The other parameters are Lx = 3,  = 5, and  =  = 10-3, unless otherwise noted.
The study of this example is split into two parts in the following discussion. In the first part, the solver
is verified on uniform meshes against previous work including some physics scaling studies, and tested in
parallel up to 4096 processors. In the second part, the solver is tested with dynamic AMR.

8.3.1. Uniform-mesh results







J

Figure 7: Tearing mode. The poloidal flux , streaming function , vorticity  and parallel current J at t = 250 are presented. Here  =  = 10-3. The computational grid is 96 × 96 and the polynomial degree used is p = 2. The time step is t = 5 and DIRK2 is used as the time integrator.
The problem is first solved on a uniform mesh to compare with the results given in [3]. In Figure 7, the solutions of the tearing mode at t = 250 are presented. In this test, a computational grid of 96 × 96 is used with a large time step t = 5. The test uses p = 2 and the DIRK2 time integrator. For such a resolution, the preconditioner performs very well with large time steps (the time step corresponds to roughly 160tCF L based on the definition given in [3]). In particular, the averaged number of Newton iterations is 2 and the averaged total Krylov linear solver iterations are about 5. Both the numerical solutions and performance of the solver are in excellent agreement with the results given in [3].
As a part of our verification tests, we also perform the same scaling tests as presented in [3]. The first verification test is the time history of the magnetic perturbation log  2 (global measure) and the current
22

ln  2 
J(Lx/2, Ly/2)

 in tearing mode,  =  =10-3 -3 -4

J in tearing mode,  =  =10-3 5 4.9

-5

4.8

-6

.0433

4.7

1 -7

p=2

4.6

p=3

p=2 p=3

0 50 100 150 200 250
t

0 50 100 150 200 250
t

Figure 8: Time histories of the L2-norm of the magnetic perturbation (global measure, left plot) and the current at the grid center point (local measure, right plot) for the tearing instability. The corresponding growth rate of  = 0.0433 is also presented. A computational grid of 96 × 96 is used with different polynomial degrees. The time step is t = 5 with DIRK2 for p = 2 and DIRK3 for p = 3.

density at the midpoint of the computational domain (local measure). We present the results of p = 2 with the DIRK2 time integrator and p = 3 with the DIRK3 time integrator in Figure 8. We note that both curves match well with each other, as well as with the second-order large time-step solution given in [3]. The kink in the current plot of Figure 8 is due to the large time step taken (t = 5). A simple linear regression is used to estimate the growth rate of the tearing mode as 0.0433, which is also comparable with the reported growth rate in [3].
Tearing mode growth rate

10-1 10-2

1 3/5
1

10-3

5/6

102

103

104

105

106

SL

Figure 9: Scaling of the tearing mode growth rate  verse the Lundquist number SL for a fixed Reynolds number Re = 103. Two theoretical scalings (SL-3/5 and SL-5/6) are shown for comparison.

The growth rate is further studied by varying the Lundquist number. An identical scaling test is performed in Fig. 3 of [3]. The Reynolds number is fixed at 103 while the Lundquist number varies between 102
to 106. The scaling result is found to be comparable to Ref. [3]. The growth rate is close to SL-3/5 in the small Lundquist number cases and approaches to SL-5/6 when the Lundquist number increases. We find the SUPG stabilization on the  equation to be necessary when the Lundquist number is greater than or equal to 105. Note the Lundquist number for this case is simply defined as 1/.
The last important test on the uniform meshes is the parallel weak scaling test. The Reynolds number

23

Table 2: Preconditioner performance study for the tearing mode. Re = SL = 104. DIRK2 is used that has two solves per time step. A fixed relative tolerance of 10-4 and a fixed relative solution tolerance of 10-6 are used as the
Newton convergence criterion. The steps are averaged over 10 time steps and p = 2, t = 1. Dofs stands for total
degrees of freedoms in one scalar component.

Grid
96 × 32 192 × 64 384 × 128 768 × 256 1536 × 512 3072 × 1024 6144 × 2048

dofs
12480 49536 197376 787968 3.1M 12.6M 50.3M

t/tCF L
32 64 128 256 512 1024 2048

CPUs
1 4 16 64 256 1024 4096

Tearing mode Newton/solve
2.0 2.0 2.0 2.0 2.0 2.0 3.0

Krylov/solve
4.5 4.9 6.5 8.5 15.5 29.5 35.5

Time(s)/t
11.5 16.0 20.5 30.1 38.7 58.5 72.3

Time(s)/Krylov
1.3 1.6 1.6 1.8 1.2 1.0 1.0

and Lundquist number are taken as 104 as representative, and the general trend of the scaling results is very similar for other choices of Reynolds/Lundquist numbers. The weak scaling result is reported in Table 2. Note the number of dofs on each processor is fixed to 12480. We choose a uniform mesh such that the grid spacings along x- and y-directions are identical, which we found gives a more consistent performance than the case of different grid spacings considered in [3]. The reported dofs are the number of dofs of a scalar unknown. The time step is fixed to t = 1 and thus the problem becomes more and more stiff as we refine the mesh. The problem has been normalized such that the shear Alfven speed is 1. As a result, the CFL number of the mesh 6144 × 2048 is about 2048. The number of Newton iterations, Krylov iterations and CPU time per time step are reported in Table 2. The dominant computational cost of the current solver stems from inverting the preconditioner (representing 80-90% of the total cost). The computational cost is found to be proportional to the number of Krylov iterations per solve. The wall-clock time per Krylov solve (Time(s)/Krylov) is found to stay more or less constant, which indicates a scalable parallel implementation (attributed to the domain decomposition of MFEM, algebraic multigrid of Hypre, etc.). The more interesting and exciting result is the algorithmic scalability of the overall algorithm. It is found the number of Krylov iterations grows weakly with respect to the number of dofs. Note that Krylov iterations grow in the last few iterations, largely due to growth of the condition number of the system. In fact, for the last resolution of 6144 × 2048, the solution update in the last few Newton iterations is close to "noise". To avoid solving on the top of noise, we add a fixed relative solution tolerance of 10-6, i.e., the iteration stops when the relative solution update is less than 10-6. This solution tolerance is only necessary in the finest resolution. The overall performance of the current work is very comparable with [3], in which the solver shows perfect scaling until the very last few refined meshes. Note the convergence criteria used in the current work is tighter compared to other works such as [13], where comparable finite element schemes are used to solve extended MHD. A looser tolerance can improve the scaling result significantly, particularly with fine meshes. Overall, the weak scaling performance is excellent. The CFL number from a single processor to 4096 processors grows by a factor of 4096, while the computational time only grows by a factor of 6.3.
We would like to comment that, despite the growth of iterations in the last few resolutions, this kind of uniform resolution is not necessary in practice. AMR significantly reduces the total dofs and guarantees a smaller workload on the preconditioner. This will be discussed in the next section.
8.3.2. AMR results In the second part of this numerical example, we examine the performance of the fully implicit solver
with dynamic AMR. We will consider accuracy and performance with respect to the uniform mesh solver, and we will compare our solver against existing ones for the same system [25, 27].
We choose a simple tearing mode test problem to perform all the AMR performance studies in this section, while a more interesting and challenging test will be considered in the next section. Our setup follows the same test given in the finite-difference-based AMR solver of [25]. The test case uses relatively large resistivity and viscosity  =  = 10-3. As indicated by the uniform mesh results in Figure 7, the solutions are rather smooth and the current sheet does not have very sharp structures. Despite its simplicity, this is a very useful
24

test to verify accuracy improvements in the AMR solutions. The error estimator is chosen as a combination

of the error in  and 

eK :=  K () + K (),

(11)

where  = 0.1. We choose the refinement ratio r = 0.1 and the coarsening ratio c = 0.001 as described in Section 6.2.

Note that the indicator here uses the error estimator as the refinement criterion. As a comparison,

the AMR solver in [25] uses an ad hoc refinement/coarsening indicator, while the work in [27] chooses the

Hessian matrix of certain solution as the indicator, which according to Ref. [27] is an indirect reflection of

some local error estimator. In general, an indicator based on the error estimator is preferred since it needs

less tuning compared to the approach in [25] and it can be easily extensible to other test problems. The

AMR approach in the current work is non-conforming and it is octree-based using the quadrilateral elements

as the building block. The octree-based approach only changes the mesh very locally in the re-gridding, and

the parallel communication and rebalancing is very minimal. As a result, the overhead of the AMR stage

of the current solver needs less than 1% of the total computational time. This contrasts with the approach

described in [25, 27]. The former uses a patch-based AMR package while the latter uses an unstructured

grid. But, as we previously found in [42], due to the conforming constraint, the conforming unstructured

adaptive grid is generally less effective compared to the octree-based adaptive grid. On the other hand,

since the work of [27] is based on FreeFem++ [52], the adaptive mesh stage there regenerates the entire

unstructured mesh every time and performs a global projection, which can be a lot more expensive than our

AMR algorithm.

Thanks to the implicit scheme and physics-based preconditioning, the time step can be kept fixed

throughout the entire simulation. We use the polynomials of p = 3 in all the AMR tests and fix time

step to 0.5. Note in [25] the time step is fixed to be grid-dependent at 140tCF L, which is comparable to the time step used here. On the other hand, [27] uses a much smaller time step since it does not focus

on preconditioning and needs to respect the CFL constraint. In the current test, the solver starts with a

uniform mesh of 96 × 32 and uses three levels of refinement. Every 10 time steps, the solver checks the error

in the elements and mark those for the refinement and coarsening. For better performance, the refinement

and coarsening stages ensure the added and removed cells do not lead to a jump more than one level across

neighboring cells.

The solutions, adaptive meshes and domain partitioning are presented at several times in Figure 10.

Note the final result at t = 250 matches well with the uniform solution result in Figure 7. The adaptive

meshes are found to follow the solution structures very well. For instance, at t = 50, the mesh shows two

levels of refinement. The mesh at level 1 largely follows the initial perturbation  and the mesh at level

2 has some local structures. It is interesting to observe that the structures in  are very well captured in

the level 2 mesh at t = 50. This indicates the proposed error estimator is effective in capturing the features

of the solutions. As time evolves, the adaptive mesh changes slowly, and, eventually, all three levels of

refinement are fully utilized at t = 120 and t = 250. The mesh still follows the structures in  and  well.

Even with continuous re-gridding and load balancing, the overall meshes still keep a symmetric structure

and are largely comparable with the adaptive mesh in [25]. 72 CPUs are used to perform the AMR test. The

domain partitioning is presented along with the solution to show the evolution of partitioning. To better

visualize the partitioning, a random number is assigned to each partition for easily distinguishing different

sub-domains. It is observed that the partitioning forms connected sub-domains.

Next, we examine the algorithmic performance of the AMR solver. The averaged number of dofs for a

scalar unknown in the AMR simulation is about 213329, while the total number of dofs for a scalar unknown

on the uniform mesh of the same finer resolution needs 1771776. It follows that the AMR solver only needs

12% of the total number of dofs on the uniform fine mesh. This is comparable with the performance of the

same AMR test in [25], in which 14% of the total number of dofs on the fine mesh is reported. Note since a

high-order finite-element method is used, the dofs in our implementation is more than 10 times more than

the dofs in [25] for a similar mesh resolution. Thus, the computational time is considerably longer than the

time reported in [25]. The uniform mesh of 768 × 256 needs 24 Krylov iterations on average per solve, and

takes 106250s on 72 CPUs. As a comparison, the AMR solver needs 29 Krylov iterations and 12451s on 72

CPUs, or 11.7% of the computational time of the uniform grid solver.

The final examination focuses on the potential accuracy improvement of AMR. While it is challenging

to compare two finite-element solutions on two different meshes, MFEM provides limited support for such

25

J at t = 250



mesh

DD

J at t = 120



mesh

DD

J at t = 50



mesh

DD

Figure 10: Tearing mode. Parallel current J, Vorticity , adaptive meshes and corresponding domain decompositions at various times are presented. Here  =  = 10-3. The base grid is 96 × 32 with three levels of refinements and the polynomial degree used is p = 3. The time step is t = 0.5 and DIRK2 is used as the time integrator. 72 CPUs are used in this test.
a task. We rely on the gslib tool on MFEM, which provides a high-order interpolation on any given point in the domain. We use this tool to pick 40000 points in the entire domain to compare the two interpolated solutions on the coarse mesh and the finer mesh and collect its averaged difference and maximal difference. The reference solution is computed on a 1536 × 512 grid and we choose  to compute the errors. We focus on a given time (t = 50) as the demonstration and compare four different meshes (a very coarse uniform mesh of 96 × 32, and adaptive meshes with a base mesh of 96 × 32 with one, two and three levels of refinement, respectively). The solutions and their differences are presented in Figure 11, and the corresponding difference values are reported in Table 3. To easily visualize the differences in Figure 11, they are only evaluated at the node point of the 1536 × 512 grid during visualization. Nevertheless, this provides us a good indication
26

 1-level mesh 2-level mesh 3-level mesh

Error on uniform mesh Error Error Error

Figure 11: Tearing mode. Errors on different meshes are presented for comparison. Top row: the solution  at t = 50 and the point value errors on the mesh of 96 × 32. Second row: the mesh using one refinement level and the error. Third row: the mesh using two refinement levels and the error. Bottom row: the mesh using three refinement levels and the error. The errors are computed as point value difference between the solution and highly resolved solution on a uniform grid of 1536 × 512. To visualize the errors, they are all evaluated at the nodes of the grid 1536 × 512. For easy comparison, all the error plots share the same colormap in the range of [0, 10-7].

of where error concentrates in the solutions.

Table 3: Maximal and average differences on 40000 points between solutions on coarse meshes and the reference solutions.

Grid

Tearing mode AMR test at t = 50 Average dofs Maximal error Average error

96 × 32 1-level mesh 2-level mesh 3-level mesh

27936 63247 157432 225487

2.69e-7 5.35e-8 3.92e-8 2.64e-8

3.09e-8 8.13e-9 9.38e-9 3.36e-9

We first note the uniform mesh has a very large error in the center region, which is largely proportional to the solution . The adaptive mesh with one-level refinement improves the error significantly by effectively locating the region of larger error and reduce the error in the center region. The adaptive mesh with two-level refinement has a mixed result, in which some part of the error increase and some part of the error decreases compared to the one-level mesh. It is however still noticeably much better than the error on the uniform mesh. Such a mixed result is likely the outcome of frequent refinement and coarsening steps. The adaptive mesh with three-level refinement is found to be better than all the other meshes. It is also observed that the errors closely follow the pattern of the refined regions in all the meshes, which indicates the effectiveness
27

of the AMR solver. For a quantitative comparison of performance, we present the averaged and maximal errors in Table 2. First note the error in the current work is much smaller than the errors reported in [25] (Fig.10 in [25] indicates a relative error of 10-3 to 10-4 compared to the relative error in the level of 10-8 to 10-9 in the current work of the same test). We note that the maximal and averaged errors among all the solutions decay as the number of refinement levels increases. It is also important to stress that the proposed error estimator (11) is used in all the tests considered in this study. Although it is not an optimal error estimator, it provides a satisfactory result based on the above accuracy study. (The optimal error estimator here refers to the error estimator using a true solution or a reference solution.) We conclude that the AMR solver provides a more accurate solution with increasing levels of refinement.

8.4. Island coalescence
The final example we consider is the island coalescence, originally considered in studies such as [53, 54]. The problem shows true time and space multiscale behaviors when the Lundquist number increases including plasmoid development, and therefore serves as a great test to challenge our solver.
The coalescence instability, which drives the island coalescence example, is an ideal instability, which is driven by the attractive force between parallel currents. For low and moderately high Lundquist number, the simulation can be divided into two phases [54]. In the first phase, the current-density ropes are freely accelerated toward each other and a current sheet starts to form, while in the second phase the current sheet at the reconnection layer stays somewhat stationary and gets slowly damped until two islands fully merge. For the high-Lundquist numbers, current sheet tears after the aspect ratio of the current sheet becomes sufficient large, and result in plasmoids forming along the sheet [20]. The plasmoids can further excite more plasmoids and eventually lead to very complicated turbulence-like local structures.
The physics properties described above indicate the numerical solutions for the test can be very challenging. The first challenge is due to the stiffness of the Alfv´en wave. The earlier stage of the dynamics is very slow, and one would like to step over the CFL constraint from the Alfv´en speed using implicit time stepping. The second challenge is the evolving small spatial scales due to the formulation of a localized current sheet. In particular, we found that the current sheet has to be well-resolved for two reasons: (i) an under-resolved current sheet may lead to oscillations, resulting in the failure of the solver; (ii) although one may reduce the time step so that the implicit solver may still converge, we found those oscillations may lead to unphysical solutions. For instance, a large "plasmoid" may form at the reconnection site for small Lundquist numbers. We observe one giant plasmoid forms for Lundquist number of 5 × 104 when it is under-resolved, and similar unphysical solutions were observed previously in [55, 56]. Note the current sheet thickness scales as  (according to the Sweet-Parker model), becoming thinner as the Lundquist number increases. The third challenge is due to large flow speed along the current sheet at the peak reconnection time. As a result, it needs adequate stabilization. Last, the violent plasmoid instability poses a significant challenge to the solver. The spatial structure becomes more complicated than a simple current sheet, which requires the dynamic AMR to capture fully those small structures. The dynamics also happens much faster than the coalescence instability, requiring the time step be reduced. All those challenges will be carefully studied and addressed here.
Many of the previous studies applied various numerical schemes with some preconditioning techniques to this problem; see [4, 9, 13, 25, 26, 55, 57­59] for instance. Most of those studies however focus on the moderately high Lundquist number cases (the maximal Lundquist number considered is typically under 105). Among them, [26, 55] also develop an implicit AMR FEM solver, but owing to low Lundquist numbers in their tests the plasmoid instability is not observed. One of the highest Lundquist number considered in previous studies is [59] in which SL up to 109 is studied for this example using a resistive MHD model, while [13] considers a simulation of SL = 107 for the same example. The approach proposed in [13, 59] does not use AMR to capture plasmoids. Instead, [59] relied on a locally packed structured grid and [13] used a very high-order HDG scheme with stabilization to resolve plasmoids and currents sheets. Those techniques are quite different from the approach proposed here.
The same problem setup as in [25, 57] is adopted. The initial condition is a Fadeev magnetic equilibrium [60] given by

y

x

0 = - ln

cosh( ) + 

cos( ) 

,

0 = 0,

0 = 0,

28

balanced with a source term of E0 = 20. The perturbation exciting the instability is
 = 10-3 cos(y/2) cos(x),
The computational domain is [-1, 1]×[-1, 1] and the boundary conditions are the PEC wall (Dirichlet) along the y boundaries and periodic in x. Other parameters are = 0.2, and  =  = 10-4, unless otherwise noted. The definitions of Reynolds and Lundquist numbers also follow [4, 25, 57], i.e., Re = 1/ and SL = 1/. The reference length scale and Alfv´en speed are both 1. Thus, the time scale for the Alfv´en wave is A = 1. Note unlike the computations in [4, 57, 59], our computational domain is the entire domain and we do not assume symmetry in either direction. This is important to capture the full solutions structures when plasmoids start to appear and break the symmetry. The readers are referred to [57] for more discussions on physics.
The example is again split into two parts in the following discussion. The first part focuses on the results on static meshes, while, in the second part, the solver is truly pushed to the limit with dynamic adaptivity, studying the cases of high Lundquist numbers and particular the one with violent plasmoid instabilities.
8.4.1. Static-mesh results

t=0

t=6

t=7

t=8

Figure 12: Island coalescence. Solutions of  =  = 10-4 at different times are presented. The parallel current J is presented as pseudocolor and the magnetic flux  is presented as black contours. The colormap and contour values are fixed over time. The presented computation uses a uniform grid of 512 × 512 with p = 2 and DIRK2 with a fixed time step of t = 0.1.
The problem is first solved on a uniform mesh of 512 × 512 with the results presented in Figure 12. The test uses  =  = 10-4, p = 2 and the DIRK2 time integrator with a large time step t = 0.1. Again, the preconditioner performs very well with a large time step. In particular, the averaged Newton iteration is 2 and the averaged total Krylov linear solver is about 11. The numerical solutions are very comparable with the results given in Ref. [4] but with better performance. Starting from time of t = 5, a current sheet starts to form in the center and the number of iterations grows from 5 iterations to 10 to 20 iterations.
The solution at peak reconnection is presented in Figure 13. With this Lundquist number, the current sheet structure follows the Sweet-Parker model. The current sheet is found to be well resolved and the largest value in the current sheet is about 140. The largest flow speed in the outgoing jet regions of the reconnection diffusion region is about 0.38. Note that, for this test, since the Lundquist and Reynolds numbers are relatively small, the solvers work well without the stabilization term and produce good results.
We further test the solver by varying the Reynolds and Lundquist numbers following [4, 57] and study the reconnection rate at the reconnection X-point, i.e., x = [0, 0]. We simulate the same test in [4, 57] by varying the Lundquist number from 102 to 2 × 105. In this test, the Reynolds number is always chosen to be identical to the Lundquist number. We use a locally packed grid for all the simulations given in Figure 14. The mesh has a uniform background mesh of 512 × 512 and is gradually refined towards the center region. The final level is packed around the current sheet and has an effective grid spacing identical to the
29

J

v

Figure 13: Island coalescence. The current and flow speed of  =  = 10-4 at first peak time t = 6.9 are presented. The peak current at the center is about 140 and the highest flow speed is about 0.38 (with normalized Alfv´en speed of 1). The presented computation uses a uniform grid of 512 × 512 with p = 2 and DIRK2 with a fixed time step of t = 0.1.

Figure 14: Locally packed mesh for the island coalescence scaling tests. The grid is visualized between y  [-0.5, 0.5]. The base uniform grid is refined by four levels gradually from the external region to the center. Here the finest grid corresponds to a uniform grid of 2048 × 2048.

mesh of 2048 × 2048. This is a technique commonly used in the finite element simulation for this test when AMR is not utilized; see [4] for instance. Such a mesh indeed saves computational cost considerably. For the third-order polynomials used, the packed mesh has 2.8M dofs in a scalar variable while the uniform mesh of 2048 × 2048 has 38M dofs. Thus, the packed mesh only needs 7.4% of the comparable uniform mesh.

·10-2Reconnection Rate at X-point 3
 = 10-3
 = 10-4
 = 10-5 2

Peak Reconnection Rate at X-point 10-1

t at [0, 0] Peak t at [0, 0]

10-2 .48
1

0 0 2 4 6 8 10 12 14
t

10-130-6

1 10-5

10-4


10-3

10-2

Figure 15: Left plot: time histories of the reconnection rate, defined as /t, at the X-point for different resistivity. Right plot: scaling result between the peak reconnection rate at X-point and resistivity. A locally packed grid described in Figure 14 is used in all the tests. The scaling results match well with [4, 57]. The linear regression of the smallest three resistivity data points indicate the scaling of reconnection rate is t  0.48.

30

The reconnection rate results are presented in Figure 15. In all these tests, a smaller time step of 0.025

is used. When a large time step of 0.1 is used, we found the reconnection rate may not be accurate in the

large Lundquist number runs, which has been also pointed out in the study of [55]. Our results match very

well with [4, 57]. The shape of the reconnection rate in the left plot of Figure 15 match well with the result

presented in [4]. The runs with  = 10-5 are found to have a slightly smaller peak reconnection rate than

those in Fig. 17 of [4]. The peak reconnection rate also matches well with [4, 57]. The linear regression of

the smallest three resistivity data points in the scaling plot indicates the scaling of reconnection rate satisfies

t  0.48, which reconnection rate

is of

idet ntical.toWtheefoscuanldingthraetsuthltefostuanbdiliiznat[4io]nantedrmcosmapraernabecleeswsaitrhy

the for

Sweet-Parker slow all the simulations

with   5 × 10-5. The above scaling test validates our solver and also indicates that SUPG is not affecting

the quality of the physics results.

Table 4: Preconditioner performance study with different grid resolutions for the island coalescence. Re = SL = 104. DIRK2 is used with two solves per time step. A fixed relative tolerance of 10-4 and a fixed relative solution tolerance of 10-6 are used as the Newton convergence criterion. The steps are averaged over 10 time steps and p = 2, t = 0.1.
Dofs stands for the total degrees of freedoms for one scalar component.

Grid
64 × 64 128 × 128 256 × 256 512 × 512 1024 × 1024 2048 × 2048 4096 × 4096

dofs
16512 65792 262656
1M 4.2M 16.8M 67.1M

t/tCF L
3.2 6.4 12.8 25.6 51.2 102.4 204.8

Island Coalescence CPUs Newton/solve

1

2.0

4

2.0

16

2.0

64

2.2

256

2.1

1024

3.0

4096

3.1

Krylov/solve
3.9 3.9 3.9 4.0 5.3 11.2 19.5

Time/t
8.9 16.5 19.3 23.3 21.3 41.1 64.5

Time/Krylov
1.1 2.1 2.4 2.9 2.0 1.8 1.6

The last uniform-mesh test is the weak parallel scaling test. The Reynolds number and Lundquist number are chosen as 104. The weak scaling result is reported in Table 4. The result is very comparable to the tearing-mode test. The weak scaling is almost optimal until we reach the finest resolutions. We also found the performance to be quite comparable with the previous work of the finite difference version [3]. (Note the original work [3] does not perform a weak scaling test with as many as CPUs as the current work. But a similar weak scaling test has been performed using the code developed in [3] during the development of the current work.) Overall, the performance of the solver is satisfactory. The CFL number from a single processor to 4096 processors grows by a factor of 4096 while the computational time only grows by a factor of 7.2. We also would like to highlight the current solver is better than the weak scaling result reported in Table 7 of [4]. The setup between two weak scaling tests are almost identical but the number of Krylov iterations grows much faster for the purely algebraic based multilevel smoother in [4].
8.4.2. AMR results We consider next numerical results using the implicit dynamic AMR solver. The simulation considered
here is a first step to demonstrate viability of dynamically resolving current sheets and plasmoid structures in fusion devices using AMR. Note some plasmoid related simulations for practical devices can be found in [61, 62].
In the following AMR tests, the error estimator (10) is used with  = 0.1. The error estimator is chosen since the solutions such as Figure 12 indicate structures are mostly observed in the current and vorticity. We choose the refinement ratio r = 0.1 and the coarsening ratio c = 0.001 as described in Section 6.2. The refinement and coarsening steps are called every 10 time steps but are switched to every 4 time steps later at t = 5.5 when the simulation becomes more dynamic. This is particularly necessary when plasmoids form and collide into each other. The base computational grid is 128 × 128, and all the simulation starts with the same base mesh but with different refinement levels. Our coarsening algorithm is implemented such that it will not coarsen the element after it reaches the level of the base mesh. We found that coarsening beyond some base level may introduce a lot of oscillations during the projection. All the AMR tests use a polynomial order of three. The number of processors used are between 180 to 3600 processors for all the
31

runs presented.
t=5
t=6
t=7
t=8

mesh mesh mesh mesh

Figure 16: Island coalescence of  =  = 10-5. Solutions at different times and the corresponding adaptive meshes are presented. The parallel current J is presented as pseudocolor plots and the magnetic flux  is presented as black contours. The colormap and contour values are fixed over time. Note the adaptive meshes capture the interesting structure in the current very well.
We first consider the case of  =  = 10-5. The solutions and the corresponding adaptive meshes at different times are presented in Figure 16. The time step is fixed to t = 0.05 throughout the simulation. The number of refinement levels is four, and thus the adaptive grid is effectively comparable to the grid in Figure 14. Throughout the entire simulation, the adaptive meshes capture the interesting structure in the current very well and the meshes are very localized with the base mesh covering the majority of the computational domain. Therefore, the adaptive mesh saves a lot of computational cost even compared to an optimized locally packed grid like the one in Figure 14. In fact, the averaged number of dofs in the entire run is 472961, which is 17% of the locally packed grid in Figure 14 and 1.2% of the effective uniform mesh. The adaptive meshes are largely symmetric even at a later time. At t = 8, the solution is found to be slightly asymmetric, which is also observed in the fixed-mesh solution. It is also interesting to observe the AMR algorithm starts to capture this asymmetry in the mesh at t = 8. The test indicates the AMR algorithm performs as expected, and the techniques such as refinement, coarsening and the error estimator are very effective and suitable for the practical simulations like this.
The solution of the above run of the first reconnection peak time is presented in Figure 17. With a Lundquist number of 105, the structure still follows the current sheet in the Sweet-Parker model. The current sheet is found to be well resolved and the largest value in the current sheet is about 700. The largest speed in the top and bottom jet regions of the current sheet is about 0.58. The aspect ratio of this peak current sheet is about 70. The plasmoid is not observed in the entire simulation since the aspect ratio is not
32

J



peak

v

B

Figure 17: Island coalescence of  =  = 10-5. The current, vorticity, flow speed and magnetic field (with their zoomed-in views around the center sheet) are presented at the peak time of t = 7.25. The zoomed-in views are focused on the top half part of the current sheet region. The peak current at the center is about 700 and the highest flow speed is about 0.58 (with a normalized Alfv´en speed of 1).
large enough. (The plasmoid will appear for aspect ratios greater than 100 [20]). Figure 17 also illustrates the importance of including the stabilization terms in the physics-based precon-
ditioners for these strongly hyperbolic operators. The stabilization terms are needed for two reasons. The obvious one is inversion of the advection-diffusion operator A + T in the preconditioner. Without the SUPG term, the scheme becomes unstable and the inversion of the operator A through AMG will fail. The second subtle reason is the inversion of the Schur complement S = A + S - NB0 D-1NB0 . We note that around the jet region the magnetic-field strength is very small. Therefore, the Schur complement has the same property as the advection-diffusion operator A, i.e., the diffusion with the extra Alfven wave parabolic operator is dominated by the strong advective term.
Next we consider the case of  =  = 10-6. In this test, we use 6 levels of refinement on top of the 128 × 128 base mesh, p = 3 and an initial time step of 0.05. Unlike the previous case, plasmoids develop in the simulation. To accommodate the fast dynamics of plasmoids, we use an approach similar to [13, 59] that reduce time step dynamically when the local dynamics become too fast. Specifically, if the maximum number of Newton iterations is reached, we reduce the time step by a factor of two. After the time step is reduced, the simulation runs with the new time step. If the solver successfully runs with the same time step for 10 steps, we start to slowly increase the time step by 10%. The time step will continuously increase every 10 time steps until the solver fails again or reaches the original target maximum time step. This approach is found to be sufficient to run through the entire simulation. The smallest time step for this run is found to be 0.003125. The computational cost is significantly reduced compared to the uniform mesh case. The averaged number of dofs in this run is 1.30M, while a uniform mesh of the same resolution needs 604M dofs. As a result, the implicit adaptive solver needs only 0.21% of the total dofs compared to a comparable uniform mesh.
The solutions and mesh with their zoomed-in views at t = 6.5 are depicted in Figure 18. The zoomed-in views show that the current sheet and jet regions are very well resolved, indicating the error estimator is very effective. The time slice of t = 6.5 is right before the first plasmoid appears at t = 6.525. At t = 6.5, the aspect ratio is estimated as 200 and the appearance of the plasmoid is consistent with the estimate given in [20]. The highest current in the center peak region is about 2400. In the next time step, the first plasmoid appears right above the central peak region.
The time evolution of the current sheet is presented in Figure 19. We select a few time slides to demonstrate the dynamics of the plasmoids. Between t = 6.5 and t = 7.0, the plasmoid dynamics are rather simple. There are a few isolated plasmoids (typically two to three) appearing along the current sheet. The plasmoids then propagate towards the top or bottom jet regions. When the plasmoids hit the jet regions, the
33

t = 6.5 zoomed-in 1

mesh

zoomed-in 1

zoomed-in 2

zoomed-in 2
Figure 18: Island coalescence of  =  = 10-6. Top row: current at t = 6.5 and the corresponding adaptive mesh. Here 6 levels of refinement are used and p = 3. The mesh captures the interesting structure in the current very well. Second row: the zoomed-in views around the current sheet region. The left plot focuses on the center region while the right plot zooms in further to visualize the bottom jet and current sheet. Note the jet and the current sheet are very well captured by AMR.
plasmoids collapse and separate into two small bumps to propagate away along the current layers to the left and right, respectively. At this stage, the movement of the plasmoids is rather straightforward, and they are driven by the flow velocity, i.e., the plasmoids appearing on the top part of the current sheet move upward and those appearing on the bottom move downward. At the later time, starting around t = 7.08, there are multiple plasmoids appearing. For instance, at t = 7.21, a few plasmoids with different sizes appear at the same time. It is observed that the plasmoids merge into a larger plasmoid, which starts to move upward at t = 7.28. Eventually, all the plasmoids merge together and form a giant plasmoid in the center at t = 7.43. At this time, the current sheet has passed the peak time and the plasmoid dynamics becomes less violent until the second peak starts to form. One important takeaway here is the simulation in the high Lundquist number breaks symmetry in both x and y directions. Therefore, this test suggests it is necessary to perform the simulation in the entire domain instead of assuming symmetry as in previous studies [4, 57].
In the final test, we consider the most challenging case of  =  = 10-7. In this test, we use 7 levels of refinement. The current sheet is found to be much thinner and the plasmoid dynamics are much more violent. As a result, the smallest time step is found to be 0.00091 for this run. The computational cost is again significantly reduced compared to the uniform mesh. The averaged number of dofs in this run is 1.77M, while a uniform mesh needs 2416M dofs. As a result, the implicit adaptive solver only needs 0.07% of the total dofs compared to a comparable uniform mesh. The current density and mesh at t = 6 is presented at Figure 20. From the zoomed-in view, we found the plasmoid in the center is very well captured by AMR. There are 8 plasmoids already along this current sheet and the maximum current values is 7086, which is found between two isolated plasmoids. The plasmoids first appear around t = 5.75 with current sheet aspect ratio of 250 and the maximum current density in the center around 3000.
The time evolution of the current sheet is presented in Figure 21. We select a few time slices to demonstrate the dynamics of the plasmoids. At an earlier stage, the plasmoid dynamics are similar to the dynamics of  =  = 10-6, i.e., there are many plasmoids of different sizes forming along the current sheet and they move and merge together, roughly between t = 5.75 and 6.26. However, this dynamics become different at the later time. Starting at t = 6.31, the current sheet of higher value at the center continuously generates small plasmoids along both directions. When those plasmoids move up or down, they deform into an asymmetric drop-like shape, with the front speed faster than the tail speed. As a result, the tail is
34

t = 6.86

t = 7.08
Plasmoids

t = 7.21

t = 7.28

t = 7.36

t = 7.43
Merged plasmoid

Figure 19: Island coalescence of  =  = 10-6. The current sheet and plasmoid evolution around the center region after the sheet becomes unstable. A giant plasmoid forms in the very end after several plasmoids merge together.

t=6

mesh

zoomed-in

zoomed-in

Figure 20: Island coalescence for  =  = 10-7. 7 levels of refinement are used with p = 3. Left: the current sheet at the center. Middle: the corresponding adaptive mesh. Right: the zoomed-in view around the central plasmoid.
caught up by plasmoids generated at a later time, leading to their merging. We find that this process occurs continuously, and as a result a stable X-shape structure forms in the center of the current sheet, which is very similar to the structure in the Petschek reconnection model (as marked in Figure 21). Note such a structure is also observed at the plot of t = 7.28 in Figure 19 (along the bottom current sheet) for the case of  =  = 10-6. Similar results were previously observed in [24, 63] for moderately high Lundquist number simulations. Discussions on forming of such a structure can be found in [63]. We defer a deeper investigation
35

t = 6.08

t = 6.13

t = 6.21

t = 6.26

t = 6.31

t = 6.36

t = 6.44

t = 6.56

Figure 21: Island coalescence for  =  = 10-7. Evolution around the current sheet regions. The dynamics become very fast and the current sheet breaks into many smaller structures.

on high Lundquist number cases to a future physics study.

Table 5: Comparison between averaged dofs in AMR meshes and dofs in the corresponding uniform meshes of comparable fine grid resolutions

Averaged scalar dofs in island coalescence tests

SL AMR uniform meshes AMR/uniform meshes

105 0.47M

38M

1.2%

106 1.30M

604M

0.21%

107 1.77M

2416M

0.07%

36

Based on the above tests, we conclude the proposed implicit AMR solver is suitable for simulations of ultra high Lundquist numbers. It is found that the implicit AMR solver is robust and efficient for different Lundquist number regimes, ranging from no plasmoids and simple dynamics to many colliding plasmoids and very complicated dynamics. The highest Lundquist number tested with the proposed solver is 108, and in a future physics study we plan to explore this limit further. The savings in dofs have been summarized in Table 5, demonstrating the ability of the approach to evolve previously inaccessible dissipation regimes.
9. Conclusions
We have developed a fully implicit, stabilized finite-element formulation for 2D incompressible reduced resistive MHD, incorporating important techniques of physics-based preconditioning and adaptive mesh refinement. An SUPG-based stabilization approach is added into the continuous finite-element formulation to handle high Lundquist numbers. The proposed physics-based preconditioner handles the stiff hyperbolic Alfv´en wave, the diffusive terms and the stabilization terms in the formulation. An octree-based parallel AMR algorithm is employed, for which we propose an error estimator and a proper refining/coarsening strategy. The algorithm is implemented in the scalable finite-element framework, MFEM, with which we achieve good performance up to thousands of cores.
The performance of the proposed algorithm is carefully studied with several numerical examples. The major points of interest include the accuracy of the discretization, the efficiency and validation of the associated solver, and the performance improvement of the AMR algorithm (both in accuracy and computational cost). Our study confirms the expected high-order convergence rates in both space and time. Weak parallel scaling results up to 4096 CPUs for moderately high Lundquist-number simulations demonstrate excellent algorithmic and parallel scalability. Results using the implicit AMR solver for low Lundquist-number (tearing mode) to moderately-high and high Lundquist-number problems (island coalescence) demonstrate the AMR algorithm improves the accuracy of numerical solutions and reduces computational cost significantly over the implicit solver on a uniform mesh. In particular, the AMR algorithm is capable of capturing the vast scale separations and dramatic dynamics in the solution when the thin current sheet breaks into small plasmoids.
The proposed solver provides a unique and promising tool to study the physics of high Lundquist numbers, such as magnetic reconnection when plasmoid instabilities become violent. Those results will be investigated in a future physics study. Another important future direction is to adapt the solver for tokamak simulations of practical interest.
Acknowledgement
We would like to thank the MFEM, PETSc and deal.II teams for a number of helpful discussions. We also extend gratitude to Barry Smith for the helpful discussion on JFNK and Sriramkrishnan Muralikrishnan for the helpful discussion on island coalescence simulations. This research used resources provided by the Los Alamos National Laboratory Institutional Computing Program, which is supported by the U.S. Department of Energy National Nuclear Security Administration under Contract No. 89233218CNA000001. In particular, most of the scaling tests are performed on the LANL Grizzly supercomputer. This research also used resources of the National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility, in some long runs of the final example.
References
[1] D. Biskamp, Nonlinear magnetohydrodynamics, no. 1, Cambridge University Press, 1997.
[2] S. C. Jardin, Review of implicit methods for the magnetohydrodynamic description of magnetically confined plasmas, Journal of Computational Physics 231 (3) (2012) 822­838.
[3] L. Chaco´n, D. A. Knoll, J. Finn, An implicit, nonlinear reduced resistive mhd solver, J. Comput. Phys. 178 (1) (2002) 15­36.
37

[4] J. N. Shadid, R. P. Pawlowski, J. W. Banks, L. Chac´on, P. T. Lin, R. S. Tuminaro, Towards a scalable fully-implicit fully-coupled resistive mhd formulation with stabilized fe methods, Journal of Computational Physics 229 (20) (2010) 7649­7671.
[5] L. Chac´on, D. A. Knoll, A 2d high- hall mhd implicit nonlinear solver, Journal of Computational Physics 188 (2) (2003) 573­592.
[6] L. Chac´on, A non-staggered, conservative,  · b = 0, finite-volume scheme for 3d implicit extended magnetohydrodynamics in curvilinear geometries, Computer Physics Communications 163 (3) (2004) 143­171.
[7] L. Chac´on, A. Stanier, A scalable, fully implicit algorithm for the reduced two-field low- extended mhd model, Journal of Computational Physics 326 (2016) 763­772.
[8] J. N. Shadid, R. P. Pawlowski, E. C. Cyr, R. S. Tuminaro, L. Chac´on, P. D. Weber, Scalable implicit incompressible resistive mhd with stabilized fe and fully-coupled newton­krylov-amg, Computer Methods in Applied Mechanics and Engineering 304 (2016) 1­25.
[9] E. C. Cyr, J. N. Shadid, R. S. Tuminaro, R. P. Pawlowski, L. Chac´on, A new approximate block factorization preconditioner for two-dimensional incompressible (reduced) resistive mhd, SIAM Journal on Scientific Computing 35 (3) (2013) B701­B730.
[10] E. G. Phillips, H. C. Elman, E. C. Cyr, J. N. Shadid, R. P. Pawlowski, A block preconditioner for an exact penalty formulation for stationary mhd, SIAM Journal on Scientific Computing 36 (6) (2014) B930­B951.
[11] E. G. Phillips, J. N. Shadid, E. C. Cyr, H. C. Elman, R. P. Pawlowski, Block preconditioners for stable mixed nodal and edge finite element representations of incompressible resistive mhd, SIAM Journal on Scientific Computing 38 (6) (2016) B1009­B1031.
[12] J. J. Lee, S. J. Shannon, T. Bui-Thanh, J. N. Shadid, Analysis of an HDG method for linearized incompressible resistive mhd equations, SIAM Journal on Numerical Analysis 57 (4) (2019) 1697­1722.
[13] S. Muralikrishnan, S. Shannon, T. Bui-Thanh, J. N. Shadid, A multilevel block preconditioner for the hdg trace system applied to incompressible resistive mhd, arXiv preprint arXiv:2012.07648.
[14] K. Hu, Y. Ma, J. Xu, Stable finite element methods preserving  · b = 0 exactly for mhd models, Numerische Mathematik 135 (2) (2017) 371­396.
[15] Y. Ma, K. Hu, X. Hu, J. Xu, Robust preconditioners for incompressible mhd models, Journal of Computational Physics 316 (2016) 721­746.
[16] L. Li, D. Zhang, W. Zheng, A constrained transport divergence-free finite element method for incompressible mhd equations, Journal of Computational Physics 428 (2021) 109980.
[17] A. J. Christlieb, J. A. Rossmanith, Q. Tang, Finite difference weighted essentially non-oscillatory schemes with constrained transport for ideal magnetohydrodynamics, Journal of Computational Physics 268 (2014) 302­325.
[18] A. J. Christlieb, X. Feng, D. C. Seal, Q. Tang, A high-order positivity-preserving single-stage single-step method for the ideal magnetohydrodynamic equations, Journal of Computational Physics 316 (2016) 218­242.
[19] J. Donea, A. Huerta, Finite element methods for flow problems, John Wiley & Sons, 2003.
[20] D. Biskamp, Magnetic reconnection in plasmas, Astrophysics and Space Science 242 (1) (1996) 165­207.
[21] A. Bhattacharjee, Y.-M. Huang, H. Yang, B. Rogers, Fast reconnection in high-lundquist-number plasmas due to the plasmoid instability, Physics of Plasmas 16 (11) (2009) 112102.
38

[22] Y.-M. Huang, A. Bhattacharjee, Scaling laws of resistive magnetohydrodynamic reconnection in the high-lundquist-number, plasmoid-unstable regime, Physics of Plasmas 17 (6) (2010) 062104.
[23] B. Lynch, J. Edmondson, M. Kazachenko, S. Guidoni, Reconnection properties of large-scale current sheets during coronal mass ejection eruptions, The Astrophysical Journal 826 (1) (2016) 43.
[24] H. Baty, Petschek-type reconnection in the high-lundquist-number regime during nonlinear evolution on the tilt instability, arXiv preprint arXiv:2005.04221.
[25] B. Philip, L. Chac´on, M. Pernice, Implicit adaptive mesh refinement for 2d reduced resistive magnetohydrodynamics, Journal of Computational Physics 227 (20) (2008) 8855­8874.
[26] J. Adler, T. A. Manteuffel, S. F. McCormick, J. Nolting, J. W. Ruge, L. Tang, Efficiency based adaptive local refinement for first-order system least-squares formulations, SIAM Journal on Scientific Computing 33 (1) (2011) 1­24.
[27] H. Baty, Finmhd: An adaptive finite-element code for magnetic reconnection and formation of plasmoid chains in magnetohydrodynamics, The Astrophysical Journal Supplement Series 243 (2) (2019) 23.
[28] H. R. Strauss, Nonlinear, three-dimensional magnetohydrodynamics of noncircular tokamaks, The Physics of Fluids 19 (1) (1976) 134­140.
[29] H. Strauss, D. Longcope, An adaptive finite element method for magnetohydrodynamics, Journal of Computational Physics 147 (2) (1998) 318­336.
[30] S. Lankalapalli, J. E. Flaherty, M. S. Shephard, H. Strauss, An adaptive finite element method for magnetohydrodynamics, Journal of Computational Physics 225 (1) (2007) 363­381.
[31] M. D. Gunzburger, Finite element methods for viscous incompressible flows: a guide to theory, practice, and algorithms, Elsevier, 2012.
[32] E. C. Cyr, J. N. Shadid, R. S. Tuminaro, Stabilization and scalable block preconditioning for the navier­stokes equations, Journal of Computational Physics 231 (2) (2012) 345­363.
[33] B. Owren, H. Simonsen, Alternative integration methods for problems in structural dynamics, Computer Methods in Applied Mechanics and Engineering 122 (1-2) (1995) 1­10.
[34] T. A. Manteuffel, J. Ruge, B. S. Southworth, Nonsymmetric algebraic multigrid based on local approximate ideal restriction (lair), SIAM Journal on Scientific Computing 40 (6) (2018) A4105­A4130.
[35] R. S. Dembo, S. C. Eisenstat, T. Steihaug, Inexact newton methods, SIAM Journal on Numerical analysis 19 (2) (1982) 400­408.
[36] D. A. Knoll, D. E. Keyes, Jacobian-free newton­krylov methods: a survey of approaches and applications, Journal of Computational Physics 193 (2) (2004) 357­397.
[37] M. Ainsworth, J. T. Oden, A posteriori error estimation in finite element analysis, Vol. 37, John Wiley & Sons, 2011.
[38] D. Arndt, W. Bangerth, B. Blais, T. C. Clevenger, M. Fehling, A. V. Grayver, T. Heister, L. Heltai, M. Kronbichler, M. Maier, et al., The deal. ii library, version 9.2, Journal of Numerical Mathematics 1 (ahead-of-print).
[39] D. Kelly, J. De SR Gago, O. Zienkiewicz, I. Babuska, A posteriori error analysis and adaptive processes in the finite element method: Part i-error analysis, International journal for numerical methods in engineering 19 (11) (1983) 1593­1619.
[40] O. C. Zienkiewicz, J. Z. Zhu, The superconvergent patch recovery and a posteriori error estimates. part 1: The recovery technique, International Journal for Numerical Methods in Engineering 33 (7) (1992) 1331­1364.
39

[41] O. C. Zienkiewicz, J. Z. Zhu, The superconvergent patch recovery and a posteriori error estimates. part 2: Error estimates and adaptivity, International Journal for Numerical Methods in Engineering 33 (7) (1992) 1365­1382.
[42] Z. Peng, Q. Tang, X.-Z. Tang, An adaptive discontinuous petrov­galerkin method for the grad­shafranov equation, SIAM Journal on Scientific Computing 42 (5) (2020) B1227­B1249.
[43] J. Bonilla, S. Badia, Monotonicity-preserving finite element schemes with adaptive mesh refinement for hyperbolic problems, Journal of Computational Physics (2020) 109522.
[44] R. Anderson, J. Andrej, A. Barker, J. Bramwell, J.-S. Camier, J. Cerveny, V. Dobrev, Y. Dudouit, A. Fisher, T. Kolev, et al., MFEM: a modular finite element methods library, Computers & Mathematics with Applications 81 (2021) 42­74.
[45] S. Balay, S. Abhyankar, M. Adams, J. Brown, P. Brune, K. Buschelman, L. Dalcin, A. Dener, V. Eijkhout, W. Gropp, et al., PETSc users manual.
[46] R. D. Falgout, U. M. Yang, hypre: A library of high performance preconditioners, in: International Conference on Computational Science, Springer, 2002, pp. 632­641.
[47] J. Cerveny, V. Dobrev, T. Kolev, Nonconforming mesh refinement for high-order finite elements, SIAM Journal on Scientific Computing 41 (4) (2019) C367­C392.
[48] R. J. Moreau, Magnetohydrodynamics, Vol. 3, Springer Science & Business Media, 2013.
[49] N. Ben Salah, A. Soulaimani, W. G. Habashi, M. Fortin, A conservative stabilized finite element method for the magneto-hydrodynamic equations, International Journal for Numerical Methods in Fluids 29 (5) (1999) 535­554.
[50] A. H. Glasser, X. Tang, The sel macroscopic modeling code, Computer physics communications 164 (1-3) (2004) 237­243.
[51] P. T. Lin, J. N. Shadid, P. H. Tsuji, Krylov smoothing for fully-coupled amg preconditioners for vms resistive mhd, in: Numerical Methods for Flows, Springer, 2020, pp. 277­286.
[52] F. Hecht, New development in freefem++, J. Numer. Math. 20 (3-4) (2012) 251­265.
[53] J. M. Finn, P. Kaw, Coalescence instability of magnetic islands, The Physics of Fluids 20 (1) (1977) 72­78.
[54] D. Biskamp, H. Welter, Coalescence of magnetic islands, Physical Review Letters 44 (16) (1980) 1069.
[55] J. H. Adler, M. Brezina, T. A. Manteuffel, S. F. McCormick, J. W. Ruge, L. Tang, Island coalescence using parallel first-order system least squares on incompressible resistive magnetohydrodynamics, SIAM Journal on Scientific Computing 35 (5) (2013) S171­S191.
[56] G. T´oth, Y. Ma, T. I. Gombosi, Hall magnetohydrodynamics on block-adaptive grids, Journal of Computational Physics 227 (14) (2008) 6967­6984.
[57] D. Knoll, L. Chac´on, Coalescence of magnetic islands, sloshing, and the pressure problem, Physics of Plasmas 13 (3) (2006) 032307.
[58] P. T. Lin, J. N. Shadid, R. S. Tuminaro, M. Sala, G. L. Hennigan, R. P. Pawlowski, A parallel fully coupled algebraic multilevel preconditioner applied to multiphysics pde applications: Drift-diffusion, flow/transport/reaction, resistive mhd, International Journal for Numerical Methods in Fluids 64 (1012) (2010) 1148­1179.
[59] J. Shadid, R. Pawlowski, L. Chac´on, D. Knoll, Current sheet break-up via fast plasmoid formation in the island coalescence problem the ultra-high lundquist number regime (s 109), in: APS Division of Plasma Physics Meeting Abstracts, Vol. 52, 2010, pp. CP9­152.
40

[60] V. Fadeev, I. Kvabtskhava, N. Komarov, Self-focusing of local plasma currents, Nuclear fusion 5 (3) (1965) 202.
[61] F. Ebrahimi, R. Raman, Large-volume flux closure during plasmoid-mediated reconnection in coaxial helicity injection, Nuclear Fusion 56 (4) (2016) 044002.
[62] A. Ali, P. Zhu, Effects of plasmoid formation on sawtooth process in a tokamak, Physics of Plasmas 26 (5) (2019) 052518.
[63] Z. Mei, C. Shen, N. Wu, J. Lin, N. Murphy, I. Roussev, Numerical experiments on magnetic reconnection in solar flare and coronal mass ejection current sheets, Monthly Notices of the Royal Astronomical Society 425 (4) (2012) 2824­2839.
41

