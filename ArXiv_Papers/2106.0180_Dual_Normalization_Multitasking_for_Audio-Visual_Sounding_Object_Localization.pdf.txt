Dual Normalization Multitasking for Audio-Visual Sounding Object Localization
Tokuhiro Nishikawa, Daiki Shimada, Jerry Jun Yokono Sony Group Corporation, Tokyo, Japan
{tokuhiro.nishikawa, daiki.shimada, jerry.jun.yokono}@sony.com

arXiv:2106.00180v1 [cs.CV] 1 Jun 2021

Abstract
Although several research works have been reported on audio-visual sound source localization in unconstrained videos, no datasets and metrics have been proposed in the literature to quantitatively evaluate its performance. Defining the ground truth for sound source localization is difficult, because the location where the sound is produced is not limited to the range of the source object, but the vibrations propagate and spread through the surrounding objects. Therefore we propose a new concept, Sounding Object, to reduce the ambiguity of the visual location of sound, making it possible to annotate the location of the wide range of sound sources. With newly proposed metrics for quantitative evaluation, we formulate the problem of Audio-Visual Sounding Object Localization (AVSOL). We also created the evaluation dataset (AVSOL-E dataset) by manually annotating the test set of well-known Audio-Visual Event (AVE) dataset [36]. To tackle this new AVSOL problem, we propose a novel multitask training strategy and architecture called Dual Normalization Multitasking (DNM), which aggregates the Audio-Visual Correspondence (AVC) task and the classification task for video events into a single audiovisual similarity map. By efficiently utilize both supervisions by DNM, our proposed architecture significantly outperforms the baseline methods.
1. Introduction
When we hear a dog barking, we associate the sound of the bark with the appearance of the dog, and naturally perceive it as a single event. A certain visual input comes in conjunction with a certain auditory input, and we can relate both information to localize the source object. By properly processing visual and audio multimodal inputs complementary, we better understand and explore the world around us.
If we could realize the same ability on machines, behaviors of the real-world robots will become more intelligent. For example, a robot can pay attention to the person who is actually talking to it among multiple people, or a robot and a person can pay joint attention to a specific object that is

Figure 1. Examples of Sounding Object. The visual location of sound source is inherently ambiguous. The sound of a horse walking is actually the sound of its hooves hitting the ground (left). The sound of a chainsaw is produced by the engine, moving chain, vibration of the log being cut, flying wood chips, etc. (right). We introduce the concept of Sounding Object to avoid this ambiguity, and make it possible to annotate their locations in the similar way to regular object detection (green bounding-box).
making a sound, which gives the person an opportunity to teach the robot the name of the object.
There have been a lot of research in the field of audiovisual sound source localization, i.e. a technique for visually locating a sound source. One of the biggest problems in the field is there is no method to quantitatively evaluate the performance of localization in unconstrained videos. Therefore, each study can only qualitatively evaluate the performance by visualization. Senocak et al. [33] proposed to create annotations with the weighted ground truth of the sound location by the consensus of multiple annotators. However, it is often seen that the opinions of each annotators differ, due to the ambiguity of the location of sound sources. Since sound is generated by vibrations of materials, it is not possible to specify exactly where the sound is produced.
We tackle this problem with the following ideas: (1) locate the object which is the original cause of the sound; (2) target the whole object, not a part of it; (3) exclude ambient sounds and limit our target to object sounds. As we describe in Section 3.1, these ideas eliminate much of the ambiguity about determining the location of the sound. We call such an object a Sounding Object (Figure 1).
Our goal is to localize the sounding object in unconstrained videos. We call this problem the Audio-Visual

1

Sounding Object Localization (AVSOL) and create the AVSOL evaluation (AVSOL-E) dataset for quantitative evaluation. We manually annotated ground truth of sounding objects by bounding-boxes for the videos in the test set of the AVE dataset which is a widely used dataset created by Tian et al. [36] We also propose three new evaluation metrics for AVSOL: HmBoxAUC, PiBR and PNSR. Details are described in Section 3.3. Since the definition of ground truth is similar to that of visual-only object localization or detection, it has high affinity with industrial applications. The AVSOL-E dataset will be publicly available on GitHub1.
To solve AVSOL, we propose a novel algorithm with a double-head architecture, called Dual Normalization Multitasking (DNM). DNM connects the one audio-visual similarity map (AVSM) with two different tasks (AVC and event classification) simultaneously to efficiently utilize the information obtained from these two tasks.
The contributions of this research are twofold: (1) For the first time in this field, we create AVSOL-E dataset and evaluation metrics, to make it possible to quantitatively evaluate the performance of AVSOL methods; (2) We propose a novel DNM architecture for AVSOL which significantly outperforms baseline methods.
2. Related Work
Sound Source Localization. Sound source localization by deep models is well studied in recent years. Deep learning requires a large labeled dataset, but it is difficult to prepare labels indicating the sound source locations in such a scale. Therefore, the mainstream is based on self- or weaklysupervised learning, which achieves localization without any supervisory data indicating the location of sound.
In early studies, the activation of a visual network trained with audio as supervisory signals is used to estimate the location[27, 1]. Arandjelovic´ and Zisserman [1] introduced AVC task, i.e. binary classification of whether the sound and video match or not. However, since these methods do not use sound input during inference, they only indicate the location of objects that are likely to produce sound.
Many of the recent techniques[26, 2, 33, 36, 31, 28] use MIL, Class Activation Mapping (CAM) or attention to localize the sound source. In [26], an early fusion network of sound and video is introduced and CAM is used to localize the sound source. In [2], a late fusion network which compares sound and pixel-wise visual features is proposed and trained by the AVC task. In their architecture, each pixel of the visual feature corresponds to a single instance of MIL, and succeeded to recognize the rough shape of the sound source object. In [33] audio-visual attention map which indicates the sound source is calculated and applied to visual feature to solve AVC task. Although these methods success-
1https://github.com/sony/ai-research-code

fully combined the AVC task with MIL or attention, they used still images, not video, as visual input.
Tian et al. [36] created AVE dataset and used event classification for training. Their system learns to align the attention to the audio-visual event. Although they also proposed an audio-visual distance learning similar to the AVC, they did not combine it with event classification. We show the localization performance is further boosted by multitasking AVC and event classification with our DNM architecture.
Dealing with multiple sound sources is a difficult issue. Methods have been proposed for localizing multiple sound sources[21, 22, 30]. In [30], multitask learning of AVC and event classification is used in the two-stage learning framework and a fine-grained audio-visual alignment is performed. We also leverage the supervision from these two tasks but in much simpler way. In spite of its simplicity, we show localizing multiple sound sources is possible. Evaluating Sound Source Localization. As described in Section 1, [33] proposed a consensus based annotation and collected the sound source localization dataset. Their dataset is based on Flickr-SoundNet[3] which consists of sound and still images. Recently the dataset of [33] is reported to be problematic. [28] showed the sound source in the dataset can be estimated using only visual keys. Hu et al. [22] used Faster RCNN[32] trained to detect instruments, to generate ground truth for a dataset of music play (MUSIC[41], AudiosSet-Instrument[16]). Since instruments are objects which clearly designed for producing sound, it is easy to use the detector to make ground truth. Weakly Supervised Object Localization. Weakly supervised object localization (WSOL) is commonly set up as a problem to extract object regions in the image given only a supervised signal of its category. Traditionally WSOL have been studied using MIL[7] and is improved by introducing prior knowledge such as symmetry[6] or motion[29], or by addressing the problem of local optimization[19].
Recently, CAM[42] is often used to extract objects location at pixel level. CAM is shown to be a kind of MIL that instantiates a receptive field corresponding to each pixel on image features[11]. Attention[24] is also shown to be an another form of MIL[23]. CAM and attention, as WSOL, has been criticized for highlighting mainly discriminative regions, and technics such as data augmentations and architectural solutions have been proposed to extract the entire object[35, 38, 39, 12]. However, recent study[11] reported a series of CAM based methods rely on implicit full supervision for setting hyperparameter (e.g. score map threshold  ), and did not reach CAM when evaluated by fair protocol.
Based on these findings, we propose evaluation metrics that is not affected by the threshold  for the heatmap. Our DNM aggregates MIL for AVC and attention for event classification into a single AVSM, and make it possible to train our model by these two tasks without inconsistencies.

2

AVE Non-AVE

single object multiple objects
visible audible neither (noise)

24,005 6,786
5,183 860 366

30,791 6,509

Total: 373 videos × 10 sec. × 10 fps = 37, 300

Table 1. # of annotated video frames in AVSOL-E dataset.

3. Audio-Visual Sounding Object Localization
3.1. Sounding Object
We define the sounding object as described in Section 1. For example, in Figure 1, the sound of a horse running is produced not by the horse itself, but by the collision of the horse's hoof with the ground. Therefore, in a physical sense, the sound source localization should output a certain range that includes the horse's hoof and the ground. Since the physical vibration is spread out through materials, it is never possible to spatially define a precise answer to the location of sound source. In this situation, however, if we ask the original cause of the sound, then it will be a "horse running", and the bounding-box annotation should be given to the horse by the definition of sounding object.
In a scene where a chainsaw is used to cut logs, the sound of the engine, the blade rotating and the vibration of the wood being cut, are all mixed together, making it difficult to determine the actual physical source of the sound. However, if we follow the definition of sounding object, estimating only the region of the chainsaw would be a desirable. Note that this "object" oriented annotations are very important for many real-world applications such as robot interactions and object-level image retrievals.
3.2. Dataset Description
With the definition of sounding object, we can now annotate a wide range of video. We create annotations for the videos in the test set of unconstrained AVE dataset[36]. AVE denotes an event which is both visible and audible in the scene. The AVE dataset is a subset of Audioset[18], and contains 4143 unconstrained videos each of 10 seconds, across 28 different categories which cover a wide range of AVEs from different domains e.g. humans, animals, vehicles, musics and machines. All categories of events are with object sounds, and there is no ambient sound event. Each video contains at least one 2 seconds long AVE, i.e. it may contains non-AVE segments. The AVSOL-E dataset. For each 10 seconds video in the test set of AVE dataset, we created bounding-box annotations for sounding objects at 10 fps. The test set contains 403 videos but only 373 videos were annotated for AVSOLE because we excluded irregular videos (e.g. Game CGs,

cartoons, videos whose audio is edited in post-processing). Each video may show multiple sounding objects at the same time in a single frame. In scenes like those, we annotated each sounding object, regardless of whether they are of the same category or a different category.
The Non-AVE frames may contain the event which is only visible (i.e. potential sounding object which is not making sound) or only audible (i.e. out-of-view sound), or neither (i.e. noise). bounding-boxes are given as long as the target is visible, regardless of whether or not the target is making a sound. Only when the visible object is making a sound at the moment, a 'sounding' tag is given to the target's bounding-box. To identify the only audible part, we introduce a 'out-of-view' tag. When the sound is audible and there is no visible object corresponds to it, we create a dummy bounding-box and attach the 'out-of-view' tag. Table 1 shows the statistics of AVSOL-E dataset.
3.3. Evaluation Metrics
We evaluate performance of AVSOL in three perspectives. In the frames of the AVE segments, we evaluate (1) whether the heatmap is spread out in the target area, and (2) whether the location of the heatmap peak is correct. In non-AVE frames, as it's desirable not to localize target, we evaluate (3) how well the output heatmap is suppressed. HmBoxAUC. Heatmap vs Bounding-Box Area Under the Curve (HmBoxAUC) is introduced to evaluate localization performance in AVE frames. For a single frame, let the index of each pixel be i. Each pixel value gi of ground truth is either gi = 1 for foreground (i.e. the pixel is in the bounding-box of sounding object) or gi = 0 for background. The each pixel value of heatmap hi is binarized by the threshold  . We define precision and recall between heatmap and bounding-box ground truth as follows.
precision( ) = |{hi   }  {gi = 1}| (1) |{hi   }|
recall( ) = |{hi   }  {gi = 1}| (2) |{gi = 1}|
For threshold independence, we use HmBoxAUC = d precision(d)(recall(d) - recall(d-1)). PiBR. Peak in Box Ratio (PiBR) is to quantify the pinpoint localization performance. It is simply the percentage of the number of frames where the peak of the heatmap is within the ground truth bounding-boxes, for all evaluation frames in the AVE segments. If there are multiple sounding objects, it is counted as correct if the peak is in one of them. PNSR. Since our model learn to generate localization map of the sounding object by the AVC task, the discrimination between AVE and non-AVE frames can be predicted by the output level of the map. In AVE frames, the peak of the map should be on the sounding objects and the value should be high. Conversely in non-AVE frames, the peak should

3

Figure 2. Proposed architecture with Dual Normalization Multitasking (DNM). Video is converted to sequential images and two CNNs (backbone and sub) extract visual features. Video is converted to sequential images and two CNNs (backbone and sub) extract visual features. Audio waveform is converted to log-mel spectrogram and sound features are extracted by two CNNs. These features are fused based on similarity in pixel-wise mannar and the AVSM is obtained. The AVSM is normalized in two ways, globally and locally. Globally normalized map is used as an attention for the visual feature to solve event classification. Each pixel of locally normalized map is dealt as an instance of MIL to solve AVC. This simple strategy allows to directly connect the two training tasks into a single map for the purpose of localization, and effectively boost the performance.

be suppressed. We introduce Peak Noise to Signal Ratio (PNSR) to measure this performance.
For a single image frame, let the number of sound sources be L, and the bounding-boxes of sounding object be B = {B1, B2, ..., BL}. We assign the number j = 1, 2, ..., J to all the evaluation frames in the test videos. Let FAVE be the set that contains all AVE frames, then the PNSR is as follows.

PNSR

=

averagej/FAVE maxi(hji ) averagejFAVE maxiBj (hji )

(3)

Unlike the general signal-to-noise ratio, the closer the PNSR is to zero, the better the performance.

4. Proposed Algorithm
As shown in Figure 2, our proposed AVSOL algorithm consists of the following sequences of steps: (1) Extraction of video and audio features by CNN; (2) Create pixel-wise AVSM by the fusion of both features; (3) Normalize the AVSM in two different ways and connect to AVC task and event classification task respectively (DNM).
4.1. Notations
Following [36], we split a video into non-overlapping segments of one second length each. The event category in AVE Dataset is annotated at second-level. The video and

audio input of each one second segment are denoted as V and A, respectively. Note that a single video segment may contain events of multiple categories at the same time, but the label is only attached to one event in AVE dataset.
4.2. Visual Feature Network
The visual feature network consists of a pretrained backbone network and a sub network. It has been shown that pretrained CNN features (VGG19 or I3D in our experiments) by a large-scale dataset are generic for other tasks. A video segment V is converted into a sequential RGB images, and the backbone extracts the visual features of them.
In order to preserve the spatial dimension, we use up to the middle layer of backbone where the spatial size of the features is W × H. The extracted features are further input to a sub network using 3D CNN to obtain the 4D visual feature v  RI×T ×D, where I(= W × H) is the vectorized spatial dimension of each feature map, T denotes the time dimension and D denotes the dimension of the feature vector. Note that, unlike [36] or [31], the visual features extracted from one second segment retain the temporal dimensionality T .
4.3. Audio Feature Network
The audio feature network is used to extract audio features from one second raw monaural audio A. Like the visual feature network, the audio feature network consists of a backbone network and a sub network.
The log mel spectrogram of audio, which can be treated as a single channel image, is input to the pretrained backbone CNN to extract features. In order to preserve the temporal information, features with the spatial size of B ×T are extracted using up to the middle layer of the CNN. B and T denotes the dimension of Mel Bin and time respectively. They are further input to the sub network of 2D CNN to extract the audio features a  RT ×D, where D represents the dimension of the audio feature vector.
4.4. Audio-Visual Similarity Fusion
The features extracted from each audio and visual feature network are fused to create pixel-wise AVSM. We propose two fusion methods: the static fusion and the Combined Dynamic Fusion (CDF). Various types of audio and video fusion have been proposed so far[36, 15, 31, 26]. However in sound localization, previous works [36, 31] did not look at fine temporal variations within one second. These fusion methods may also be considered a type of static fusion. Static Fusion The static fusion, which serves as a baseline, simply takes the similarity between static appearance and sound feature. As described in Figure 3 (a), we use the 1D vector audio feature of final layer output of audio backbone. Combined Dynamic Fusion From the findings in speaker estimation[15] and sound separation[40], we believe it is

4

(a) Static Fusion

(b) Combined Dynamic Fusion (CDF)

Figure 3. Audio-Visual Similarity Fusion. (a) In the static fusion, we follow [36] for feature extraction. After the visual backbone, the feature is averaged to crush the dimension of time T . In audio, the last layer output of the VGGish[20] is used. AVSM is obtained by pixelwise dot product without taking care of T . (b) The CDF is to capture the relation of detailed temporal changes between audio and video. The visual and sound features are fused in two ways, statically and dynamically with GRU[10][4], and further combined to get the AVSM. The map is expected to represent the pixel-wise locations where video and audio have static correspondences (i.e. visual appearances to sound types) and dynamic relations (i.e. visual motions to sound variations) at the same time.

important to extract the dynamic correlation between audio and video for AVSOL. For example, the movement of the hand playing an instrument and the change in the music sound will be highly correlated. At the same time, as the mechanism of sound generation varies[17], even objects with no apparent movement, such as car engines, produce sound. Therefore, we propose CDF (Figure 3 (b)) which uses both static and dynamic features to compute the similarity between audio and video features.
Let vt  RI×D be the sliced visual feature v along the time T , and vti  RD be the feature vector at each pixel on vt, i.e. v = {vt}Tt=1 and vt = {vti}Ii=1. For audio features, we have a = {at}Tt=1 in the same manner. at  RD denotes the audio feature vector of the time period t.
For a one second clip, a static map is obtained as the time average of the pixel-wise dot product as MStatic = {averaget(vti · at)|i = 1, ..., I}  RI . To make a dynamic map, each feature v and a is encoded by GRU[10] and then pixel-wise dot product is computed for the final output of each GRU. For v, we use convolutional GRU (ConvGRU)[4] since it has a spatial dimension. ConvGRU and GRU takes vt and at as input to encode temporal dependencies by processing them in unidirectional manner, for the two modalities respectively:

vT , hvT = ConvGRU(v0, hv0)

(4)

aT , haT = GRU(a0, ha0)

(5)

where vT  RI×D and aT  RD refer to final output of visual and sound feature and hv and ha represent hidden

states. The dynamic similarity is computed from vT and aT

as

MDynamic

=

{v

i T

·

aT )|i

=

1, ..., I}



RI .

By

taking

Hadamard product of static and dynamic maps, we obtain

the final AVSM MCDF  RI as follows.

MCDF = MStatic  MDynamic

(6)

4.5. Dual Normalization Multitasking

Our model is trained using both the AVC task and the event classification task. The advantage of the AVC is that it is completely self-supervised. It is also compatible with MIL formulation, as it is positive/negative classification. The disadvantage is that the audio is often mixed with out-of-view sounds or ambient sounds that is not directly related to the visible events. In fact, based on our experience with manually annotating AVSOL-E dataset, it is sometimes difficult even for expert human annotators to determine whether a certain sound comes from the object in the screen or not.
We believe this drawback can be compensated for by additional supervision, e.g. the category of events shown in the video. Since category annotations are made by humans, it's guaranteed the audio contains the sound from the target object. The disadvantage is a request for manual annotation, but category annotations are much easier to correct than other types of annotations[5].
Each of AVC and classification is commonly used in sound localization, but we don't yet have a known strategy on how to utilize them together to improve localization. Therefore, we propose a method to effectively connect them: Dual Normalization Multitasking (DNM).
In DNM, we normalize the AVSM in two ways; locally and globally. In local normalization, each pixel of the similarity map is processed independently. Each normalized value is used as an instance for MIL to estimate AVC. Specifically, sigmoid function is used for normalization, and then global max pooling (GMP) is applied to the entire map to produce the AVC estimation:

zavc = GMP(Mloc)

(7)

where Mloc = {Sigmoid (si)}Ii=1 is the localization map and si is the similarity score for each pixel in AVSM. Mloc
is used as the localization result of AVSOL.

5

The purpose of global normalization is to select important pixels by considering their relation to the entire AVSM. We use the globally normalized map as the attention to visual features. Specifically, softmax function is used to generate attention weights, which will be applied to obtain a weighted sum of the feature vectors vatt  RD :

vatt = Ii=1wai ttvcils

(8)

where vcls  RI×D is an output from the branch of sub CNN, and watt = Softmax ({si}Ii=1)  RI is attention obtained by global normalization. Finally, vatt is passed to
fully connected layer to obtain classification output.

5. Experiments
5.1. Implementation Details
For the AVC, we make positive (AVC=yes) and negative (AVC=no) data. The positive data is the correct combination of audio and video. For the negative data, the video and audio are separated, and the video is combined with the audio of a different clip or time. The number of positive and negative data is set to 1:1. For each training epoch, the combination of video and audio in the negative data is shuffled.
During training, the error of event classification is backpropagated only when AVC=yes. Random horizontal flip and resized crop are used for video data augmentation. No data augmentation is applied for audio. We used a binary cross-entropy loss for both AVC and event classification, and trained our network with the sum of these two losses.
For the visual backbone, we compare VGG19[34] pretrained with ImageNet[13], and I3D's RGB stream [8] pretrained with Kinetics Dataset[8]. VGG19 is commonly used in image recognition, and I3D is commonly used in video action recognition. VGGish[20] pretrained by Audioset[18] is used to extract audio features. During training, the parameters of the backbone networks are fixed.
5.2. Baselines and Evaluation Method
We set two baselines for model comparison; CAM [42] and the model proposed by Tian et al. [36]. CAM is a basic and powerful method in WSOL. We modified our proposed model to take only visual input and output CAM. Specifically, we applied global average pooling (GAP) to the feature vcls from visual sub CNN followed by the fully connected layer to make CAM.
In [36], the audio-guided visual attention is used to localize sounding objects. We used the model trained and provided by the authors. With referring to WSOL evaluation, we normalized the output attention by min-max for each frame individually[11]. [36] uses whole 10-second video as input, and their temporal interrelationships for 10 seconds are coded in bidirectional LSTM. However, we humans can recognize the type of event and the location of

sound source instantly and in real-time[17]. Since our algorithm is intended to be used in real-world and real-time applications, we use only 1-second videos as input to solve the problems. Note that our algorighm solves tasks with much less information than Tian et al.'s[36] and other methods that follow[31, 14, 37, 25].
In addition to the two baselines, to validate the effectiveness of DNM, we compared models trained on a single task, either AVC or event classification, to the model trained using DNM. For AVC and DNM, the MIL map Mloc is used as the localization result. For the classification only models, the attention weight watt is used as localization result.
HmBoxAUC, PiBR, and PNSR are used for evaluation. We choose the model with the maximum accuracy in AVC and classification in the validation set respectively, and evaluate them for AVSOL in the test set. Multiple trainings are conducted and the average result is reported.
5.3. experimental comparisons
Table 2 and Figure 4 shows the quantitative and qualitative results respectively. The result of two baseline methods and eight conditions of our proposed models are presented. Training Methods. For the models trained only with event classification (VGG19-Static-Cls, I3D-Static-Cls.), the HmBoxAUC are significantly lower than those of the baselines. The qualitative results show that the globally normalized audio-visual attention map wvatt tends to concentrate on a point of the most discriminative region, and the shape of the target object is not extracted. In spite of low HmBoxAUC, the PiBR is as good as those of baselines.
When trained with the AVC task only (VGG19-StaticAVC, I3D-Static-AVC), the performances are not as good as Tian et al. and for VGG19-Static-AVC, the result is worse than I3D-Static-AVC and CAM. The qualitative results show that the model tends to extract not the sounding object itself, but the area around its edges. We saw this occur in most of the trials of training. This appears to be same to the case reported by Senocak et al. [33]. For example, in music play, the visual feature of the edges of instrument which is almost always present on the screen at the same time with the instrument, may be learned to be more strongly associated with the sound. This phenomenon is also known to be appear in the field of WSOL as the inherent ill-posedness of weakly-supervised learning [11] .
The two DNM models with static fusion (VGG19-StaticDNM, I3D-Static-DNM) showed a significant improvement in HmBoxAUC from the single-task conditions. The qualitative results show that the models with DNM did not show the problems which occurred in single task conditions. Since the MIL is good at extracting wide region, and the attention is good at finding globally important points, we think the combination of these two worked well. Audio-Visual Fusion Methods. The two CDF condi-

6

Models
CAM[42] (visual only) Tian et al. [36]
VGG19-Static-Cls. VGG19-Static-AVC VGG19-Static-DNM VGG19-CDF-DNM
I3D-Static-Cls. I3D-Static-AVC I3D-Static-DNM I3D-CDF-DNM

HmBoxAUC

all single multi.

0.416 0.433 0.348 0.508 0.531 0.432

0.049 0.328 0.571 0.573

0.046 0.350 0.604 0.598

0.061 0.250 0.419 0.467

0.278 0.480 0.576 0.581

0.279 0.499 0.619 0.616

0.292 0.414 0.422 0.451

all
0.629 0.671
0.672 0.450 0.630 0.672
0.661 0.717 0.709 0.744

PiBR

single multi.

0.625 0.644 0.687 0.617

0.691 0.484 0.667 0.688

0.604 0.331 0.550 0.617

0.672 0.746 0.739 0.768

0.620 0.615 0.602 0.659

all
-
0.805 0.660 0.605
0.590 0.626 0.550

PNSR

visible audible

-

-

-

-

0.881 0.610 0.677

0.816 0.701 0.619

0.679 0.637 0.587

0.604 0.658 0.563

noise
-
0.317 0.298 0.307
0.268 0.241 0.328

Table 2. Comparisons of our models and baselines on AVSOL-E dataset. HmBoxAUC and PiBR are evaluated for all the AVE frames (all). They are also separately evaluated for the scenes with single sounding object (single) and for the scenes with multiple sounding objects (multi.). PNSR for all the frames and three separated conditions (visible, audible and noise) are reported for the models with AVC output.

Annotations

CAM

Tian et al.

VGG19

VGG19

VGG19

VGG19

I3D

I3D

I3D

I3D

Static-Cls. Static-AVC Static-DNM CDF-DNM Static-Cls. Static-AVC Static-DNM CDF-DNM

Figure 4. Qualitative comparisons of baselines and our models with AVSOL-E annotations. AVSOL-E annotations are shown in the leftmost column. Sounding objects (green bounding-box) and potential sounding objects which is not making sound at the frame (magenta bounding-box) are drawn. Heatmap is min-max normalized for better visibility.

tions (VGG19-CDF-DNM, I3D-CDF-DNM) showed better performance compared to their static fusion counterparts (VGG19-Static-DNM, I3D-Static-DNM) for all the three metrics. The qualitative results show that the method is able to extract the entire area of the target object better than others (Figure 4 top and 2nd row). However, compared to static fusion, strong heatmap output sometimes appeared on the area of the moving parts. As shown in Figure 4 (bottom), moving fingers playing the flute are also localized.

Multiple Sounding Objects. As shown in Table 2, in most cases, the HmBoxAUC and PiBR in single sounding object condition is better than that of multiple sounding objects. Qualitative results in Figure 5 shows that both the baselines and the proposed methods with CDF and DNM succeed in localizing multiple sounding objects, when those objects are in the same or similar categories (Figure 5 (a) and (b)). In such a case, the model simply has to find the location where the visual feature corresponds to the sound feature.

7

Annotations

CAM

Tian et al.

VGG19

I3D

CDF-DNM CDF-DNM

(a)

(a)

(b)
(b) (c)

(d)
Figure 5. Comparisons in scenes with multiple sounding objects. Sounding objects (green bounding-box) and objects not making sound (magenta bounding-box) are visualized in annotations.
It is more difficult to localize multiple sounding objects when they are in different categories. The model has to represent two types of sound features together in a single sound feature vector, and associate them with different visual features in different positions. Figure 5 (c) and (d) shows scenes with playing instrument and singing at the same time. Our models only managed to locate the instrument being played, not the singing face. Non-AVE Scenes.
PNSR for all different types of non-AVE scenes are shown in Table 2. For all the tested models, in the scene with no sounding object nor no out-of-view sound (noise), the PNSR are better compared to other non-AVE scenes (visible or audible). For each backbone, the model with CDF and DNM showed the best performance. I3D-CDFDNM was the best of all the models.
The qualitative results of our I3D-CDF-DNM model is shown in Figure 6. Our proposed model successfully suppresses the heatmap output in the two types of non-AVE frames. In (a), the type of event changes from "speaking" to "playing flute" before and after the pause. Our proposed model was able to follow the change and localize the correct sounding object. In (c), at first, we hear the sound of a bus running but we don't see the bus, then the bus appears. Our model was able to suppress the heatmap output while the bus is not visible.
6. Discussion and Conclusion
For the first time in this field, we created the annotations (AVSOL-E dataset) and the evaluation metrics for quantitative comparisons of AVSOL methods. As the AVSOL-E

(c)
Figure 6. Qualitative results of a proposed model (I3D-CDFDNM) for videos containing non-AVE scenes. Audio waveforms are also drawn. Frames that contain at least one sounding object (green bounding-box) are AVE frames. Frames only with objects not making sound (magenta bounding-box) or out-of-view sound (red bounding-box surrounding the frame) are non-AVE frames.
dataset and evaluation source code will be publicly available, we hope it will be useful to other researchers. The idea of sounding object presented in this paper is effective. Although we have annotated 28 categories of events in AVE dataset this time, we think that datasets containing more types of events (e.g. VGGSound[9] with 310 categories) can also be annotated using the same idea.
Inspired by the findings from WSOL and previous audiovisual multimodal studies, we proposed DNM which uses MIL and attention at the same time to effectively connect AVC task and event classification task. We evaluated two baseline methods and proposed models on AVSOL-E dataset. Our models with DNM outperformed the baselines in all the evaluation metrics. They also showed better performance compared to other methods and baselines in difficult scenes, i.e. multiple sounding objects and three types of non-AVE segments. Thanks to the new AVSOL-E dataset and the evaluation metrics, we were able to quantitatively and qualitatively demonstrate the superiority of our proposed method.
Our algorithm can be implemented to operate in realtime manner. The ability to find sounding objects is a fundamental ability for living creatures. The results of this research are expected to be applied to many applications, including real-world robots.

8

References
[1] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In Proceedings of the International Conference on Computer Vision, pages 609­617, 2017. 2
[2] Relja Arandjelovic and Andrew Zisserman. Objects that sound. In Proceedings of the European Conference on Computer Vision, pages 435­451, 2018. 2
[3] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: learning sound representations from unlabeled video. In Proceedings of the International Conference on Neural Information Processing Systems, pages 892­900, 2016. 2
[4] Nicolas Ballas, Li Yao, Chris Pal, and Aaron C Courville. Delving deeper into convolutional networks for learning video representations. In Proceedings of the International Conference on Learning Representations, 2016. 5
[5] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. What's the point: Semantic segmentation with point supervision. In Proceedings of the European conference on computer vision, pages 549­565, 2016. 5
[6] Hakan Bilen, Marco Pedersoli, and Tinne Tuytelaars. Weakly supervised object detection with posterior regularization. Proceedings of the British Machine Vision Conference, pages 1­12, 2014. 2
[7] Marc-Andre´ Carbonneau, Veronika Cheplygina, Eric Granger, and Ghyslain Gagnon. Multiple instance learning: A survey of problem characteristics and applications. Pattern Recognition, 77:329­353, May 2018. 2
[8] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 6299­6308, 2017. 6
[9] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale audio-visual dataset. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, 2020. 8
[10] Kyunghyun Cho, Bart van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder­decoder for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724­1734, 2014. 5
[11] Junsuk Choe, Seong Joon Oh, Seungho Lee, Sanghyuk Chun, Zeynep Akata, and Hyunjung Shim. Evaluating weakly supervised object localization methods right. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 3133­3142, 2020. 2, 6
[12] Junsuk Choe and Hyunjung Shim. Attention-based dropout layer for weakly supervised object localization. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 2219­2228, 2019. 2
[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. ImageNet: A Large-Scale Hierarchical Image Database. In Proceeding of the Conference on Computer Vision and Pattern Recognition, 2009. 6
[14] Bin Duan, Hao Tang, Wei Wang, Ziliang Zong, Guowei Yang, and Yan Yan. Audio-visual event localization via recursive fusion by joint co-attention. In Proceedings of

the Winter Conference on Applications of Computer Vision, pages 4013­4022, 2021. 6 [15] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018. 4 [16] Ruohan Gao and Kristen Grauman. Co-separating sounds of visual objects. In Proceedings of the International Conference on Computer Vision, pages 3879­3888, 2019. 2 [17] William W Gaver. What in the world do we hear?: An ecological approach to auditory event perception. Ecological psychology, 5(1):1­29, 1993. 5, 6 [18] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and humanlabeled dataset for audio events. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pages 776­780, 2017. 3, 6 [19] Ramazan Gokberk Cinbis, Jakob Verbeek, and Cordelia Schmid. Multi-fold mil training for weakly supervised object localization. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 2409­2416, 2014. 2 [20] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn architectures for large-scale audio classification. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pages 131­135, 2017. 5, 6 [21] Di Hu, Feiping Nie, and Xuelong Li. Deep multimodal clustering for unsupervised audiovisual learning. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 9248­9257, 2019. 2 [22] Di Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding, Weiyao Lin, and Dejing Dou. Discriminative sounding objects localization via self-supervised audiovisual matching. arXiv preprint arXiv:2010.05466, 2020. 2 [23] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In Proceedngs of the International Conference on Machine Learning, pages 2127­2136. PMLR, 2018. 2 [24] Saumya Jetley, Nicholas A Lord, Namhoon Lee, and Philip HS Torr. Learn to pay attention. arXiv preprint arXiv:1804.02391, 2018. 2 [25] Yan-Bo Lin, Yu-Jhe Li, and Yu-Chiang Frank Wang. Dualmodality seq2seq network for audio-visual event localization. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pages 2002­2006, 2019. 6 [26] Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisensory features. In Proceedings of the European Conference on Computer Vision, pages 631­648, 2018. 2, 4 [27] Andrew Owens, Jiajun Wu, Josh H McDermott, William T Freeman, and Antonio Torralba. Ambient sound provides supervision for visual learning. In Proceedings of the European Conference on Computer Vision, pages 801­816. Springer, 2016. 2

9

[28] Takashi Oya, Shohei Iwase, Ryota Natsume, Takahiro Itazuri, Shugo Yamaguchi, and Shigeo Morishima. Do we need sound for sound source localization? In Proceedings of the Asian Conference on Computer Vision, 2020. 2
[29] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid, and Vittorio Ferrari. Learning object class detectors from weakly annotated video. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 3282­3289, 2012. 2
[30] Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, and Weiyao Lin. Multiple sound sources localization from coarse to fine. arXiv preprint arXiv:2007.06355, 2020. 2
[31] Janani Ramaswamy and Sukhendu Das. See the sound, hear the pixels. In Proceedings of the Winter Conference on Applications of Computer Vision, pages 2970­2979, 2020. 2, 4, 6
[32] Shaoqing Ren, Kaiming He, Ross B Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proceedings of the International Conference on Neural Information Processing Systems, 2015. 2
[33] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 4358­4366, 2018. 1, 2, 6
[34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of the International Conference on Learning Representations, 2015. 6
[35] Krishna Kumar Singh and Yong Jae Lee. Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In Proceedings of the International Conference on Computer Vision, pages 3544­3553, 2017. 2
[36] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European Conference on Computer Vision, pages 247­263, 2018. 1, 2, 3, 4, 5, 6, 7
[37] Yu Wu, Linchao Zhu, Yan Yan, and Yi Yang. Dual attention matching for audio-visual event localization. In Proceedings of the International Conference on Computer Vision, pages 6292­6300, 2019. 6
[38] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the International Conference on Computer Vision, pages 6023­6032, 2019. 2
[39] Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, and Thomas S Huang. Adversarial complementary learning for weakly supervised object localization. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 1325­1334, 2018. 2
[40] Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba. The sound of motions. In Proceedings of the International Conference on Computer Vision, pages 1735­1744, 2019. 4
[41] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound

of pixels. In Proceedings of the European Conference on Computer Vision, pages 570­586, 2018. 2
[42] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 2921­2929, 2016. 2, 6, 7

10

