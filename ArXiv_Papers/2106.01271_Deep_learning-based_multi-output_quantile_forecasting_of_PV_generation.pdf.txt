Deep learning-based multi-output quantile forecasting of PV generation
Jonathan Dumas, Colin Cointe, Xavier Fettweis, Bertrand Corne´lusse Departments of computer science, electrical engineering, and geography University of Lie`ge, Belgium
{jdumas, xavier.fettweis, bertrand.cornelusse}@uliege.be, colin.cointe@mines-paristech.fr

arXiv:2106.01271v2 [cs.LG] 7 Jun 2021

Abstract--This paper develops probabilistic PV forecasters by taking advantage of recent breakthroughs in deep learning. A tailored forecasting tool, named encoder-decoder, is implemented to compute intraday multi-output PV quantiles forecasts to efficiently capture the time correlation. The models are trained using quantile regression, a non-parametric approach that assumes no prior knowledge of the probabilistic forecasting distribution. The case study is composed of PV production monitored on-site at the University of Lie`ge (ULie`ge), Belgium. The weather forecasts from the regional climate model provided by the Laboratory of Climatology are used as inputs of the deep learning models. The forecast quality is quantitatively assessed by the continuous ranked probability and interval scores. The results indicate this architecture improves the forecast quality and is computationally efficient to be incorporated in an intraday decision-making tool for robust optimization.
Index Terms--Quantile forecasting, probabilistic PV forecasting, LSTM, deep learning, encoder-decoder
I. INTRODUCTION
The Intergovernmental Panel on Climate Change special report1 on the impacts of global warming of 1.5°C above preindustrial levels and related global greenhouse gas emission pathways' presents several scenarios of decarbonisation of the electricity sector with a renewable share that should reach in 2030 the interquartile range [47, 65] ([69, 86] in 2050). Therefore, the development of renewable generations, typically from wind and photovoltaic (PV) sources has been facilitated by policy makers. However, the intermittent and uncertain nature of these sources is challenging the traditional operation of electricity networks. Market players require reliable decisionmaking tools to deal with uncertainty based on forecasts of renewable generation.
In contrast to point predictions, probabilistic forecasts aim at providing decision-makers with full information about potential future outcomes [1]. The various types of probabilistic forecasts range from quantile to density forecasts, and through prediction intervals. This paper focuses on quantile forecasts that provide probabilistic information about future renewable power generation, in the form of a threshold level associated with a probability [1]. Quantile regression [2] is one of the most famous non-parametric approaches. It does not assume
1https://www.ipcc.ch/sr15/
978-1-6654-3597-0/21/$31.00 ©2021 IEEE

the shape of the predictive distributions and is implemented with neural networks, linear regression, gradient boosting, or any other regression techniques.
The following papers have gained our attention in PV probabilistic forecasting. At the Global Energy Forecasting Competition 2014 [3] solar forecasts were to be expressed in the form of 99 quantiles with various nominal proportions between zero and one. A systematic framework for generating PV probabilistic forecasts is developed by [4]. A non-parametric density forecasting method based on Extreme Learning Machine is adopted to avoid restrictive assumptions on the shape of the forecast densities. A combination of bidirectional Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) resulting in Bidirectional LSTM (BLSTM) is proposed by [5]. It has the benefits of both longrange memory and bidirectional processing. The BLSTM is trained by minimizing the quantile loss to compute quantile forecasts of aggregated load, wind and PV generation, and electricity prices on a day-ahead basis. Finally, an innovative architecture, referred to as encoder-decoder (ED), is developed by [6] to generate reliable predictions of the future system imbalance used for robust optimization.
In this study, the forecast quality of the models is evaluated. It corresponds to the ability of the forecasts to genuinely inform of future events by mimicking the characteristics of the processes involved. We follow the framework proposed by [7], [8] for evaluating the quality of solar irradiance probabilistic forecasts based on visual diagnostic tools and a set of scoring rules. Overall they indicate that two main attributes, reliability and resolution, characterize the quality of quantile forecasts.
This work exploits recent breakthroughs in the field of data science by using advanced deep learning structures, such as the encoder-decoder architecture [6], and quality metrics [8], [9] to develop a tailored deep learning-based multi-output quantile forecaster. The goal is to capture the time correlation between time periods and to use this forecaster as input of a robust optimization model. For instance, to address the energy management system of a grid-connected renewable generation plant coupled with a battery energy storage device [10].
Overall, the contributions can be summarized as follows. First, a deep learning-based multi-output quantile architecture is used to compute prediction intervals of PV generation on a day-ahead and intraday basis. Specifically, the goal is to implement an improved probabilistic intraday forecaster,

the encoder-decoder, to benefit from the last PV generation observations. This architecture is compared to a feed-forward neural network that is used as the benchmark model. Second, the weather forecasts of the MAR climate regional model [11] are used. It allows to directly take into account the impact of the weather forecast updates generated every six hours. Finally, a proper assessment of the quantile forecasts is conducted by using a k-fold cross-validation methodology and probabilistic metrics. It allows computing average scores over several testing sets and mitigating the dependency of the results to specific days of the dataset.
The remainder of this paper is organized as follows. Section II provides the non-parametric quantile forecasting framework considered. Section III presents the forecasting techniques used to compute the quantile forecasts. Section IV details the different metrics used to evaluate the quality of PV quantile forecasts. Section V describes the case study and presents the results. Finally, Section VI summarizes the main findings and highlights ideas for further work.

II. NON-PARAMETRIC QUANTILE FORECASTING
Let yt be the PV power generation measured at time t, which corresponds to a realization of the random variable Yt. Then let ft and Ft be the probability density function (PDF) and related cumulative distribution function (CDF) of Yt, respectively. Following the definition of probabilistic forecasting of [1], a probabilistic forecast issued at time t for time t + k consists of a prediction of the PDF (or equivalently, the CDF) of Yt+k, or of some summary features. Various types of probabilistic forecasts have been developed, from quantile to density forecasts, and through prediction intervals. This study focuses on quantile regression [2], which is the most widely used type of probabilistic forecasting method. It is a non-parametric approach with no restrictive assumption on the shape and features of the predictive distributions. Indeed, empirical investigations [4] showed that PV power forecast errors do not follow common, e.g. Gaussian, Beta, distributions. Following the definition of [1], a (model-based) forecast y^t+k|t of PV power generation is an estimate of some of the characteristics of the stochastic process Yt+k given a model g, its estimated parameters ^ t and the information set t gathering all data and knowledge about the processes of interest up to time t.

A. Point forecasting

A point prediction y^t+k|t is a single-valued issued at time
t for t + k. It corresponds to the conditional expectation of Yt+k given g, ^ t, and the information set t

y^t+k|t = E Yt+k|t|g, t, ^ t .

(1)

A multi-output point forecast computed at t for t+k1 to t+kT is the vector y^t+k1|t, · · · , y^t+kT |t of size T .

B. Quantile forecasting
A quantile forecast y^t(+q)k|t with nominal level q is an estimate, issued at time t for time step t + k of the quantile

yt(+q)k|t for the random variable Yt+k|t given a model g, its estimated parameters ^ t and the information set t

P [Yt+k|t  y^t(+q)k|t|g, t, ^ t] = q,

(2)

or equivalently y^t(+q)k|t = F^t-+1k|t(q), with F^ the estimated cumulative distribution function of the continuous random
variable Y . Finally, a multi-output quantile forecast computed at t for t + k1 to t + kT is the matrix Z^t of dimensions T × Q

 y^t(+q=k11|)t . . . y^t(+q=k1Q|t) 



Z^t

=

 

...

...

...

 , 

(3)





y^t(+q=kT1)|t . . . y^t(+q=kTQ|)t

with Q quantiles per time period.

C. Quantile loss function
Quantile regression consists of estimating quantiles by applying asymmetric weights to the mean absolute error. Following [2], the quantile loss function is

q × (x - y)

x>y

q(x, y) =

(1 - q) × (y - x)

. xy

(4)

For a given time period t, q is summed over the forecasting time periods k1  k  kT and quantiles q  Q to compute multi-output quantile forecasts at t for t + k1 to t + kT

Lt =

q yt+k, y^t(+q)k|t .

(5)

k1kkT qQ

Finally,

the

model

g

is

trained

by

minimizing

1 |t |

t t Lt .

Note in the case of perfect prediction, the quantile loss cannot

be differentiated, and a smooth approximation of (4) using the

Huber norm [6] is built.

D. Prediction intervals (PIs)

Prediction intervals (PIs) define the range of values within
which the observation is expected to be with a certain proba-
bility, i.e., its nominal coverage rate [9]. Formally, a prediction interval I^t(+)k|t issued at t for t+k, defines a range of potential values for Yt+k, for a certain level of probability (1 - ),   [0, 1]. Its nominal coverage rate is

P Yt+k  I^t(+)k|t|g, t, ^ t = 1 - .

(6)

A central PI consists of centering the PI on the median where there is the same probability of risk below and above the median. A central PI with a coverage rate of (1 - ) is estimated by using the quantiles q = (/2) and q = (1-/2). Its nominal coverage rate is

I^t(+)k|t = y^t(+q=k|t/2), y^t(+q=k1|t-/2) .

(7)

For instance, central PIs with a nominal coverage rate of 90 %, i.e., (1 - ) = 0.9, are defined by quantile forecasts with nominal levels of 5 and 95 %.

III. FORECASTING TECHNIQUES
A. Gradient boosting regression (GBR)
Gradient boosting builds an additive model in a forward stage-wise fashion [12]. It allows for the optimization of arbitrary differentiable loss functions. In each stage, a regression tree is fit on the negative gradient of the given loss function. The Scikit-learn [13] Python library is used to implement a gradient boosting regressor (GBR) with the quantile loss function. The learning rate is set to 10-2, the max depth to 5, and the number of estimators to 500. There is a GBR model per quantile as the library does not support multi-output quantile regression.
B. Multilayer perceptron (MLP)
A description of the most widely used "vanilla" neural network, the Multilayer perceptron (MLP), is provided by [12]. A MLP with a single hidden layer is considered for the day-ahead forecasts and as the benchmark for the intraday forecasts. MLPs with two and three hidden layers did not provide any significant improvement. The activation function is the Rectified Linear Unit (ReLU). The number of neurons of the hidden layer is ninput +(noutput -ninput)/2, with ninput and noutput the number of neurons of the input and output layers, respectively. The learning rate is set to 10-2 and the number of epoch to 500 with a batch size of 8. It is implemented using the PyTorch Python library [14].

encoder and a MLP as the decoder. In a second version (ED2) the decoder is a LSTM followed by an additional feedforward layer. Both versions of the encoder-decoder are used as intraday forecasters. In ED-1, the encoder has 2×ninput units with ninput the number of neurons of the encoder input layer, features from the past. Then, the encoder output is merged with the weather forecasts becoming the decoder input layer that has noutput/2 neurons. In ED-2, the decoder has the same number of cells as the encoder, and the feed-forward layer is composed of noutput/2 neurons. The LSTM, ED-1, and ED-2 models are implemented using the TensorFlow Python library [15]. The activation functions are the ReLU, the learning rate is set to 10-3, the number of epoch to 500 with a batch size of 64 for the three models.
A sensitivity analysis has been conducted to select the hyperparameters: number of hidden layers, neurons, epochs, and learning rate. Overall, increasing the number of hidden layers and neurons increases the model complexity. It can enhance the accuracy, but only up to a limited number of layers and neurons due to overfitting issues. In addition, the hyperparameter solution is closely related to the size of the historical database [5]. A deep learning model with a larger amount of hidden layers and neurons requires a large amount of data to accurately estimate the parameters. In the case study considered, there are only 157 days of data with a 15 minutes resolution. Thus, we decided to restrict the number of layers and neurons to select a smaller model that performs better with the available information.

C. Encoder-decoder (ED)
Several technical information about recent advances in neural networks is provided by [5], [6]. In particular, recurrent neural networks, have shown a high potential in processing and predicting complex time series with multi-scale dynamics. However, RNNs are known to struggle in accessing time dependencies more than a few time steps long due to the vanishing gradient problem. Indeed, back-propagated errors during the training stage either fades or blows up over time. Long Short-Term Memory and Gated Recurrent Units networks tackle this problem by using internal memory cells [6]. A neural network composed of a LSTM and feed-forward layers, referred to as LSTM in the rest of the paper, is implemented for the day-ahead and intraday forecasts. The number of LSTM units is ninput + (noutput - ninput)/3, and the number of neurons of the feed-forward layer ninput + 2 × (noutput - ninput)/3.
An innovative architecture, referred to as encoder-decoder [6], is composed of two different networks and has recently shown promising results for translation tasks and speech recognition applications and imbalance price forecasting. The encoder-decoder processes features from the past, such as past PV observations, to extract the relevant historical information that is contained into a reduced vector of fixed dimensions, based on the last hidden state. Then, the decoder processes this representation along with the known future information such as weather forecasts. A version of the encoder-decoder architecture (ED-1) is implemented with a LSTM as the

IV. PROBABILISTIC FORECASTING QUALITY ASSESSMENT
For predictions in any form, one must differentiate between their quality and their value [1]. Forecast quality corresponds to the ability of the forecasts to genuinely inform of future events by mimicking the characteristics of the processes involved. Forecast value relates, instead, to the benefits from using forecasts in a decision-making process such as participation in the electricity market. This section proposes quality metrics based on the framework proposed by [8]. The value assessment is not in the scope of this paper as it would require to consider a decision-making process.
A. Continuous rank probability score (CRPS)
A score is said to be proper if it ensures that the perfect forecasts should be given the best score value [16]. It is the case of the Continuous Rank Probability Score (CRPS) that penalizes the lack of resolution of the predictive distributions as well as biased forecasts. For deterministic forecasts, the CRPS turns out to be the Mean Absolute Error (MAE). The energy form [16] of the CRPS (NRG) is selected in this study
CRPSNt,RkG(F^t+k|t, yt+k) =EF^t+k|t |X - yt+k| 1
- 2 EF^t+k|t |X - X |, (8)
where X and X are two independent copies of a random variable with distribution function F^t+k|t and finite first moment. A CRPS estimator (eNRG) of the energy form is provided

by [17] when the CDF is only known at t + k through a Qensemble of quantile forecasts Qt,k = {y^t(+q)k|t}qQ

CRPSet,NkRG(Qt,k, yt+k)

=

1 Q

|y^t(+q)k|t - yt+k|

qQ

1 - 2Q2

|y^t(+q)k|t - y^t(+q k)|t|.

qQ q Q

(9)

Finally, CRPSeNRG(k) is the mean over the evaluation set for a given forecasting time period k, and CRPSeNRG is the average
over all k with k1  k  kT .

B. Interval score (IS)

The Interval Score (IS) is a proper score proposed by [16] to specifically assess the quality of central (1 - ) prediction interval forecasts. The IS rewards narrow prediction intervals but penalizes, with the penalty term that depends on , the forecasts for which the observation is outside the interval. The averaged IS over an evaluation set of length M and k, k1  k  kT is

1 IS = M

M1 T

kT
(y^t(+1-k|t/2) - y^t(+/k2|t))

t=1 k=k1

+

2 

(y^t(+/k2|t)

-

yt+k )1(yt+k



y^t(+/k2|t))

+

2  (yt+k

-

y^t(+1-k|t/2))1(yt+k



y^t(+1-k|t/2)).

(10)

V. THE ULIE` GE CASE STUDY

A. Case study description

The ULie`ge case study is composed of a PV generation plant with an installed capacity of 466.4 kW. The PV generation has been monitored on a minute basis from April 4, 2020 to September 14, 2020, 157 days in total, and the data is resampled to 15 minutes. The set of quantiles is Q = {0.1, 0.2, . . . , 0.9} for both the day-ahead and intraday forecasts. Numerical experiments are performed on an Intel Core i7-8700 3.20 GHz based computer with 12 physical CPU cores and 32 GB of RAM running on Ubuntu 18.04 LTS.

B. Numerical settings
The MAR regional climate model [11] provided by the Laboratory of Climatology of the Lie`ge University is forced by GFS (Global Forecast System) to compute weather forecasts on a six hours basis, four-time gates per day at 00:00, 06:00, 12:00, and 18:00 with a 10 day horizon and a 15 minutes resolution. The solar irradiance and air temperature at 2 meters are normalized by a standard scaler and used as inputs to the forecasting models.
A k-fold cross-validation is strategy is used to compute average scores over several testing sets to mitigate the dependency of the results to specific days of the dataset. The dataset is divided into k parts of equal length, and there are k possible testing sets 1  i  k. For a given testing set i, the models are trained over the k - 1 parts of the dataset. Eleven pairs

of fixed lengths of 142 and 15 days are built. One pair is used to conduct the hyperparameters sensitivity analysis, and the ten others for testing where the scores are averaged. The Mean Absolute Error (NMAE) and Root Mean Squared Error (NRMSE) are introduced to evaluate the point forecasts. The MAE, RMSE, CRPS, and IS are normalized by the PV total installed capacity with NMAE and NRMSE the normalized MAE and RMSE.
The day-ahead models, MLP, LSTM, and GBR compute forecasts at 12:00 for the next day. Four intraday time gates are considered at 00:00, 06:00, 12:00, and 18:00. The intraday forecasts of time gate 00:00 are computed by the day-ahead models using only the weather forecasts. Then, the next three intraday forecasts are computed by intraday models where the MLP, ED-1, and ED-2, models use the weather forecasts and the last three hours of PV generation.
The day-ahead and the first intraday predictions are delivered for the 96 quarters of the next day from 00:00 to 23:45 indexed by time steps 0  k  95. The prediction horizons span from 12 to 36 hours, for the day-ahead gate 12:00, and 0 to 24 hours, for the intraday gate 00:00. The prediction horizon is cropped to 11  k  80 because the PV generation is always 0 for time steps 0  k  10 and 81  k  95 on the ULie`ge case study. The next three intraday predictions are performed for the 72, 48, and 24 next quarters of the day corresponding to the gates 06:00, 12:00, and 18:00. Therefore, the prediction horizons span from 0 to 18 hours, 0 to 12 hours, and 0 to 6 hours. The intraday forecasting time periods are 24  k  80, 48  k  80, and 72  k  80. Table I compares the mean and the standard deviation of the computation times, over the ten learning sets, to train the point and quantile forecast models2.

day-ahead point quantile intraday point quantile

MLP 5.3 (0.1) 7.6 (0.2)
MLP 5.0 (0.1) 17.9 (0.2)

LSTM 23.7 (0.3) 69.0 (0.6)
ED-1 5.2 (0.1) 6.4 (0.2)

GBR 3.4 (0.1) 44.6 (0.4)
ED-2 17.2 (0.2) 18.0 (0.3)

TABLE I: Training computation time (s).

C. Day-ahead results
Figure 1a compares the NMAE (plain lines), NRMSE (dashed lines), and Figure 1b the CRPS per forecasting time periods k of the day-ahead models of gate 12:00. Table II provides the mean and standard deviation of the NMAE, NRMSE, and CRPS. The LSTM achieved the best results for both point and quantile forecasts. Figures 3a, 3c, and 3e compare the MLP, LSTM, and GBR day-ahead quantile and point forecasts (black line named dad 12) of gate 12:00 on August 2, 2020 with the observation in red. One can see that the predicted intervals of the LSTM model better encompass
2The day-ahead and intraday LSTM training times are identicals for both point and quantile forecasts as they only take the weather forecasts as inputs.

the actual realizations of uncertainties than the MLP and GBR.

25 20 %15

MLP LSTM

14 12

GBR 10

%8

MLP LSTM GBR

10

6

5

4 2

0 10 20 30 40 50 60 70 80 0 10 20 30 40 50 60 70 80

(a) NMAE and NRMSE.

(b) CRPS.

Fig. 1: Day-ahead models results with the NMAE (plain lines), NRMSE (dashed lines), and CRPS.

Score

Gate

MLP LSTM

GBR

NMAE 12 24

8.2 (1.2) 7.6 (1.5) 7.9 (1.2) 7.7 (1.6)

9.2 (0.9) 9.0 (0.8)

12 NRMSE
24

10.2 (1.4) 9.2 (1.6) 11.2 (0.9) 9.7 (1.2) 9.4 (1.8) 10.9 (0.8)

12 CRPS
24

6.2 (1.1) 4.4 (0.2) 6.2 (1.0) 4.4 (0.2)

6.4 (0.7) 6.3 (0.6)

TABLE II: Day-ahead models results.

D. Intraday results
Table III provides the averaged NMAE, NRMSE, and CRPS per gate of intraday models. The LSTM achieved the best NMAE and NRMSE for the 06:00 gate and the ED-1 achieved the best NMAE and NRMSE for the 12:00 gate and the best CRPS for both gates. Figure 2 compares the CRPS per forecasting time periods k of the intraday models. The ED1 benefits from the last PV generation observations. Indeed, some CRPS values for both 06:00 and 12:00 gates are below the ones of 00:00 gate. Table IV provides the Interval score of intraday models for 80 %, 60 %, 40 %, and 20 % width of central intervals. The ED-1 model achieved the best results for both 06:00 and 12:00 gates and all prediction intervals except for the 06:00 gate and the prediction interval width of 80 % where it is ED-2. The LSTM achieved close results to the ED1. Figures 3b, 3d, and 3f compare the ED-1, LSTM, and ED-2 intraday quantile and point forecasts (black line named intra 6) of 06:00 gate on August 2, 2020 with the observation in red. Generally, one can see that the predicted intervals of ED1 and LSTM models better encompass the actual realizations of uncertainties than ED-2.
VI. CONCLUSION
An encoder-decoder architecture is implemented on the intraday scale to produce accurate forecasts. It efficiently captures the contextual information composed of past PV observations and future weather forecasts, while capturing the temporal dependency between forecasting time periods

Score

Gate

MLP

ED-1

ED-2 LSTM

6 NMAE
12

8.9 (1.0) 8.5 (1.4) 9.4 (1.0 ) 7.6 (1.5) 6.7 (1.4) 6.4 (1.3 ) 7.1 (1.1) 7.2 (1.1)

NRMSE 6 12

10.9 (0.9) 10.3 (1.3) 11.3 (1.1) 7.7 (1.6) 8.7 (1.3) 7.8 (1.2) 8.5 (1.2) 9.4 (1.8)

6 CRPS
12

8.1 (0.7) 5.8 (1.2)

5.9 (0.9) 4.5 (0.7)

6.6 (0.7) 6.2 (0.7) 5.6 (1.8) 4.7 (0.5)

TABLE III: Intraday models NMAE, NRMSE, CRPS results.

20.0 17.5 15.0 %1102..05

00:00 06:00 12:00 18:00

20.0 17.5 15.0 %1102..05

00:00 06:00 12:00 18:00

7.5

7.5

5.0

5.0

2.5

2.5

0.0 10 20 30 40 50 60 70 80 0.0 10 20 30 40 50 60 70 80

(a) ED-1.

(b) ED-2.

20.0 17.5 15.0 %1102..05

20.0 17.5 15.0 %1102..05

00:00 06:00 12:00 18:00

7.5 5.0 2.5

00:00 06:00 12:00 18:00

7.5 5.0 2.5

0.0 10 20 30 40 50 60 70 80 0.0 10 20 30 40 50 60 70 80

(c) MLP.

(d) LSTM.

Fig. 2: Intraday models CRPS results.

Width Gate

MLP

ED-1

ED-2

LSTM

6 80 %
12

24.4 (2.9) 17.4 (3.5)

14.9 (4.0) 10.6 (1.8)

13.9 (4.9) 11.6 (10.1)

19.3 (4.2) 9.6 (2.0)

6 60 %
12

37.6 (3.2) 27.2 (4.3)

29.9 (5.0) 22.4 (4.2)

32.2 (4.2) 27.5 (10.8)

30.7 (4.6) 22.6 (3.1)

6 40 %
12

58.0 (4.5) 42.4 (6.9)

50.1 (6.5) 37.7 (5.9)

57.2 (6.0) 48.1 (16.8)

51.6 (5.8) 39.2 (4.9)

20 % 6 12

111.8 (8.4) 97.1 (11.7) 112.1 (10.3) 99.5 (10.4) 81.5 (13.8) 72.7 (10.0) 94.8 (32.1) 76.5 (8.0)

TABLE IV: Intraday models IS results.

over the entire forecasting horizon. The models are compared by using a k-fold cross-validation methodology and quality metrics on a real case study composed of the PV generation of the parking rooftops of the Lie`ge University. The best dayahead model for both point and quantile forecasts is a neural network composed of a LSTM cell and an additional feedforward layer. Then, the encoder-architecture composed of a LSTM-MLP yields accurate and calibrated forecast distributions learned from the historical dataset in comparison with the MLP and LSTM-LSTM models for the intraday point and quantile forecasts. However, the LSTM produced similar

500 400

qqPm1Q ==

0.1 0.9

500 400

kW300

dad 12 kW300

200

200

100

100

0 10 20 30 40 50 60 70 80 0

(a) MLP day-ahead.

500 400

qq1Q

= =

0.1 0.9

500 400

Pm

kW300

dad 12 kW300

200

200

100

100

0 10 20 30 40 50 60 70 80 0

(c) LSTM day-ahead.

500 400

qq1Q

= =

0.1 0.9

500 400

Pm

kW300

dad 12

kW300

200

200

100

100

0 10 20 30 40 50 60 70 80 0

(e) GBR day-ahead.

qqPm1Q ==

0.1 0.9

intra 6

30 40 50 60 70 80

(b) ED-1 intraday.

qq1Q

= =

0.1 0.9

Pm

intra 6

30 40 50 60 70 80

(d) LSTM intraday.

qq1Q

= =

0.1 0.9

Pm

intra 6

30 40 50 60 70 80 (f) ED-2 intraday.

[7] P. Lauret, M. David, and H. T. Pedro, "Probabilistic solar forecasting using quantile regression models," energies, vol. 10, no. 10, p. 1591, 2017.
[8] P. Lauret, M. David, and P. Pinson, "Verification of solar irradiance probabilistic forecasts," Solar Energy, vol. 194, pp. 254­271, 2019.
[9] P. Pinson, H. A. Nielsen, J. K. Møller, H. Madsen, and G. N. Kariniotakis, "Non-parametric probabilistic forecasts of wind power: required properties and evaluation," Wind Energy: An International Journal for Progress and Applications in Wind Power Conversion Technology, vol. 10, no. 6, pp. 497­516, 2007.
[10] J. Dumas, B. Corne´lusse, A. Giannitrapani, S. Paoletti, and A. Vicino, "Stochastic and deterministic formulations for capacity firming nominations," in 2020 International Conference on Probabilistic Methods Applied to Power Systems (PMAPS), pp. 1­7, IEEE, 2020.
[11] X. Fettweis, J. Box, C. Agosta, C. Amory, C. Kittel, C. Lang, D. van As, H. Machguth, and H. Galle´e, "Reconstructions of the 1900­2015 greenland ice sheet surface mass balance using the regional climate MAR model," Cryosphere (The), vol. 11, pp. 1015­1033, 2017.
[12] T. Hastie, R. Tibshirani, and J. Friedman, The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media, 2009.
[13] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al., "Scikit-learn: Machine learning in python," the Journal of machine Learning research, vol. 12, pp. 2825­2830, 2011.
[14] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, "Automatic differentiation in pytorch," in NIPS-W, 2017.
[15] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al., "Tensorflow: Large-scale machine learning on heterogeneous systems," 2015.
[16] T. Gneiting and A. E. Raftery, "Strictly proper scoring rules, prediction, and estimation," Journal of the American statistical Association, vol. 102, no. 477, pp. 359­378, 2007.
[17] M. Zamo and P. Naveau, "Estimation of the continuous ranked probability score with limited information and applications to ensemble weather forecasts," Mathematical Geosciences, vol. 50, no. 2, pp. 209­234, 2018.

Fig. 3: Quantiles vs point forecasts of day-ahead models of gate 12:00 (left), and intraday models of gate 06:00 (right) on August 2, 2020, the observations are in red.

results. Several extensions are under investigation. First, considering a larger dataset of at least one full year to take into account the entire PV seasonality. Second, developing a PV scenario approach based on the encoder-decoder architecture.
REFERENCES
[1] J. M. Morales, A. J. Conejo, H. Madsen, P. Pinson, and M. Zugno, Integrating renewables in electricity markets: operational problems, vol. 205. Springer Science & Business Media, 2013.
[2] R. Koenker and G. Bassett Jr, "Regression quantiles," Econometrica: journal of the Econometric Society, pp. 33­50, 1978.
[3] T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and R. J. Hyndman, "Probabilistic energy forecasting: Global energy forecasting competition 2014 and beyond," 2016.
[4] F. Golestaneh, P. Pinson, and H. B. Gooi, "Very short-term nonparametric probabilistic forecasting of renewable energy generation--with application to solar energy," IEEE Transactions on Power Systems, vol. 31, no. 5, pp. 3850­3863, 2016.
[5] J.-F. Toubeau, J. Bottieau, F. Valle´e, and Z. De Gre`ve, "Deep learningbased multivariate probabilistic forecasting for short-term scheduling in power markets," IEEE Transactions on Power Systems, vol. 34, no. 2, pp. 1203­1215, 2018.
[6] J. Bottieau, L. Hubert, Z. De Gre`ve, F. Valle´e, and J.-F. Toubeau, "Very-short-term probabilistic forecasting for a risk-aware participation in the single price imbalance settlement," IEEE Transactions on Power Systems, vol. 35, no. 2, pp. 1218­1230, 2019.

