semopy 2: A Structural Equation Modeling Package with Random Effects in Python

Georgy Meshcheryakov 

iam@georgy.top

Anna A. Igolkina 

Maria G. Samsonova

igolkinaanna11@gmail.com m.samsonova@spbstu.ru

June 1, 2021

arXiv:2106.01140v3 [stat.AP] 9 Jun 2021

Abstract
Structural Equation Modeling (SEM) is an umbrella term that includes numerous multivariate statistical techniques that are employed throughout a plethora of research areas, ranging from social to natural sciences. Until recently, SEM software was either commercial or restricted to niche languages, and the lack of SEM packages compatible with more mainstream programming languages was dire. To combat that, we introduced a Python package semopy v1 that surpassed other state-of-the-art software in terms of performance and estimation accuracy. Yet, it was lacking in functionality and its usage was burdened with unnecessary boilerplate code. Here, we introduce a complete overhaul of semopy that improves upon the previous results and comes with lots of new capabilities. Furthermore, we propose a novel SEM model that combines in itself a notion of random effects from linear mixed models (LMMs) to model numerous phenomena, such as spatial data, time series or population stratification in genetics.
1 Introduction
Over the past years, structural equation modelling software has grown in number, but neither of the numerous SEM tools has been developed natively in any of the mainstream languages, with R being the most common choice. Notable examples are R packages lavaan (Rosseel, 2012), openMX (Neale, Hunter, Pritikin, Zahery, Brick, Kirkpatrick, Estabrook, Bates, Maes, and Boker, 2016), sem (Fox, 2006), with the former being the most popular one. According to PYPL (Carbonelle, 2021) and TIOBE (TIOBE, 2021) indices, Python is easily among the most popular languages, whereas R popularity is stagnating at best. We believe that the lack of SEM packages in Python prevents or makes it harder for some researchers to contribute to the area of SEM.
Previously, we created the first Python package for SEM semopy that outperformed the most popular package lavaan in terms of performance and estimates accuracy (Igolkina and Meshcheryakov, 2020). However, at the time, we didn't anticipate significant interest in semopy, and the first versions of the package were more of an ad-hoc solution for our own specific research interests in bioinformatics, namely, in applying SEM to genome-wide association studies (Igolkina, Meshcheryakov, Gretsova, Nuzhdin, and Samsonova, 2020) than a package that is designated to a wider audience. We were proved wrong by vast unexpected feedback, and the presence of a public desire for a better and easier to use Python package became clear. In semopy 2, we aim to bring together the most requested functionalities that were lacking previously, while maintaining an easy-to-use-and-extend codebase.
Also, we introduce a notion of random effects to SEM that comes from the area of linear mixed models (LMMs). Random effects are a powerful technique that helps to model different types of dependencies in data. For example, those dependencies can be caused by observations being separated into groups (e.g. pupils polled from different schools), time series, spatial data and genetic kinship. Although some of the cases listed are studied and implemented as separate SEM techniques, such as latent curve analysis (Meredith and Tisak, 1990) or spatial SEM (Liu, Wall, and Hodges, 2005), no one has provided a generalized framework or an easy-to-use solution. Here, we propose a novel generalized SEM model that can be used to take into account any of the above phenomena.

2 BRIEFLY ON SEM HISTORY

x4 x6 x1 x2 x3
x5 x7

x ­ observed variable;

­ loading;

Figure 1: PA model example; xk are observed variables whereas arrows are regressions. To fit PA model, or SEM model, to data, is to find regression coefficients on those arrows, and, possibly, variances of xk.

This article aims to serve as a tutorial on package semopy with a strong emphasis on algebra behind its methods. The latest version semopy is available at PyPi repository (https://pypi.org/projects/ semopy). A comprehensive guide on its usage can be found at semopy website (https://semopy.com), albeit without rigorous mathematical details. The rest part of the article is structured as follows: first, to better highlight the power of SEM technique and some strengths of the semopy package, we provide a short overview of the SEM history; second, we explain in detail four mathematical models behind the package and their use cases; third, we describe secondary features of semopy; fourth, we provide results of numerical experiments that showcase that the 2.0+ version of the package improves upon results of the previous 1.3.1 version in Section 14. Finally, a reader can find some tedious derivations of formulae in appendices, alongside tabular data of numerical experiments.

2 Briefly on SEM history
SEM is not a new field and its roots date back to works of geneticist Sewall Green Wright and his technique of path analysis (PA) (Wright, 1921). PA is best explained by Equation 1:

x = Bx + ,

(1)

where x is a nx × 1 vector of observed variables, is a random vector of errors and B is a parameterized

loadings matrix. Notice that Equation 1 can be reformulated as a special case of a linear regression model

by inferring x:

x = (Inx - B)-1 = C =

C

If we further assume that  N (0, ), where  is a covariance matrix of shape nx × nx, then x  N (Cµ, CCT ). For brevity, we also assume that data is centred. Usually,  and B/C are unknown and

estimated via maximum likelihood scheme, although at the times of Wright such a computational technique

was unfeasible and was not considered in his original works (Wright, 1934). Also, often some kind of structure

is imposed on . In the context of PA, it is natural to assume it to be diagonal. An example of the PA

model can be seen in Figure 2.

Some phenomena can not be observed, or, at least, were not at the time of data collection, yet the

researcher seeks to incorporate its effects into a model via the introduction of a latent variable (or, in other

terms, a factor). The first task is to confirm if there is an underlying latent factor in the first place, as

designed by a researcher. The most common approach is to fit a linear model with latent factors in it defined

explicitly as in Equation 2

y =  + ,

(2)

where y is an ny ×1 vector of indicators/observed variables (also known as manifest variables),  is a random vector of errors,  is a n×1 vector of latent factors and  is a parameterized loading of factors onto indicators matrix. Per usual, it is assumed that   N (0, ) and   N (0, ), where  is usually restricted to diagonal matrix, however extra non-diagonal covariances can be introduced at researcher's disposal. This approach, named confirmatory factor analysis (CFA), is due to J¨oreskog (1967). An example of the CFA model can be seen in Figure 2. Please notice, that despite the fact indicator and observed variables are effectively

2

3 SEMOPY MODELS

1

2

y1 y2 y3 y4 y5 y6 y7

 ­ latent factor; ­ loading;

y ­ indicator; ­ covariance;

Figure 2: CFA model example; yk are indicators/observed variables, k are latent factors, black arrows are regressions and bidirectional arrows are parameterized covariances.

synonymous, at the moment we are using different notation for them both in formulae and on figures. The

reason for this is historic and will be discussed in Section 3.1.

The model from Equation 2 is not identifiable, though. This comes from an observation that we can

imagine latent variables of an arbitrary magnitude that will still fit the data exactly the same. Indeed, we

can write down

1

1

y =  +  =   +  =  () +  =  + ,





where  is an arbitrary scalar. This issue is dealt with by fixing some of the loading or variance parameters to a certain value; the most common approach (and the one followed by semopy) is to fix a first loading for each of the latent variables to 1.0 (for example, at Figure 2 two loadings are fixed: the loading between y1 and 1 and between y4 and 2).
CFA models, however, do not allow for casual interactions between observed variables or latent factors. SEM, on the other hand, can model arbitrary interactions between variables. Although there are no strict definition of SEM and it is rather a broad term, here we shall assume that by "SEM model" we mean a hybrid of PA and CFA, with LISREL (Joreskog and van Thiilo, 1972) being the notable example and one of pioneers of SEM. Following the idea of SEM = PA + CFA, we get the Equation 3:



  

=  = B + ,

 N (0, )

 

x

,

(3)

y  

= z =  + ,

  N (0, )

 

x

where , x, y are the same vectors from Equation 1 and Equation 2, w is a n × 1 (where n = n + nx) vector of latent variables  and observed variables x, z is a nz × 1 (where nz = ny + nx) vector of all observed variables (including indicators y), and  are random vectors of shapes n × 1 and nz × 1 respectively. The vector x is separated into subvectors x(1), x(2) where x(1) is a vector of endogenous xi and x(2) is a vector of exogenous xi. Covariances between x(2) in  are fixed to their sample values. This model is also used by lavaan, for instance.
One can infer covariance matrix  of shape nz × nz from Equation 3:

 = E[zzT ] = CCT T + ,

(4)

and then fit to a sample covariance matrix S using either maximum likelihood (ML) or weighted least squares (WLS) (Hoyle, 2015).

3 semopy models
semopy cornerstones are model classes. At the moment, there are 4 models present: ` Model ', ` ModelMeans ', ` ModelEffects ' and

3

3 SEMOPY MODELS

3.1 Model

y1

y2 1

y3 2 x3

4

x4 x5

x1

y4 y6

x6

3

x2

y5

 ­ latent factor; x ­ observed variable; y ­ indicator;

­ loading;

­ covariance;

Figure 3: SEM model example; yk are indicators/observed variables, k are latent factors, xk are observed variables (but not indicators) unidirectional arrows are regressions and bidirectional arrows are parameterized covariances.

` ModelGeneralizedEffects '. Each of them treats data and estimation differently, has it is own distinct set of hyperparameters, but they all share the core interface, specifically, they all posses key methods fit (fits model to data), inspect (produces a dataframe with parameters estimates) , predict (predicts observed variables values from given data), predict_factors (estimates factor scores/latent variables values). Next, we describe each of the 4 models in detail.

3.1 Conventional SEM: ` Model '

This model is closest to the classical SEM and is almost the same for an end user as one present in Equation 3. However, there are some technical differences that result in a better performance and a possibility to define a broader class of models. The proposed model is:



  

=  = B + ,

 N (0, )

 

x

,

(5)

y



  

x(1)

= z =  + ,

  N (0, )

the biggest change here is that y is not a vector of indicator variables, it is a vector of output observed variables ­ those that depend on other variables but do not cast any regression arrows to other variables. In fact, there is no concept of "indicators"/"manifest" variables in semopy unlike in other SEM software. We think that the sole reason such a separation of observed variables exists is due to historical leftovers from CFA. Although not crucial, having a class of indicator variables over plain output variables has three drawbacks:

1. Creates confusion among new SEM researchers, as there is no apparent reason to move out variables that are loaded onto only by latent variables to a class of indicators;
2. Once a variable is specified as an indicator, it is impossible to regress onto it by any observable variable;
3. The missing opportunity to rule out output variables from x vector result in a larger size of C = (I - B)-1 matrix, and its inversion is numerically expensive.

4

3 SEMOPY MODELS

3.1 Model

y1

y2 1

y3 2 x3

4

x4 y7

x1

y4 y6

x6

3

x2

y5

 ­ latent factor; x ­ observed variable;

­ loading;

­ covariance;

Figure 4: ` Model ' representation of the same SEM model presented previously. Notice, that there is no disambiguation between yk and xk now.
The other change in Equation 5 is reduced size of z as it now consists of x(1) instead of x. This change is natural as covariances of exogenous variables are usually not parameterized and hence, a part of  that corresponds to exogenous variables is static and therefore not of interest. This results in further performance gains as in some objective functions  has to be inverted.
It may appear that with ` Model ' one can't define a covariance parameter between an exogenous variable and some other variable. Although the authors of semopy can't think of a reason to do that, it is still possible, but such exogenous variable is moved out of the class of "exogenous" variables x(2) to "endogenous" variables x(1). It should not affect an end-user experience other than a slightly longer estimation time.
To help reader understand how matrices are parameterized, compare Figure 3.1 to Figure 3.1 for clarification.
In some SEM implementations covariances between "output" observed variables that are not indicators and "output" latent factors are parameterized too. To achieve this behavior in semopy, one can pass argument mimic_lavaan=True to the constructor of ` Model '.

3.1.1 Estimation methods

` Model ' provides several loss functions to minimize for parameters estimation. Here and in other parts of the article, we shall often assume that parameterized matrices are parameterized by a certain vector . Rigorously, we should always write B(), (), () etc, but we often omit explicit declaration of matrices as functions of parameter vector  for brevity.
Maximum likelihood methods assume a normal distribution of variables:

· Wishart maximum likelihood

F (|S) = tr{S-1()} + ln |()|

(6)

It is a maximum likelihood method for Wishart distribution.

5

3 SEMOPY MODELS

3.1 Model

B
1 2 3 4 x2 x3 x4 x1 x6 1 2 3 4 x2 x3 x4 x1 x6


1 2 3 4 x2 x3 x4 x1 x6 1 2 3 4 x2 x3 x4 x1 x6



1 2 3 4 x2 x3 x4 x1 x6

y1 1

y2

y3 1

y4

11

y5

y6

y7

x2

1

x3

1

x4

1


y1 y2 y3 y4 y5 y6 y7 x2 x3 x4 y1 y2 y3 y4 y5 y6 y7 x2 x3 x4

­ zero entry; 1 ­ fixed 1.0 entry;

­ parameter;

­ sample covariance;

Figure 5: Matrices parameterized as implemented in ` Model ' class. Parameterized entries in symmetric matrices (, ) have identical parameters in their lower and upper triangular parts.

Wishart ML equivalent to multivariate normal ML, see that:

n

n

l(|Z) = (z(i) - µ)T -1(z(i) - µ) + ln|| = tr{ (z(i) - µ)T -1(z(i) - µ)}+

i=1

i=1

n

,

+nln|| = tr{(z(i) - µ)T -1(z(i) - µ)} + n ln || =

i=1

= tr

n
(z(i) - µ)(z(i) - µ)T
i=1

-1 + n ln || = tr M M T -1 + n ln || =, = ntr S-1 + n ln ||  tr S-1 + ln ||

where

M

=

[z(1), z(2), . . . , z(n)] - µ1

­

centered

data

matrix,

S

=

1 n

M

M

T

is

a

biased

sample

covariance

matrix.

If

we

were

to

use

unbiased

S

=

1 n-1

M

M

T

,

then

F (|Z) = (n - 1)tr{S-1()} - nln||

(7)

To estimate model parameters with Wishart ML, no extra actions are necessary as it is the default 6

3 SEMOPY MODELS

3.1 Model

method in ` Model ', but if that changes, you can supply 'MLW' to argument method of Model.fit : model.fit(..., method='MLW') .

· Full information maximum likelihood (FIML)
All of the approaches above handle missing data naturally at the stage of computing S. However, one might consider FIML to be a more viable approach.
FIML at its core is very similar to multivariate normal ML, but for each term in loglikelihood sum, we cull columns and rows in  that correspond to missing variables. In case when all of the data is present, FIML degrades to multivariate normal as in Equation 7.
To estimate model parameters with FIML, supply 'FIML' to argument method of Model.fit : model.fit(..., method='FIML') .

The least-squares method might be more robust when the normality assumption is violated:

· Unweighted least squares (ULS)

F (|S) = tr{(() - S)(() - S)T } = tr{(() - S)2},

(8)

where S is a sample covariance matrix.
To estimate model parameters with ULS, supply 'ULS' to argument method of Model.fit : model.fit(..., method='ULS') .

· Generalized least squares (GLS)

F (|S) = tr{(Inz - ()S-1)2}

(9)

It is the same as minimizing Mahalanobis distance.
To estimate model parameters with GLS, supply 'GLS' to argument method of Model.fit : model.fit(..., method='GLS') .

· Weighted least squares (WLS)

F (|Z) = (vech(() - vech(S))T W -1(vech(() - vech(S)),

(10)

where

vech

is

a

half-vectorization

operator

(i.e.

vech(S)

transforms

nz

× nz

matrix

to

a

nz (nz +1) 2

×1

vector of elements from an upper triangular part of S), W is a weight matrix that is usually chosen

to be an asymptotic covariance matrix for . A user can pass any custom W matrix to Model.fit

as an wls_w argument, but by default it will use the fourth-moments matrix as proposed by Browne

(1984). In the latter case, it is also known as asymptotic distribution-free (ADL) estimator.

Let zk be a n × 1 vector of observations for k-th component of the nz × 1 vector z. The algorithm for computing is W is following:

1.

For

each

{zk}nk=z 1

compute

sample

mean

z^k

=

1 N

n i=1

zk(i);

2. Center each {zk}nk=z 1: z~k = zk - z^k;

3. Compute matrix X of Cartesian Hadamard products for {z~k}nk=z 1:

X = [z~1 z~1, z~1 z~2, . . . , z~1 z~nz , z~2 z~1, . . . , z~nz z~nz ]

4. Compute W as a covariance matrix of X.

WLS is often used when assumptions on data normality are violated.
If W can't be successfully inverted, then a warning is printed and a nearest positive-definite matrix of W is used instead.
To estimate model parameters with WLS, supply 'WLS' to argument method of Model.fit : model.fit(..., method='WLS') .

7

3 SEMOPY MODELS

3.1 Model

Module name univariate regression univariate regression many multivariate regression
example article political democracy
holzinger39

Description Univariate regression wtih 1 independent varaible. Univariate regression with 3 independent variables. Multivariate regression with 5 independent and 3 dependent variables. Toy model from Figure 3.1. Political Democracy dataset that is frequently used as a testing dataset in SEM software (Bollen, 1980). The classic Holzinger and Swineford dataset consists of mental ability test scores of seventh- and eighth-grade children from two different schools that is frequently used for showcasing in SEM software (J¨oreskog, 1969).

Table 1: Toy SEM models + dataset that are used in semopy for showcasing and testing. All modules reside in submodule semopy.examples , each of them has functions get_model() , get_data() , get_params() that return string description of the model in semopy syntax (see Section 4), dataset and true parameter values respectively. The latter is obviously not returned for political_democracy and holzinger39 as they are real datasets and hence no true parameter estimates are available. A reader can check semopy results for those models in Appendix A.

· Diagonally weighted least squares (DWLS)
It is the same as WLS, but all non-diagonal elements of W are zeroed. It might be helpful if W is ill-conditioned or too big and inverting it is not an option. DWLS is also sometimes referred to as robust weighted least squares (DiStefano and Morgan, 2014).
To estimate model parameters with DWLS, supply 'DWLS' to argument method of Model.fit : model.fit(..., method='DWLS') .
If some of the variables are ordinal, then in all of the methods above it is possible to substitute sample covariance matrix S with a heterogeneous correlation matrix S computed from polychoric and polyserial correlations (Drasgow, 2004): it can be done through model syntax as explained in Section 4. Sometimes, however, S is degenerate: in that case, semopy finds a nearest positive-definite matrix and uses it instead of S. The latter also happens with the sample covariance matrix when ill data is passed. In either case, semopy will output a warning that informs the user of possible problems with data.

3.1.2 Usage example

semopy has some built-in SEM models to help its users dive into it: see list at Table 1. Throughout most of this article (namely, in Section 3.1, Section 3.2, Section 3.3 and Section 3.4), we shall use only example_article as it is the model showcased at Figure 3.1 (or Figure 3.2). The reason why we chose this model for showcasing is because it has all variety of interactions supported by semopy. Also, it is not identifiable with respect to some of its parameters ­ usually, semopy provides hints to users that model is incorrect and some of parameter estimates should not be trusted (see below).
First, we get a built-in dataset and toy model:

In [ ]:

import semopy ex = semopy.examples.example_article desc , data = ex.get_model(), ex.get_data() print(desc)

Out [ ]:

# Measurement part eta1 = y1 + y2 + y3 eta2 = y3 + y2 eta3 = y4 + y5 eta4 = y4 + y6 # Structural part

8

3 SEMOPY MODELS

3.1 Model

eta3  x2 + x1 eta4  x3 x3  eta1 + eta2 + x1 x4  eta4 + x6 y7  x4 + x6 # Additional covariances y6  y5 x2  eta2

In [ ]: print ( data . head ())

Out [ ]:

y1

y2

y3 ...

0 0.729838 -0.781150 -0.473951

1 -1.895332 0.313026 -1.861669

2 0.771990 -2.019936 -0.452560

3 -0.956471 -0.374326 0.040394

4 0.959640 -0.997909 -0.299834

x4

x1

x6

... 1.984575 -1.187765 -0.025494

... 2.139032 -0.397323 -0.217159

... 1.908656 0.534365 0.058370

... -0.089787 0.091094 -0.603859

... 2.284195 -0.851540 -0.343206

[5 rows x 12 columns]

Then, we instantiate a ` Model ' and fit it to data. We also print an optimization result:
In [ ]: m = semopy . Model ( desc )
r = m.fit(data) print(r)

Out [ ]:

Name of objective: MLW Optimization method: SLSQP Optimization successful. Optimization terminated successfully Objective value: 0.091 Number of iterations: 58 Params: -0.488 -0.782 -0.183 1.225 1.444 -1.147 -1.344 1.223 1.071 -0.348 1.291 1.454 0.840 -0.388 -0.625 -0.106 1.252 -0.084 1.097 0.870 0.696 0.844 0.654 1.114 0.871 0.824 1.010 0.804 1.182 -0.499 1.264

Finally, we can print a fancy table with parameter estimates and their p-values:
In [ ]: ins = m. inspect ()
print(ins)

Out [ ]: WARNING : root : Fisher Information Matrix is not PD . Moore - Penrose inverse
will be used instead of Cholesky decomposition. See 10.1109/ TSP .2012.2208105.

lval op

0 eta3

1 eta3

2 eta4

3

x3

4

x3

5

x3

6

x4

7

x4

8

y1

9

y2

10

y2

11

y3

12

y3

13

y4

14

y4

15

y5

16

y6

rval Estimate Std. Err z-value p-value

 x2 -1.146663 0.065317 -17.55527

0.0

 x1 -1.344422 0.076917 -17.478884

0.0

 x3 1.222542 0.038071 32.112318

0.0

 eta1 1.070822 0.287943 3.718868

0.0002

 eta2 -0.347555 0.146593 -2.370895 0.017745

 x1 1.291230 0.075725 17.051592

0.0

 eta4 1.454421 0.041067 35.41557

0.0

 x6 0.839923 0.06817 12.320923

0.0

 eta1 1.000000

-

-

-

 eta1 -0.488414 0.664931 -0.734533 0.462624

 eta2 -0.781996 0.912859 -0.856646 0.391641

 eta1 -0.182725 0.140074 -1.304484 0.192069

 eta2 1.000000

-

-

-

 eta3 1.000000

-

-

-

 eta4 1.000000

-

-

-

 eta3 1.224550 0.048392 25.304791

0.0

 eta4 1.443567 0.040942 35.258544

0.0

9

3 SEMOPY MODELS

3.2 ModelMeans

17

y7 

x4 -0.387558 0.01444 -26.8399

0.0

18

y7 

x6 -0.624882

0.058 -10.773807

0.0

19

x2  eta2 -0.084431 0.087237 -0.967832 0.333128

20 eta3  eta3 0.869520 0.110941 7.837675

0.0

21

x3 

x3 1.114065 0.566346 1.967111 0.04917

22

x4 

x4 1.009523 0.136551 7.393021

0.0

23 eta4  eta4 0.803514 0.090644 8.864495

0.0

24 eta2  eta2 1.181504 0.855015 1.381853 0.167017

25 eta2  eta1 -0.498966 0.239579 -2.082676 0.037281

26 eta1  eta1 1.263544 0.456489 2.767959 0.005641

27

y6 

y5 -0.105931 0.101857 -1.039999 0.29834

28

y6 

y6 1.251659 0.151825 8.244097

0.0

29

y7 

y7 1.096623 0.089539 12.247449

0.0

30

y1 

y1 0.695780 0.435022 1.599413 0.109729

31

y3 

y3 0.844282 0.961208 0.878355 0.379751

32

y4 

y4 0.654485 0.11071 5.911725

0.0

33

y2 

y2 0.871375 0.751912 1.158879 0.246505

34

y5 

y5 0.823609 0.143472 5.740541

0.0

Notice the warning ­ semopy has managed to spot identification issues. Still, most parameters are correctly estimated. We can check mean absolute percentage error (MAPE) for true parameter values:

In [ ]:

import numpy as np params = ex.get_params() mape = np.mean(semopy.utils.compare_results(m, params)) print ('MAPE: {:.2f}%'.format(mape * 100))

Out [ ]: MAPE : 19.94% For an example of a clean working session, see Appendix A.

3.2 SEM with fixed effects: ` ModelMeans '

As it is evident from the Equation 5, ` Model ' has no support for modeling intercepts nor any kinds of fixed effects. The former is not crucial as it is rarely of any interest, the latter, however, might be a problem. ` Model ' analyzes sample covariance information only, and it will fail if a variation of a variable is too low to be captured effectively by a covariance function. Furthermore, it loses information on a relative scale of a variable: for instance, it will produce the same results for an ordinal variable encoded as {0, 1, 2} and an ordinal variable encoded as {0, 1, 10}. Also, it will have problems with non-normal variables in the first place. In Equation 11 we propose a model that lacks most of those downfalls and can be used with non-normal variables as long as they are exogenous:







  

x(1)

=  = 1x(2) + B + ,

 N (0, )

,

(11)

y



  

x(1)

= z = 2x(2) +  + ,

  N (0, )

where 1 is a n × nx(2) loading matrix of exogenous variables x(1) onto latent factors  and endogenous non-output variables x(1), and 2 is a nz × nx(2) loading matrix of exogenous variables onto output variables
y. 1 and 2 are parameterized. By default, there is a fake 1 exogenous variable that loads onto all
endogenous observed variables to model intercepts, but if user seeks to reduce parameter space, he/she can pass intercepts=False argument to the ModelMeans.fit method. However, in that case, data has to be centered beforehand.
Previously, for Equation 5, E[z] = 0, but for Equation 11:
E[z] = E[2x(2) +  + ] = E[2x(2) + C1x(2) + C ] = (2 + C1) x(2)
As for the covariance matrix  = E[(z - E[Z])(z - E[Z])T ], it attains the same expression as previously in Equation 4.

10

3 SEMOPY MODELS

3.2 ModelMeans

y1

y2 1

y3 2 x3

4

x4 y7

x1

y4 y6

x6

3

x2

y5

 ­ latent factor;

x ­ observed variable; x ­ exogenous observed variable;

­ loading;

­ covariance;

Figure 6: ` ModelMeans ' representation of the same SEM model presented previously. Notice that exogenous observed variables are now ruled out into a separate class.

1
x1 x6 1 1 2 3 4 x2 x3 x4

B
1 2 3 4 x2 x3 x4 1 2 3 4 x2 x3 x4


1 2 3 4 x2 x3 x4 1 2 3 4 x2 x3 x4

2
x1 x6 1 y1 y2 y3 y4 y5 y6 y7 x2 x3 x4



1 2 3 4 x2 x3 x4

y1 1

y2

y3 1

y4

11

y5

y6

y7

x2

1

x3

1

x4

1


y1 y2 y3 y4 y5 y6 y7 x2 x3 x4 y1 y2 y3 y4 y5 y6 y7 x2 x3 x4

­ zero entry; 1 ­ fixed 1.0 entry;

­ parameter;

­ sample covariance;

Figure 7: Matrices parameterization as implemented in ` ModelMeans ' class. Parameterized entries in
symmetric matrices (, ) have identical parameters in their lower and upper triangular parts. 1 vector of
ones are used for intercepts.

11

3 SEMOPY MODELS

3.2 ModelMeans

3.2.1 Estimation methods

Although generalization of ULS, GLS and WLS methods to ` ModelMeans ' is straightforward, efficient optimization of those functions is not. At the moment, ` ModelMeans ' provides only methods that assume a normal distribution of endogenous variables and developing an efficient least-squares approach for it is a subject for further research.
Before we proceed to methods, let's denote mean matrix M as

M () = (2() + ()C()1()) X(2),

(12)

where X(2) is a sample matrix of observed exogenous variables. Notice that if Z is a matrix of observations, then Z - M () is a "centered" (in a sense that is corrected for exogenous effects) matrix of observations. We need M to write down objective functions in a more compact, readable way.

· Full information maximum likelihood (FIML)/Maximum likelihood (ML) Similar to FIML for ` Model '. When no missing data is present, FIML reduces to ML:

F (|Z) = tr (Z - M ())T -1()(Z - M ()) + n ln ||

(13)

To estimate model parameters with FIML, supply 'FIML' to argument method of Model.fit : model.fit(..., method='FIML') . It is the default option.

· Restricted maximum likelihood (REML)

REML was proposed by Thompson (1962) for variance component estimation in linear models. Here, we adapt it to our SEM model.

Let's infer z from 11:

z = (2 + C1) x(2) + C + 

For each of n observations holds

z(i) = (2 + C1) x((i2)) + C (i) + (i), i = 1..n

We want to get rid of the fixed part (2 + C1) x((i2)) to lessen the computational burden. That is possible if we find a matrix P such that X(2)P = 0 (that's the same as x(i)P = 0 for all i = 1..n), P = 0 and the rank of P is as big as possible. Good candidate for P is

P0 = (In - X(2)T (X(2)X(2)T )-1X(2))

However, for reasons that will be more evident in Section 3.3, we shall use different P . First, we do

an eigendecomposition on P0:

P0 = QDQT ,

where Q is an orthogonal matrix of eigenvectors and D is a diagonal matrix of eigenvalues. Let number of non-zero eigenvalues be r, (at least nx(2) of eigenvalues are guaranteed to be zero), then a following low-rank decomposition is possible:
 D = D D = D1 D2 ,
n×r r×n

 As D is diagonal, D1 and D2 are just D with last r columns and rows deleted respectively. Moreover, we can notice that Px = X(2)T (X(2)X(2)T )-1X(2) is a projection operator as

Px2 = X(2)T (X(2)X(2)T )-1X(2)X(2)T (X(2)X(2)T )-1X(2) = Px

12

3 SEMOPY MODELS

3.2 ModelMeans

It is known that all eigenvalues of projection operators are either 0 or 1. Hence, eigenvalues of P0 = In - Px = QQT - QDQT = Q(I - D)QT are either 0 or 1 too. Therefore, the only non-zero elements of D1 and D2 are ones. Then, we can chose P1 as P :
X(2)P0 = 0  GQD1D2QT  Q = 0  Q  X(2) QD1 D2 = 0  X(2)P1 = 0
P1

P1 = QD1

(14)

Next, we transform data by P1 = [p1, p2, . . . , pn]T :

z(i)p(i) = z^(i) = C p(i) + (i)p(i), i = 1..r

Notice that cov[z^] = cov[z] = , as





cov[z^(i)] = E (C (i) + (i)) p1pT1 (C (i) + (i))T  = 

1

However, a total number of observations "decreases" to r. It can be interpreted as taking degrees of freedom that are wasted on mean component into an account.
So, at first we estimate variance components for transformed data as given my a REML likelihood:

F (|ZP1) = tr{P1T ZT -1()ZP1} + r ln ||  tr{S-1()} + ln ||,

(15)

where S is a biased sample covariance matrix for transformed data ZP1.

After estimating ^ from Equation 15, we can estimate mean components in M () by fitting a likelihood

of untransformed data:

F (|Z, ^ ) = tr{M ()T ^ -1M ()}

(16)

REML has some advantages over ML:

1. It separates one big optimization problem into 2 smaller independent problems, hence practically decreasing computational burden;
2. ML produces biased estimates of variance components, whereas REML provides unbiased estimates.

The latter, however, does not necessarily mean that estimates will have lower mean squared error (MSE). In our tests (see Section 14), ML outperformed REML in terms of accuracy for exogenous loadings in 1, 2, yet was significantly behind in performance and a bit behind in estimates accuracy for the rest of the parameters.
An interested reader can check literature review on REML given by Harville (1977). An extensive coverage of REML is also present in "Variance components" book (Searle, Casella, and McCulloch, 1992).
To estimate model parameters with REML, supply 'REML' to argument method of ModelMeans.fit : model.fit(..., method='REML') . Note that fit then returns a pair of optimization results ­ one for Equation 15 and the other for Equation 16.

3.2.2 Usage example

In [ ]:

import semopy ex = semopy.examples.example_article desc , data = ex.get_model(), ex.get_data() m = semopy.ModelMeans(desc) m.fit(data) print (m. inspect ())

13

3 SEMOPY MODELS

3.3 ModelEffects

Out [ ]:

lval op

0 eta3

1 eta4

2

x3

3

x3

4

x4

5 eta3

6

x3

7

x4

8

y7

9

y1

10

y2

11

y2

12

y3

13

y3

14

y4

15

y4

16

y5

17

y6

18

y7

19

x3

20

x4

21

x2

22

y7

23

y1

24

y2

25

y3

26

y4

27

y5

28

y6

29

x2

30 eta3

31

x3

32

x4

33 eta4

34 eta2

35 eta2

36 eta1

37

y6

38

y6

39

y7

40

y1

41

y3

42

y4

43

y2

44

y5

rval Estimate

 x2 -1.146671

 x3 1.222544

 eta1 1.831707

 eta2 -0.345268

 eta4 1.454411

 x1 -1.344433

 x1 1.291013

 x6 0.839896

 x6 -0.624860

 eta1 1.000000

 eta1 1.212918

 eta2 -0.753647

 eta1 -2.416658

 eta2 1.000000

 eta3 1.000000

 eta4 1.000000

 eta3 1.224545

 eta4 1.443583

 x4 -0.387550



1 0.457809



1 1.519828



1 -1.007762



1 0.490313



1 -0.994713



1 -0.646721



1 -0.507852



1 -1.045236



1 1.038446



1 1.001274

 eta2 -0.091548

 eta3 0.869757

 x3 1.131809

 x4 1.009375

 eta4 0.803528

 eta2 5.369147

 eta1 2.356964

 eta1 1.277327

 y5 -0.105989

 y6 1.251810

 y7 1.096517

 y1 0.681144

 y3 0.813174

 y4 0.654815

 y2 0.894850

 y5 0.823550

Std. Err

z-value p-value

0.065539 -17.495993

0.0

0.03795 32.214424

0.0

566870.787859 0.000003 0.999997

0.143255 -2.410154 0.015946

0.040911 35.550814

0.0

0.07717 -17.421604

0.0

0.075713 17.051312

0.0

0.068167 12.321151

0.0

0.057998 -10.773906

0.0

-

-

-

1237359.017737 0.000001 0.999999

0.830757 -0.90718 0.364311

1641829.10264 -0.000001 0.999999

-

-

-

-

-

-

-

-

-

0.049702 24.637799

0.0

0.040787 35.393578

0.0

0.014441 -26.837254

0.0

0.101276 4.520432 0.000006

0.097107 15.651084

0.0

0.062283 -16.180429

0.0

0.06906 7.099783

0.0

0.080798 -12.311191

0.0

0.071053 -9.101999

0.0

0.086607 -5.863842

0.0

0.113045 -9.246199

0.0

0.113512 9.148353

0.0

0.10067 9.946154

0.0

0.087398 -1.047491 0.294873

0.111259 7.817419

0.0

0.548416 2.063777 0.039039

0.136543

7.39234

0.0

0.090594 8.869577

0.0

7739465.375667 0.000001 0.999999

2097152.0 0.000001 0.999999

0.451611 2.828376 0.004678

0.101863 -1.040511 0.298103

0.15184 8.244278

0.0

0.08953 12.247449

0.0

0.429614

1.58548 0.112857

0.941338 0.863849 0.387671

0.111022 5.898087

0.0

0.685349 1.305685 0.19166

0.144001 5.719065

0.0

Notice new regression of 1 onto observed variables - that's intercepts.
We also check again that the obtained estimates, including intercepts, are sensible:

In [ ]:

import numpy as np params = ex.get_params() mape = np.mean(semopy.utils.compare_results(m, params)) print ('MAPE: {:.2f}%'.format(mape * 100))

Out [ ]: MAPE : 20.24%

3.3 SEM with random effects: ` ModelEffects '

In ` Model ' and ` ModelMeans ' it is assumed that all observations are obtained independently. This as-

sumption, however, is not reasonable in some cases, and some dependence structure between data samples

is present. One way to model this dependence is to assume an extra random term U (a random effect)with

some covariance matrix K of shape n × n. K matrix is not estimable in a general case, as the number of

parameters

is

n(n+1) 2



n,

therefore

some

kind

of

a

prior

assumption

on

the

K

structure

should

be

provided.

14

3 SEMOPY MODELS

3.3 ModelEffects

The best-case scenario is when we already know K matrix up to some scalar multiplier ­ for instance, this scenario often arises in bioinformatics and genomic studies, where K plays a role of genomic relatedness matrix and is computed from genotypes (VanRaden, 2008). That's the case ` ModelEffects ' is concerned with.



H



  

X (1)

= W = 1X(2) + BW + E,

E  MN (0, , In)

(17)

Y



  

X (1)

= Z = 2X(2) + W +  + U,

  MN (0, , In), U  MN (0, D, K)

Previously, we have employed vectors to describe our models, but introduction of dependence across
observations makes matrix notation much more convenient. In Equation 17, H is a n × n matrix of latent variables, X(1) and X(2) are matrices of endogenous and exogenous observable variables of shapes nx(1) × n and nx(2) × n respectively, D is a covariance matrix of random effect components U that incorporates scale with respect to each of the observable variables and K is a fixed covariance across-observations matrix.
MN is a notation for matrix-variate normal distribution (Gupta and Nagar, 2000). Actually, it is just a
synonym for a form of multivariate normal distribution:

  MN (M, U, V )  vec()  N (vec(M ), V  U ),

(18)

where vec is a vectorization operator and  is a Kronecker product symbol. It follows that

1. E[] = M ;

2. cov[] = E ( - E[])(X - E[X])T = tr{V }U ;

3. cov[T ] = E ( - E[])T (X - E[X]) = tr{U }V ;

4.

U

and V

are not estimable

as V

U

= cV



1 c

U

,

where

c

is

an

arbitrary

scalar.

The latter, however, is not bad news, as it will be shown next in Section 3.3.1. Using the first 3 properties we infer mean and covariances of Z:

M = E[Z] = (C1 + 2) X(2)

(19)

Covariance across rows (i.e. covariance between observable variables):

L = cov[Z] = n + tr{K}D

(20)

Covariance across columns (i.e. covariance between observations):

T = cov[ZT ] = tr{}In + tr{D}K

(21)

There are multiple options to parameterize D matrix in semopy: 1. Full parameterization: pass d_mode='full' argument to the constructor of ` ModelMeans ';

2. Only diagonal parameterization: pass d_mode='diag' argument to the constructor of ` ModelMeans ' (the default);
3. Single scale parameter D = 2Inz : pass d_mode='scale' argument to the constructor of ` ModelMeans '.
4. Custom parameterization: use any of the above parameterizations as a base and then introduce custom parameters using semopy syntax (see Section 4).

15

3 SEMOPY MODELS

3.3 ModelEffects

3.3.1 Estimation methods · Matrix-variate maximum likelihood/ML Negative loglikelihood for variable Z  MN (M (), L(), T ()):

f (|Z) = tr T -1(Z - M ())T L-1(Z - M ()) + n ln |L| + nz ln |T |

(22)

Note that if Z from Equation 17 follows matrix-variate normal distribution MN (M, L, T ), it doesn't hold that L = L or T = T . Hence, we cant' just substitute L (20) and T (21) to Equation 22 and minimize it. As follows from properties 2 and 3 of MN above,

1

L = tr{T }L  L =

L

tr{T }

1

(23)

T = tr{L}T  T =

T

tr{L}

From expression for L we can infer tr{L}:

tr{L} tr{L} =
tr{T }

Then:

1

L=

L

tr{T }

(24)

tr{T }

T=

T

tr{L}

Next, we substitute Equation 24 to Equation 22:



-1

-1



 tr{T }

tr

T

 tr{L}

(Z - M )T

1 L

tr{T }



1

tr{T }

(Z - M ) + n ln 

L tr{T }

+ nz ln

T tr{L}

A1

A2

There is only one thing that we can't compute: tr{T }. Fortunately, it cancels out in both terms A1

and A2:

A1 = tr{L}tr T -1(Z - M )T L-1(Z - M )

In A2, as

1 tr{T

}

L

=

1 tr{T }

nz
|L| and

tr{T } tr{L}

T

=

tr{T } tr{L}

n
|T |:

A2 = n ln

1

nz

tr{T } |L| + nz ln

n
tr{T } |T | =
tr{L}

= nln|L| + nzln|T | - nnz ln tr{L}

Therefore, the negative likelihood to minimize attains the form:

F (|Z) = tr{L()}tr T -1()(Z - M ())T L-1()(Z - M ()) + (25)
+nln|L()| + nzln|T ()| - nnz ln tr{L()}

It may appear that there is an asymmetry in Equation 25 that arises from tr{L()}. Indeed, we could have inferred from Equation 23 tr{T ()} instead and then we would get seemingly different likelihood from of the Equation 25. However, they are actually the same as tr{L} = tr{T }. We've chosen to write down the likelihood function in terms of tr{L} because it is faster to compute.
This is the default method. If that changes, one can supply 'ML' to argument method of ModelEffects.fit : model.fit(..., method='ML') .

16

3 SEMOPY MODELS

3.3 ModelEffects

· Restricted maximum likelihood (REML)

REML for ` ModelEffects ' follows the same logic as for ` ModelMeans '. First, we transform data by

P1 (14):

Z = ZP1 = CE +  + U ,

where E  MN (0, , Ir),   MN (0, , Ir), U  MN (0, D, P1T KP1). Second, we show that

K

LR = cov[Z] = r + tr{K}D,
and TR = cov[ZT ] = tr{}Ir + tr{D}K
Third, we minimize the REML function:

F1(|Z) = tr{LR()}tr TR-1()ZT L-R1()Z +

(26)

+rln|LR()| + nzln|TR()| - rnz ln tr{L()}

Fourth, after obtaining parameter estimates in covariance structures, we use them to obtain estimates for non-transformed L and T ­ L and T respectively. Finally, we solve a kind of least squares problems to estimate parameters in M ():

F2(|Z, L, T ) = tr{T -1(Z - M ()T )L-1(Z - M ())}

Here, we can significantly decrease the cost of evaluating this function by transforming it to a conventional GLS problem as it was in Equation 16. To do it, we using Cholesky decomposition on T -1 = R-1R-T :
F2(|Z, L, T ) = tr{T -1(Z - M ())T L-1(Z - M ())} = = tr{R-1R-T (Z - M ())T L-1(Z - M ())} =
= tr{(ZR-1 - M ()R-1)T L-1(ZR-1 - M ()R-1)}

We replace ZR-1 with Z and M ()R-1 = (C1 + 2)X(2)X(2)R-1 with M () and get the GLS

equation:

F2(|Z, L) - tr{(Z - M ())T L-1(Z - M ())}

(27)

3.3.2 Diagonalization trick

Both ML and REML objective functions require to compute inverse and determinant of T () matrix at each evaluation. The complexity of those operations operation is O(n3), which is troublesome in cases when a number of observations n is large or when high performance is demanded (in GWAS, for instance). We, similarly to FaST-LMM (Lippert, Listgarten, Liu, Kadie, Davidson, and Heckerman, 2011), use the fact that matrices matrices In and K from Equation 21 are simultaneously diagonalizable, i.e.
T = tr{}In + tr{D}K = tr{}In + tr{D}QSQT = Q(tr{}QT Q + tr{D}S)QT = = Q(tr{}In + tr{D}S)QT ,

where S is a diagonal matrix of eigenvalues. If we rotate data Z by QT , covariance across rows remains

unchanged





cov[ZQT ] = E (Z - E[Z]) QQT (Z - E[Z])T  = cov[Z] = L,

In

and covariance across columns transforms to

cov[QZT ] = QT Q(tr{}In + tr{D}S)QT Q = tr{}In + tr{D}S,

which is a diagonal matrix, hence the complexity of inversion and computing determinant is O(n).

17

3 SEMOPY MODELS

3.3 ModelEffects

3.3.3 Groups

In ` ModelEfects ', user doesn't have to provide an n × n K matrix. Instead, one can pass a p × p covariance

between groups matrix V , provided that data is clustered into p distinct groups and labeled accordingly.

Then, K is obtained as

K = ZT V Z,

(28)

where Z is a n × p design matrix that assigns each of the n observations to some of the p groups.

Furthermore, no V can be provided at all. In that case, semopy assumes V = Ip, which should be roughly equivalent to centering data by groups. The latter can be done automatically in ` Model ' or ` ModelMeans ' by passing groups argument to the fit method

3.3.4 Usage example

Like previously, we start by loading data and model from the examples submodule, but this time we pass
random_effects=1 argument to the get_data function to return the same data as before, but "spoiled" with unknown random effects, alongside with covariance matrix for those random effects:

In [ ]:

import semopy ex = semopy.examples.example_article desc = ex.get_model() data , k = ex.get_data(random_effects =1) print(data.shape , '\n', data.head())

Out [ ]:

(300, 13)

y1

y2

y3

y4

...

0 0.570959 -0.014071 0.467279 -0.815086

1 -2.292133 -0.907395 -1.894549 0.939781

2 0.129772 -0.591307 0.112034 -0.157425

3 -1.010035 -0.863170 -1.924978 -0.921666

4 0.489841 -2.039194 0.264125 0.563737

x6

group

... -0.025494

0

... -0.217159

1

... 0.058370

2

... -0.603859

3

... -0.343206

4

[5 rows x 13 columns]

In [ ]: print (k)

Out [ ]:

group group 0 1 2 3 4 ... 295 296 297 298 299

0
0.597172 0.193626 0.054028 0.656677 -0.718798
... -0.015002 -0.142605
0.509318 -0.056519
0.073060

1

2

...

...

0.193626 0.054028 ...

1.558588 -0.246282 ...

-0.246282 0.561246 ...

0.143660 -0.529918 ...

-0.211845 0.048397 ...

... ...

...

0.151484 0.082362 ...

-0.285580 0.190506 ...

0.590645 -0.147389 ...

0.318278 0.015261 ...

-0.012598 -0.206373 ...

297
0.509318 0.590645 -0.147389 0.409124 -0.498914
... -0.028149 -0.552173
0.770882 -0.078281
0.364578

298
-0.056519 0.318278 0.015261 0.001428 0.326037 ... 0.040969
-0.126101 -0.078281
1.369580 -0.319364

299
0.073060 -0.012598 -0.206373 -0.295317
0.183095
-0.046783 -0.523155
0.364578 -0.319364
0.689287

[300 rows x 300 columns]

An extra column 'group' is provided with the data frame. In this case, it is just equal to the data index as a number of "groups" is equal to a number of observations. Each unique group entry must have a corresponding row and column in 'k' data frame.
Then, we fit ` ModelEffects ' instance to data:
In [ ]: m = semopy . ModelEffects ( desc ,)
m.fit(data , group='group', k=k,)

where group argument is a name of a column in dataframe that assigns individuals to groups.

18

3 SEMOPY MODELS

3.4 ModelGeneralizedEffects

Finally, we verify the results on true parameter values:

In [ ]:

import numpy as np params = ex.get_params() mape = np.mean(semopy.utils.compare_results(m, params)) print ('MAPE: {:.2f}%'.format(mape * 100))

Out [ ]: MAPE : 20.69%

Note that fitting ` ModelMeans ' (or Model) to this data will provide worse results:

In [ ]:

m = semopy.ModelMeans(desc) m.fit(data) mape = np.mean(semopy.utils.compare_results(m, params)) print ('MAPE: {:.2f}%'.format(mape * 100))

Out [ ]: MAPE : 57.85%
It will be demonstrated in Section 14 that not taking random effects into an account can lead to arbitrary worse results.

3.4 SEM with multiple generalized random effects: ` ModelGeneralizedEffects '

` ModelEffects ' lets us account for only one random effect, which is sufficient in many cases. However, some studies might benefit from the inclusion of multiple effects. For example, in GWAS, one might include geographical information as a random effect alongside genetic relatedness. Furthermore, many population structure models don't assume a known K matrix, but merely assume a structure of it; the exact form is supposed to be discovered. Moving average of first order (MA(1)) model is an example:

ut = et-1 + et, E[et] = 0, E[etet-1] = 0

Here, structure of covariance matrix K is determined by an autocorrelation function of ut:



1, 

ti - tj = 0



Kij = cov(uti , utj ) =

 1+2

,

|ti - tj| = 1

,

 0,

|ti - tj| > 1

 coefficient, however, is unknown. Hence, we propose ` ModelGeneralizedEffects ', as defined by Equation 29:

W = 1X(2) + BW + E,

E  MN (0, , In)

Z = 2X(2) + W +  +

p i=1

U(i), 



MN (0, ,

In),

U(i)



MN (0,

D(i),

K(i)),

(29)

where p is a number of random effects and {K(i)}i=1..p matrices are parameterized. Similarly to ` ModelEffects ', we infer expectation of model-implied data matrix M , covariance across

rows (between variables) L and covariance across columns (between observations) T :

M = E[Z] = (C1 + 2) X(2)

(30)

p

L =cov[Z] = n + tr{K(i)}D(i)

(31)

i=1

p

T =cov[ZT ] = tr{}In + tr{D(i)}K(i)

(32)

i=1

3.4.1 Estimation methods
At the moment, only one optimization method is implemented for ModelGeneralizedEffects :
· Matrix-variate normal maximum likelihood (ML) REML is likely to appear in future versions.

19

3 SEMOPY MODELS

3.4 ModelGeneralizedEffects

3.4.2 Random effects models

` ModelGeneralizedEffects ' constructor accepts a list of different effect model instances, each element in the list describes an U(i) from the Equation 29. All available effect models require a columns argument passed to the constructor that contains a list (or just a string if applicable) of column names with information on a population structure. For example, it can be group labels or times when a sample was observed. Next, we describe effect models that are available in the semopy as of version 2.2.2.

· Fixed effects model: ` EffectStatic ' ` EffectStatic ' assumes a fixed covariance matrix K just like in ` ModelEffects '. Model-specific arguments:

­ k : K matrix; indices and columns must have group labels.
· Fully parameterized model: ` EffectFree ' With ` EffectFree ', entire V from Equation 28 is estimated. This problem might be feasible if p <<< n. Model-specific arguments:

­ diagonal : if True , then V is constrained to be a diagonal matrix. The default is False . ­ correlation : if True , then V is constrained to be a correlation matrix. The default is False .
· Moving average model: ` EffectMA ' Moving average is a classical model in time series analysis, where the variable at present time is explained by a weighted sum of random independent errors from past times. The first-order moving average model, denoted by MA(1), is:

ut =  t-1 + t,

where { t}t=1..T are independent and centered. Autocorrelation function for MA(1) model:



1, 

|ti - tj| = 0



cor(uti , utj ) =

 1+2

,

|ti - tj| = 1

,

0, |ti - tj| > 1

The second-order moving average model, denoted by MA(2), is:

ut = 2 t-2 + 1 t-1 + t,

where t, t-1, t-2 are independent and centered. Autocorrelation function for MA(2) model:

1, 

|ti - tj| = 0



cor(uti ,

utj )

=

 

1

+1

2

 1+21+22

2

  

1+21

+22

, ,

|ti - tj| = 1 |ti - tj| = 2

,



0,

|ti - tj| > 2

And general case of p-th order moving average model, denote by MA(p):

p

p

ut = t + i t-i = i t-i

i=1

i=0

where { i}i=t..t-p are independent and centered, 0 = 1. The autocorrelation function for MA(p)

model:


 cor(uti , utj ) =

, p-k
i=0

i i+p

p i=0

2i

|ti - tj| = k, k  p

,

0,

|ti - tj| > p

Model-specific arguments:

20

3 SEMOPY MODELS

3.4 ModelGeneralizedEffects

­ order : Order of moving-average model. The default is 1. ­ dt_bounds : Optional list of tuples that contain bounding boxes for different lags of autocorre-
lation function, i.e. if it is [(0, 1), (1, 3)] , then all observations with time difference in interval [0, 1) are assumed to have lag 0 (similarly, if the difference is in [1, 3), then the lag is 1). In other words, if the time difference is in k-th interval, then the lag is k. It provides a mapping from a continuous time domain to a discrete domain of lags. The default is [(0, 1), (1, 2), ... (p, p + 1)] .
­ params : Optional list of {i}i=1..p parameters. If it is not provided, then parameters are estimated (that's the default approach).
· Autoregressive model: ` EffectAR '
Autoregressive model of the first order AR(1) is defined as

ut = ut-1 + t

Its autocorrelation function is

cov(uti , utj ) = k, k = |ti - tj|

(33)

Although higher-order AR(q) models are possible, at the moment semopy supports only first-order

AR models.

Model-specific arguments:

­

dt : k from Equation 33 is calculated as

|ti-tj | dt

.

The default is 1.

­ param : Same as params in ` EffectMA '.

· Spatial model: ` EffectMatern '

For the task of modeling spatial dependencies, Matern autocovariance function has been proposed by

Stein (1999):

21- cov(ui, uj) = C(dij)  ()

 2

dij




K

 2

dij



,

where dij is a Minkowski distance between i-th and j-th individual,  is the gamma function, K is the modified Bessel function of the second kind,  and  are positive hyperparameters (though  can be turned into an active parameter). The special case of Matern covariance is when   :

C  exp

-

d2ij 22

,

which is Gaussian radial-basis function.

The

other

special

case

is

absolute

exponential

kernel

that

is

achieved

when



=

1 2

:

C 1  exp - dij

2



Model-specific arguments:

­ nu : The  hyperparameter. The default is . ­ rho : The  parameter. The default is 1. ­ active : If True , then  is an active parameter that is optimized. Otherwise, it is fixed. The
default is False .

By default (i.e. if active=False ), ` EffectMatern ' has no active parameters, hence, it can be seen as a special case of ` EffectStatic '. One might wish to enjoy the higher performance of ` ModelEffects ' if only one random effect is needed. If that's the case, a user can just call the calc_zkz method after instantiating and loading data into the effect, and then pass it as a K matrix to ` ModelEffects '.

21

3 SEMOPY MODELS

3.4 ModelGeneralizedEffects

· Custom kernel model: ` EffectKernel ' semopy provides a general effect model that is compatible with kernels of sklearn (Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay, 2011) from the gaussian_process.kernels submodule. In fact, ` EffectMatern ' is a convenient specialization of ` EffectKernel ' for the Matern kernel. Model-specific arguments:
­ kernel : Any sklearn.gaussian_process.kernels compatible class. ­ params : Dictionary of kernel parameters.
­ active : If True , then parameters are active and estimated. The default is False .
· Custom random effects models Do the models above cover all possible dependence structures? Probably not. The user is invited to build his own custom effect models when necessary. All effect classes in semopy are derived from the base class ` EffectBase '. A developer has to implement the following methods:
­ load A method that loads data and builds model-specific parameters vector of length pm.
­ calc_k Method that returns n × n K matrix.
­ grad_k A method that returns a list of length pm where i-th element is a derivative of K matrix with respect to the i-th parameter.
­ (Optional) get_bounds A method that returns a list of length pm where i-th element is a bounding box of the i-th parameter. This method is actually already implemented in the base class and if not overridden, then models parameters are unconstrained.
We advise a prospective developer to first take a look at source code of the demo class ` EffectBlank ' (an effect with K = const = In and 0 parameters) before proceeding to any of the "useful" effect models.

3.4.3 Usage example

We are still working with the same model, but this time we load data that is "spoiled" with a mix of 2 random effects with different known K matrices and an MA(2) process:

In [ ]:

import semopy ex = semopy.examples.example_article desc = ex.get_model() data , (k1 , k2) = ex.get_data(random_effects =2, moving_average=True)

Then, we build a list of effect models:
In [ ]: from semopy . effects import EffectStatic , EffectMA
ef = [EffectStatic('group', k1), EffectStatic('group', k2), EffectMA('time', 2)]

Finally, we build an instance of ` ModelGeneralizedEffects ' using desc and list of effect models ef :
In [ ]: m = semopy . ModelGeneralizedEffects ( desc , ef )
r = m.fit(data)

Let's see if parameter estimates are any close to the true ones:

In [ ]:

import numpy as np params = ex.get_params() mape = np.mean(semopy.utils.compare_results(m, params)) print ('MAPE: {:.2f}%'.format(mape * 100))

22

3 SEMOPY MODELS

3.5 Comparison and limitations

Model name Performance ranking Nonnormal continuous and ordinal variables
Intercepts
Missing data
Population structure
Maximal number of random effects Generalized random effects (time series, spatial analysis, etc)

` Model '
1st
ULS, GLS, WLS, DWLS; Polychoric and polyserial correlations No; Possible to estimate intercepts after fitting model Pairwise deletions when computing S; FIML Limited: group-wise means 0
No

` ModelMeans ' ` ModelEffects '

2nd

3rd

Only exogenous variables, but any method

Only exogenous variables, but any method

Yes

Yes

FIML

No; Only group labels for random effects

Limited: group-wise means 0

Random effects 1

No

No

` ModelGeneralizedEffects ' 4th Only exogenous variables, but any method
Yes
No; Only group labels for random effects Random effects 
Yes

Table 2: Comparison of models.

Out [ ]: MAPE : 34.63%
A reader can verify that exclusion of any of the random effect models from ef will produce estimates with significantly higher MAPE.
3.5 Comparison and limitations
` ModelGeneralizedEffects ' can be seen as the most general model and all other classes can be interpreted as subsets of it. One cay say that ` Model '  ` ModelMeans '  ` ModelEffects '  ` ModelGeneralizedEffects '. However, generality comes at price of lower performance, and at the moment, some features present for ` Model ' are lacking for other classes. A short review of semopy models can be seen at Table 2.
Also, there are often identifiability issues of some variance components in ` ModelEffects ' and ` ModelGeneralizedEffects '. Wang (2013) showed that the exact conditions for variance components being non-identifiable in univariate LMMs depend on the structure of K matrix, however, no research has been conducted in this area in the case of SEM and parameterized K matrices. Yet, it appears that non-
23

4 SYNTAX

identifiable parameters always reside in  and {D(i)}i=1..p matrices, the sum of those variances is always the same, and all other parameters are estimated correctly (in a sense that there is always a local minimum).
Therefore, we don't consider the identifiability of random effects variance components to be an issue.

4 Syntax
To specify SEM models, semopy uses syntax that borrows from formula syntax known to R userbase for describing regression models. lavaan package, for instance, enjoys the similar approach. SEM models are specified as a multiline string, where each line either constitutes a relationship between variables with operators, or explains a properties of variables or parameters by means of operation commands.
4.1 Operators
Three core operators are supported across all of the models:
· "  ": regression operator. For example, assume the following linear equation:

y = a1x1 + a2x2 + a3x3 + , where y and x1, x2, x3 are observable variables, and a1, a2, a3 are regression coefficients. In semopy syntax it can be rewritten as:
y  x1 + x2 + x3
Here, regression coefficients are implicitly added to the model as active parameters.
· '  ': covariance operator. It is used to parameterize covariance between variables. For example, in model from Figure 3.2, covariances between 2, x2 and between y5, y6 are defined with those lines:
eta2  x2 y5  y6

· " = ": measurement operator.
This operator is used to define latent variables and regressions onto their indicators. For example, if a latent factor is explained by a set of equations


y1 = 1.0   + 1  y2 = a1   + 2

y3 = a2   + 3



 



N

(0,

2 ),

k  N (0, 2)

where  is a latent factor,n semopy it can be written down as:

eta = y1 + y2 + y3

In other words, " = " operator does this:
1. Defines l-value as a latent variable; 2. Regresses l-values onto r-values; 3. Sets the first regression coefficient as appears in the formula to 1.0 to make regression parameters
estimable. This happens unless a user manually set any other regression in right part to some fixed value.

24

4 SYNTAX

4.2 Operation commands

It can be thought of as a syntax sugar. Later, in Section 4.2, we will show how it can be rewritten in terms of "  " and DEFINE operation.
There are also covariance operators that are specific to ` ModelEffects ' and ` ModelGeneralizedEffects ':
· (` ModelEffects ') " RF " covariance operator. Used to define covariances in D matrix.
· (` ModelGeneralizedEffects ') " RFk " covariance operator. Here, " k " stands for the order in which random effects are specified. It is the same as " RF ", but for Dk matrix only.
When specifying relationships between variables by means of operators, we always implicitly introduce new parameters. This can be done explicitly, however, by typing the name of the parameter in front of an r-value separated by an asterisk/multiplication " * " symbol. It can be used for further constraint specification, or to reuse the same parameter in other equations. For example,
y  a1 * x1 + a1 * x2 + x3
Here, the same regression coefficient is forced for x1 and x2 . The same approach can be used to fix parameters, for example:
y  2.0*x1 + x2 + x3
Also, for brevity, you can specify multiple r-values separated by comma symbol " , " ­ it will be separated into multiple formulas with the same r-values. For example,
y1  x1 + x2 + 3 * x3 y2  x1 + x2 + 3 * x3 y3  x1 + x2 + 3 * x3
can be rewritten as
y1 , y2 , y3  x1 + x2 + 3 * x3
Let's see how the model that is depicted on Figure 3.2 is described in semopy syntax: In [ ]: import semopy
desc = semopy.examples.example_article.get_model() print(desc)

Out [ ]:

# Measurement part eta1 = y1 + y2 + y3 eta2 = y3 + y2 eta3 = y4 + y5 eta4 = y4 + y6 # Structural part eta3  x2 + x1 eta4  x3 x3  eta1 + eta2 + x1 x4  eta4 + x6 y7  x4 + x6 # Additional covariances y6  y5 x2  eta2

4.2 Operation commands
· DEFINE(latent) Defines a list of variables that follow the command as latent variables. For example,

25

4 SYNTAX

4.2 Operation commands

In [ ]: Out [ ]:

y1  1.0 * eta1 y2  eta1 + 1.0 * eta2 y3  eta1 + eta2 y4  eta2 DEFINE(latent) eta1 eta2
Notice that that's the same as
eta1 = y1 + y2 + y3 eta2 = y2 + y3 + y4
· DEFINE(ordinal) This operation has effect only for ` Model '. When variables are defined as ordinal, polychoric and polyserial will be used for ordinal variables instead. An example:
y  x1 + cat1 + cat2 cat3  x1 cat2  cat3 DEFINE(ordinal) cat1 cat2 cat3
· START(v) Sets float value v as a starting value for a list of parameters that goes after the command. Parameters must be given a name beforehand. An example:
y  a1 * x1 + a2 * x2 + a3 * x3 START (0.0) a1 a2 START (10) a3
· BOUND(l, r) BOUND operator restricts parameter values to an interval [ l , r ]. All variance parameters are restricted to an open interval [0, ] by default. An example:
y  a1 * x1 + a2 * x2 + a3 * x3 y  v * y BOUND(-1, 1) a2 a2 a3 BOUND(0, 10) v
· CONSTRAINT(constr) Imposes a non-linear constraint constr of type equality or inequality on the model parameters. constr can be any sympy-compatible (Meurer, Smith, Paprocki, C ert´ik, Kirpichev, Rocklin, Kumar, Ivanov, Moore, Singh, Rathnayake, Vig, Granger, Muller, Bonazzi, Gupta, Vats, Johansson, Pedregosa, Curry, Terrel, Roucka, Saboo, Fernando, Kulal, Cimrman, and Scopatz, 2017) string. All parameters in the constraint must be named beforehand. An executable example: First, we load univariate regression model:
import semopy ex = semopy.examples.univariate_regression_many desc , data = ex.get_model(), ex.get_data() print(desc)
y  x1 + x2 + x3
Instead of using this desc , we name some of the parameters, apply constraints and then fit the model to the data:

26

5 PREDICTION

In [ ]:

desc = '''y  a*x1 + b * x2 + c # We explicitly define variance parameter: y  v * y CONSTRAINT(exp(a) + exp(b) = 10) CONSTRAINT(v < cos(a)^2 + sin(b)^2)''' m = semopy.Model(desc) r = m.fit(data) m. inspect ()

Out [ ]:

lval op rval Estimate Std. Err

0

y  x1 1.987709 0.090504

1

y  x2 0.993699 0.097202

2

y  x3 1.214214 0.085897

3

y 

y 0.866303 0.122514

z-value 21.962690 10.223016 14.135694
7.071068

p-value 0.000000e+00 0.000000e+00 0.000000e+00 1.537437 e -12

It can be seen that parameter estimates satisfy the imposed constraints.

5 Prediction
SEM can be interpreted as a data generating process, and the problem of predicting certain variables from given data is of interest. Specifically, given model parameters estimate, we want to:
1. Predict missing entries in a dataset from, at least, some of the observed data; 2. Predict factor scores/provide estimates of latent variables.

5.1 SEM Regression/Imputation
All semopy models can do regression by means of a conditional expectation of multivariate normal variable. Let z be a vector of random variables distributed as µ, ±. We partition z into two subvectors z1 and z2, and, consequently, do the same with mean vector µ and the covariance matrix :

z = z1 , µ = µ1 ,  = 11 12

z2

µ2

21 22

It can be shown (Eaton, 1983) that the expectation of z1 conditional on z2 is

E[z1|z2] = µ1 + 12-221(z2 - µ2)

(34)

If we interpret z2 as observed variables that are present in the dataset and z1 as missing variables, then Equation 34 provides a regression formula. It can also be thought of as an imputation method if parameter estimation was done with either ` Model 's pairwise deletions or the FIML method on the incomplete data.
In semopy, it is done via predict method. See an example:
First, we load the political_democracy dataset and then we randomly fill 10 data entries with NaNs:

In [ ]:

import semopy import numpy as np import random random . seed (123) ex = semopy.examples.political_democracy desc , data = ex.get_model(), ex.get_data() inds = l i s t (np.ndindex(data.shape)) inds = tuple(zip(*random.sample(inds , 10))) true_vals = data.values[inds] data.values[inds] = np.nan

Then, we fit ` ModelMeans ' to the data with FIML, predict/impute the missing values and compare them to true values in terms of MAPE and mean-squared error (MSE):

27

6 SOLVERS

5.2 Factor scores

In [ ]:

m = semopy.ModelMeans(desc) r=m.fit(data , ) pred = m.predict(data ,)[data.columns].values[inds] mape = np.mean(abs((pred - true_vals ) / true_vals )) * 100 mse = np.mean((pred - true_vals) ** 2) print ('MAPE: {:.2f}%, MSE: {:.2f}'.format(mape , mse ))

Out [ ]: MAPE : 14.59% , MSE : 0.41 In practice, usually, predictions can be only as good as a variance of predicted variables.

There is a limitation of predict method for ` ModelMeans ', ` ModelEffects ' and
` ModelGeneralizedEffects ', however: it can't be used to predict exogenous observed variables that reside in x(2). Hence, if a user needs to predict x(2), he/she should either move the predicted variables from x(2) to x(1), or resort to ` Model '.

5.2 Factor scores
Factor scores, in context of our generalized SEM models, are the most likely values that latent variables would attain if we had observed them given data and parameter estimates. Numerous approaches have been proposed for different models, starting with exploratory factor analysis (EFA) (Bartlett, 1937) to generalizations of those approaches to CFA (Bollen, 1989) and SEM (Yung and Yuan, 2013). Yet, none of them can tackle ` ModelMeans ', let alone to take random effects into an account. Hence, we propose our own generalized factor prediction scheme that is based on maximum a posteriori (MAP) approach. Namely, we assume a joint distribution function of data Z and latent factors H:

f (Z, H|) = f (Z|H, )f (H|),

(35)

where f (Z|H, ) is a conditional distribution of Z given H,  and f (H|theta) is a distribution of H given ,  is a parameters vector. Then, we maximize Equation 54 with respect to H and, finally, obtain MAP estimate of latent factors. In case of ` ModelEffects ' and ` ModelGeneralizedEffects ', solving the maximization problem requires solving Sylvester equation of the form A1H - HA2 = A3, which is done by means of Bartels-Stewart algorithm (Bartels and Stewart, 1972). In case of ` Model ' and ` ModelMeans ', the problem reduces to solving a system of linear equations.
The derivation of our approach and an exact analytical formula is, unfortunately, too tedious to be included here, but an interested reader can refer to Appendix D.
In semopy, method predict_factors is used for factor scores estimation. See an example:

In [ ]:

import semopy ex = semopy.examples.holzinger39 data , desc = ex.get_data(), ex.get_model () m = semopy.Model(desc) r = m.fit(data) m. predict_factors ( data ). head ()

Out [ ]:

speed textual

visual

0 0.061510 -0.137550 -0.817675

1 0.625501 -1.012725 0.049531

2 -0.840548 -1.872284 -0.761421

3 -0.271325 0.018486 0.419333

4 0.194325 -0.122253 -0.415898

6 Solvers
All the estimation methods require solving some sort of the maximization/minimization problem. semopy relies on scipy (Virtanen, Gommers, Oliphant, Haberland, Reddy, Cournapeau, Burovski, Peterson, Weckesser, Bright, van der Walt, Brett, Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey,
28

8 TESTING FRAMEWORK: MODEL AND DATA GENERATION

Polat, Feng, Moore, VanderPlas, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris, Archibald, Ribeiro, Pedregosa, van Mulbregt, and SciPy 1.0 Contributors, 2020) optimization methods; any of the methods available to scipy can be chosen via setting the solver argument of the fit method. However, not all scipy solvers support constraints. The default solver in semopy is SLSQP (Kraft, 1988) as previously (Igolkina and Meshcheryakov, 2020) it has demonstrated the best results. However, sometimes other solvers can provide better results.
The one solver that deserves special attention is the differential evolution solver that is due to Storn and Price (1997); it is a global solver and might be useful in big and complex models with non-optimal local minima. However, it has two pitfalls that make it less attractive:
1. A user might need to provide bound constraints for parameters, as differential evolution solver looks for optima inside of a box in a parameters space. If no bounding boxes are provided, then the package automatically assumes that they lie in the interval [-10, 10], which is not always reasonable, or too large and will take a toll on performance;
2. Computation times are huge and not suitable for applications where high performance is demanded.
To use differential evolution solver, one should pass "de" as the solver argument to the fit method. Also, the default [-10, 10] bound can be redefined as [-b, b] by passing an extra b_max=b argument.

7 Parametric bootstrap resampling for bias reduction

Maximum likelihood estimators (MLE) are linchpins of statistical models due to their properties, however,

it is known that

MLE produces unbiased variance

estimates up to

the order

1 n

.

The bias

becomes

more

noticeable in cases when the number of data samples n is small. Fortunately, it is possible to reduce the bias

to

the

order

1 n2

.

One

way

to

do

it

is

to

generate

pseudo-random

samples

from

distribution

to

estimate

bias

and then subtract it from the previously obtained estimates. This approach, called parametric bootstrap

resampling (PBE), is due to Efron (1982).

Let z  N (0, ()), where  is a parameter vector obtained from MLE. Next, we sample data from N (0, ()) k times and for each of sampled datasets we calculate another ML estimate (i). Then, we can

estimate bias b of  as

1k b=

 - (i)

k

i=1

Finally, we can obtain ML estimates corrected for bias as

1 P BE =  - b = 2 - k

k

(i)

i=1

To correct parameter estimates for bias, one should call function bias_correction : In [ ]: semopy . bias_correction ( model , n =1000)

where n is a number of bootstrap iterations (the default is 100). After that, parameters in model should be corrected for bias with the above PBE method.

By default, semopy uses biased sample covariance matrix similarly to lavaan. The bias might become big enough when number of samples is very small, therefore when incorporating the above PBE approach in scenarios with tiny datasets, it is advised to pass unbiased sample covariance matrix to the fit method via cov argument and/or using FIML.

8 Testing framework: model and data generation
For numerical experiments with semopy, we implemented a versatile system that can: 1. Given a configuration (i.e. the desired number of endogenous and exogenous variables, number of latent factors, etc), generate a random model description in semopy syntax;
29

8 TESTING FRAMEWORK: MODEL AND DATA GENERATION

2. Given a model description, generate a random set of parameters for the model;

3. Given a model, sample a random dataset that could be generated by the model;

Let's see how it works on an example: First, we generate a random model description:

In [ ]:

import random import semopy random . seed (12345) modgen = semopy.model_generation desc = modgen.generate_desc(n_endo=3, n_exo=2, n_lat=2,
n_inds=3, n_cycles=1, p_join =0.1) print(desc)

Out [ ]:

# Measurement part: eta1 = y1 + y2 + y3 eta2 = y4 + y5 + y6 + y2 # Structural part: eta2  eta1 + x3 x3  g1 x1  g1 + x3 + x2 x2  g2 eta1  g2 + x3 + x2 + x1 g1  x3

Here, parameters have the following meaning: n_endo is a number of endogenous observable variables, n_exo is a number of exogenous observable variables, n_lat is a number of latent variables, n_inds is a number (or tuple, then for each factor number of indicators is chosen randomly from an interval) of indicators per random latent variable, n_cycles is a number of cycles in a model, p_join is a probability that an indicator will be shared with another latent variable (here, y2 is an example).
Then, we generate a set of parameters for the given model:
In [ ]: params , tmp = modgen . generate_parameters ( desc )

params is a dataframe that is similar to ones returned by the inspect method, tmp is an auxiliary variable (to be more precise, it is a ` ModelMeans ' instance with parameters loaded from the params table) that is useful only for the next step:
In [ ]: data = modgen . generate_data ( tmp , n =200)
print ( data . head ())

Out [ ]:

y1

y2

y3 ...

0 3.276384 0.525599 4.127978

1 0.083196 1.600804 1.080848

2 -4.294538 -1.885474 -1.581973

3 7.844931 5.640587 11.752653

4 -3.326591 -2.099521 -3.087801

x2

x3

g2

... 4.763574 2.031664 -2.128789

... -0.512638 -1.273207 1.008115

... -1.025223 2.078174 -0.550860

... 4.871314 -3.475223 -1.385154

... 0.575090 3.693363 -0.408098

[5 rows x 11 columns]

where n is a number of samples in a generated dataset. So, now we have a triplet of model description desc , dataset data and "true" parameter values
params . Let's verify that semopy provides reasonable estimates for the generated model:

In [ ]:

m = semopy.Model(desc) r = m.fit(data) mape = np.mean(semopy.utils.compare_results(m, params)) * 100 print ('MAPE: {:.2f}%'.format(mape ))

Out [ ]: MAPE : 15.45%

30

9 SIGNIFICANCE TESTING

Note that generate_data can be applied to any Model , ModelMeans or even ModelEffects and
ModelGeneralizedEffects instance. It can be used to simulate different datasets that could be spewed out by a data generating process that is described by a SEM model.

9 Significance testing

9.1 Standard errors and p-values

The semopy utilizes the Z-test to calculate p-values for parameters' estimates under the assumption that

parameters are normally distributed; H0: the value of a parameter is equal to zero. This approach considers

z-score:

Z (^)

=

^ SE(^) ,

where ^ is a to variance:

vector SE(^)

of =

vpaarr(a^m)/etenr .esBtiamseadteosn,

SE(^) is the standard error the Cram´er­Rao bound,

of

estimates,

which

is

proportional

var(^)  FIM()-1,

where FIM() is a Fisher information matrix (FIM). The FIM() matrix can be defined as an observed or expected FIM. The observed FIM is the Hessian of a likelihood function at ^, H(^). The expected FIM is:

F IM (^) = -E

2

ln f (X, ^)

n
,

ij

i,j

where f (x, ^) is a likelihood function. For instance, if x  N (µ(), ()), the FIM is calculated as follows (Mardia and Marshall, 1984):

F IM () = E



f ()

n
=

µT ()-1() µ () n +

ij

i,j

i

j

i,j

+ -tr  (^)(^)-1  (^)(^)-1 n

(36)

i

j

i,j

Applying property in the Equation 18 to x, it is trivial to generalize the Equation 36 to a case of matrix-variate normal distribution, hence making it applicable to ` ModelEffects ' and ` ModelGeneralizedEffects '. Unfortunately, this straightforward approach requires O(n3m3) operations for each of the FIM components, where n is a number of observations and m is a number of output observable variables. Therefore, we have inferred an alternative form of the FIM for the case of matrix-variate normal distributions that are computable in O(n3 + m3) operations (see derivation in Appendix C):

F IM ()i,j = tr{T }tr

M T L-1 M T -1

i

i

1 + 2 (ntr{AiAk} + mtr{BiBk}+

+tr{Ai}tr{Bk} + tr{Ak}tr{Bi} + nmik - nktr{Ai}-

-mktr{Bi} - nitr{Ak} - mitr{Bk}),

where

Ai

=

T

-1

T i

,Ak

=

T

-1

T k

,

Bi

=

L-1

L i

,

Bk

=

L-1

L k

,

i

=

tr{

T i

}

tr{T }

and

k

=

tr{

T k

}

tr{T }

.

The p-values for parameter estimates are based on the Z-test assumption, that Z() follows a multivariate
normal distribution. This assumption follows from properties of ML estimators, namely, that parameter estimates ^ of a parameter vector  converge in distribution to N (, F IM -1()):

 n(i)

d

M(0,

F

IM

-1(^))

31

10 REGULARIZATION

9.2 Robust standard errors

Name "l1-naive"
"l1-smooth"
"l1-thresh"
"l2-naive" "l2-square"

Description Naive implementation of L1 regularization: the gradient does not exist at 0. Instead of L1 penalty, a smooth approximation is used (Schmidt, Fung, and Rosales, 2007):  (ln(1 + exp(-x/)) + ln(1 + exp(x/))). l1 regularization, but the gradient is a soft-thresholding operator (Daubechies, Defrise, and De Mol, 2004) with hyperparemter . L2 regularization, the gradient is not differentiable at 0. L2-squared regularization, the gradient exists everywhere.

Table 3: The list of available penalty functions. In all cases, the less  is, the better is approximation, but too small values lead to numerical instability. is a hyperparemter that can be submitted via alpha argument. The default value for the alpha is 10-6.

In seompy, standard errors, alongside p-values, are calculated automatically when inspect method is called. By default, expected FIM is used, but a user may want to estimate standard errors with the observed FIM instead by setting the information argument to "observed" :
In [ ]: model . inspect ( information = " observed " )

9.2 Robust standard errors
When the assumption of data normality is violated, standard errors should not be trusted. The problem can be mitigated by using the so-called "Huber sandwich estimator" of an asymptotic covariance matrix proposed by Huber (1967). Standard errors, obtained from the sandwich estimator, are often referred to as "robust standard errors". The intuition behind this method and discussion on whether it is reasonable is provided by Freedman (2006). In semopy, user can substitute conventional standard errors with robust ones by passing robust=True argument to the inspect method:
In [ ]: model . inspect ( robust = True )

10 Regularization
Regularization, in the context of SEM, is not well studied, neither have we did any substantial research that could contribute to the field. So far, its applications are limited, but not non-existent. For example, penalized SEM has been applied for variable (and model in a broader sense) selection by Jacobucci, Brandmaier, and Kievit (2019). Therefore, we added support of regularization to semopy as a promising feature for an ingenious user and as a basis for possible further research. We consider a regularized problem of the form
G() = F () + cR()  min, where F is an objective function such as log-likelihood or a least-squares loss, R is a regularization constant, R is a penalty function and  is a subvector of  that is penalized.
To apply regularization in semopy, a user should first call the create_regularization function to instantiate an auxiliary structure that will be used by the fit method of a model. This function has the following parameters of interest:
· model ­ a model instance;
· regularization ­ a name of penalty function (see Table 3);
· c ­ a regularization constant (the default is 1.0);
· alpha ­ an  parameter that is used in "l1_smooth" and "l1_thresh" (the default is 10-6);

32

11 SPARSE EXPLORATORY FACTOR ANALYSIS

· param_names ­ an optional list of parameter names (as provided by a model syntax) that are penalized;
· mx_names ­ an optional list of matrix names whose parameters are penalized.

At least one of param_names or mx_names arguments must be provided.
The list of supported penalty functions is provided in Table 3. Note that as regularization accepts a list of regularization instances, one can combine different penalties on the same subset of parameters. It can be used to obtain the elastic net penalty, for instance. See an example:

In [ ]:

import semopy ex = semopy.examples.political_democracy desc , data = ex.get_model(), ex.get_data() m = semopy.Model(desc) reg1 = semopy.create_regularization(m, 'l1 -thresh ', mx_names =['Beta ']) reg2 = semopy.create_regularization(m, 'l2 -square ', c=0.5, mx_names =[ ' Beta ']) regs = [reg1 , reg2] r = m.fit(data , regularization=regs)

11 Sparse exploratory factor analysis
Although an exact definition for Exploratory Factor Analysis (EFA) is arguable, it is fair to generalize and say that EFA is a set of multivariate statistical techniques used to extract an underlying latent structure from observed variables. EFA is often used as the first step in SEM model specification in cases when the researcher can't provide a reasonable prior latent structure. Numerous implementations of EFA exist, the most popular is probably a two-step procedure of firstly determining the number of latent factors via a kind of scree test and secondly performing a factor analysis (FA) to discover factors' indicators. That's easily achievable with great number of statistical packages,such as statsmodels (Seabold and Perktold, 2010) and sklearn.
Though not necessarily a con, the above approach is prone to a researcher subjective reasoning: first, a researcher has to select a number of latent factors, second, he has to decide what loading values obtained from FA are significant. The latter can be dealt with using a sparse variation of EFA, for example, sparse PCA (SPCA) (Zou, Hastie, and Tibshirani, 2006). In semopy, we provide an enhanced heuristic approach that not only manages to uncover latent structures with a greater degree of success than plain SPCA but also provides a way to automatically detect a number of latent factors hidden in data.
First, we compute correlations {ij}mi,j=1 between phenotypes. Then we construct a distance matrix D, where Dij = 1-|ij|. Then, we pass D to OPTICS (Ankerst, Breunig, Kriegel, and Sander, 1999) clustering algorithm with minimal cluster size set to 2. We chose OPTICS as a cluster due to its ability to identify clusters of uneven sizes and densities: it was observed that first 3 eigenvectors of covariance matrix might explain up to 90% of the variance while the number of latent factors in an actual model is significantly higher in our synthetic datasets, thus making it impossible to obtain even a correct number of latent factors using conventional EFA approaches and clustering algorithms. A number of clusters identified by OPTICS are considered to be a number of latent factors (and variables' labels can be interpreted as loadings onto a respective latent factor; in fact, this representation is often already good enough, but in this approach, we don't use it further).
Second, we run SPCA to determine loadings of latent factors onto observed variables. At this point, we've obtained a CFA model, but it is usually quite overidentified in the sense that there are some abundant loadings.
Finally, we refine the CFA model by evaluating it through ` Model ' and dropping loadings with high p-values.
The goal of the proposed algorithm is to provide a convenient EFA technique within a semopy framework, rather than to contribute to the field of sparse learning or EFA, as the algorithm is very heuristic and not theoretically sound, yet performed well in our tests (see Section 14.2). To use it, user should call the explore_cfa_model function of the efa submodule. See an example:

33

12 VISUALIZATION

Before all else, we generate a random model that consists only of CFA part and a 100 exogenous variables that load onto latent factors. The exogenous variables here play a role as variance contributors to latent factors, we omit them shortly from the analysis for a cleaner example:

In [ ]:

import semopy import numpy as np np.random.seed (123) modgen = semopy.model_generation desc = modgen.generate_desc(0, 100, 4, 3, 0, 0.05) params , t = modgen.generate_parameters(desc) data = modgen.generate_data(t, 200, ) print(desc.split('# ')[1])

Out [ ]:

Measurement part: eta1 = y1 + y2 + y3 + y9 eta2 = y4 + y5 + y6 eta3 = y7 + y8 + y9 + y5 eta4 = y10 + y11 + y12 + y2

In [ ]: cols = [ c f o r c in data . columns i f not c . startswith ( 'g ' )]
cols = sorted ( cols , key =lambda x : i n t ( x . split ( 'y ' )[ -1])) data = data[cols]
Next, to make the task of discovering the underlying latent structure a bit harder, we introduce 40 fake variables into a dataset.
In [ ]: f o r i in range (40):
data[f'fake{i+1}'] = np.random.normal(size=len(data))
Finally, we run EFA procedure on data :
In [ ]: desc = semopy . efa . explore_cfa_model ( data , mode = ' spca ')
print('Predicted CFA model:\n', desc)

Out [ ]:

Predicted CFA model: eta1 = y7 + y8 + y5 + y9 eta2 = y3 + y1 + y2 + y9 eta3 = y10 + y11 + y12 + y2 eta4 = y6 + y4 + y5

It is the same model as that was generated previously up to indicator permutations.

12 Visualization

It is often tiresome to analyze SEM results in a form of plain text, hence semopy provides a simpler wrapper around graphviz (Ellson, Gansner, Koutsofios, North, and Woodhull, 2003). See an example:
First, we load and fit the model:

In [ ]:

import semopy ex = semopy.examples.political_democracy desc , data = ex.get_model(), ex.get_data() m = semopy.Model(desc) r = m.fit(data)

Then, we plot it and save to "out.pdf" file: In [ ]: g = semopy . semplot (m , ' out . pdf ')

The result is shown on Figure 8. Also, in g resides graph in dot-format (Gansner, Koutsofios, and North, 2015), that a user is free to adjust for a more fancy look.
Alternatively, one can just pass a model description to semplot instead of a ` Model ' instance. For example:

34

13 REPORTING RESULTS

ind60

1.482 p-val: 0.00

1.000

2.180 p-val: 0.00

1.819 p-val: 0.00

0.572 p-val: 0.01

dem60

x1

x2

x3

0.838 p-val: 0.00

1.000

1.257 p-val: 0.00

1.058 p-val: 0.00

1.265 p-val: 0.00

dem65

y1

y2

y3

y4

1.000

1.186 p-val: 0.00

1.280 p-val: 0.00

1.266 p-val: 0.00

y5

y6

y7

y8

Figure 8: Visualization of estimated Political Democracy model.

In [ ]:

desc = ''' eta = y1 + y2 + y3 eta  x1 + x2 x1 , x2  x3 ''' g = semopy.semplot(desc , 'out.pdf')

The result is shown on Figure 9

13 Reporting results
Although most of information of interest can be extracted pragmatically via inspect method and fit indices can be computed via calc_stats function, semopy provides a convenient report generation feature: In [ ]: semopy . report ( model , " Title or model name " )
For example, let's fit ` ModelMeans ' to the political_democracy dataset and then generate a report:

35

14 NUMERICAL EXPERIMENTS

x3

x1

x2

eta

y1

y2

y3

Figure 9: Visualization of a model given only text description.

In [ ]:

from semopy import examples , ModelMeans , report ex = examples.political_democracy model = ModelMeans(ex.get_desc ()) r = model.fit(ex.get_data ()) report(model , "Political Democracy")

Afterwards, a report with parameter estimates, visualization and fit indices is generated in HTML format. The resulting HTML file can be found at https://semopy.com/report_example/report.html or in supplementary files. A small fraction of the report can be seen in Figure 10.

14 Numerical experiments
14.1 Estimation accuracy
To test semopy performance, we evaluated different models and methods on the series of synthetic datasets and SEM models generated by our testing framework (see Section 8). We generated 33 synthetic sets of varying configurations (i.e. parameters that were passed to generate_description and generate_data ): each set consists of 40 unique SEM model descriptions and for each of the models 10 random pairs of parameters and data were generated (in total, that's 400 testing subjects per set). To measure the quality of estimates, we employed two metrics:
· Distance between parameter estimates and the true parameter values. Throughout most of this section (with an exception of Table 5), we shall consider only the MAPE statistics as it is easy to interpret, but root-mean-square error (RMSE) also can be found in supplementary data.
· Number of non-convergent models. We say that an optimization process failed to converge to the true parameter estimates if any of the 3 conditions are met:
1. MAPE > 40%;

36

14 NUMERICAL EXPERIMENTS

14.1 Estimation accuracy

Figure 10: A portion of the HTML report for the Political Democracy dataset.
2. Numerical optimizer returns an error; 3. The resulting objective function value is NaN. Throughout this section, wherever MAPE or RMSE are provided, they were calculated only on the intersection of sets of converged models from all of the compared methods.
14.1.1 Comparison to the previous version First, we examine both estimates accuracy and working times of the new semopy to the older version of 1.3.1 on the testing subset A: as evident from the Table 5, the newer version has an edge not only in terms of working times, but also in terms of estimation accuracy. Here, we also would like to remind a reader that previously (Igolkina and Meshcheryakov, 2020) we have demonstrated that semopy outperforms the most
37

14 NUMERICAL EXPERIMENTS

14.1 Estimation accuracy

# n n exo n endo n lat n cycles Random effects

A 1 100 4

3

3

0

No

2 100 2

3

3

0

No

3 100 3

3

3

0

No

B

4 5

100 4 100 5

3 3

3

0

3

0

No No

6 100 6

3

3

0

No

7 100 7

3

3

0

No

8 100 8

3

3

0

No

9 100 4

2

3

0

No

10 100 4

3

3

0

No

C

11 12

100 100

4 4

4 5

3

0

3

0

No No

13 100 4

6

3

0

No

14 100 4

7

3

0

No

15 100 4

3

1

0

No

16 100 4

3

2

0

No

D

17 18

100 100

4 4

3 3

3

0

4

0

No No

19 100 4

3

5

0

No

20 100 4

3

6

0

No

21 100 4

3

3

1

No

E

22 23

100 100

4 4

3 3

3

2

3

3

No No

24 100 4

3

3

4

No

25 50 4

3

3

0

No

26 100 4

3

3

0

No

27 150 4

3

3

0

No

F

28 29

200 250

4 4

3 3

3

0

3

0

No No

30 300 4

3

3

0

No

31 350 4

3

3

0

No

32 400 4

3

3

0

No

G 33 100 4

3

3

0

Yes

Table 4: Configurations of generated models and their respective datasets. The generated sets are separated into subsets A, B, C, D, E, F, G with respect to a varying parameter. Parameters n_inds and p_join are fixed to 3 and 0.05 respectively.

popular SEM package lavaan, therefore the new results solidify the past result.
14.1.2 Performance on testing sets For subsets B -F, we omit results for the older version of semopy and leave only some of the best-performing methods as reported by Table 5, and provide plots of number of non-covergent models N in Figure 11. Tables with exact N and MAPE statistic can be found in Appendix F.

38

14 NUMERICAL EXPERIMENTS

14.1 Estimation accuracy

N , number of non-convergent models

N , number of non-convergent models

20 18 16 14 12 10 8 6
2
90 80 70 60 50 40 30 20 10
0 1

B

3

4

5

6

7

n_exo

D

2

3

4

5

n_lat

N , number of non-convergent models

200 180 160 140 120 100 80 60
1

C

24

Model [FIML]

Model [MLW]

23

ModelMeans [ML]

22

21

20

19

18

17

16

15

8

2

3

4

5

6

7

n_endo

E

55

50

45

40

35

30

25

20

15

10

5

0

6

50

100

150

200

250

300

350

400

n

F

2

3

4

n_cycles

Figure 11: Number of non-convergent models for sets B-F.

39

15 CONCLUDING REMARKS

14.2 EFA

N MAPE RMSE Time, s

` Model '[DWLS]

78 14.76

` Model '[FIML]

18 11.70

` Model '[GLS]

63 13.30

semopy1[GLS]

68 15.07

` Model '[MLW]

18 11.82

semopy1[MLW]

21 13.11

` ModelMeans '[ML] 19 11.82

` ModelMeans '[REML] 35 15.76

` Model '[ULS]

93 15.96

semopy1[ULS]

112 18.09

0.11 0.09 0.10 0.12 0.09 0.10 0.09 0.11 0.14 0.15

0.18 0.35 0.17 0.22 0.12 0.21 0.60 0.15 0.28 0.30

Table 5: Comparison of the core semopy methods to the older 1.3.1 version.

14.1.3 Performance on the testing set G with random effects

N MAPE RMSE

` Model '

195 26.67 0.17

` ModelEffects ' 59 15.36 0.12

Table 6: Models performances on data with random effects.

Here, we draw a comparison of ` ModelEffects ' and ` Model ' evaluations on datasets "contaminated" with random effects. The purpose of this comparison is to showcase a drastic negative effect of not taking random effects into an account: see Table 6. As expected, ` Model ' demonstrates very poor performance in this setting, whereas ` ModelEffects ' tackles the situation considerably better.

14.2 EFA

We also used our testing framework to generate K datasets and ran our EFA procedure on each of them

(see Table 7). Then, we compared the resulting CFA models to the measurement part of true models. To

perform this comparison, we invented a simple metric



=

m n

,

where

m

is

a

number

of

falsely

identified/not-

identified loadings and n is a total number of loadings in the true model. Note that if the EFA procedure

failed to correctly estimate the number of latent factors,  can be greater than 1.0. The results are depicted

in Figure 12 that is built by points from tables in Appendix F.

15 Concluding remarks
Many options and features that are present in the package, didn't make it to this article, for the sake of not bloating it. semopy is better documented at the project's website (https://semopy.com), although without technical and mathematical details.
Despite that semopy may appear as a rather big project, we abstain from declaring it complete. We welcome prospective users and we expect any kind of feedback, as well as feature requests.

Computational details
All numerical results that are present in this article were obtained with Python 3.9.4 and semopy version 2.2.2. OpenBLAS was used as an backend for BLAS/LAPACK APIs. Computations were done on a Intel(R) Core(TM) i5-3230M CPU @ 2.60GHz

40

15 CONCLUDING REMARKS

A

0.55

0.5

0.45

0.4

0.35



0.3

0.25

0.2

0.15

0.1

5 · 10-2

2

3

4

5

6

n_lat

C



0.4

0.35

0.3

0.25

0.2

0.15

0.1 5

10

15

20

25

30

35

40

n_exo

B

0.23 0.22 0.21
0.2 0.19 0.18 0.17 0.16 0.15
50

100

150

200

250

300

350

n

D

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.5 8

16

32

64

sampler_var_psi

Figure 12: EFA procedure performance on different datasets.

41

15 CONCLUDING REMARKS

# n lat n n exo 

11

22

A

3 4

3 4

55

66

73

83

93

B 10 3

11 3

12 3

13 3

14 3

15 3

16 3

C

17 18

3 3

19 3

20 3

21 3

22 3

24 3

25 3

26 3

D 27 3

28 3

29 3

30 3

200 20 200 20 200 20 200 20 200 20 200 20 50 20 100 20 150 20 200 20 250 20 300 20 350 20 200 5 200 10 200 15 200 20 200 25 200 30 200 35 200 40 200 0 200 0 200 0 200 0 200 0 200 0 200 0 200 0

1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.5 1.0 2.0 4.0 8.0 16.0 32.0 64.0

Table 7: Configurations of generated models and their respective datasets. The generated sets are separated into subsets A, B, C, D with respect to a varying parameter. Parameters n_inds and p_join are fixed to 3 and 0.05 respectively.  is a variance of latent variables: variance of latent variable is sampled from the distribution   U (0.7, 1.4).

42

REFERENCES
Data availability
Randomly generated datasets and models that were used in Section 14 are available at the file storage https: //drive.google.com/file/d/1MRZ1_HrDjsEQL2tgnABRafF02XWmlwpI/view?usp=sharing, alongside with scripts that were used to generate and evaluate them. Alternatively, one can fetch scripts only from the repository https://gitlab.com/georgy.m/semopy-tests, re-generate datasets and then reproduce the results.
The package semopy can be retrieved either directly from the PyPi repository via pip, i.e. pip i n s t a l l semopy or via git repository https://gitlab.com/georgy.m/semopy.
To ensure that results are exactly reproducible, version 2.2.2 should be installed: pip i n s t a l l semopy==2.2.2
Acknowledgments
This work was supported by RFBR Grant No. 18­29­13033.
References
Ankerst M, Breunig MM, Kriegel HP, Sander J (1999). "OPTICS: Ordering Points to Identify the Clustering Structure." SIGMOD Rec., 28(2), 49­60. ISSN 0163-5808. doi:10.1145/304181.304187. URL https: //doi.org/10.1145/304181.304187.
Bartels RH, Stewart GW (1972). "Solution of the Matrix Equation AX + XB = C [F4]." Commun. ACM, 15(9), 820­826. ISSN 0001-0782. doi:10.1145/361573.361582. URL https://doi.org/10.1145/361573. 361582.
Bartlett MS (1937). "THE STATISTICAL CONCEPTION OF MENTAL FACTORS." British Journal of Psychology. General Section, 28(1), 97­104. doi:https://doi.org/10.1111/j.2044-8295.1937. tb00863.x. https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8295.1937. tb00863.x, URL https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8295. 1937.tb00863.x.
Bollen KA (1980). "Issues in the Comparative Measurement of Political Democracy." American Sociological Review, 45(3), 370­390. ISSN 00031224. URL http://www.jstor.org/stable/2095172.
Bollen KA (1989). Structural equations with latent variables. Wiley series in probability and mathematical statistics. Wiley, New York. ISBN 9780471011712.
Browne MW (1984). "Asymptotically distribution-free methods for the analysis of covariance structures." British Journal of Mathematical and Statistical Psychology, 37(1), 62­83. doi:https://doi. org/10.1111/j.2044-8317.1984.tb00789.x. https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/ 10.1111/j.2044-8317.1984.tb00789.x, URL https://bpspsychub.onlinelibrary.wiley.com/doi/ abs/10.1111/j.2044-8317.1984.tb00789.x.
Carbonelle P (2021). "PYPL PopularitY of Programming Language." [Online; accessed 10-April-2021], URL https://pypl.github.io/PYPL.html.
Daubechies I, Defrise M, De Mol C (2004). "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint." Communications on Pure and Applied Mathematics, 57(11), 1413­ 1457. doi:https://doi.org/10.1002/cpa.20042. https://onlinelibrary.wiley.com/doi/pdf/10.1002/ cpa.20042, URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20042.
43

REFERENCES
DiStefano C, Morgan GB (2014). "A Comparison of Diagonal Weighted Least Squares Robust Estimation Techniques for Ordinal Data." Structural Equation Modeling: A Multidisciplinary Journal, 21(3), 425­438. doi:10.1080/10705511.2014.915373. https://doi.org/10.1080/10705511.2014.915373, URL https: //doi.org/10.1080/10705511.2014.915373.
Drasgow F (2004). Polychoric and Polyserial Correlations. American Cancer Society. ISBN 9780471667193. doi:https://doi.org/10.1002/0471667196.ess2014. https://onlinelibrary.wiley. com/doi/pdf/10.1002/0471667196.ess2014, URL https://onlinelibrary.wiley.com/doi/abs/10. 1002/0471667196.ess2014.
Eaton ML (1983). Multivariate statistics: a vector space approach. Wiley series in probability and mathematical statistics. Wiley, New York. ISBN 9780471027768.
Efron B (1982). The Jackknife, the Bootstrap, and Other Resampling Plans. Society for Industrial and Applied Mathematics. ISBN 9780898711790.
Ellson J, Gansner ER, Koutsofios E, North SC, Woodhull G (2003). "Graphviz and dynagraph ­ static and dynamic graph drawing tools."
Fox J (2006). "TEACHER'S CORNER: Structural Equation Modeling With the sem Package in R." Structural Equation Modeling: A Multidisciplinary Journal, 13(3), 465­486. doi:10.1207/s15328007sem1303\ 7. https://doi.org/10.1207/s15328007sem1303_7.
Freedman DA (2006). "On The So-Called "Huber Sandwich Estimator" and "Robust Standard Errors"." The American Statistician, 60(4), 299­302. doi:10.1198/000313006X152207. https://doi.org/10.1198/ 000313006X152207, URL https://doi.org/10.1198/000313006X152207.
Gansner E, Koutsofios E, North S (2015). "Drawing graphs with dot." Accessed: 2021-05-05, URL https: //www.graphviz.org/pdf/dotguide.pdf.
Gupta AK, Nagar DK (2000). Matrix variate distributions. Number 104 in Chapman & Hall/CRC monographs and surveys in pure and applied mathematics. Chapman & Hall, Boca Raton, FL. ISBN 9781584880462.
Harville DA (1977). "Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems." Journal of the American Statistical Association, 72(358), 320­338. doi:10.1080/01621459. 1977.10480998. https://www.tandfonline.com/doi/pdf/10.1080/01621459.1977.10480998, URL https://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480998.
Hoyle RH (ed.) (2015). Handbook of structural equation modeling. Paperback ed edition. Guilford Press, New York, NY. ISBN 9781462516797 9781606230770. OCLC: 904462113.
Huber PJ (1967). "The behavior of maximum likelihood estimates under nonstandard conditions." Proc. 5th Berkeley Symp. Math. Stat. Probab., Univ. Calif. 1965/66, 1, 221-233 (1967).
Igolkina AA, Meshcheryakov G (2020). "semopy: A Python Package for Structural Equation Modeling." Structural Equation Modeling: A Multidisciplinary Journal, 27(6), 952­963. doi:10.1080/10705511.2019. 1704289. https://doi.org/10.1080/10705511.2019.1704289.
Igolkina AA, Meshcheryakov G, Gretsova MV, Nuzhdin SV, Samsonova MG (2020). "Multi-trait multilocus SEM model discriminates SNPs of different effects." BMC Genomics, 21(S8). doi:10.1186/ s12864-020-06833-2.
Jacobucci R, Brandmaier AM, Kievit RA (2019). "A Practical Guide to Variable Selection in Structural Equation Modeling by Using Regularized Multiple-Indicators, Multiple-Causes Models." Advances in Methods and Practices in Psychological Science, 2(1), 55­76. doi:10.1177/2515245919826527. URL https: //doi.org/10.1177/2515245919826527.
J¨oreskog K (1969). "A General Approach to Confirmatory Factor Analysis." Psychometrika, 34, 183­202. doi:10.1007/BF02289343.
44

REFERENCES
J¨oreskog KG (1967). "A GENERAL APPROACH TO CONFIRMATORY MAXIMUM LIKELIHOOD FACTOR ANALYSIS." ETS Research Bulletin Series, 1967(2), 183­202. doi: https://doi.org/10.1002/j.2333-8504.1967.tb00991.x. https://onlinelibrary.wiley.com/doi/pdf/10. 1002/j.2333-8504.1967.tb00991.x, URL https://onlinelibrary.wiley.com/doi/abs/10.1002/j. 2333-8504.1967.tb00991.x.
Joreskog KG, van Thiilo M (1972). "LISREL A GENERAL COMPUTER PROGRAM FOR ESTIMATING A LINEAR STRUCTURAL EQUATION SYSTEM INVOLVING MULTIPLE INDICATORS OF UNMEASURED VARIABLES." ETS Research Bulletin Series, 1972(2), i­71. doi: https://doi.org/10.1002/j.2333-8504.1972.tb00827.x. https://onlinelibrary.wiley.com/doi/pdf/10. 1002/j.2333-8504.1972.tb00827.x, URL https://onlinelibrary.wiley.com/doi/abs/10.1002/j. 2333-8504.1972.tb00827.x.
Kraft D (1988). "A software package for sequential quadratic programming." DFVLR-FB, 88(28).
Lippert C, Listgarten J, Liu Y, Kadie CM, Davidson RI, Heckerman D (2011). "FaST linear mixed models for genome-wide association studies." Nature Methods, 8(10), 833­835. doi:10.1038/nmeth.1681. URL https://doi.org/10.1038/nmeth.1681.
Liu X, Wall MM, Hodges JS (2005). "Generalized spatial structural equation models." Biostatistics, 6(4), 539­557. ISSN 1465-4644. doi:10.1093/biostatistics/kxi026. https://academic.oup. com/biostatistics/article-pdf/6/4/539/617114/kxi026.pdf, URL https://doi.org/10.1093/ biostatistics/kxi026.
Mardia KV, Marshall RJ (1984). "Maximum likelihood estimation of models for residual covariance in spatial regression." Biometrika, 71(1), 135­146. doi:10.1093/biomet/71.1.135. URL https://doi.org/ 10.1093/biomet/71.1.135.
Meredith W, Tisak J (1990). "Latent curve analysis." Psychometrika, 55(1), 107­122. doi:10.1007/ bf02294746.
Meurer A, Smith CP, Paprocki M, C ert´ik O, Kirpichev SB, Rocklin M, Kumar A, Ivanov S, Moore JK, Singh S, Rathnayake T, Vig S, Granger BE, Muller RP, Bonazzi F, Gupta H, Vats S, Johansson F, Pedregosa F, Curry MJ, Terrel AR, Roucka v, Saboo A, Fernando I, Kulal S, Cimrman R, Scopatz A (2017). "SymPy: symbolic computing in Python." PeerJ Computer Science, 3, e103. ISSN 2376-5992. doi:10.7717/peerj-cs.103. URL https://doi.org/10.7717/peerj-cs.103.
Neale MC, Hunter MD, Pritikin JN, Zahery M, Brick TR, Kirkpatrick RM, Estabrook R, Bates TC, Maes HH, Boker SM (2016). "OpenMx 2.0: Extended structural equation and statistical modeling." Psychometrika, 81(2), 535­549. doi:10.1007/s11336-014-9435-8.
Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E (2011). "Scikit-learn: Machine Learning in Python." Journal of Machine Learning Research, 12, 2825­2830.
Rosseel Y (2012). "lavaan: An R Package for Structural Equation Modeling." Journal of Statistical Software, 48(2), 1­36. URL https://www.jstatsoft.org/v48/i02/.
Schmidt M, Fung G, Rosales R (2007). "Fast Optimization Methods for L1 Regularization: A Comparative Study and Two New Approaches." In JN Kok, J Koronacki, RLd Mantaras, S Matwin, D Mladenic, A Skowron (eds.), Machine Learning: ECML 2007, pp. 286­297. Springer Berlin Heidelberg, Berlin, Heidelberg. ISBN 978-3-540-74958-5.
Seabold S, Perktold J (2010). "statsmodels: Econometric and statistical modeling with python." In 9th Python in Science Conference.
Searle SR, Casella G, McCulloch CE (1992). Variance components. Wiley series in probability and mathematical statistics. Wiley, New York. ISBN 9780471621621.
45

REFERENCES
Stein ML (1999). Interpolation of Spatial Data. Springer New York. doi:10.1007/978-1-4612-1494-6. URL https://doi.org/10.1007/978-1-4612-1494-6.
Storn R, Price K (1997). "Differential Evolution ­ A Simple and Efficient Heuristic for global Optimization over Continuous Spaces." Journal of Global Optimization, 11(4), 341­359. doi:10.1023/a:1008202821328. URL https://doi.org/10.1023/a:1008202821328.
Thompson WA (1962). "The Problem of Negative Estimates of Variance Components." The Annals of Mathematical Statistics, 33(1), 273 ­ 289. doi:10.1214/aoms/1177704731. URL https://doi.org/10. 1214/aoms/1177704731.
TIOBE (2021). "TIOBE Index." [Online; accessed 10-April-2021], URL https://www.tiobe.com/ tiobe-index/.
VanRaden P (2008). "Efficient Methods to Compute Genomic Predictions." Journal of Dairy Science, 91(11), 4414­4423. ISSN 0022-0302. doi:https://doi.org/10.3168/jds.2007-0980. URL https://www. sciencedirect.com/science/article/pii/S0022030208709901.
Virtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, Burovski E, Peterson P, Weckesser W, Bright J, van der Walt SJ, Brett M, Wilson J, Millman KJ, Mayorov N, Nelson ARJ, Jones E, Kern R, Larson E, Carey CJ, Polat I, Feng Y, Moore EW, VanderPlas J, Laxalde D, Perktold J, Cimrman R, Henriksen I, Quintero EA, Harris CR, Archibald AM, Ribeiro AH, Pedregosa F, van Mulbregt P, SciPy 10 Contributors (2020). "SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python." Nature Methods, 17, 261­272. doi:10.1038/s41592-019-0686-2.
Wang W (2013). "Identifiability of linear mixed effects models." Electronic Journal of Statistics, 7(none), 244 ­ 263. doi:10.1214/13-EJS770. URL https://doi.org/10.1214/13-EJS770.
Wright S (1921). "Correlation and causation." Agricultural Research, 20, 557­558. Wright S (1934). "The Method of Path Coefficients." The Annals of Mathematical Statistics, 5(3), 161­215.
doi:10.1214/aoms/1177732676. Yung YF, Yuan KH (2013). "Bartlett Factor Scores: General Formulas and Applications to Structural
Equation Models." In RE Millsap, LA van der Ark, DM Bolt, CM Woods (eds.), New Developments in Quantitative Psychology, pp. 385­401. Springer New York, New York, NY. ISBN 978-1-4614-9348-8. Zou H, Hastie T, Tibshirani R (2006). "Sparse Principal Component Analysis." Journal of Computational and Graphical Statistics, 15(2), 265­286. doi:10.1198/106186006X113430. https://doi.org/10.1198/ 106186006X113430, URL https://doi.org/10.1198/106186006X113430.
46

A TOY DATASETS
A Toy datasets
A reader can use results below to double-check output of semopy on their machine.
` Model '
In [ ]: from semopy import Model , examples

univariate_regression

In [ ]:

ex = examples.univariate_regression desc , data = ex.get_model(), ex.get_data() m = Model(desc) r = m.fit(data) print (m. inspect ())

Out [ ]: lval op rval Estimate

0

y

x -1.221069

1

y 

y 0.670367

Std. Err

z-value

0.083165 -14.682538

0.094804 7.071068

p-value 0.000000e+00 1.537437 e -12

univariate_regression_many

In [ ]:

ex = examples.univariate_regression_many desc , data = ex.get_model(), ex.get_data() m = Model(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:

lval 0y 1y 2y 3y

op rval  x1  x2  x3  y

Estimate 1.399551 0.450561 1.190470 0.878486

Std. Err 0.091138 0.097883 0.086499 0.124237

z-value 15.356385
4.603051 13.762839
7.071068

p-value 0.000000e+00 4.163465 e -06 0.000000e+00 1.537437 e -12

multivariate_regression

In [ ]:

ex = examples.multivariate_regression desc , data = ex.get_model(), ex.get_data() m = Model(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:

lval 0 y1 1 y1 2 y1 3 y2 4 y2 5 y2 6 y3 7 y3 8 y3 9 y1 10 y3 11 y2

op rval  x1  x2  x3  x1  x2  x3  x1  x2  x3  y1  y3  y2

Estimate -1.389754 -1.138405 -0.317893 -0.745837
1.074436 -1.130890
0.702778 1.235044 -0.920469 0.637755 0.488735 1.135729

Std. Err 0.073417 0.087966 0.072576 0.097974 0.117388 0.096851 0.064270 0.077006 0.063534 0.090192 0.069118 0.160616

z-value -18.929470 -12.941462
-4.380132 -7.612623
9.152855 -11.676597
10.934755 16.038334 -14.487925
7.071068 7.071068 7.071068

p-value 0.000000e+00 0.000000e+00 1.186073 e -05 2.686740 e -14 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.537437 e -12 1.537437 e -12 1.537437 e -12

47

A TOY DATASETS

political_democracy

In [ ]:

ex = examples.political_democracy desc , data = ex.get_model(), ex.get_data() m = Model(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:

lval op rval Estimate Std. Err

z-value p-value

0 dem60  ind60 1.482379 0.399024 3.715017 0.000203

1 dem65  ind60 0.571912 0.221383 2.583364 0.009784

2 dem65  dem60 0.837574 0.098446 8.507992

0.0

3

x1  ind60 1.000000

-

-

-

4

x2  ind60 2.180494 0.138565 15.736254

0.0

5

x3  ind60 1.818546 0.151993 11.96465

0.0

6

y1  dem60 1.000000

-

-

-

7

y2  dem60 1.256819 0.182687 6.879647

0.0

8

y3  dem60 1.058174 0.151521 6.983699

0.0

9

y4  dem60 1.265186 0.145151 8.716344

0.0

10

y5  dem65 1.000000

-

-

-

11

y6  dem65 1.185743 0.168908 7.020032

0.0

12

y7  dem65 1.279717 0.159996

7.99841

0.0

13

y8  dem65 1.266084 0.158238 8.001141

0.0

14 dem60  dem60 3.950849 0.920451 4.292296 0.000018

15 dem65  dem65 0.172210 0.214861 0.801494 0.422846

16 ind60  ind60 0.448321 0.086677 5.172345

0.0

17

y1 

y5 0.624423 0.358435 1.742083 0.081494

18

y1 

y1 1.892743 0.44456 4.257565 0.000021

19

y2 

y4 1.319589 0.70268 1.877937 0.06039

20

y2 

y6 2.156164 0.734155 2.936934 0.003315

21

y2 

y2 7.385292 1.375671 5.368501

0.0

22

y3 

y7 0.793329 0.607642 1.305585 0.191694

23

y3 

y3 5.066628 0.951722 5.323646

0.0

24

y4 

y8 0.347222 0.442234 0.785154 0.432363

25

y4 

y4 3.147911 0.738841 4.260605 0.00002

26

y6 

y8 1.357037

0.5685 2.387047 0.016984

27

y6 

y6 4.954364 0.914284 5.418843

0.0

28

x3 

x3 0.466732 0.090168 5.176276

0.0

29

y8 

y8 3.256389 0.69504 4.685182 0.000003

30

y7 

y7 3.430032 0.712732 4.812512 0.000001

31

y5 

y5 2.351910 0.480369 4.896044 0.000001

32

x2 

x2 0.119894 0.069747 1.718973 0.085619

33

x1 

x1 0.081573 0.019495 4.184317 0.000029

holzinger39

In [ ]:

ex = examples.holzinger39 desc , data = ex.get_model(), ex.get_data() m = Model(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:
0 1 2 3 4 5 6 7 8 9

lval op

rval

x1  visual

x2  visual

x3  visual

x4  textual

x5  textual

x6  textual

x7  speed

x8  speed

x9  speed

textual  textual

Estimate 1.000000 0.554421 0.730526 1.000000 1.113076 0.926120 1.000000 1.179980 1.082517 0.980034

Std. Err -
0.099727 0.10918 -
0.065392 0.055425
0.165045 0.151354 0.112145

z-value -
5.559413 6.691009
17.021522 16.709493
7.149459 7.152197 8.739002

p-value -
0.0 0.0
0.0 0.0
0.0 0.0 0.0

48

A TOY DATASETS

10 textual 

11 textual 

12 visual 

13

speed 

14

speed 

15

x8 

16

x3 

17

x7 

18

x9 

19

x5 

20

x6 

21

x4 

22

x2 

23

x1 

speed visual visual
speed visual
x8 x3 x7 x9 x5 x6 x4 x2 x1

0.173603 0.408277 0.808310 0.383377 0.262135 0.487934 0.843731 0.799708 0.565804 0.446208 0.356171 0.371117 1.133391 0.550161

0.049316 0.073527 0.145287 0.086171 0.056252 0.074167 0.090625 0.081387 0.070757 0.058387
0.04303 0.047712 0.101711 0.113439

3.520223 5.55273
5.563548 4.449045 4.659977 6.578856
9.31016 9.825966 7.996483 7.642264 8.277334 7.778264 11.143202
4.84983

0.000431 0.0 0.0
0.000009 0.000003
0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000001

` ModelMeans '

In [ ]:

from semopy import ModelMeans , examples

univariate_regression

In [ ]:

ex = examples.univariate_regression desc , data = ex.get_model(), ex.get_data() m = ModelMeans(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:

lval 0y 1y 2y

op rval Estimate  x -1.221251  1 -1.421354  y 0.670406

Std. Err 0.083167 0.082150 0.094810

z-value -14.684298 -17.301864
7.071068

p-value 0.000000e+00 0.000000e+00 1.537437 e -12

univariate_regression_many

In [ ]:

ex = examples.univariate_regression_many desc , data = ex.get_model(), ex.get_data() m = ModelMeans(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:

lval 0y 1y 2y 3y 4y

op rval  x1  x2  x3 1  y

Estimate 1.399662 0.450742 1.190553 1.120529 0.877835

Std. Err 0.091104 0.097847 0.086467 0.094777 0.124145

z-value 15.363298
4.606606 13.768895 11.822861
7.071068

p-value 0.000000e+00 4.092950 e -06 0.000000e+00 0.000000e+00 1.537437 e -12

multivariate_regression

In [ ]:

ex = examples.multivariate_regression desc , data = ex.get_model(), ex.get_data() m = ModelMeans(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:

49

A TOY DATASETS

lval 0 y1 1 y1 2 y1 3 y2 4 y2 5 y2 6 y3 7 y3 8 y3 9 y1 10 y2 11 y3 12 y1 13 y3 14 y2

op rval Estimate  x1 -1.389720  x2 -1.138398  x3 -0.317953  x1 -0.745740  x2 1.074526  x3 -1.130938  x1 0.702778  x2 1.235047  x3 -0.920454  1 -1.456621  1 0.929294  1 0.992040  y1 0.637755  y3 0.488724  y2 1.135630

Std. Err 0.073417 0.087966 0.072576 0.097969 0.117383 0.096847 0.064269 0.077005 0.063533 0.080268 0.107110 0.070266 0.090192 0.069116 0.160602

z-value -18.929022 -12.941384
-4.380965 -7.611965
9.154020 -11.677611
10.934870 16.038535 -14.487837 -18.147061
8.676035 14.118332
7.071068 7.071068 7.071068

p-value 0.000000e+00 0.000000e+00 1.181546 e -05 2.708944 e -14 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.537437 e -12 1.537437 e -12 1.537437 e -12

political_democracy

In [ ]:

ex = examples.political_democracy desc , data = ex.get_model(), ex.get_data() m = ModelMeans(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:

lval op rval Estimate Std. Err

z-value p-value

0 dem60  ind60 1.482999 0.399149 3.715401 0.000203

1 dem65  ind60 0.572322 0.221313 2.586027 0.009709

2 dem65  dem60 0.837346 0.098351 8.513859

0.0

3

x1  ind60 1.000000

-

-

-

4

x2  ind60 2.180375 0.13851 15.741609

0.0

5

x3  ind60 1.818522 0.15196 11.967144

0.0

6

y1  dem60 1.000000

-

-

-

7

y2  dem60 1.256753 0.182439 6.888611

0.0

8

y3  dem60 1.057746 0.151385 6.987136

0.0

9

y4  dem60 1.264790 0.145006 8.722314

0.0

10

y5  dem65 1.000000

-

-

-

11

y6  dem65 1.185687 0.16881 7.023806

0.0

12

y7  dem65 1.279531 0.159903 8.001935

0.0

13

y8  dem65 1.265935 0.15811

8.00667

0.0

14

x1 

1 5.054393 0.084062 60.126661

0.0

15

x2 

1 4.792218 0.17327 27.657545

0.0

16

x3 

1 3.557715 0.161233 22.065738

0.0

17

y1 

1 5.464665 0.301854 18.10368

0.0

18

y2 

1 4.256439 0.44987 9.461478

0.0

19

y3 

1 6.563146 0.375891 17.460248

0.0

20

y4 

1 4.452537 0.38391 11.597855

0.0

21

y5 

1 5.136254 0.300511 17.091727

0.0

22

y6 

1 2.978057 0.385931 7.716551

0.0

23

y7 

1 6.196289 0.377202 16.426972

0.0

24

y8 

1 4.043383 0.371319 10.889254

0.0

25 dem60  dem60 3.956039 0.921185 4.294508 0.000018

26 dem65  dem65 0.172487 0.214803 0.803001 0.421974

27 ind60  ind60 0.448436 0.086692 5.172765

0.0

28

y1 

y5 0.623671 0.358319 1.740548 0.081763

29

y1 

y1 1.891402 0.444423 4.255861 0.000021

30

y2 

y4 1.313085 0.70198 1.870545 0.061408

31

y2 

y6 2.152828 0.733771 2.933925 0.003347

32

y2 

y2 7.372791 1.373882 5.366395

0.0

33

y3 

y7 0.794961 0.607702 1.308143 0.190825

34

y3 

y3 5.067487 0.951738 5.324453

0.0

35

y4 

y8 0.348246 0.442238 0.787464 0.43101

36

y4 

y4 3.147907 0.738783 4.260936 0.00002

37

y6 

y8 1.356165 0.568281 2.386434 0.017013

50

A TOY DATASETS

38

y6 

y6 4.953952 0.914241 5.418647

0.0

39

x3 

x3 0.466708 0.090157 5.176583

0.0

40

y8 

y8 3.254068 0.694596 4.684837 0.000003

41

y7 

y7 3.431334 0.712843 4.813589 0.000001

42

y5 

y5 2.350969 0.480238 4.895427 0.000001

43

x2 

x2 0.119802 0.069721 1.718314 0.085739

44

x1 

x1 0.081551 0.01949 4.184259 0.000029

holzinger39

In [ ]:

ex = examples.holzinger39 desc , data = ex.get_model(), ex.get_data() m = ModelMeans(desc) r = m.fit(data) print (m. inspect ())

Out [ ]:

lval op

rval Estimate Std. Err z-value p-value

0

x1  visual 1.000000

-

-

-

1

x2  visual 0.553493 0.099663

5.55366

0.0

2

x3  visual 0.729357 0.109106 6.684861

0.0

3

x4  textual 1.000000

-

-

-

4

x5  textual 1.113076 0.06542 17.014187

0.0

5

x6  textual 0.926147 0.055449 16.702625

0.0

6

x7  speed 1.000000

-

-

-

7

x8  speed 1.179973 0.164992 7.151679

0.0

8

x9  speed 1.081572 0.151176 7.154404

0.0

9

x1 

1 4.935774 0.067178 73.47271

0.0

10

x2 

1 6.088037 0.067754 89.854497

0.0

11

x3 

1 2.250415 0.06508 34.57913

0.0

12

x4 

1 3.060910 0.066987 45.694203

0.0

13

x5 

1 4.340535 0.074258 58.45232

0.0

14

x6 

1 2.185574 0.063044 34.667251

0.0

15

x7 

1 4.185915 0.062695 66.76601

0.0

16

x8 

1 5.527082 0.058269 94.854886

0.0

17

x9 

1 5.374125 0.05807 92.546353

0.0

18 textual  textual 0.979483 0.112105 8.737173

0.0

19 textual  speed 0.173487 0.049313 3.518059 0.000435

20 textual  visual 0.408245 0.073525 5.552498

0.0

21 visual  visual 0.809338 0.145464 5.563848

0.0

22

speed 

speed 0.383726 0.086207 4.451217 0.000009

23

speed  visual 0.262232 0.056277 4.659693 0.000003

24

x8 

x8 0.487697 0.074193 6.573326

0.0

25

x3 

x3 0.844326 0.090622 9.316999

0.0

26

x7 

x7 0.799415 0.081382 9.822992

0.0

27

x9 

x9 0.566112 0.070737 8.003081

0.0

28

x5 

x5 0.446256 0.058393 7.642312

0.0

29

x6 

x6 0.356202 0.043035 8.277039

0.0

30

x4 

x4 0.371174 0.047718 7.778523

0.0

31

x2 

x2 1.133843 0.101723 11.146333

0.0

32

x1 

x1 0.549053 0.113601

4.83318 0.000001

51

B GRADIENT OF MATRIX-VARIATE NORMAL LOGLIKELIHOOD

B Gradient of matrix-variate normal loglikelihood

l() = tr{L}tr{T -1ZT L-1Z} + m ln |T | + n ln |L| - nm ln tr{L}

(37)

We are seeking to find a derivative of l() from Equation 37 with respect to a parameter i from the vector . Next, we are using the product rule for the differentiation and the cyclic permutation property of the trace operator:

 l() = tr{ L }tr{T -1ZT L-1Z} + tr{L}tr{T -1 ZT L-1Z + T -1ZT L-1 Z }-

i

i

i

i

-tr{L}tr{T -1 T T -1ZT L-1Z + T -1ZT L L-1 L-1Z}+

i

i

Ai

Bi

+mtr{T

-1

T

}

+

ntr{

L

L-1}

-

nm

tr{

L i

}

i

i

tr{L}

Ai

Bi

Notice that

tr{T -1 ZT L-1Z + T -1ZT L-1 Z } = tr{T -1 ZT L-1Z} + tr{T -1ZT L-1 Z } =

i

i

i

i

tr{ZT L-1 Z T -1} + tr{T -1ZT L-1 Z } = tr{T -1ZT L-1 Z } + tr{T -1ZT L-1 Z } =

i

i

i

i

2tr{T -1ZT L-1
C0

Z i

}

=

2tr{C0

Z i

},

here, we have used the invariance of trace under transposition and cyclic permutations properties. We continue to simplify the derivative:



L

Z

i l() = tr{ i }tr{C0Z} + 2tr{L}tr{C0 i }-

C1

-tr{L}tr{Ai

C0

Z

+Bi

Z

C0}

+

mtr{Ai

}

+

ntr{Bi}

-

nm

tr{

L i

}

tr{L}

C1

C2

The final compact form of the gradient:



L

Z

i

l()

=

tr{

i

}tr{C1}

+

2tr{L}tr{C0 i } - tr{L}tr{Ai +mtr{Ai} + ntr{Bi}

C1 + BiC2}+

-

nm

tr{

L i

}

,

tr{L}

(38)

where C0 = T -1ZT L-1, C1 = C0Z, C2 = ZC0 are matrices that need to be calculated only once, and

Ai

=

T

-1

T i

,

Bi

=

L i

L-1

have

to

be

calculated

for

each

of

elements

of

the

parameter

vector

.

52

C FISHER INFORMATION MATRIX

C Fisher information matrix

For the task of deriving Fisher Information Matrix (FIM) for the matrix-variate distribution, we use the fact that vec(Z)  MN (M, L, T ) iff Z  N (vec(M ), T  L), where  is a Kronecker product and vec is a vectorization operator. The result for multivariate-normal FIM I() is known. If x  N (µ, ), then

I ()i,j

=

µT i

-1 µ k

+

1 tr
2

-1  -1  i k

(39)

One could compute I() for Z just by using the Equation 39 by plugging µ = vec(M ),  = T × L, however,
we don't have actual L or T available, therefore  is not known. Instead, we are working with some model-implied matrices L = cov(Z), T = cov(ZT ). This is a problem, as for matrix-variate distributions,
L = tr{T }L, T = tr{L}T . Although we can't estimate tr{T }, tr{L}, it is not necessary: this is completely
similar to the problem we've encountered when computing likelihood function. Firstly, we observe that tr{T } = tr{L}tr{T }  tr{L} = tr{T } . Secondly, we substitute
tr{T }

1

tr{T }

L=

L, T =

T

tr{T }

tr{T }

So the  will have the following form:

tr{T }

1

1

=T L=

T

L=

T L

(40)

tr{T }

tr{T }

tr{T }

Most importantly, this straightforward approach would be very inefficient: each component of I is computed in O(n3m3).
Next, we'll show that FIM can be computed very efficiently. First, we analyze the trace term of the Equation 39, and then tackle the first quadratic term. We shall use the following 5 properties of the Kronecker product and vec operator:

1. Inverse of Kronecker product:

(A  B)-1 = A-1  B-1

2. Mixed product:

(A  B)(C  D) = (AC)  (BD)

3. Trace of Kronecker product:

tr{A  B} = tr{A}tr{B}

4. Link of Kronecker product to vectorization operator: vec(ABC) = (CT  A)vec(B)

5. Link between vectorization operator and dot product: vec(AT )vec(B) = tr{AT B}

Trace term

We start off by rewriting the trace term in terms of  from Equation 40. First, we use the first property to

show that

-1 = tr{T }T -1  L-1

Second,

we

shall

use

the

product

rule

for

differentiation

to

obtain

an

expression

for

 i

:

 1 =

T

L+T



L

-

tr{

T i

}

T

L

i tr{T } i

i tr{T }

53

C FISHER INFORMATION MATRIX

And

similarly

for

 k

:



1

=

T

L+T 

L

-

tr{

T k

}

T



L

k tr{T } k

k tr{T }

Now, for brevity, we rename some parts in the trace term as C1 and C2:









1

tr

 
-1





2  i

-1



 

k 

=

1 2 tr{C1C2}

(41)









C1

C2

We expand C1:

C1 = T -1  L-1

T

L+T



L

-

tr{

T i

}

T

L

i

i tr{T }

=

= (T -1  L-1)

T L

+ (T -1  L-1)

L T

-

tr{

T i

}

(T

-1



L-1)(T



L)

i

i tr{T }

C11

C12

C13

We will take advantage of the mixed product property 2 to tackle each of the terms individually:

C11 = (T -1  L-1)

T L
i

=

T -1 T i

 L-1L =

T -1 T i

 In

C12 = (T -1  L-1)

L T
i

= (T -1T )  L-1 L i

= Im 

L-1 L i

C13

=

tr{

T i

}

(T

-1

tr{T }

 L-1)(T

 L)

=

tr{

T i

}

(T

-1T

)



(L-1

L)

tr{T }

=

=

tr{

T i

}

tr{T }

Im



In

=

tr{

T i

}

tr{T }

Inm

In the latter case of C13, we used the trivial fact that Im  In = Inm.

For an extra brevity, we also rename replace some terms with a singular matrices, namely Ai =

T

-1

T i

,Ak

=

T

-1

T k

,

Bi

=

L-1

L i

and

Bk

=

L-1

L k

.

We also

denote scalars

tr{

T i

}

tr{T }

as

i

and

tr{

T k

}

tr{T }

as

k .

So, after the above simplifications, C1 takes the following form:

Likewise, we can infer C2:

C1 = Ai  In + Im  Bi - iInm

C2 = Ak  In + Im  Bk - kInm

54

C FISHER INFORMATION MATRIX

We substitute the obtained results for C1, C2 to the Equation 41 and expand it:

1

1

2 tr{C1C2} = 2 tr{(Ai  In + Im  Bi - iInm) (Ak  In + Im  Bk - kInm)} =

1 = 2 tr{(Ai  In)(Ak  In) + (Ai  In)(Im  Bk) - k(Ai  In)Inm+

+(Im  Bi)(Ak  In) + (Im  Bi)(Im  Bk) - k(Im  Bi)Inm-

1

-iInm(Ak



In)

-

iInm(Im



Bk )

+

i k Inm }

=

tr{ 2

(Ai  In)(Ak  In) + (Ai  In)(Im  Bk) - k(Ai  In)+

+(Im  Bi)(Ak  In) + (Im  Bi)(Im  Bk) - k(Im  Bi)-

1

-i(Ak



In)

-

i(Im



Bk )

+

i k Inm }

=

( 2

tr{(Ai  In)(Ak  In)} + tr{(Ai  In)(Im  Bk)} - ktr{Ai  In} +

D1

D2

D3

+ tr{(Im  Bi)(Ak  In)} + tr{(Im  Bi)(Im  Bk)} - ktr{(Im  Bi)} -

D4

D5

D6

itr{Ak  In} - itr{Im  Bk} + iktr{Inm}) =

D7

D8

D9

1 = 2 (D1 + D2 - D3 + D4 + D5 - D6 - D7 - D8 + D9)

Now, we shall deal with each of the {Dk}k=1..9 terms. For D1, D2, D4, D5 we use the second property followed by the third property. As for terms D3, D6,
D7, D8, just the third property suffices for them, and D9 is trivial.

D1 = tr{(Ai  In)(Ak  In)} = tr{(AiAk)  In} = ntr{AiAk} D2 = tr{(Ai  In)(Im  Bk)} = tr{Ai  Bk} = tr{Ai}tr{Bk}
D3 = ktr{Ai  In} = nktr{Ai} D4 = tr{(Im  Bi)(Ak  In)} = tr{Ak  Bi} = tr{Ak}tr{Bi}
D5 = tr{(Im  Bi)(Im  Bk)} = mtr{BiBk} D6 = ktr{Im  Bi} = mktr{Bi} D7 = itr{Ak  In} = nitr{Ak} D8 = itr{Im  Bk} = mitr{Bk} D9 = iktr{Inm} = nmik
So, the trace term takes the final form of

1

2 (ntr{AiAk} + mtr{BiBk} + tr{Ai}tr{Bk} + tr{Ak}tr{Bi}+

(42)

+nmik - nktr{Ai} - mktr{Bi} - nitr{Ak} - mitr{Bk})

Quadratic term

Let's rewrite the quadratic term in L, T, M matrices:

µT -1 µ

vec(M )T = tr{T }

T -1  L-1

vec(M )

i k

i

k

55

C FISHER INFORMATION MATRIX

First, we use the fourth property on

T -1  L-1

vec(M k

)

:

vec(M )T tr{T }

T -1  L-1

vec(M )

vec(M )T

= tr{T }

vec

L-1 M T -1

i

k

i

k

Second, we use the fifth property and obtain the final form for the quadratic term:

vec(M )T

tr{T }

vec

L-1 M T -1

= tr{T }tr

M T L-1 M T -1

(43)

i

k

i

k

Final result
We combine Equations 42 and 43 to obtain the final representation for Fisher Information Matrix:

I()i,j = tr{T }tr

M T L-1 M T -1

i

i

1 + 2 (ntr{AiAk} + mtr{BiBk}+

,

(44)

+tr{Ai}tr{Bk} + tr{Ak}tr{Bi} + nmik - nktr{Ai}-

-mktr{Bi} - nitr{Ak} - mitr{Bk})

where

Ai

=

T

-1

T i

,Ak

=

T

-1

T k

,

Bi

=

L-1

L i

,

Bk

=

L-1

L k

,

i

=

tr{

T i

}

tr{T }

and

k

=

tr{

T k

}

tr{T }

.

The complexity of computing the Equation 44 is O(n3 + m3 + nm2 + mn2). The most practically

troublesome term of complexity n3 arises from the necessity of computing Bi, Bk. This is, however, a big improvement on the original complexity of O(n3m3).

56

D GENERALIZED LATENT FACTOR PREDICTION SCHEME

D Generalized latent factor prediction scheme

We propose a MAP-like (maximum a posteriori) approach for estimating factor scores. The core idea is to express the joint density f (Z, H) of observed variables and latent variables through a product of conditional densitiy f (Z|H) and an unconditional density f (H) of latent variables H:

f (Z, H) = f (Z|H)f (H)  ln f (z, H) = ln f (z|H) + ln f (H),

(45)

and then, maximizing it with respect to H. The task of deriving those densities is cumbersome, however.



H

 
X

= W = 1G + BW + E

 Z = 2G + W +  +

p i=1

Ui



H 

=

F1 C (1 G

+

E)



X = F2C(1G + E)

,

 Z

=

2G

+

H H

+

X X

+



+

p i=1

Ui

Here, Ui  MN(0, Di, Ki), F1 = In 0n,nx , F2 = 0nx,n Inx , and 0a,b is a zero matrix of shape a × b.

Derivation of lnf (z|H)
First, we infer Z from the system of equations under the assumption of known H matrix:

X = F2C(1G + E)

Z = 2G + H H + X X +  +

p i=1

Ui

X = F2C(1G + E) Z = (2 + X F2C1)G + H H + X F2CE +  +

Expectation of Z:

E[Z|H] = (2 + X F2C1)G + H H

Covariance matrix across rows (across variables):

p
LZ|H = nX F2CCT F2T TX + n + tr{Ki}Di
i=1

Covariance matrix across columns (across observations):

p

TZ|H = E[ET CT F2T TX X F2C E] + tr{}In + tr{Di}Ki =

A0

i=1

p

= tr{A0}In + tr{}In + tr{Di}Ki

i=1

The formula for ln f (z|H) (constants omitted):

p i=1

Ui

ln f (z|H) = tr{LZ|H }tr{TZ-|1H (Z - E[Z|H])T L-Z|1H (Z - E[Z|H])}

(46)

We rearrange the term Z - E[Z|H] this way:

Z - E[Z|H] = Z - (2 + X F2C1)G -H H = MH - H H
MH

57

D GENERALIZED LATENT FACTOR PREDICTION SCHEME

With this rearrangement in mind, we can now more easily expand the formula 46 (it will be helpful later):

ln f (z|H) = tr{LZ|H }tr{TZ-|1H (MH - H H)T L-Z|1H (MH - H H)} = = tr{LZ|H }tr{(MHT - HT TH )T (L-Z|1H MH TZ-|1H - L-Z|1H H HTZ-|1H )} =
= tr{LZ|H }tr{MHT L-Z|1H MH TZ-|1H - MHT L-Z|1H H HTZ-|1H - -HT TH L-Z|1H MH TZ-|1H + HT TH L-Z|1H H HTZ-|1H }

(47)

To make Equation 47 look less intimidating, we group some matrices together and substitute them with

single matrices:

A = tr{LZ|H }TH L-Z|1H MH TZ-|1H A0 = tr{LZ|H }TH L-Z|1H H

(48)

Then, the Equation 47 transforms to:

ln f (z|H) = tr{MHT LZ-|1H MH TZ-|1H - AT H - HT A + HT A0HTZ-|1H }

(49)

Derivation of ln f (H)

H = F1C1G + F1CE

(50)

From Equation 50, we can infer expectation of H and both of the covariance matrices. Expectation of H: E[H] = M = F1C1G

Covariance matrix across rows (variables):

LH = nF1CCT F1T Covariance matrix across columns (observations):

TH = E[ET CT F1T F1CE] = tr{CT F1T F1C}In

(51)

Remember, that we did make independence assumption when talking about latent variables. Hence,

as it can be seen from both model and the Equation 51, the matrix-variate distribution here degenerates

straightforwardly to a multivariate normal with LH covariance matrix and a zero mean. Constants n and

tr{CT F1T F1C}

will

canceled

out,

therefore,

for

the

sake

of

simplicity,

we

shall

use

normalized

by

1 n

matrix

LH instead:

LH

=

1 n LH

=

F1CCT F1T

Then, the loglikelihood lnf (H) is:

ln f (H) = tr{(H - M )T L-H1(H - M )} = tr{(HT - M T )(L-H1H - L-H1M )} = = tr{HT L-H1H - HT L-H1M - M T L-H1H + M T L-H1M }

(52)

Here, we collect only one term into a single matrix, namely, L-H1M = A1:

ln f (H) = tr{HT L-H1H - HT A1 - AT1 H + M T A1}

(53)

Estimation of latent factors

We can hope to get analytical expression for factor scores estimates. For it to happen, solution to the

problem of maximizaiton of the Equation 45 should have closed form, or, equivalently, we should solve the

system of equations:

lnf (Z, H) lnf (Z|H) lnf (H)

=

+

=0

(54)

H

H

H

To

compute

derivatives

lnf (Z|H) H

,

lnf (H) H

,

one

can

refer

to

matrix

calculus

handbooks,

such

Matrix

Calculus by G. Golub, or simply to the Matrix Cookbook.

58

D GENERALIZED LATENT FACTOR PREDICTION SCHEME

So, the Equation 54 becomes:

lnf (Z|H) H

=

2A0 H TZ-|1H

-

2A

lnf (H) H

=

2L-H1H

-

2A1

2A0HTZ-|1H - 2A + 2L-H1H - 2A1 = 0  A0HTZ-|1H + L-H1H = A + A1

(55)

Let's multiply left and right sides of the Equation 55 by A-0 1:

H TZ-|1H + A-0 1L-H1 H = A-0 1 (A + A1) = A

=-A3

=A2

(56)

The Equation 56 is actually a form of the Sylvester equation:

A2H - HA3 = A

(57)

Sylvester equations, like in 57, can be solved in O(n3 + m3) with Bartels­Stewart algorithm.

If

there

are

no

random

effects

in

the

model,

TZ-|1H

is

effectively

1 tr{}

In.

With

that

in

mind,

we

can

solve

Equation 55 just like we solve regular systems of linear equations:

H=

1 tr{}

A0

+

L-H1

-1
(A1 + A)

(58)

59

E MULTIVARIATE BLUP

E Multivariate BLUP

Assume that we have already estimated model parameters  that reside in covariance matrices L, T, {Di}i=1..p, {Ki}i and mean components matrix M . We would like to estimate i-th random effect. We can deduce it in the same fashion as we did when predicting latent factors, i.e. by using the MAP-estimator.

f (Z, U |) = f (Z|U, )f (U |)  ln f (Z, U |) = ln f (Z|U, ) + ln f (U |),

(59)

where

ln f (Z, U |)  tr{T -1(Z - M -U )T L-1(Z - M -Ui)} =

Z

Z

(60)

= tr{T -1(Z - Ui)T L-1(Z - Ui)},

where L and T are computed without the i-th Ui.

ln f (U |)  tr{Ti-1U T L-i 1U }, Ti = tr{Di}Ki, Li = tr{Ki}Di

Maximizing

Equation

59

with

respect

to

U

is

the

same

problem

as

inferring

U

from

 U

ln f (Z, U |)

=

0.

Before computing the derivative, we start off by expanding the Equation 60:

ln f (Z, U |)  tr{T -1(Z - Ui)T L-1(Z - Ui)} = tr{(ZT - UiT )(L-1ZT -1- -L-1UiT -1)} = tr{ZT L-1ZT -1 + UiT L-1UiT -1 - 2UiT L-1ZT -1}

Here, we used the trace invariance under transposition and cyclic permutation properties to to combine two negative terms into 2ZT L-1UiT -1.
Next, we compute the derivative of the Equation 59 and set it to zero:

 U

ln f (Z, U |)

=

2L-1UiT -1

+

2L-i 1UiTi-1

-

2L-1ZT -1

=

0



L-1UiT -1 + L-i 1UiTi-1 = L-1ZT -1

(61)

We multiply both sides of the Equation 61 by Ti from the right and by L from the left:

UiT -1Ti-1 + LL-i 1Ui = ZT -1Ti

(62)

Again, we've obtained a from of the Sylvester Equation that is solvable by Stewart-Bartels algorithm. Notice, that the Equation 62 has been obtained for the case of ` ModelGeneralizedEffects '. It is
easy to restrict it to the case of ` ModelEffects ': the f (Z|U, ) will degenerate to multivariate-normal distribution (as T becomes proportional to In), and the resulting BLUP estimator is

U T -1Tu-1 + LL-u 1Uu = ZTu, Lu = tr{K}D, Tu = tr{D}K

60

F NUMERICAL EXPERIMENTS TABLES

F Numerical experiments tables

n exo

2

3

4

5

6

7

8

N MAPE N MAPE N MAPE N MAPE N MAPE N MAPE N MAPE

Model[FIML]

19 13.27 12 13.02 17 13.10 8 12.85 13 12.25 14 12.18 7 12.07

Model[MLW]

18 13.34 10 13.08 17 13.23 10 12.94 14 12.29 15 12.30 10 12.12

ModelMeans[ML] 18 13.34 10 13.08 18 13.23 10 12.94 14 12.29 14 12.30 10 12.12

ModelMeans[REML] 24 16.75 21 16.83 34 16.89 31 17.50 31 17.37 30 16.91 35 17.25

Table 8: N and MAPE for the subset B: varying number of exogenous variables.

n endo

2

3

4

5

6

7

N MAPE N MAPE N MAPE N MAPE N MAPE N MAPE

Model[FIML]

24 13.05 17 13.28 18 12.87 20 12.56 17 12.79 15 13.28

Model[MLW]

22 13.14 17 13.43 18 12.99 20 12.68 18 12.87 17 13.34

ModelMeans[ML] 22 13.14 18 13.43 18 12.99 20 12.68 17 12.87 17 13.34

Table 9: N and MAPE for the subset C: varying number of endogenous variables.

n lat

1

2

3

4

5

6

N MAPE N MAPE N MAPE N MAPE N MAPE N MAPE

Model[FIML]

5 12.45 17 13.28 7 14.20 35 14.22 47 14.49 82 15.45

Model[MLW]

4 12.58 17 13.43 11 14.27 35 14.27 53 14.40 83 15.51

ModelMeans[ML] 4 12.58 18 13.43 10 14.27 33 14.25 48 14.42 79 15.51

Table 10: N and MAPE for the subset D: varying number of latent variables.

n cycles

1

2

3

4

N MAPE N MAPE N MAPE N MAPE

Model[FIML]

60 15.09 93 17.46 148 18.00 180 20.10

Model[MLW]

70 15.39 96 17.87 161 18.37 191 20.67

ModelMeans[ML] 57 15.30 86 17.65 147 17.95 179 20.49

Table 11: N and MAPE for the subset E: varying number of cycles.

n

50

100

150

200

250

300

350

400

N MAPE N MAPE N MAPE N MAPE N MAPE N MAPE N MAPE N MAPE

Model[FIML]

47 19.15 17 13.28 5 11.08 2 9.25 2 8.26 3 7.24 1 6.59 1 6.43

Model[MLW]

53 19.49 17 13.43 6 11.10 2 9.20 3 8.19 2 7.18 1 6.62 1 6.39

ModelMeans[ML] 53 19.48 18 13.43 7 11.16 2 9.30 3 8.30 3 7.28 1 6.68 1 6.44

Table 12: N and MAPE for the subset F: varying number of data samples.

n lat

2

3

4

5

6

 Time, s  Time, s  Time, s  Time, s  Time, s  = 0.75 0.08 0.49 0.17 1.14 0.27 3.02 0.43 6.21 0.60 14.46  = 1 0.07 0.41 0.17 0.92 0.27 2.50 0.42 4.96 0.57 12.10  = 1.25 0.07 0.35 0.16 0.83 0.27 2.09 0.40 4.18 0.54 10.63

Table 13:  and working time for the subset A: varying number of latent factors.

61

F NUMERICAL EXPERIMENTS TABLES

n

50

100

150

200

250

300

350

 Time, s  Time, s  Time, s  Time, s  Time, s  Time, s  Time, s  = 0.75 0.26 0.85 0.21 0.99 0.19 1.08 0.17 1.14 0.18 1.37 0.18 1.46 0.16 1.44  = 1 0.24 0.74 0.20 0.82 0.18 0.91 0.17 0.96 0.18 1.08 0.16 1.19 0.15 1.18  = 1.25 0.23 0.57 0.18 0.70 0.18 0.74 0.16 0.78 0.18 0.88 0.16 0.96 0.15 1.01

Table 14:  and working time for the subset B: varying number of data samples.

n exo

5

10

15

20

25

30

35

40

 Time, s  Time, s  Time, s  Time, s  Time, s  Time, s  Time, s  Time, s  = 0.75 0.44 0.73 0.26 0.96 0.20 1.27 0.17 1.17 0.16 1.40 0.13 1.52 0.14 1.52 0.15 1.70  = 1 0.42 0.63 0.25 0.75 0.19 0.96 0.17 0.90 0.15 1.11 0.12 1.13 0.14 1.23 0.14 1.45  = 1.25 0.41 0.51 0.23 0.64 0.18 0.80 0.16 0.82 0.15 0.93 0.12 1.01 0.14 0.97 0.14 1.17

Table 15:  and working time for the subset C: varying number of exogenous variables.

k

0.5

1.0

2.0

4.0

8.0

16.0

32.0

64.0

 Time, s  Time, s  Time, s  Time, s  Time, s  Time, s  Time, s  Time, s  = 1.25 1.10 0.19 0.78 0.27 0.48 0.43 0.28 0.57 0.19 0.85 0.15 1.04 0.15 1.54 0.17 1.95

Table 16:  and working time for the subset D: varying k multiplier.

In Tables 13-16,  is a value of regularization constant for SPCA.

62

