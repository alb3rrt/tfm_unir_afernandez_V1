
# Parallelizing Thompson Sampling

[arXiv](https://arxiv.org/abs/2106.01420), [PDF](https://arxiv.org/pdf/2106.01420.pdf)

## Authors

- Amin Karbasi
- Vahab Mirrokni
- Mohammad Shadravan

## Abstract

How can we make use of information parallelism in online decision making problems while efficiently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision making problems, namely, stochastic multi-arm bandit and linear contextual bandit with finitely many arms. Over a time horizon $T$, our \textit{batch} Thompson Sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only $O(\log T)$ batch queries. To achieve this exponential reduction, i.e., reducing the number of interactions from $T$ to $O(\log T)$, our batch policy dynamically determines the duration of each batch in order to balance the exploration-exploitation trade-off. We also demonstrate experimentally that dynamic batch allocation dramatically outperforms natural baselines such as static batch allocations.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/parallelizing-thompson-sampling](https://paperswithcode.com/paper/parallelizing-thompson-sampling)

## Bibtex

```tex
@misc{karbasi2021parallelizing,
      title={Parallelizing Thompson Sampling}, 
      author={Amin Karbasi and Vahab Mirrokni and Mohammad Shadravan},
      year={2021},
      eprint={2106.01420},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

