arXiv:2106.01134v1 [cs.AI] 2 Jun 2021

Smooth Q-learning: Accelerate Convergence of Q-learning Using Similarity
Wei Liaoa,b, Xiaohui Wei a,b, and Jizhou Laic
a Key laboratory of Fundamental Science for National Defense-Advanced Design Technology of Flight Vehicle, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China
bState Key Laboratory of Mechanics and Control of Mechanical Structures, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China
cCollege of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China
Abstract An improvement of Q-learning is proposed in this paper. It is different from classic Q-learning in that the similarity between different states and actions is considered in the proposed method. During the training, a new updating mechanism is used, in which the Q value of the similar stateaction pairs are updated synchronously. The proposed method can be used in combination with both tabular Q-learning function and deep Q-learning. And the results of numerical examples illustrate that compared to the classic Q-learning, the proposed method has a significantly better performance.
Keywards: Reinforcement learning; Q-learning; Similarity
1 Introduction
Reinforcement learning aims to learn good policies for sequential decision problems [1] [2]. By maximizing the cumulative future reward, the performance of policies enhance as the training steps increase. Qlearning [3] is set of the most popular model-free reinforcement learning algorithms. During the training, the state-action value function, known as Q value, is updated and the action with the biggest Q value is choosen at every time step.
A key point in the design of a Q-learning algorithms is the choice of a structure to store estimates of qualities [4]. In the original Q-learning method, a tabular-value function is used to represent the Q value [4]. In the tabular-value function, one element represents the Q value of oen state-action pair [5]. In paper [6], a box-pushing problem is solved via the Q-learning method with tabular-value function. And in some examples of book [7], tabular-value function is used and excellent results were achieved. However, the size of the table may be considerable because of the excessive amount of memory needed to store the table [8]. In order to deal with the continuous state space, some approximation methods are taken. The linear combination of some linear function of state is used to express the Q value in [9]. With the development of deep learning, neural network (NN) with several layers of nodes, called deep Q-network (DQN), is used to build up progressively more abstract representations of the Q value [10] [1]. For the past few years, with the going deep of the research work, some improved form of DNQ were proposed. In order to overcome the overestimation of action values caused by insufficiently flexible function approximation a new new mechanism of evaluating the action values was proposed in [11]. A dueling architecture was proposed in [12], where the Q value estimates is represented as the combination of the state-value and the advantages for each action. In order to make the most effective use of the replay memory for learning, instead of random sampling from the replay memory, paper [13] proposed a new sampling method based on priority. Recently, the generative antagonistic network (GAN) is also used in Q-learning, paper [14] proposed GAN Q-learning and analyzed its performance in simple tabular environments.
Corresponding author: wei xiaohui@nuaa.edu.cn
1

Q-learning has been applied successfully in many engineering fields [15] [16] [17]. However, there are still some shortcomings. Tabular Q-learning can only solve the problems with limited and discrete states and actions, too large amounts of states or actions always leads to an extended period training time. Although DQN can deal with the problems with continuous state space, it can only handle discrete action spaces. An obvious approach to adapting DQN to continuous domains is to simply discretize the action space [18]. This approach has a limitation: The more parts the action space is discretized into, the more time is spent in training. A truth is that, intuitively, taking similar actions at similar states always obtain roughly equal cumulative future reward. Based on this truth, an improvement of Q-learning is proposed in this paper: When a Q value of a state-action pair is updated, the Q value of the similar state-action pairs are updated synchronously. It is worth pointing out that the proposed method can be used in combination with both tabular-value function and DQN.
The rest of the paper is organized as follows. Background and some preliminaries are introduced in Section 2. Section 3 presents the details of the proposed method. Some numerical examples are taken to demonstrate the effectiveness of our method in Section 4. The results are summarized in Section 5.

2 Background

Consider a standard reinforcement learning problem consisting of an agent interacting with an environment E in discrete timesteps. At each timestep t the agent receives an observation st  S, takes an action at  A and receives a scalar reward rt.
An agent's behavior is defined by a policy, , which maps from state space S to action space A,  : S  A. The environment E can be modeled as a Markov decision process (MDP) with a state space S, an action space A, a transition dynamics st+1 = f (st, at) and a reward function rt = r(st, at). The Q value function is used in many reinforcement learning algorithms. It describes the expected return after taking an action a in state s and thereafter following policy :



Q(s, a) =

trt|s0 = s, a0 = a, 

(1)

t=0

where   [0, 1] is a discount factor that trades off the importance of immediate and later rewards. Q-learning aims to accurately estimate and maximize the Q value of each state-action pair and then an optimal policy is easily derived from the optimal values by selecting the highest valued action in each state.

3 Method Details

3.1 Tabular-value Function
For the reinforcement learning problems with limited states and actions, the Q value of each state-action pair can be stored in a tabular-value function. In the classic Q learning method, after taking action at in state st and getting the immediate reward rt and resulting state st+1, the tabular-value function is updated as follows:

Q(st, at) = Q(st, at) +  rt +  max Q(st+1, a) - Q(st, at)

(2)

a

where  is a scalar step size. The similarity between different states and actions can be measured by norm. In our method, the Q
value of a state-action pair (s, a) is updated synchronously if it satisfy

s - st  s

(3)

or

a - at  a

(4)

where s > 0 and a > 0 are two threshold values. The details are shown in Algorithm 1 and Algorithm 2, where the former takes the similarity of different states into consideration during the training and the

2

similarity of different actions is taken into consideration in the latter, s > 0 and a > 0 are parameters of the algorithms named smooth rate, which determine the updating step size of the Q value of similar state-action pairs.
Algorithm 1 State Space Smooth Tabular Q-learning Initialize Q value function as 0 for all state-action pairs; s = s0; for t = 1, 2, ... do With probability select a random action a, otherwise a = arg maxa Q(s, a ; ); Execute action a and observe reward r and observe new state snext; q = r +  maxa Q(snext, a ); Q(s, a) = Q(s, a) + [q - Q(s, a)]; for s  S do if s - s  s then q = (1 - s)Q(s , a) + sq; Q(s , a) = Q(s , a) +  [q - Q(s , a)]; end if end for s = snext; end for
Algorithm 2 Action Space Smooth Tabular Q-learning Initialize Q value function as 0 for all state-action pairs; s = s0; for t = 1, 2, ... do Generate an equally distributed random real number d uniformly distributed on the interval from 0 to 1; With probability select a random action a, otherwise a = arg maxa Q(s, a ; ); Execute action a and observe reward r and observe new state snext; q = r +  maxa Q(snext, a ); Q(s, a) = Q(s, a) + [q - Q(s, a)]; for a  A do if a - a  a then q = (1 - a)Q(s, a ) + aq; Q(s, a ) = Q(s, a ) +  [q - Q(s, a )]; end if end for s = snext; end for

3.2 Deep Q Network
Due to the limitation of tabular-value function, DQN is widely used in engineering practice. For each time step, the DQN with parameters  of standard Q-learning method, say Q(s, a; ), is updated as follows

 =  +  [Y - Q(st, at; )] Q(st, at; )

(5)

where

Y = rt+1 +  max Q(st+1, a; )

(6)

a

3

Neural networks can fit the continuous functions, therefore, the similarity of different states is naturally taken into consideration when the DQN is used to represent the Q value. However, the action space is still considered as discrete space in deep Q-learning and its improved versions. Based on the deep Q-learning proposed in [nature DQN], know as Nature DQN, and considered similarity between different actions, a new updating mechanism of DQN is introduced in this subsection, see Algorithm 3.

Algorithm 3 Smooth Deep Q-learning

Initialize replay memory D to capacity N ;

Initialize state-action value function Q with random parameters ; Initialize target state-action value function Q^ with random parameters  = ;

s = s0; for t = 1, 2, ... do

With probability select a random action a,

otherwise a = arg maxa Q(s, a ; );

Execute action a and observe reward r and new state snext;

Store transition (s, a, r, snext) in D;

Sample random minibatch of transitions {(sj, aj, rj, snext,j)} from D;

qj = rj +  maxa Q^(snext,j , a ; ); l0 = j [qj - Q(sj, aj; )]2;

l1 =  j

a -aj a,a =aj [qj - Q(sj , a ; )]2

Perform a gradient descent step on l0 + l1 with respect to ;

q = r +  maxa Q(snext, a );

Q(s, a) = Q(s, a) + [q - Q(s, a)];

Every C steps reset  = ;

s = snext; end for

4 Numerical Examples

4.1 Smooth Tabular Q-learning
Consider a MDP involved a 64x64 gridworld, the state of the agent is expressed as a position (x, y) in the gridworld. For every time step, the action (ax, ay) can be choosen from the set {(-1, 0), (0, 1), (1, 0), (0, -1)}. The transition dynamics is

xt+1 = min[max(xt + ax, 0), 63]

(7)

yt+1 = min[max(yt + ay, 0), 63]

The reward function is

rt =

100, xt = 63 and yt = 63 -1, otherwise

(8)

At the beginning of each training episode, the state of agent is set as (0, 0), and the training episode is end when the agent get the state (63, 63). The simulation results of Algorithm 1 with different s and Algorithm 2 with different a are shown in Figure 1 and Figure 2 respectively. During the simulations, the other parameters are set as follows

= 0.1  = 0.1  = 0.9 s = 1
It can be seen from Figure 1 and Figure 2 that with the consideration of the similarity between different states and actions, the Q value function has a significantly faster convergence speed.

4

Figure 1: Simulation results of Algorithm 1.

Figure 2: Simulation results of Algorithm 2.

4.2 Smooth Deep Q-learning

In this subsection, some classical RL examples with continuous action space are taken. The first is the Pendulum problem [gym website]. The transition dynamics is

t+1 = min{max

t

-

3g 2l

sin t

+

3u ml2

0.05, -8 , 8}

(9)

t+1 = (t + 0.05t+1 + )%(2) - 

where  and  are angle and angle velocity respectively. g = 10, m = 1 and l = 1 are acceleration of gravity, mass and length respectively. The operator "%" is modulo operator. u  [-2, 2] is action. In order to adapt DQN to this problem, the action space is discretized into several parts, that is

4

u  -2 + i|i = 0, 1, ..., 63

(10)

63

And the reward function is

r(t, t, ut) = -t2 - t2 - 0.001u2t

(11)

The second example is MountainCar. The transition dynamics is

vt+1 = min

max

vt

+

0.0015u

-

cos 3pt 400

,

-0.07

, 0.07

(12)

pt+1 = min [max(pt + vt+1, -1.2), 0.6]

Where p and v are position and velocity respectively, and u  [-1, 1] is the action. Similar to the previous example, the action space is discretized into several parts

2

u  -1 + i|i = 0, 1, ..., 63

(13)

63

And the reward function is

r(pt, vt, ut) =

20pt+1 - 0.1u2, pt+1 < 0.45 100 - 0.1u2, pt+1  0.45

(14)

The simulation results of the two examples are shown in Figure 3 and Figure 4. It is clear that the method with a smooth rate greater than 0 (smooth Q-learning) does substantially better than the classic Q-learning.

5

Figure 3: Simulation results of Pendulum problem.
Figure 4: Simulation results of MountainCar problem.
5 Conclusions
This paper presents an improvement of Q-learning method, in which the similarity of different states or/and actions is considered. Through a new updating mechanism, the convergence is obviously accelerated. Besides the chosen states and actions, the Q value of the similar state-action pairs are updated synchronously in every training step. The simulation results illustrate that, compared to the existing Q-learning method, the performance of the proposed method has remarkable advantage. However, over the course of the study, some problems were found. For example, the convergence speed is not monotonely increasing along the smooth rate, to determine the optimal smooth rate is necessary. The proposed method has a poor performance when it is adapted in the problems with high-dimensional continuous action space. These problems will be considered in our future work.
Acknowledgements
The authors gratefully acknowledge support from National Defense Outstanding Youth Science Foundation (Grant No. 2018-JCJQ-ZQ-053), and Research Fund of State Key Laboratory of Mechanics and Control of Mechanical Structures (Nanjing University of Aeronautics and astronautics) (Grant No. MCMS-0217G01). Also, the authors would like to thank the anonymous reviewers, associate editor, and editor for their valuable and constructive comments and suggestions.
References
[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529­33, 02 2015.
[2] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998.
[3] Ben J.A. Kr¨ose. Learning from delayed rewards. Robotics and Autonomous Systems, 15(4):233 ­ 235, 1995. Reinforcement Learning and Robotics.
6

[4] Philippe Preux. Propagation of q-values in tabular td(). 08 2002. [5] Christopher J.C.H. Watkins and Peter Dayan. Technical note: Q-learning. Machine Learning,
8(3):279­292, May 1992. [6] K. S. Hwang, J. L. Ling, and W. Wang. Adaptive reinforcement learning in box-pushing robots.
In 2014 IEEE International Conference on Automation Science and Engineering (CASE), pages 1182­1187, Aug 2014. [7] R Sutton and A Barto. Reinforcement Learning:An Introduction. 1998. [8] Yan Cheng. Dynamic packaging in e-retailing with stochastic demand over finite horizons: A qlearning approach. Expert Systems with Applications, 36(1):472 ­ 480, 2009. [9] Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst. Reinforcement Learning and Dynamic Programming Using Function Approximators. 01 2010. [10] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. 12 2013. [11] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. 09 2015. [12] Ziyu Wang, Nando Freitas, and Marc Lanctot. Dueling network architectures for deep reinforcement learning. 11 2015. [13] Tom Schaul, Jonh Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. 1 2016. [14] Thang Doan, Bogdan Mazoure, and Clare Lyle. Gan q-learning, 05 2018. [15] Prases Mohanty, Arun Sah, Vikas Kumar, and Shubhasri Kundu. Application of deep q-learning for wheel mobile robot navigation. pages 88­93, 10 2017. [16] J. Bernhard, R. Gieselmann, K. Esterle, and A. Knol. Experience-based heuristic search: Robust motion planning with deep q-learning. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pages 3175­3182, Nov 2018. [17] Z. Yijing, Z. Zheng, Z. Xiaoyi, and L. Yang. Q learning algorithm based uav path learning and obstacle avoidence approach. In 2017 36th Chinese Control Conference (CCC), pages 3397­3402, July 2017. [18] Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR, 09 2015.
7

