
# The Limitations of Limited Context for Constituency Parsing

[arXiv](https://arxiv.org/abs/2106.01580), [PDF](https://arxiv.org/pdf/2106.01580.pdf)

## Authors

- Yuchen Li
- Andrej Risteski

## Abstract

However, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind. In this work, we answer representational questions raised by the architectures in (Shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent? Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision. We show that with limited context (either bounded, or unidirectional), there are PCFGs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any PCFG.

## Comments

To be published in ACL 2021 ( this https URL ) as a long paper

## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/the-limitations-of-limited-context-for](https://paperswithcode.com/paper/the-limitations-of-limited-context-for)

## Bibtex

```tex
@misc{li2021limitations,
      title={The Limitations of Limited Context for Constituency Parsing}, 
      author={Yuchen Li and Andrej Risteski},
      year={2021},
      eprint={2106.01580},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

