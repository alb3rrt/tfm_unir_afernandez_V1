What Can I Do Here? Learning New Skills by Imagining Visual Affordances

Alexander Khazatsky1, Ashvin Nair1, Daniel Jing1, Sergey Levine1

arXiv:2106.00671v1 [cs.RO] 1 Jun 2021

Abstract-- A generalist robot equipped with learned skills "1 : drawer opening "2 : lid on pot

"3 : relocate bag

must be able to perform many tasks in many different en-

vironments. However, zero-shot generalization to new settings

is not always possible. When the robot encounters a new

environment or object, it may need to finetune some of its previously learned skills to accommodate this change. But

"4 : drawer closing

"5 : grasping cup

"6 : pushing/pulling

crucially, previously learned behaviors and models should still

be suitable to accelerate this relearning. In this paper, we aim to

study how generative models of possible outcomes can allow a

robot to learn visual representations of affordances, so that the robot can sample potentially possible outcomes in new situations, and then further train its policy to achieve those

"7 : grasp football

"8 : relocate tiger
Prior Dataset D {1, 2, ..., N }

outcomes. In effect, prior data is used to learn what kinds of Prior Dataset D {1, 2, ..., N }

outcomes may be possible, such that when the robot encounters

an unfamiliar setting, it can sample potential outcomes from its

model, attempt to reach them, and thereby update both its skills
and its outcome model. This approach, visuomotor affordance
learning (VAL), can be used to train goal-conditioned poOliciesine

Oine LeaRrnepirnegsentation LeRaerpnriensegntation z = (s)

z

=A(os)rdance p (zg |zs )

p(zBge|zhsa)vior (a|z, zg) (a|z, zg)

that operate on raw image inputs, and can rapidly learn to

manipulate new objects via our proposed affordance-directed exploration scheme. We show that VAL can utilize prior data to solve real-world tasks such drawer opening, grasping, and

Online Learning in "new
"new : new drawer

placing objects in new scenes with only five minutes of online

experience in the new scene.

3. train  online

I. INTRODUCTION

Suppose that you need to learn to open a new kind of drawer in a kitchen. While this new skill might demand some amount of trial and error, you would likely be able to use your mental model and past experience to imagine the drawer in the open position, and perhaps even imagine likely intermediate steps, such as grasping the handle, even if you do not yet know precisely how to perform the task. Borrowing the terminology put forward by Gibson [15], the drawer presents the affordance of being "openable," and you are aware of this affordance from your past experience with other similar objects. In fact, infants learn about affordances such as movability, suckability, graspability, and digestibility through interaction and sensory feedback [6]. Learning and utilizing affordances through interaction allows an agent to acquire diverse, meaningful experiences even in unfamiliar situations. However, this way of learning new skills differs

s0

1. sample

goal zg 

zg  p(·|z0)

2. rollout (a|z, zg)

Fig. 1: We propose a system for efficient self-supervised robot learning in an unseen environment Enew by utilizing prior data D of trajectories from related similar environments Ei  p(E). Tasks are specified via a target goal image. From prior data, we learn an encoder z = (s) of images (representation) for compressing observations and self-generating rewards, a model of what tasks might be tested in the new environment (affordance), and goalconditioned policy to accomplish a given task (behavior). While this provides reasonable performance in some test environments, perfecting the policy in test environments may require additional interaction in the test environment. Dropped in a test environment Enew without a given goal, we run RL online in order to practice potential tasks, with goals sampled from the affordance model. Online behavior learning allows us to improve the policy  for Enew even when it contains new and unseen objects.

markedly from the approach taken by most robotic learning algorithms: the most widely used exploration methods are generally undirected, and focus more on seeking out novelty and surprise [18], [41], [32], rather than familiar and previously seen outcomes. In this paper, we study how robots operating entirely from pixel input can learn about affordances and, when faced with a new and unfamiliar environment, can utilize a previously trained model of possible

outcomes to propose potential goals that they can practice in this new environment, so as to explore and update their policy efficiently.
We study affordance learning through the framework of self-supervised goal-conditioned reinforcement learning (RL). Learning a new skill in this framework requires generalization in terms of goal setting (affordances) and generalization in terms of goal reaching (behavior). Prior

First two authors contributed equally. 1UC Berkeley. Correspondence methods for goal-conditioned RL learn a policy to reach a

to anair17@berkeley.edu

goal state without the need for an externally provided reward

function, and are able to master skills such as pushing and grasping objects from image observations [1], [31], [26], [29]. They learn skills by setting goals to explore, and learning a policy to reach them. However, while these prior works have studied how to learn goal-conditioned policies in individual environments, they do not consider what happens when the robot enters a new environment where the policy does not simply generalize zero-shot and needs to be trained further. Our approach to solving tasks in this setting is to learn affordances, represented by a generative model of possible outcomes, and then sample possible affordances in a new environment to explore the new environment efficiently.
Our method, visuomotor affordance learning (VAL), uses expressive conditional models for learning generalizable affordances, along with off-policy RL to learn generalizable behaviors. First, we propose to use more expressive generative models for RL to learn compressed representations of images that can reconstruct unseen objects and help the policy generalize to them. Second, we propose to learn an expressive conditional generative model that generalizes past data to set meaningful goals for novel environments, enabling an understanding of object affordances. Third, we demonstrate how we can use off-policy RL with all prior data to initialize a general-purpose goal-conditioned policy, then fine-tune the policy with additional data to master a skill in a new environment. Combining RL and representation learning, we show that we are able to learn skills with a small amount of online exploration on novel objects in realworld robotic scenarios such as object grasping, relocation, and drawer opening and closing.
The main contribution of this work is to present a learning system that can learn robotic skills entirely from image pixels in novel environments by utilizing prior data to generate goals and generalize behavior in new settings. We demonstrate that our method can learn complex manipulation skills including grasping, drawer opening, pushing, and object repositioning for a diverse set of objects in simulation. We also demonstrate our method in the real world on a Sawyer robot, where our method is able to learn tasks such as grasping and placing unseen objects and opening and closing unseen drawers after only five minutes of online interaction.
II. RELATED WORK
RL has been applied previously to robotic manipulation [21], [34], [24], and also various other applications from playing games [27], [38] to locomotion [5], [22], [10], [45]. Such approaches require an external reward function, but obtaining this reward function itself poses a challenge to exploring in novel uninstrumented environments as we consider in this paper. Thus, in this work we focus on selfsupervised RL methods that that do not assume externally provided reward functions.
When learning without an externally provided reward function, one common idea is to use novelty-based intrinsic reward functions [7], [25], [4], [18], [40], [32]. State noveltybased methods eventually visit all possible states, but do not necessarily learn a useful policy from purely optimizing the exploration objective. An alternative exploration framework

that learns a useful policy even solely from the intrinsic objective is goal reaching [19], [37], [3], [2], [31], [28], [17], [33], [44], [35]: by picking a distance measure between states, setting goals, and attempting to reach them, an agent can discover all potential goals in its environment. We refer the reader to the survey of Colas et al. [8] for a full classification and discussion of these methods. However, these prior methods do not study the question of how to set goals in new environments, which is vital for collecting coherent experience when faced with a new task. Our method utilizes representation learning and off-policy RL to generalize prior experience to set exploration goals in new settings.
Most similar to our work, context-conditioned reinforcement learning with imagined goals (CCRIG) learns a conditional variational auto-encoder (CVAE) [39] that generates goals conditional on the current scene [29]. CCRIG was able to learn pushing skills that generalized mainly to object color and partially to object geometry. Our work differs from CCRIG in a number of ways. First, we learn expressive generative models that are able to generate goals in scenes with significantly more visual diversity. Second, we learn a diverse set of skills (e.g., grasping, drawer opening, object placing) that require the goal generation to understand affordances of the environment. Finally, we show that we can use off-policy RL on prior experience, in addition to fine-tuning further on a single specific task to learn new skills. These differences allow our method to better operate in real-world scenarios, as borne out in our experiments.

III. PRELIMINARIES

In this section, we cover preliminaries on RL, goal-

conditioned RL, and self-supervised visual RL.

Goal-conditioned reinforcement learning. In goal-

conditioned RL, we augment the standard Markov decision

process (MDP), which is defined in terms of states

st  S, actions at  A, and environment dynamics st+1  p(·|st, at), with goals g  G that represent the
agent's intention to perform one of a variety of tasks drawn

from the task family p(g). The reward function is also

goal-conditioned, and given by some function r(s, g). The

discounted return is defined as Rt =

H i=0

ir(si,

g),

where

 is a discount factor and H is the horizon, which may

be infinite. The aim of the agent is to optimize a policy

(at|st, g) to maximize the expected discounted return J() = Eg[R0]. Efficient off-policy RL algorithms have

been proposed to learn goal-conditioned policies [37], [2].

Self-supervised visual reinforcement learning. For scalable

robot learning, we cannot always assume known shaped

reward functions for tasks. In the absence of such reward

functions, Andrychowicz et al. propose goal-state reaching

as a natural objective [2]: tasks are defined by state outcomes,

where the goal space G = S, the state space, and the reward

is a goal-reaching objective r(s, g) = -1||s-g||> . The task distribution p(g) is chosen to be the feasible states of the

robot and objects it interacts with.

But when states are high-dimensional (e.g., images), two

issues arise: we do not know the task distribution p(g),

and exact reaching of a target goal state is impractical.

Reinforcement learning with imagined goals (RIG) [31] addresses these issues with a generative model, which is used to learn a latent space of observations and similarity metric on images. Specifically, a variational auto-encoder [20] with encoder (zt|st) and prior p(z) is learned. At training time, the robot sets goals for itself in latent space by sampling a goal latent zg  p(z) and learns a policy (zt, zg) to reach latent goals. At test time, the robot can be tasked with a goal image g and execute the learned policy with goal latent zg  (·|g) to match the goal image. In this way, goal-conditioned RL with generative models enables selfsupervised learning in a single environment E where the task distribution p(g) is not known apriori.
IV. PROBLEM SETTING
In this paper, we now consider acting in a distribution of environments p(E) with shared structure.1 Each environment Ei has its own task distribution pi(g). As before, tasks are defined by goal states, so pi(g) represents the potential outcomes of interest in that environment. We assume that the outcomes of interest depends only on the appearance of the environment. When E is fully observed, this means p(g|s0) is shared across environments. As prior data, the robot has access to a training dataset D = {1, 2, . . . N } of trajectories from prior environments, where the trajectories achieve a final outcome sT  pi(g) ­ that is, they succeed on tasks from the underlying task distribution for that environment. Now, the robot is placed in a new environment Enew  p(E), and must learn to solve tasks in the new environment through self-supervised practice at training time, such that it can accomplish tasks sampled from pi(g) at test time. The environments we consider vary visually and dynamically in terms of the objects that are present and potential tasks they afford, such as being able to lift various objects and open different drawers; such real-world variation is presented in Figure 1.
Thus, the agent in this new setting must generalize its prior experience D to practice potential skills it may be asked to perform at test time in the new environment efficiently, even when it encounters novel objects. At test time, the robot is evaluated in terms of its ability to accomplish a task in Enew specified by a goal image. To perform well in this setting, a method must attempt to infer pnew(g), which should be possible since p(g|s0) is common across environments and D contains trajectories that achieve goals from the same distribution. Note that given observations from Enew, the agent may still have to practice the various behaviors the environment affords if there are multiple, since the test task distribution is unknown at training time.
V. VISUOMOTOR AFFORDANCE LEARNING
In this section, we present visuomotor affordance learning (VAL), our method for self-supervised learning in novel
1Meta-learning approaches have also studied learning a new task quickly, given experience on a set of related MDPs [13], [14], [36]. The aims of our method are related to meta-learning, in that we also aim to learn in new environments more quickly, but we do not assume being given user-specified tasks or rewards to solve in the new environment.

Algorithm 1 Visual Affordance Learning

Require: Dataset D, policy (a|z, zg), Q-function Q(z, a, zg), RL algorithm A, replay buffer R, relabeling strategy pRS(z), environment family p(E).
1: Learn encoder (z|s) by generative model of D

2: Learn affordances p(zt|z0) by generative model of D

3: Add latent encoding of D to the replay buffer

4: Initialize  and Q by running A offline

5: Sample Enew  p(E ), Enew = (pnew(s0), new(st+1|s, a)) 6: for 1, . . . , Nepisodes do 7: Sample initial state s0  pnew(s0). 8: Sample goal zg  p(zt|z0) 9: for t = 0, . . . , H do

10:

Sample at  (·|zt, zg)

11:

Sample st+1  pnew(·|st, at)

12: end for

13: Store trajectory (z1, a1, . . . , zH ) in replay buffer R.

14: for 1, . . . , Ntrain steps do

15:

Sample transition (zt, at, zt+1, zg)

16:

Relabel with zg  pRS(zg) and recompute reward

17:

Update  and Q with relabeled transition using A

18: end for

19: end for

environments utilizing prior data from related environments. VAL consists of three learning phases: (A) an affordance learning phase to learn affordances from the prior data, (B) an offline behavior learning phase to learn behaviors from the prior data, and (C) an online behavior learning phase where the agent actively interacts with the test environment using affordances and learns potential behaviors in the new environment. The overall method is summarized in Algorithm 1.
A. Affordance Learning
The affordance learning system must generate goals that induce coherent exploration trajectories even in new environments. We would like to learn a model that, given an observation s0 in a new environment, generates a potential goal state the agent might be tasked to reach. With high-dimensional image observations, attempting to sample such goal states directly is a difficult generative modeling problem. Instead, we first learn a lower-dimensional latent space p(zt|st) by training a generative model through image reconstruction. With sufficiently expressive models and enough data, the generative model represents images in a manner that it can reconstruct even unseen objects. Given such a latent space, we can then learn affordances by training a conditional model p(zt|z0) to generate goals that are plausible outcomes of an initial state, even for unseen environments.
To instantiate these two models, we must choose a class of latent variable generative models for p(st|zt) and conditional affordance models p(zt|z0). For the first, while many choices would be suitable, including models such as variational autoencoders (VAEs) [20] and generative adversarial networks (GANs) [16], [12], in our implementation we use the VQVAE model [43]. The VQVAE is expressive enough to represent very diverse datasets, and be able to reconstruct

even unseen objects with a high level of detail. We do not require image reconstructions for our method, but the ability to partially reconstruct unseen objects suggests that model expressively represents geometry and color information that may be important for learning affordances and behaviors. In the VQVAE case,  is deterministic, so we will use the shorthand for the latent embedding zt = (st), where zt is the continuous latent resulting after quantization. In our experiments, we compare this choice of model to other expressive models: VAE [20], CVAE [39], and BiGAN [12].
Next, given a latent space, we need to sample potential goals from this space that is predictive of which goals might be tasked at test time in the new environment Enew. We use a conditional PixelCNN model [42] in the latent space to do so, conditioned on the initial state s0. The PixelCNN model is trained to maximize ED,(s0,st),z(·|s)[log p(zt|z0)], where  is the parameters of the PixelCNN density model. To generate exploration goals, we sample zg  p(·|z0), where z0 = (s0) is the encoding of an image of the current setting. The PixelCNN model in VAL being conditional allows us to generate meaningful goals that might be achievable with a novel object. As we show in Section VI-A, the conditional model allows us to sample affordances such as opening drawers and lifting objects, even for new environments that are visually complex with variation in the identity, color, geometry, and functionality of objects.
B. Offline Behavior Learning
Given learned affordances of what behaviors the agent may be tasked with, the agent needs to learn how to actually accomplish those behaviors. In this phase, we wish to learn a reasonable goal-conditioned policy with offline RL. While trained on a limited fixed offline dataset, the hope is that the policy can generalize learning from offline data to accomplish desired behaviors in a new environment, allowing us to either perform tasks successfully zero-shot without further learning, or collect meaningful exploration data in the next phase (Section V-C).
To learn with offline RL while allowing the possibility of quickly fine-tuning in a new environment, we use advantage weighted actor critic (AWAC) [30] as the underlying RL algorithm. AWAC is an off-policy RL algorithm that has shown strong performance in utilizing prior data for offline pretraining while still being amenable to online finetuning. The aim of behavior learning is to optimize a goalconditioned policy (a|z, zg) which is able to solve any task in the task distribution p(zg). We do not assume an external reward function, so we require a reward function to optimize. Following prior work [2], [31], we optimize a goal reaching objective: to maximize the density pZg (z = zg); in practice we use the sparse reward function r(z, zg) = -1||z-zg||> , where is a fixed threshold as it encourages fully solving tasks in a binary fashion. For a particular transition in the replay buffer (zt, at, zt+1, zg, r), we can also relabel the goal with a new goal zg and the recompute the reward. In practice we keep zg with 20% probability, future hindsight experience replay with 40% probability, and sample zg  p(zt|z0) with 40% probability. Importantly, due to using a compressed

Initial Image
CVQVAE (Ours)
CCVAE
Fig. 2: Samples on unseen objects in the real world. In each column, the top image is the conditioning image s0 and the images below are conditionally sampled images from the corresponding generative model. Our model, CVQVAE, generates clear and diverse samples.
representation, we can expect that zt in any new environment will be semantically similar to past experience. Thus, in this phase, we can obtain a policy that generalizes partially to a new environment.
C. Online Behavior Learning
Guaranteeing zero-shot generalization for every possible new environment is in general impossible. Instead, we will discuss how to finetune in a specific environment Enew using affordances. In this phase, we utilize the learned affordances which inform what tasks to perform, and the offline learned behaviors that inform how to perform those tasks.
At online training time, the new task distribution pnew(g) is unknown, so we use the affordance model to sample potential tasks. Thus, to collect coherent exploration data, we sample goals from the affordance module zg  p(·|z0) and roll out the goal-conditioned policy (a|z, zg). We then iterate between improving the policy with off-policy RL, and collecting exploration data and appending it to the replay buffer. To learn a new task, exploration data in the new environment for fine-tuning the policy is extremely valuable, so this iteration allows us to quickly fine-tune. The online learning process is illustrated in the bottom box in Figure 1.
In summary, VAL enables self-supervised learning in novel environments utilizing prior data. We first learn a generative model for learning a latent space and affordances, and learn how to accomplish tasks with off-policy RL. Then, both are used in an online behavior learning phase to perfect potential behaviors in a new environment. The overall method is summarized in Algorithm 1.
VI. REAL-WORLD EXPERIMENTAL EVALUATION
We first evaluate our method in a real-world manipulation setting with a Sawyer robot and diverse objects and tasks. This setting is very challenging due to the variety of objects, scenes, and tasks that the robot interacts with, and being limited to using image inputs.
Real-world setting. The real-world setting is shown in Figure 1. A Sawyer robot is controlled at 5Hz with 4 degrees of control: 3 dimensions of end-effector velocity control in Euclidean space and one dimension to open and close the gripper. The environments span 10 drawer handles, 10 pot handles, 40 toys, and 60 distractor objects. To create a new environment E in the real world, we randomly sample one interaction object as well as 2-3 distractor objects.

Task
(1) Pickup shoe (2) Drawer closing

VAL (Ours)

CCRIG

Offline  Online Offline  Online

VAL training

s0

sT d(zg)

VAL testing

s0

sT

sg

(1)

12.5%  50%

0%  16.6%

25%  100%

0%  12.5%

(2)

(3) Drawer opening

62.5%  100% 0%  0%

(3)

(4) Place object in tray 25%  75%

0%  0%

(4)

(5) Lid on pot

37.5%  87.5% 0%  0%

(5)

Fig. 3: Real-world results. Left, success rates per method for the five tasks tested. We report the offline performance, followed by the performance after five minutes of online fine-tuning. With VAL, we see reasonable initial offline performance followed by significant improvement on all tasks. Meanwhile CCRIG fails to succeed any of the tasks. Right, film strips of VAL during training (left, with decoded affordance proposals) and testing (right, with goal images) are visualized. Videos are available at https://sites.google.com/view/val-rl

The behaviors that the environments afford therefore span grasping and relocating toys, opening and closing drawers, as well as covering and uncovering pots. Each test environment contains an unseen interaction object and a random set of 2-3 distractor objects, with all positions randomized.
Our experiments aim to answer the following questions:
1) Does the learned generative model sample plausible affordances in new scenes?
2) Is VAL able to accelerate learning of real-world manipulation skills online in new settings?
A. Generative Models for Affordance Learning
Expressive generative models enable us to propose desired outcomes in new environments even when the current policy cannot yet achieve them. We can preview this capability by inspecting the training procedure of the models, which also gives insights into which models will tend to perform well when used for interactive learning. Specifically, we inspect model samples to see if the model can actually output plausible candidate affordances. We evaluate our model, which combines a VQVAE with a conditional pixel CNN, and prior models that have been used in self-supervised RL as well as other expressive generative models. We compare (1) VAE [20], (2) CVAE [39], (3) BiGAN [12], (4) Conditional BiGAN, (5) Ours, a VQVAE with a conditional PixelCNN.
Sampled affordances are shown in Figure 2. Our model is able to sample coherent tasks to perform where other methods produce indistinct or uninterruptible samples. Most interestingly, the VQVAE model turns an unseen drawer (with a different color and handle) into a red drawer, in open and closed positions. This illustrates its ability to utilize prior data for generating conducive goals for online RL. CCVAE samples are usually less coherent, often missing the object completely and not capturing the geometry of unseen objects. The CBiGAN also struggles to produce realistic images, while the VAE fails completely as it is not a conditional model cannot represent the wide diversity of potential images.

B. Real-World Visuomotor Affordance Learning
Next, we investigate whether VAL can handle real-world visual complexity and object diversity to learn control policies for varied tasks on a Sawyer robot. The setup is shown in Figure 1: the robot is tasked with fine-tuning in a particular environment, beginning with about 2000 trajectories of prior data for affordance learning and offline behavior learning. The protocol for collecting prior data, as well as examples of images from the data are shown in Appendix A.
After pretraining affordances, a policy, and a Q function on the prior data, the robot is dropped in a new test environment. We evaluate five test environments which each contain distractor objects as well as objects that may be interacted with such a shoe that can be picked up, drawers than can be opened or closed, and a lid which may be placed on a pot. These behaviors are demonstrated on similar objects in the prior dataset, but the objects during test time are previously unseen - for instance, the drawer has a different handle. The agent can interact with the environment without supervision to collect more data and improve the policy; then to evaluate, the agent is tasked with a specific goal image that corresponds to a task such as opening a closed drawer.
Results running VAL in the real world are shown in the table in Figure 3, reporting the success rates on five test tasks for our method and CCRIG both offline and after one epoch of online training, which includes 10 interactive trials, amounting to less than five minutes of real-world interaction time. On all five tasks, VAL shows nontrivial offline performance followed by strong online improvement. Qualitatively, our method is able to learn behaviors such as recovering from a missed grasp by returning to the grasp. Film strips of our method are shown on the right side of Figure 3. In contrast, CCRIG struggles to make progress on all five tasks. CCRIG fails for two reasons. First, the quality of the sampled affordances are significantly poorer. Second, as can be seen in videos, CCRIG sometimes comes close to solving the task, but does not fully solve it, preventing improvement after subsequent training.

1.0

Place Object in Tray

1.0 Open/Close Drawer, Button

1.0

Pick and Place

Success Rate

0.8

0.8

0.8

VAL

0.6

0.6

0.6

CCRIG

RIG

0.4

0.4

0.4

CBiGAN-RIG

BiGAN-RIG

0.2

0.2

0.2

0.00K 100K 20T0iKmest3e0p0sK 400K 500K

0.00K

50K Tim10e0sKteps 150K

200K 0.00K

100K 20T0iKmeste3p00sK 400K 500K

Fig. 4: Learning curves for simulation experiments, fine-tuning on an unseen environment. Our method is able to learn these tasks online, while none of the baselines or prior methods are able to make meaningful learning progress in this setting. A successful rollout of each task in a test environment is shown above the corresponding plots.

Initial Image

CVQVAE (Ours)

Fig. 5: Randomly sampled scenes from our simulated multitask environment. To practice in a sampled scene, the agent must infer the potential behaviors that the scene affords.

(1) Place object in tray

VAL training

s0

sT d(zg)

(2) Open drawer, handle

(3) Open/close drawer, button

CCVAE
CBiGAN
VAE
Fig. 7: Samples on test environments in simulation. In each column, the top image is the conditioning image s0 and the images below are conditionally sampled images. The left four columns are relatively successful samples for our affordance model, each showing the potential outcome of a behavior. The right two columns are failure modes, lacking diversity or altering an object's geometry or color.

(4) Pick and place
Fig. 6: Training rollouts from VAL. For each environment, the reconstruction of a sampled affordance is shown on the rightmost column, and frames from a trajectory attempting to achieve that affordance is shown on the left.

existence, position, color, and orientation of the following: two drawers, a box, a button, and an object. If an object is present it is chosen from a set of 84 object geometries. To successfully learn in a test environment, the method must be able to explore in the environment based on the behaviors that environment affords.

VII. EXPERIMENTAL EVALUATION IN SIMULATION
In order to further study and understand VAL, we carry out more experiments in simulation, where we can better control the quantity of data and ablate parts of the method. These experiments aim to answer the following questions:
1) Does VAL outperform prior self-supervised RL methods in accelerating learning a new task from prior data?
2) Can VAL scale with additional data for affordance and behavior learning?
Simulated setting. Randomly generated workspaces in our simulated multi-task environment are shown in Figure 7. In this PyBullet simulation [9], the robot is faced with multiple potential tasks in each environment based on which objects are present: opening and closing a drawer by the handle, opening and closing a different drawer by pressing a button, grasping objects, and moving objects into drawers or a box. To sample a new environment E, we randomize the

A. Self-Supervised Online Fine-Tuning from Prior Data
We first compare VAL against prior methods in this simulated setting. The robot receives a prior dataset D of trajectories in the pre-training environments, which is utilized for learning affordances and offline RL. The details of this dataset are explained further in Appendix B. After the pre-training phase, the robot is placed in a test setting and begins online fine-tuning. We evaluate the policy on goal images in the new environment, sampled from expert trajectories, and report whether the final state of the policy's trajectory matches the state of the goal image within a chosen threshold.
Learning curves of the online training phase for various objects are shown in Figure 4. We see that for each task, VAL outperforms prior methods which do not make progress on learning these tasks at all. On the drawer opening task, offline RL achieves nontrivial performance of about 60%, but online interaction allows fine-tuning to over 90% success rate.

Grasping, Varying Prior Data
0.7

Success Rate

0.6

3000

2000

0.5

1000

0.4

500 250

0.3

0K 25K 50K 75K 100K 125K 150K Timesteps

Fig. 8: Learning curves for simulated grasping of novel objects with VAL, using data from an increasing number of training objects for offline RL. We collect 50 trajectories per training object, and each line is labeled with the total number of training trajectories.

How is VAL able to outperform prior methods so significantly? One major reason is the quality of sampled goals. Samples for the affordance model for the simulated domain are shown in Figure 7. We compare (1) VAE [20], (2) CVAE [39], (3) BiGAN [12], (4) Conditional BiGAN, (5) Ours, a VQVAE with a conditional PixelCNN. Our model produces diverse, coherent samples of possible outcomes (i.e., affordances) in new scenes with novel objects. In comparison to our model, conditional VAE samples tend to be blurry and do not capture the geometry of unseen objects well.
B. Scalable Robot Learning with VAL
For general-purpose robot learning, we would ideally like to learn diverse skills continuously, using prior experience to perpetually improve at learning new skills. To study this possibility, we examine whether solving tasks in a new environment can be sped up by collecting larger amounts of prior experience. In this experiment conducted in simulation in order to carefully control the data quantity, the robot receives a prior dataset D of K trajectories for running VAL to grasp an unseen object in a new environment. We vary K to observe whether the method can benefit from larger amounts of prior data.
Learning curves of the online training phase, averaged over five test objects, are shown in Figure 8. First, we see from the starting point of each curve that the offline policy already generalizes to some level for grasping objects, but the average success rate is only around 35%. Importantly, note that training on more data only slightly improves generalization after offline training (at timestep 0). Then, fine-tuning results in rapid policy improvement up to around 65% success rate after only 150,000 timesteps when utilizing the most prior data, compared to 40% with the least prior data. Thus, more prior data significantly accelerates learning in new environments even when the initial performance is comparable. This suggests that VAL can be deployed in a continual learning setting, with each new task being learned faster as it benefits from the increasing dataset size.
VIII. CONCLUSION
We present visuomotor affordance learning (VAL), a method for learning tasks online in a new environment without supervision, utilizing trajectories from other related environments. VAL uses expressive generative models to

learn visual affordances, combines these affordances with off-policy goal-conditioned RL to learn skills offline, and then fine-tunes in a new environment online. Like deep learning in domains such as computer vision [23] and natural language processing [11] which has been driven by large datasets and generalization, robotics will likely require learning from a similar scale of data. In future work, VAL could enable such systems by allowing autonomous collection of coherent exploration data in diverse real-world settings.
IX. ACKNOWLEDGEMENTS
This is an extended version of a paper by the same title that appears in the IEEE International Conference on Robotics and Automation (ICRA), 2021. This research was supported by the Office of Naval Research, the National Science Foundation through IIS-1651843, and Berkeley DeepDrive. We would like to additionally thank Misha Laskin, Vitchyr Pong, Glen Berseth, and Abhishek Gupta for constructive discussions about this work, and members of RAIL for their support and collaboration.
REFERENCES
[1] P. Agrawal, A. Nair, P. Abbeel, J. Malik, and S. Levine, "Learning to Poke by Poking: Experiential Learning of Intuitive Physics," in Advances in Neural Information Processing Systems (NeurIPS), 2016.
[2] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. Mcgrew, J. Tobin, P. Abbeel, and W. Zaremba, "Hindsight Experience Replay," in Advances in Neural Information Processing Systems (NeurIPS), 2017.
[3] A. Baranes and P.-Y. Oudeyer, "Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots," Robotics and Autonomous Systems, vol. 61, no. 1, pp. 49­73, 2012.
[4] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, "Unifying count-based exploration and intrinsic motivation," in Advances in Neural Information Processing Systems (NeurIPS), 2016, pp. 1471­1479.
[5] H. Benbrahim and J. A. Franklin, "Biped dynamic walking using reinforcement learning," Robotics and Autonomous Systems, vol. 22, no. 3-4, pp. 283­302, dec 1997.
[6] K. Berger, The Developing Person Through the Life Span, 2014. [7] N. Chentanez, A. G. Barto, and S. P. Singh, "Intrinsically motivated
reinforcement learning," in Advances in neural information processing systems, 2005, pp. 1281­1288. [8] C. Colas, T. Karch, O. Sigaud, and P.-Y. Oudeyer, "Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey," Tech. Rep., 2021. [9] E. Coumans and Y. Bai, "Pybullet, a python module for physics simulation for games, robotics and machine learning," http://pybullet.org, 2016­2021. [10] M. P. Deisenroth and C. E. Rasmussen, "PILCO: A model-based and data-efficient approach to policy search," in International Conference on Machine Learning (ICML), 2011, pp. 465­472. [11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," in Association for Compuational Linguistics (ACL), oct 2019. [12] J. Donahue, P. Kra¨henbu¨hl, and T. Darrell, "Adversarial Feature Learning," in International Conference on Learning Representations (ICLR), may 2017. [13] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning," nov 2016. [14] C. Finn, P. Abbeel, and S. Levine, "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks," in International Conference on Machine Learning (ICML), 2017. [15] J. Gibson, The Ecological Approach to Visual Perception, 1979. [16] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative Adversarial Nets," in Advances in Neural Information Processing Systems (NeurIPS), 2014.

[17] D. Held, X. Geng, C. Florensa, and P. Abbeel, "Automatic Goal Generation for Reinforcement Learning Agents," in International Conference on Machine Learning (ICML), 2018.
[18] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel, "Variational Information Maximizing Exploration," in Advances in Neural Information Processing Systems (NeurIPS), 2016.
[19] L. P. Kaelbling, "Learning to achieve goals," in International Joint Conference on Artificial Intelligence (IJCAI), vol. vol.2, 1993, pp. 1094 ­ 8.
[20] D. P. Kingma and M. Welling, "Auto-Encoding Variational Bayes," in International Conference on Learning Representations (ICLR), 2014.
[21] J. Kober and J. Peter, "Policy search for motor primitives in robotics," in Advances in Neural Information Processing Systems (NeurIPS), vol. 97, 2008, pp. 83­117.
[22] N. Kohl and P. Stone, "Machine Learning for Fast Quadrupedal Locomotion," in AAAI Conference on Artificial Intelligence, 2004, pp. 611­616.
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," in Advances in Neural Information Processing Systems (NeurIPS), 2012, pp. 1097­1105.
[24] S. Levine, C. Finn, T. Darrell, and P. Abbeel, "End-to-End Training of Deep Visuomotor Policies," Journal of Machine Learning Research (JMLR), vol. 17, no. 1, pp. 1334­1373, 2016.
[25] M. Lopes, T. Lang, M. Toussaint, and P.-Y. Oudeyer, "Exploration in model-based reinforcement learning by empirically estimating learning progress," in Advances in Neural Information Processing Systems, 2012, pp. 206­214.
[26] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet, "Learning Latent Plans from Play," in Conference on Robot Learning (CoRL), mar 2019.
[27] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, "Playing Atari with Deep Reinforcement Learning," in NIPS Workshop on Deep Learning, 2013, pp. 1­9.
[28] O. Nachum, S. S. Gu, H. Lee, and S. Levine, "Data-Efficient Hierarchical Reinforcement Learning," in Advances in Neural Information Processing Systems (NeurIPS), 2018.
[29] A. Nair, S. Bahl, A. Khazatsky, V. Pong, G. Berseth, and S. Levine, "Contextual Imagined Goals for Self-Supervised Robotic Learning," in Conference on Robot Learning (CoRL), oct 2019.
[30] A. Nair, M. Dalal, A. Gupta, and S. Levine, "Accelerating Online Reinforcement Learning with Offline Datasets," jun 2020.
[31] A. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine, "Visual Reinforcement Learning with Imagined Goals," in Advances in Neural Information Processing Systems (NeurIPS), 2018.
[32] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, "CuriosityDriven Exploration by Self-Supervised Prediction," in International Conference on Machine Learning (ICML). IEEE, 2017, pp. 488­ 489.
[33] A. Pe´re´, S. Forestier, O. Sigaud, and P.-Y. Oudeyer, "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration," in International Conference on Learning Representations (ICLR), 2018.
[34] J. Peters, K. Mu¨lling, and Y. Altu¨n, "Relative Entropy Policy Search," in AAAI Conference on Artificial Intelligence, 2010, pp. 1607­1612.
[35] V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine, "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning," in International Conference on Machine Learning (ICML), 2020.
[36] K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables," in International Conference on Machine Learning (ICML), 2019.
[37] T. Schaul, D. Horgan, K. Gregor, and D. Silver, "Universal Value Function Approximators," in International Conference on Machine Learning (ICML), 2015, pp. 1312­1320.
[38] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, "Mastering the game of Go with deep neural networks and tree search," Nature, vol. 529, no. 7587, pp. 484­489, jan 2016.
[39] K. Sohn, X. Yan, and H. Lee, "Learning Structured Output Representation using Deep Conditional Generative Models," in Advances in Neural Information Processing Systems (NeurIPS), 2015.
[40] B. C. Stadie, S. Levine, and P. Abbeel, "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models," in International Conference on Learning Representations (ICLR), 2016.

[41] H. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel, "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning," in Advances in Neural Information Processing Systems (NeurIPS), 2017.
[42] A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu, "Conditional Image Generation with PixelCNN Decoders," in Advances in Neural Information Processing Systems. Neural information processing systems foundation, jun 2016, pp. 4797­4805.
[43] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, "Neural Discrete Representation Learning," in Advances in Neural Information Processing Systems, vol. 2017-Decem. Neural information processing systems foundation, nov 2017, pp. 6307­6316.
[44] D. Warde-Farley, T. Van De Wiele, T. Kulkarni, C. Ionescu, S. Hansen, and M. Volodymyr, "Unsupervised Control Through Non-Parametric Discriminative Rewards," in International Conference on Learning Representations (ICLR), 2019.
[45] G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou, "Information Theoretic MPC for Model-Based Reinforcement Learning," in IEEE International Conference on Robotics and Automation (ICRA), 2017.
[46] Y. Zhu, J. Wong, A. Mandlekar, and R. Mart´in-Mart´in, "robosuite: A modular simulation framework and benchmark for robot learning," in arXiv preprint arXiv:2009.12293, 2020.

APPENDIX
A. Real-World Experimental Details
Our real world dataset consists of 1,984 trajectories (137,111 transitions) collected by a human using a 3Dconnexion SpaceMouse device. Instructions for interfacing with the SpaceMouse is available publicly at https://github.com/vitchyr/rlkit/tree/ master/rlkit/demos/spacemouse, with the device code adapted from the RoboSuite library [46]. A full view of the robot and the view from the camera can be seen in Figure 9.
Across our dataset, we interact with 20 drawer handles, 20 pot handles, 60 toys, and 60 distractor objects. Every 10 trajectories we randomly sample one or more interaction objects as well as two or more distractor objects. Before each rollout we randomize all object positions. The trajectories can be grouped into five separate categories: picking and placing objects, putting objects into a tray, opening and closing a drawer, and placing and removing a lid on a pot. To artificially increase the size of our dataset and make our policy robust to light changes and camera nudges, we utilize color jittering and random cropping during training. As there was an unequal amount of data per category, we re-balanced the dataset by using a different number of data augmentations per category. The final amount of task-specific data used per experiment is reported in Table VI.
We additionally collected unscripted play data mixing all of the above behaviors with more object diversity, but did not use this data in this paper. The entire dataset is available on our website: https://sites.google.com/view/ val-rl.
B. Simulation Experimental Details
Our simulated dataset consists of 8,000 trajectories (400,000 transitions). Before sampling each trajectory, we randomize the existence, position, color, and orientation of the following: two drawers, a box, a button, and an object. If an object is present it is chosen from a set of 84 object geometries. The trajectories are generated by a scripted policy which collects play data by interacting with all the present objects in a random order. The scripted behavior includes: opening and closing a drawer by the handle, opening and closing a different drawer by pressing a button, and repositioning objects. All simulated RL experiments were run with 5 seeds.

Fig. 10: Images from the prior dataset. The dataset contains 1,984 trajectories. We have released the full prior dataset along with on-policy robot executions at our website, https://sites. google.com/view/val-rl
C. Algorithm Details
Visuomotor affordance learning (VAL) builds off the rlkit codebase available at https://github.com/ vitchyr/rlkit. We will release our code at our website, https://sites.google.com/view/val-rl. Below, we list the specific hyperparameters used in our experiments for each component. In VAL, we first collect an offline data D, run representation learning, then offline RL, and finally online RL for a specific environment.
In the representation learning phase, we first train the VQVAE [43] on D. We then encode the entire dataset with the VQVAE to obtain discrete latent variables, and then independently train the PixelCNN [42] on discrete latent code dataset. For the CCRIG experiments, we train a CCVAE [39] on D.
In the offline RL phase, we run advantage weighted actor critic (AWAC) [30] on the offline data to obtain a single policy and Q-function. This policy and Q-function can then be fine-tuned to a specific environment by running online RL.
All hyperparameters are provided below for these algorithms are provided below in tables I, II, III, IV, V.

Fig. 9: Left, full view of Sawyer robot setup. Right, camera view.

Hyper-parameter Training Batches Per Timestep
Exploration Noise RL Batch Size Discount Factor Reward Scaling
Replay Buffer Size Number of pretraining steps
Policy Hidden Sizes Policy Hidden Activation
Policy Weight Decay Policy Learning Rate
Q Hidden Sizes Q Hidden Activation
Q Weight Decay Q Learning Rate Target Network 

Value
1
None (stochastic policy) 1024 0.99 1
1000000 25000
[256, 256, 256, 256]
ReLU 10-4 3 × 10-4 [256, 256]
ReLU 0
3 × 10-4 5 × 10-3

TABLE I: Hyper-parameters used for RL (AWAC) experiments.

Hyper-parameter Brightness (Color Jitter) Contrast (Color Jitter) Saturation (Color Jitter)
Hue (Color Jitter) Size (Random Resized Crop) Scale (Random Resized Crop) Ratio (Random Resized Crop) Interpolation (Random Resized Crop)

Value [0.75, 1.25]
[0.9, 1.1] [0.9, 1.1] [-0.1, 0.1]
48 [0.9, 1.0] [0.9, 1.1] Antialiasing

TABLE II: Hyper-parameters used for data augmentation.

Hyper-parameter Convolution Layers Convolution Hidden Size
Residual Layers Residual Hidden Size
Embedding Size Dictionary Size Commitment Cost EMA Embedding

Value 3
128 3 64 5
512 0.25 F alse

TABLE III: Hyper-parameters used for VQVAE training.

Hyper-parameter Batch Size Layers
Learning Rate Latent Conditioning Type

Value 32 15
0.0003 C ontinuous

TABLE IV: Hyper-parameters used for PixelCNN experiments.

Hyper-parameter Convolution Layers Convolution Hidden Size
Residual Layers Residual Hidden Size
Embedding Size Conditioning Embedding Size

Value 3
128 3 64 5 1

TABLE V: Hyper-parameters used for CCVAE training.

Hyper-parameter Tray
Pick and Place Close Drawer Open Drawer
Place Lid

Value 25% task specific data 25% task specific data 20% task specific data 20% task specific data 17% task specific data

TABLE VI: Hyper-parameters used for task-specific replay buffer re-balancing (through data augmentation).

