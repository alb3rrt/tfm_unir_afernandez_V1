
# EV-VGCNN: A Voxel Graph CNN for Event-based Object Classification

[arXiv](https://arxiv.org/abs/2106.0216), [PDF](https://arxiv.org/pdf/2106.0216.pdf)

## Authors

- Yongjian Deng
- Hao Chen
- Huiying Chen
- Youfu Li

## Abstract

Event cameras report sparse intensity changes and hold noticeable advantages of low power consumption, high dynamic range, and high response speed for visual perception and understanding on portable devices. Event-based learning methods have recently achieved massive success on object recognition by integrating events into dense frame-based representations to apply traditional 2D learning algorithms. However, these approaches introduce much redundant information during the sparse-to-dense conversion and necessitate models with heavy-weight and large capacities, limiting the potential of event cameras on real-life applications. To address the core problem of balancing accuracy and model complexity for event-based classification models, we (1) construct graph representations for event data to utilize their sparsity nature better and design a lightweight end-to-end graph neural network (EV-VGCNN) for classification; (2) use voxel-wise vertices rather than traditional point-wise methods to incorporate the information from more points; (3) introduce a multi-scale feature relational layer (MFRL) to extract semantic and motion cues from each vertex adaptively concerning its distances to neighbors. Comprehensive experiments show that our approach advances state-of-the-art classification accuracy while achieving nearly 20 times parameter reduction (merely 0.84M parameters).

## Comments

10 pages, 5 Figures

## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{deng2021evvgcnn,
      title={EV-VGCNN: A Voxel Graph CNN for Event-based Object Classification}, 
      author={Yongjian Deng and Hao Chen and Huiying Chen and Youfu Li},
      year={2021},
      eprint={2106.00216},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## Notes

Type your reading notes here...

