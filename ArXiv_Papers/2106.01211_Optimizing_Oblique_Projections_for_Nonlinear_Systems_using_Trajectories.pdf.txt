arXiv:2106.01211v2 [math.OC] 8 Jun 2021

OPTIMIZING OBLIQUE PROJECTIONS FOR NONLINEAR SYSTEMS USING TRAJECTORIES 
SAMUEL E. OTTO , ALBERTO PADOVAN , AND CLARENCE W. ROWLEY
Abstract. Reduced-order modeling techniques, including balanced truncation and H2-optimal model reduction, exploit the structure of linear dynamical systems to produce models that accurately capture the dynamics. For nonlinear systems operating far away from equilibria, on the other hand, current approaches seek low-dimensional representations of the state that often neglect low-energy features that have high dynamical significance. For instance, low-energy features are known to play an important role in fluid dynamics where they can be a driving mechanism for shear-layer instabilities. Neglecting these features leads to models with poor predictive accuracy despite being able to accurately encode and decode states. In order to improve predictive accuracy, we propose to optimize the reduced-order model to fit a collection of coarsely sampled trajectories from the original system. In particular, we optimize over the product of two Grassmann manifolds defining PetrovGalerkin projections of the full-order governing equations. We compare our approach with existing methods such as proper orthogonal decomposition and balanced truncation-based Petrov-Galerkin projection, and our approach demonstrates significantly improved accuracy both on a nonlinear toy model and on an incompressible (nonlinear) axisymmetric jet flow with 69, 000 states.
Key words. model reduction, nonlinear systems, fluid dynamics, Petrov-Galerkin projection, Riemannian optimization, geometric conjugate gradient, Grassmann manifold, adjoint method
AMS subject classifications. 14M15, 15A03, 34A45, 34C20, 49J27, 49J30, 49M37, 76D55, 90C06, 90C26, 90C45, 93A15, 93C10
1. Introduction. Accurate low-dimensional models of physical processes enable a variety of important scientific and engineering tasks to be carried out. Such models can be used to make real-time forecasts as well as to shed light on the underlying physics through detailed analysis of the resulting dynamical system. The models can also serve as a building block for filters that estimate the state of the system from incomplete measurements and to design control laws to achieve desired behaviors from the system. However, many real-world systems like complex fluid flows in the atmosphere as well as around and inside aircraft are governed by extremely highdimensional nonlinear systems -- properties that make tasks like real-time forecasting, state estimation, and control computationally prohibitive using the original governing equations. Fortunately, the behavior of these systems is frequently dominated by coherent structures and patterns [14] that may be modeled with equations whose dimension is much smaller [47, 22]. The goal of "reduced-order modeling" is to obtain simplified models that are suitable for forecasting, estimation, and control from the vastly more complicated governing equations provided by physics. For reviews of modern techniques, see [6], [11] and [40]. For a striking display of coherent structures in turbulence, see the shadowgraphs in G. L. Brown and A. Roshko [14].
When the system of interest is operating close to an equilibrium point, the governing equations are accurately approximated by their linearization about the equilibrium. In this case, a variety of sophisticated and effective reduced-order modeling techniques can be applied with guarantees on the accuracy of the resulting low-
Compiled June 10, 2021. Funding: This research was supported by the Army Research Office under grant number
W911NF-17-1-0512 and the Air Force Office of Scientific Research under grant number FA955019-1-0005. S.E.O. was supported by a National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-2039656.
Princeton University, Princeton, NJ, Dept. of Mechanical and Aerospace Engineering (S.E.O.: sotto@princeton.edu, A.P.: apadovan@princeton.edu, C.W.R.: cwrowley@princeton.edu)
1

2

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

dimensional model [4, 11]. Put simply, linearity provides an elegant and complete characterization of the system's trajectories in response to inputs, disturbances, and initial conditions that can be exploited to build simplified models whose trajectories closely approximate the ones from the original system. For instance, the balanced truncation method introduced by B. Moore [32] yields a low-dimensional projection of the original system that simultaneously retains the most observable and controllable states of the system and provides bounds on various measures of reduced-order model error [4]. A computationally efficient approximation called Balanced Proper Orthogonal Decomposition (BPOD) [39] is suitable for high-dimensional fluid flow applications. Another approach is to find a stable reduced-order model (ROM) that is as close as possible to a stable full-order model (FOM) with respect to the H2 norm. Algorithms like the Iterative Rational Krylov Algorithm (IRKA) [19] are based on satisfying necessary conditions for H2-optimality.
Various generalizations of linear model reduction techniques have also been developed for bilinear [6, 8, 18] and quadratic bilinear systems [9, 10] based on truncated Volterra series expansion of the output. If enough terms are retained in the series expansions, these methods can yield reduced systems that approximate the response to arbitrary input signals. However, the computational cost increases with the number of terms retained, making them difficult to apply to fluid flows whose state dimensions can easily exceed 105.
One commonality among the above model reduction approaches based on direct input-output relationships is that they lead to reduced-order models that capture the most energetic features as well as any low-energy features that nonetheless significantly influence the dynamics at future times [11, 39]. These small, but dynamically significant features are known to play an important role in driving the growth of instabilities in "shear flows" such as mixing layers and jets. Linearizations of these shear flows often result in non-normal systems, which can exhibit large transient growth in response to low-energy perturbations [49, 45]. Some successful approaches [5, 3, 24, 25] have involved oblique projections of the nonlinear dynamics onto subspaces identified from the dynamics linearized about an equilibrium. However, this approach is often not satisfactory since the linearized dynamics become inaccurate as the state moves away from the equilibrium and nonlinear effects become significant. In this paper we illustrate how such nonlinear effects can cause reduced-order models obtained using the above approach to perform poorly, for instance on a simple three-dimensional system as well as on a high-dimensional axisymmetric jet flow.
When dealing with nonlinear systems operating far away from equilibria, nonlinear model reduction approaches tend to follow a two-step process: first identify a set, typically a smooth manifold or a subspace, near which the state of the system is known to lie, then model the dynamics in this set either by a projection of the governing equations or by a black-box data-driven approach. The most common approach to identify a candidate subspace is Proper Orthogonal Decomposition (POD), whose application to the study of complex fluid flows was pioneered by J. L. Lumley [30]. The dynamics may also be projected onto nonlinear manifolds using "nonlinear Galerkin" methods [31, 37]. Recently, more sophisticated manifold learning techniques like deep convolutional autoencoders have also been used [29].
The main obstacle encountered by the manifold-learning-based approaches described above is the presence of dynamically-significant low-energy features. Since POD and even sophisticated nonlinear manifold learning techniques like convolutional autoencoders aim to accurately reduce and reconstruct states, they will neglect features whose contribution to the overall state has a sufficiently small magnitude. But,

OPTIMIZING REDUCED-ORDER MODELS

3

as we mentioned earlier, we should not neglect all of the low-energy features since some can have a large influence on the dynamics. In fact, in our jet flow example, we shall see that a model with 50 POD modes that together capture 99.6% of the energy still yields poor predictions that rapidly diverge from the full-order model.
In order to capture significant low-energy features, while remaining tractable for very large-scale systems like fluid flows, we shall optimize an oblique projection operator defining the reduced-order model with the objective of reproducing a collection of trajectories sampled from the original system. In particular, we seek to minimize the sum of squared errors between the trajectories predicted by the model and those collected from the full system. In this framework, oblique projection operators of a fixed dimension are identified with pairs of subspaces that meet a transversality condition. Recalling that the collection of all subspaces of a given dimension can be endowed with the structure of a Riemannian manifold called the Grassmann manifold [1, 7], we show that the pairs of subspaces that define oblique projection operators are an open, dense, and connected subset of the product of two such Grassmann manifolds and we provide conditions for the existence of a minimizer. The optimization is performed using the Riemannian conjugate gradient algorithm introduced by H. Sato [41] with retraction and vector transport defined in Absil et al. [1], and we provide general conditions under which the algorithm is guaranteed to converge to a local optimum.
Related techniques based on optimizing projection subspaces have been used to produce H2-optimal reduced-order models for linear and bilinear systems. Most approaches focus on optimizing orthogonal projection operators over a single Grassmann manifold [53, 44, 26] or an orthogonal Stiefel manifold [54, 44, 50, 55, 52]. On the other hand, an alternating minimization technique over the two Grassmann manifolds defining an oblique projection is proposed by T. Zeng and C. Lu [56] for H2-optimal reduction of linear systems. For systems with quadratic nonlinearities, Y.-L. Jiang and K.-L. Xu [26] present an approach to optimize orthogonal projection operators based on the same truncated generalization of the H2 norm used by P. Benner et al. [10]. Our approach differs from the ones mentioned above in that it may be used to find optimal reduced-order models based on oblique projections for general very high-dimensional nonlinear systems based on sampled trajectories.
2. Projection-Based Reduced-Order Models. Consider a physical process, modeled by an input-output dynamical system

(2.1)

d x = f (x, u),
dt y = g(x)

x(t0) = x0

on a finite-dimensional real inner product space X = (Rn, ·, · ) with outputs y in Rm equipped with the usual inner product. We shall often refer to (2.1) as the full-order model (FOM). Our goal is to use one or more discrete-time histories of observations yl = y(tl) at sample times t0 < · · · < tL-1 in order to learn the key dynamical features of (2.1) and produce a reduced-order model (ROM) that captures these effects. Throughout the paper we assume that
Assumption 2.1. The functions (x, t)  f (x, u(t)) and x  g(x) in (2.1), along with their first-order partial derivatives with respect to x, are continuous.
We shall use our observation data to learn an r-dimensional subspace V of Rn in which to represent the state of the system (2.1). Since f (x, u) might not lie in V when x  V , we shall also learn another r-dimensional subspace W of Rn that, together

4

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

with V , uniquely defines an oblique projection operator PV,W : Rn  V according to

(2.2)

w, PV,W x = w, x , w  W, x  Rn

when no nonzero element of W is orthogonal to V . We let P denote the set of such subspaces pairs (V, W ) with the property that no nonzero element of W is orthogonal to V and give some equivalent definitions of this set in Proposition 2.2.
Proposition 2.2 (Subspaces that Define Oblique Projections). Let V and W be subspaces of Rn with dim V = dim W = r, and let ,   Rn×r be matrices such that V = Range  and W = Range . Let  denote the adjoint of , viewed as a linear operator from Rr with the Euclidean inner product into the state space X with its own inner product. Then the following are equivalent:
1. no nonzero element of W is orthogonal to V ; 2. no nonzero element of V is orthogonal to W ; 3. det () = 0; 4. for every x  Rn there exists a unique x^  V such that

(2.3)

w, x = w, x^ w  W.

Proof. The proof is an exercise in linear algebra, so we give it in Appendix F.
Applying the projection defined by (V, W )  P to the full-order model (2.1), we obtain a Petrov-Galerkin reduced-order model whose state x^  V evolves according to

(2.4)

d dt x^ = PV,W f (x^, u),

x^(0) = PV,W x0,

with observations given by y^ = g(x^). The two subspaces V, W uniquely define the projection PV,W and the reduced-order model (2.4).
Let Ly : Rm  [0, +) be a smooth penalty function for the difference between each observation yl and the model's prediction y^(tl). Let us also introduce a smooth nonnegative-valued function (V, W ), to be defined precisely in section 3, that will serve as regularization by preventing minimizing sequences of subspaces (V, W ) from approaching points outside the set P in which valid Petrov-Galerkin projections can be defined. Using this regularization with a weight  > 0 allows us to seek a minimum of the cost defined by

(2.5)

1 L-1

J(V, W ) = L

Ly (y^(tl) - yl) + (V, W )

l=0

over all pairs of r-dimensional subspaces (V, W ), subject to the reduced-order dynamics (2.4). Here we shall consider the case when there is a single trajectory generated from a known initial condition since it will be easy to handle multiple trajectories from multiple known initial conditions once we understand the single trajectory case. The cost function (2.5) defines an optimization problem, and in the following section we define a suitable regularization function  and develop a technique for iteratively solving this problem.

3. Optimization Domain, Representatives, and Regularization. The set containing all r-dimensional subspaces of Rn can be endowed with the structure of a
compact Riemannian manifold called the Grassmann manifold, which has dimension nr - r2 and is denoted Gn,r. Therefore, our optimization problem entails minimizing

OPTIMIZING REDUCED-ORDER MODELS

5

the cost given by (2.5) over the subset P of the product manifold M = Gn,r × Gn,r on which oblique projection operators are defined according to Proposition 2.2. The goal of this section will be to characterize the topology of the set P and to introduce an appropriate regularization function  so that we may instead consider the unconstrained minimization of (2.5) over M. We also describe how to work with matrix representatives of the relevant subspaces that can be stored in a computer.

3.1. Grassmann Manifold and Representatives of Subspaces. First we

describe some basic properties of the Grassmann manifold that can be found in Absil

et al.

[1].

If

n,r
R

denotes

the

smooth

manifold

of

n×r

matrices

with

linearly

independent

columns,

then

Gn,r

can

be

identified

with

the

quotient

manifold

of

n,r
R

defined by identifying matrices with the same span.

That is, two matrices X, Y



n,r
R

are defined to be equivalent if Range X = Range Y , i.e., there is an invertible matrix

M  GLr, the general linear group, such that X = Y M . The equivalence class of a

matrix

X



n,r
R

is

defined

by

(3.1)

[X] = {Y  Rn,r : Range X = Range Y } ,

and

the

set

of

these

equivalence

classes

is

the

quotient

space

n,r
R

/GLr

.

Since the

action

of

GLr

on

n,r
R

defining

a

change

of

basis

GLr

×

n,r
R



n,r
R

:

(M, X)



XM

is free and proper, it follows from the quotient manifold theorem (Theorem 21.10

in

[28])

that

n,r
R

/GLr

is

a

smooth

manifold

and

the

quotient

map

X



[X ]

is

a

smooth

submersion.

The

Grassmann

manifold

can

be

identified

with

n,r
R

/GLr

since

each

subspace

V

 Gn,r

corresponds

to

the

unique

equivalence

class

[X ]



n,r
R

/GLr

whose elements all span V and vice-versa.

The

identification

of

the

Grassmann

manifold

with

n,r
R

/GLr

is

very

useful

since

we wish to optimize the subspaces V and W using a computer that can't store abstract

subspaces in memory. Instead, we have to work with representatives of these subspaces

given

by

pairs

of

n×r

matrices

,





n,r
R

such

that

V

= Range  and W

= Range .

That is, we aim to optimize over the product manifold M = Gn,r × Gn,r by relying on

representatives

in

the

so

called

"structure

space"

M¯

=

n,r
R

×

n,r
R

.

The

"canonical

projection" map  : M¯  M is defined by

(3.2)

 : (, )  ([], []),

and it is clear that any representatives (, )  M¯ of (V, W )  M must satisfy

(V, W ) = (, ). For a pair of subspaces (V, W )  Gn,r × Gn,r, the set of all

representatives

(, )



n,r
R

×

n,r
R

such

that

(, )

=

(V, W )

is

given

by

the

pre-

image -1(V, W ). The canonical projection map is a surjective submersion since its

component maps   [] and   [] are surjective submersions. This key fact

yields the following useful result:

Lemma 3.1 (Regularity via Representatives). Let N be another smooth manifold and let F : M  N be a function. Then F is continuous if and only if F   is continuous. Furthermore, F is smooth if and only if F   is smooth.

Proof. The "only if" directions are obvious since the composition of continuous (resp. smooth) functions is continuous (resp. smooth). The converse statements follow immediately from the local submersion theorem [20] for , which provides local charts on which F can be expressed by restricting F   to a coordinate slice.

As before, we let P denote the set of subspace pairs that satisfy the conditions in Proposition 2.2, and suppose that (V, W )  P and (, )  -1(V, W ) are a choice

6

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

of representatives. We recall that  denotes the adjoint of  viewed as a linear operator from Rr into the state space X . Then it is clear from Proposition 2.2 that the r × r matrix  is invertible and the oblique projection operator corresponding
to (V, W ) is given by

(3.3)

PV,W = ()-1.

We also observe that (3.3) is independent of the choice of representatives (, )  -1(V, W ) -- as it should be, given that PV,W was originally defined by (2.2) in terms of abstract subspaces alone. Using the representatives and an r-dimensional state z defined by x^ = z  V , we obtain a representative of the reduced-order model (2.4) given by

(3.4)

d z = ()-1f (z, u) =: f~(z, u; (, )) , dt
y^ = g(z) =: g~ (z; (, )) ,

z(0) = ()-1x0

that can be simulated on a computer. Observe that the output y^(t) of (3.4) depends only on the subspaces (V, W ), and not on the representatives (, ) we choose.
Consequently, any function of (, ) that depends only on the output y^(t) of (3.4) can be viewed as a function on M composed with the canonical projection . Hence, we can evaluate our cost function (2.5) for a subspace pair (V, W ) by computing

(3.5)

J¯(, ) = J((, ))

for any choice of represenatives (, )  -1(V, W ), that is, by evaluating the sum in (2.5) using the output y^(t) generated by (3.4). Moreover, Lemma 3.1 tells us that J is smooth if and only if J¯ is smooth.
Remark 3.2 (Convenient Representatives). One choice of representatives that is especially convenient to work with are biorthonormal pairs, that is, (, )  -1(V, W ) such that  = Ir and  = Ir. Of course such representatives may always be found by first choosing any representatives ~ , ~ of the subspaces V, W and then letting  = qf(~ ), where qf denotes orthogonalization by QR factorization, and letting  = ~ (~ )-1.
3.2. Topology of the Optimization Problem Domain. The main result of this section is the following:
Theorem 3.3 (Topology of Subspaces that Define Oblique Projections). Let P denote the pairs of subspaces (V, W )  Gn,r × Gn,r that define oblique projection operators according to Proposition 2.2. Then P is open, dense, and connected in Gn,r ×Gn,r. Moreover, P is diffeomorphic to the set of rank-r projection operators

(3.6)

P = P  Rn×n : P 2 = P and rank(P ) = r .

Proof. See Appendix A.
The openness of P in Gn,r × Gn,r means that it is a submanifold of Gn,r × Gn,r with the same dimension dim P = 2nr - 2r2. The connectedness result is especially important since it means than an optimization routine can access any point in the set P by a smooth path from any initial guess without ever encountering the "bad set" Gn,r × Gn,r \ P. In other words the bad set doesn't cut off access to any region of P by an optimizer that progresses along a smooth path, e.g., a gradient flow.

OPTIMIZING REDUCED-ORDER MODELS

7

The reduced-order model (2.4) may not have a solution over the desired time interval [t0, tL-1] for every projection operator defined by (V, W )  P. The following result characterizes the appropriate domain D  P over which the ROM has a unique solution as well as the key properties of solutions when they exist.

Proposition 3.4 (Properties of ROM Solutions). When the reduced-order model

(2.4) has a solution over the time interval [t0, tL-1], it is unique. Let D  P denote the set of subspace pairs (V, W ) for which the resulting reduced-order model (2.4) has

a unique solution over the time interval [t0, tL-1]. Then

1. D is open in P, and hence D is also open in Gn,r × Gn,r.

2.

When

 x

f

(x,

u(t))

is

bounded

then

D = P.

3. If x^(t; (V, W )) denotes the solution of (2.4) with (V, W )  D, then (V, W ) 

x^(t; (V, W )) is continuously differentiable on D for every t  [t0, tL-1]. 4. If {(Vk, Wk)} k=1  D is a sequence approaching (Vk, Wk)  (V0, W0)  P \ D
and x^(t; (Vk, Wk)) are the corresponding solutions of (2.4), then

(3.7)

max x^(t; (Vk, Wk))   as k  .
t[t0 ,tL-1 ]

Proof. The claims follow from standard results in the theory of ordinary differential equations that can be found in W. G. Kelly A. C. Peterson [27]. We give the detailed proof in Appendix B.
In particular, Proposition 3.4 shows that the solutions produced by the reduced-order model are continuously differentiable over D and blow up as points outside of D are approached. In the special case when the governing equations (2.1) have a bounded Jacobian, we may dispense with D entirely since we find that the reduced-order model always has a unique, differentiable solution.
3.3. Regularization and Existence of a Minimizer. Without regularization, we cannot guarantee a priori that a sequence of subspace pairs with decreasing cost doesn't approach a point outside of the set P where projection operators are defined. That is, a minimizer for the cost function (2.5) may not even exist in P, in which case our optimization problem would have no solution. In order to address this issue, we introduce a regularization function (V, W ) into the cost (2.5) that "blows up" to + as the subspaces (V, W ) approach any point outside of P, and nowhere else. In order to do this, we use the fact that (V, W )  P if and only if all representatives (, )  -1(V, W ) have det () = 0, as shown in Proposition 2.2. While this condition characterizes the set P, we cannot use det () directly since its nonzero value depends on the choice of representatives. But this problem is easily solved by an appropriate normalization, leading us to define the regularization of (2.5) in terms of representatives according to

(3.8)

det()2   (, ) = - log det() det() .

We observe that the function  : P  R in (3.8) is well-defined because (, ) does not depend on the representatives (, ) thanks to the product rule for determinants.
The following theorem shows that the regularization defined by (3.8) has the desirable properties that it vanishes when V = W and "blows up" as (V, W ) escapes the set P. When V = W , the resulting projection operator PV,V is the orthogonal projection onto V .

8

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

Theorem 3.5 (Regularization). The minimum value of  defined by (3.8) over P is zero, and this minimum value (V, W ) = 0 is attained if and only if V = W . On the other hand, if (V0, W0)  Gn,r × Gn,r \ P and {(Vn, Wn)} n=1 is a sequence of subspaces in P such that (Vn, Wn)  (V0, W0) as n  , then limn (Vn, Wn) = .
Proof. See Appendix C.
We must also rule out the possibility that a sequence of subspace pairs with decreasing cost approaches a point where the reduced-order model does not have a unique solution. By Proposition 3.4, we do not have this problem when the full-order model has a bounded Jacobian since the reduced-order model always has a unique solution, i.e., D = P. On the other hand, when D = P we may accomplish this by choosing a cost function that blows up if the states of the reduced-order model blow up. In particular, we assume the following:
Assumption 3.6. Let D be as in Proposition 3.4 and P be the set defined by Proposition 2.2. If D = P and {(Vk, Wk)} k=1  D is any sequence producing solutions x^(t; (Vk, Wk)) of the reduced-order model (2.4) such that

(3.9)

max x^(t; (Vk, Wk))   as k  ,
t[t0 ,tL-1 ]

then we assume that J(Vk, Wk)  . Furthermore, we make the convention that J(V, W ) =  whenever (V, W )  P \ D.
In practice, this is a reasonable assumption if g(x)   as x   and Ly(y)   as y  . Alternatively, one could add a new regularization term to the cost function (2.5) that penalizes reduced-order model states with large magnitudes. In Corollary C.1 we show that a minimizer of the cost function (2.5) exists in the valid set D  P when Assumption 3.6 holds and we use the regularization described by (3.8) with any positive weight  > 0.

4. Computing the Gradient. With the reduced-order model representative (3.4) in hand, the following results (Theorem 4.1 and Corollary 4.2) allow us to compute the derivative of the cost (3.5) with respect to the matrices (, ) in the structure space M¯ = Rn×r × Rn×r. In particular, we treat the matrices (, ) as general model parameters C whose values we wish to optimize. When the initial condition x0 for the full-order model is known, the initial condition for the reduced-order model representative (3.4) is given by z0 = ()-1x0 and Corollary 4.2 provides a correction for the gradient provided by Theorem 4.1 due to the dependence of z0 on (, ).
Theorem 4.1 (Gradient with Respect to Model Parameters). Suppose we have observation data {y1, . . . , yL} generated by a dynamical system at sample times t0 < · · · < tL-1 and a parametric model for the system given by

(4.1)

d z = f~(z, u; C) dt

where u(t) is a known input signal, C are unknown parameters in a Riemannian manifold M¯ , and the initial condition z(t0) = z0 is unknown. Furthermore, suppose
that we aim to fit the unknown parameters by minimizing a cost function of the form

(4.2)

L-1
J¯0(C, z0) := Ly(g~(z(ti); C) - yi).
i=0

OPTIMIZING REDUCED-ORDER MODELS

9

Let

F (t) =

 z

f~(z(t),

u(t);

C ),

S(t) =

 C

f~(z(t),

u(t);

C ),

H(t) =

 z

g~(z(t);

C ),

and

T (t)

=

 C

g~(z

(t);

C)

denote

the

linearized

dynamics

and

observation

functions

around

a solution z(t) of (4.1) and define an adjoint variable (t) that satisfies

(4.3a) (4.3b) (4.3c)

- d (t) =F (t)(t), dt

t  (ti, ti+1],

0  i < L - 1,

(ti) = lim (t) + H(ti)  Ly(g~(z(ti); C) - yi),
tt+ i

(tL-1) =H(tL-1)  Ly(g~ (z(tL-1); C) - yL-1).

Here (·) denotes the adjoint of a linear operator. Then the gradients of the cost function (4.2) with respect to the unknown variables C and z0 subject to the dynamics (4.1) are given by

(4.4)

z0 J¯0(C, z0) = (t0)

(4.5)

tL-1

L-1

C J¯0(C, z0) =

S(t)(t) dt + T (ti)  Ly(g~(z(ti); C) - yi).

t0

i=0

Proof. See Appendix D.

Corollary 4.2 (Gradient with Parameter-Dependent Initial Condition). When the initial condition z0 = z0(C) in Theorem 4.1 is also a function of the parameters, then the gradient of J¯(C) = J¯0(C, z0(C)) with respect to the parameters C is given by

(4.6)

C J¯(C) = C J¯0(C, z0) +

 C z0(C)


z0 J¯0(C, z0).

Proof. This is little more than the chain rule. For details, see Appendix D.

In order to optimize on M = Gn,r × Gn,r, we must make sense of the gradient computed using Theorem 4.1 and Corollary 4.2 with respect to (, ) in the structure space M¯ = Rn×r × Rn×r in terms of gradients that are tangent to the quotient manifold M. The key idea is to endow M¯ with a special structure called a "horizontal distribution" that allows one to uniquely identify tangent vectors to M¯ that repre-
sent tangent vectors to M. Together with the horizontal distribution, we also must
define a Riemannian metric on the structure space that is unaffected by the choice of
representative within an equivalence class [1]. In what follows, we summarize some
useful results from Absil et al. [1], Chapter 3, which should be consulted for more
details.
Since we do not have direct access to T(V,W )M using a computer, it is necessary to work with representatives of the gradient that are tangent to the structure space M¯ . But for any   TpM and representative p¯  M¯ such that p = (p¯), there are an infinite number of possible ¯  Tp¯M¯ that could serve as representatives of  in the sense that  = D (p¯)¯. A unique representative of  is identified by observing that the pre-image -1(p) of any p  M is a smooth submanifold of M¯ yielding a decomposition of the tangent space Tp¯M¯ into a direct sum of the "vertical space" defined by Vp¯ = Tp¯-1(p) and the "horizontal space" defined as its orthogonal complement Hp¯ = Vp¯. Following Example 3.6.4 in [1], it can be shown that the orthogonal projection onto the horizontal space is given by

(4.7)

P(h,)(X, Y ) = X - ()-1X, Y - ()-1Y .

Using this horizontal distribution on the structure space, we have the following:

10

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

Definition 4.3 (horizontal lift [1]). Given   TpM and a representative p¯  -1(p), there is always a unique element ¯p¯  Hp¯ called the "horizontal lift" of  such that  = D (p¯)¯p¯.
An important consequence for optimization is that the horizontal lift of the gradient of a function J : M  R is given by the gradient of J¯ = J   [1], that is,

(4.8)

 J((p¯))p¯ =  J¯(p¯), p¯  M¯ .

In particular, this means that the gradient computed using Theorem 4.1 and Corollary 4.2 is the "correct" representative in the sense that it is the horizontal lift at (, ) of the (inaccessible) gradient of the cost function tangent to M at (V, W ). Moreover, given (V, W )  M and invertible matrices S, T  GLr, then the unique horizontal lifts of a tangent vector (, )  T(V,W )M at the representatives (, ) and (S, T ) are related by

(4.9)

(¯S, ¯T ) = (¯S, ¯T )  Rn×r × Rn×r.

This can also be shown via a trivial adaptation of Example 3.6.4 in [1]. In order for the Riemannian metric on the structure space M¯ to induce a com-
patible Riemannian metric on the quotient manifold M, we must have

(4.10)

,  p := ¯p¯, ¯p¯ p¯ = ¯q¯, ¯q¯ q¯,

,   TpM, p¯, q¯  -1(p),

so that the metric on M¯ is independent of the representative p¯. The Riemannian metric we adopt for the structure space M¯ is given by

(4.11)

(X1, Y1), (X2, Y2) (,) = Tr ()-1X1X2 + Tr ()-1Y1Y2 ,

which clearly satisfies (4.10) thanks to (4.9). Using the Riemannian metric (4.11) on the structure space, we next obtain an
explicit form of each term required to compute the horizontal lift of the gradient using Theorem 4.1 and Corollary 4.2. Proposition 4.4 below also provides us with the gradient of the regularization function (3.8). In order to simplify these expressions, we have assumed that the particular representatives are chosen to satisfy  = Ir, which is always possible thanks to Remark 3.2. The horizontal lift of the gradient computed at any equivalent point (S, T ) with S, T  GLr can be readily obtained from the horizontal lift of the gradient computed at (, ) via (4.9).
Proposition 4.4 (Required Terms for Gradient). We assume that the representatives  and  with V = Range  and W = Range  have been chosen such that  = Ir. Then the terms required to compute the gradient of the cost function using the model (3.4) with respect to the representatives in the structure space via Theorem 4.1 and Corollary 4.2 are given by

(4.12)

F (t) =  f~(z(t), u(t); (, )) T z

(4.13) S(t)v =

 f (z(t), u(t))


vz(t)T - vf~(z(t), u(t); (, ))T

,

x

f (z(t), u(t)) - f~(z(t), u(t); (, )) vT   T(,)M¯ v  Rr,

OPTIMIZING REDUCED-ORDER MODELS

11

(4.14)

H(t) =



T

g~(z(t); (, ))) ,

z

(4.15) T (t)w =

 g(z(t))


wz(t)T , 0

x

 T(,)M¯

w  Rdim y,

(4.16)





(, ) z0(, ) v

= - v(x0)T , (x0 - x0) vT   T(,)M¯

v  Rr.

The gradient of the regularization function (3.8) in terms of representatives (, ) satisfying  = Ir is given by

(4.17)

(  )(, ) = 2  - (),  - ()  T(,)M¯ .

Proof. See Appendix D.
Below, we present Algorithm 4.1 to compute the gradient according to Theorem 4.1 and Corollary 4.2, with the appropriate terms given in Proposition 4.4.

Algorithm 4.1 Compute the cost function gradient with respect to (, )

1: input: biorthogonal representatives (, )  -1(V, W ), initial condition x0, observations {yl}Ll=-01 at times {tl}Ll=-01, regularization weight .
2: Assemble and simulate the ROM representative (3.4) from initial condition z0 = 3: Initxi0a,liszteotrhineggtrhaediternatj:ectoJr¯yz(Tt)(taLn-d1)prediLcyte(dy^Lo-u1tp-uytLs -{1y^)l.}lL=-01. 4: Compute adjoint variable at final time: (tL-1) = H(tL-1)  Ly(y^L-1 - yL-1).
5: for l = L - 2, L - 3, . . . , 0 do

6: Solve the adjoint equation (4.3a) backwards in time over the interval [tl, tl+1]

using the linearized ROM dynamics (4.12) and store (t) on this interval.

7: Compute the integral component of (4.5) over the interval [tl, tl+1]:  J¯ 

8:

 J¯ +

tl+1 tl

S(t)(t)dt

Add lth element of the

using Gauss-Legendre quadrature. sum in (4.5):  J¯   J¯ + T (tl) 

Ly (y^l

-

yl).

9: Add "jump" (4.3b) to the adjoint variable: (tl)  (tl) + H(tl)  Ly(y^l - yl).

10: end for 11: Add gradient due to initial condition:  J¯   J¯ +



 (,)

z0

(,

)

(t0).

12: Normalize by trajectory length:  J¯   J¯/L.

13: Add regularization:  J¯   J¯ +  (  )(, ).

14: return  J¯

5. Optimization using a Conjugate Gradient Algorithm. In this section we provide the necessary tools to implement most gradient-based optimization algorithms [1] including stochastic gradient descent [12, 43], quasi-Newton methods [38, 23], and conjugate gradient methods [38, 41]. Here we use a conjugate gradient algorithm to illustrate the key ingredients, such as the retraction and transport of tangent vectors on the Grassmann manifold. We also provide convergence guarantees

12

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

for the algorithm on broad classes of sufficiently smooth full-order models, including all linear systems as a special case.
Line search optimization techniques in Euclidean space entail choosing a search direction k and a step size k in order to produce the next iterate of the optimization process according to

(5.1)

pk+1 = pk + kk.

The step size k is usually chosen using a backtracking or bisection approach in order to meet a sufficient decrease condition like the one proposed by P. Wolfe [51]. In the steepest descent approach, one searches along the direction of the gradient k = -  J(pk). Yet in poorly conditioned problems, the steepest descent method may converge slowly, and it can be greatly improved by choosing a search direction that incorporates second-order information about the cost function. However, in high-dimensional applications like finding optimal projection subspaces for large-scale dynamical systems, we cannot efficiently evaluate the second derivatives of the cost function. Conjugate gradient algorithms provide efficient alternatives by computing a search direction that combines the gradient at the current iterate with the previous search direction

(5.2)

k = -  J (pk) + kk-1.

There are many choices for the coefficient k; for instance, the one due to Y.-H. Dai and Y. Yuan [17] is given by

(5.3)

k(DY ) =

 J (pk),  J (pk)

.

 J (pk), k-1 -  J (pk-1), k-1

This coefficient guarantees convergence of the Euclidean conjugate gradient algorithm in the sense that the limiting infimum of the gradients at the iterates is zero when the line search satisfies the Wolfe conditions and the gradient of J is Lipschitz continuous on sub-level sets [17].

5.1. Retraction of Tangent Vectors onto the Manifold. In the setting of optimization on Riemanniann manifolds two problems with the Euclidean formulation of conjugate gradient algorithms must be addressed. First, we must have a suitable generalization of what it means to search along a "line" on the manifold. A natural, but computationally expensive choice, is the exponential map on the Riemanniann manifold. The idea of a "retraction" was introduced by [2] as a computationally efficient alternative to the exponential map that retains only the properties that are needed for the purpose of optimization. In particular, [1] gives the following Definition 5.1 for a retraction.
Definition 5.1 (Retraction [1]). A retraction on a manifold M is a smooth mapping R from the tangent bundle T M onto M with the following properties. Let Rp denote the restriction of R to TpM.
1. (Base point preservation) Rp(0) = p. 2. (Local rigidity) Rp satisfies

(5.4)

D Rp(0) =    TpM

(with the canonical identification T0TpM TpM).

OPTIMIZING REDUCED-ORDER MODELS

13

The retraction Rp parameterizes a neighborhood of p  M using elemets of the tangent space, allowing us to pull our optimization problem back to the Euclidean space TpM while the local rigidity condition ensures that the search direction is preserved. Line search may be performed on M using the retraction and a search direction k  Tpk M by defining the next iterate according to

(5.5)

pk+1 = Rpk (kk).

In our case, M is a quotient manifold of a structure space M¯ whose elements we must use as representatives. Proposition 4.1.3 in [1] says that a retraction R¯ on the structure space M¯ induces a retraction R on the quotient manifold M when   R¯

does not depend on the choice of representatives. In particular, Example 4.1.5 in [1] shows that R[]() = [ + ¯] defines a retraction on the Grassmann manifold. Therefore, for our problem on a product of Grassmann manifolds M = Gn,r × Gn,r
we have a retraction defined by

(5.6)

R(V,W )(, ) =   + ¯,  + ¯ , for any (, )  -1(V, W ).

In particular,  + ¯,  + ¯ is a representative of R(V,W )(, ). In order to avoid ill-conditioning, we work with biorthogonalized representatives obtained by applying the procedure described in Remark 3.2.

5.2. Transporting Tangent Vectors to New Points. The second problem that must be solved in order to implement conjugate gradient algorithms on Riemannian manifolds is how to make sense of the search direction defined by (5.2), which combines elements from two different tangent spaces. In particular, the previous gradient  J(pk-1) and the previous search direction k-1 lie in the tangent space Tpk-1 M, which is different from the tangent space Tpk M in which the current search direction k and gradient  J(pk) lie. To solve this problem, vectors in Tpk-1 M must be "transported" to the new tangent space Tpk M. The most natural notion of vector transport on a Riemannian manifold is parallel translation along geodesics; yet computing parallel translations can be expensive. The following Definition 5.2 of "vector transport" given by Absil et al. [1] retains only the properties that are essential in the context of optimization.
Definition 5.2 (Vector Transport [1]). Let the "Whitney sum"

(5.7)

T M  T M = {(p, p) : p, p  TpM, p  M}

denote pairs of tangent vectors sharing the same root points. A vector transport on the manifold M is a smooth mapping

(5.8)

T M  T M  T M : (p, p)  Tp (p)

satisfying the following properties:
1. (Associated retraction) There exists a retraction R, called the retraction asso-
ciated with T , such that Tp (p)  TRp(p)M for every (p, p)  T M  T M. 2. (Consistency) T0p (p) = p for all p  T M 3. (Linearity) Tp (ap + bp) = aTp (p) + bTp (p) for all a, b  R, p, p, p 
TpM, and every p  M.

As pointed out in [1], if one has a retraction, then a vector transport can be obtained by differentiating it and letting

(5.9)

d

Tp (p) := D Rp(p)p =

dt Rp(p + tp)

.
t=0

14

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

Following Example 8.1.10 in [1] and differentiating our retraction (5.6) on M = Gn,r × Gn,r, we obtain the following vector transport defined in terms of its horizontal lift

(5.10)

T(,)(V,W ) (, )(V,W ) (+¯,+¯) = P(h+¯,+¯) ¯, ¯ ,

for any (, )  -1(V, W ). We recall that P h is the orthogonal projection onto the horizontal space given by (4.7). The horizontal lifts of transported vectors at the biorthogonalized representatives computed using the procedure in Remark 3.2 are found by applying the transformation (4.9).

5.3. Geometric Conjugate Gradient Algorithm. The search directions for Riemannian conjugate gradient algorithms are computed by transporting the previous search direction according to

(5.11)

k = -  J (pk) + kTk-1k-1 (k-1).

We use the scaled Riemannian Dai-Yuan coefficient proposed by H. Sato [41] since it guarantees (under Lipschitz assumptions on D(J  R)) that the resulting conjugate gradient algorithm with line search based on Wolfe-type conditions always converges, though not necessarily to a global minimum of J. In particular, [41] defines the scaling factor

(5.12)

k = min

1,

k-1 pk-1

Tk-1k-1 (k-1) pk

,

and lets the coefficient k be defined by a scaled generalization of (5.3) given by

(5.13)

k(sRDY ) = k

k  J (pk),  J (pk) pk

.

 J (pk),

Tk-1k-1 (k-1)

-
pk

 J (pk-1),

k-1 pk-1

The Wolfe conditions to be satisfied during the line search are

(5.14) (5.15)

J (Rpk (kk))  J (pk) + c1k  J (pk), k pk d dt J (Rpk (tk)) t=k  c2  J (pk), k pk ,

with 0 < c1 < c2 < 1 and the bisection method described in [15] is used. We now have all of the machinery needed to implement a geometric conjugate gradient method, detailed below in Algorithm 5.1, to minimize the cost function (2.5) over pairs of subspaces P  M defining projection-based reduced-order models. In Appendix E we provide convergence guarantees for this algorithm applied to our optimal model reduction problem under modest conditions on the problem's setup (see Theorem E.1, Corollary E.7, and Corollary E.8). In particular, these guarantees say that for any threshold  > 0 on the gradient, Algorithm 5.1 will eventually stop.

6. Simple Nonlinear System with an Important Low-Energy Feature. In this section, we illustrate our method on a simple example system for which existing approaches to nonlinear model reduction perform poorly. In particular, we consider the system

(6.1)

x 1 = -x1 + 15x1x3 + u x 2 = -2x2 + 15x2x3 + u x 3 = -5x3 + u y = x1 + x2 + x3,

OPTIMIZING REDUCED-ORDER MODELS

15

Algorithm 5.1 Geometric conjugate gradient algorithm for model reduction

1: Input: biorthogonal representatives (0, 0) of the initial subspaces, stopping

threshold  > 0, and Wolfe condition coefficients 0 < c1 < c2 < 1

2: Compute cost J¯(0, 0) and gradient  J¯0 using Algorithm 4.1

3: Initialize the search direction (X0, Y0) =  J¯0 and set k = 0

4: while

 J¯k,  J¯k

> , given by (4.11), do
(k ,k )

5: Define line-search objective Jk() = J¯(k + Xk, k + Yk) via retraction

6: Compute step size k using the bisection method in [15], so that Jk(k) satisfies

the Wolfe conditions, namely Jk(k)  Jk(0)+c1kJk(0) and Jk(k)  c2Jk(0)

7: Compute next iterate (k+1, k+1) = (k + kXk, k + kYk) via retraction

8:

Transport

search

direction

(X~ k ,

Y~k )

=

Ph
(k+1

,k+1

)

(Xk

,

Yk )

using

(4.7)

9: Compute slim QR factorization k+1 = QR and biorthogonalizing transforma-

tion matrices S = R-1 and T = (Qk+1)-1

10: Biorthogonalize representatives (k+1, k+1)  (k+1S, k+1T ) and transform the search direction (X~k, Y~k)  (X~kS, Y~kT ) via (4.9)
11: Compute cost J¯(k+1, k+1) and gradient  J¯k+1 using Algorithm 4.1

12: Using (4.11), compute Riemannian Dai-Yuan scaling factor and coefficient

k+1 = min 1,

(Xk, Yk), (Xk, Yk) (k,k) (X~k, Y~k), (X~k, Y~k) (k+1,k+1)

,

k+1

=

k+1

k+1  J¯k+1,  J¯k+1 (k+1,k+1)  J¯k+1, (X~k, Y~k) (k+1,k+1) +  J¯k, (Xk, Yk)

(k ,k )

13: Compute next search direction (Xk+1, Yk+1) =  J¯k+1 + k+1(X~k, Y~k). 14: Update k  k + 1 15: end while 16: return biorthogonal representatives (K , K ) of the optimized projection sub-
spaces and the final cost J¯(K , K )

and we compare our method with POD/Galerkin projection onto the most energetic modes, and with Petrov-Galerkin projection onto subspaces determined by balanced truncation of the linearized system. We confine our attention to nonlinear impulseresponses with magnitudes u0  [0, 1]. These responses can be obtained by considering the output of (6.1) with u  0 and known initial condition x(0) = u0(1, 1, 1). Two such responses with u0 = 0.5 and u0 = 1 are shown in Figure 1a.
The key feature of (6.1) is that that state x3 plays a very important role in the dynamics of the states x1 and x2, while remaining small by comparison due to its fast decay rate. In fact, for u0 > 8/30 we have y(0) > 0 and the output experiences transient growth due to the nonlinear interaction of x1 and x2 with x3. These nonlinear interactions become dominant for larger u0, but are neglected completely by model reduction techniques like balanced truncation that consider only the linear part of (6.1). Figure 1a shows the result of such an approach, in which we obtain a nonlinear reduced-order model by Petrov-Galerkin projection of (6.1) onto a twodimensional subspace determined by balanced truncation of the linearized system. As shown in the figure, the resulting model wildly over-predicts the transient growth when u0 = 1.
On the other hand, a two-dimensional POD-based model retains the most ener-

16

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

y for u0 = 0.5 y for u0 = 1 (y^ - y)2/ avg (y2)

t (a) training trajectories

t

t

(b) error on testing trajectories

Fig. 1. In panel (a), we show the outputs generated by the full-order model (6.1) and the twodimensional reduced-order models found by POD Galerkin projection, balanced truncation, and our optimization approach in response to impulses with magnitudes u0 = 0.5 and u0 = 1 at t = 0. The sample points used to construct the objective function (6.2) used to optimize the projection operator are shown as black dots. In panel (b), we show the normalized square errors of the reduced-order model predictions in response to 50 impulses at t = 0 whose magnitudes u0 were drawn uniformly at random from the interval [0, 1].

getic states, which align closely with x1 and x2, and essentially ignores the important low-energy state x3. Consequently, the POD-based model of (6.1) does not predict any transient growth as shown in Figure 1a.
In order to find a two-dimensional reduced-order model of (6.1) using our new approach, we collected the two impulse-response trajectories shown in Figure 1a and used the L = 21 equally spaced samples shown for each trajectory to define the cost function

(6.2) J(V, W ) =
u0 {0.5,1.0}

1

L l=0

(y

|u0

(tl

))2

L
(y^|u0 (tl) - y|u0 (tl))2
l=0

+ (V, W ),

with  = 10-3 (although we note that the results were not sensitive to the choice of ). The normalizing factor in the cost for each trajectory was used to penalize the error relative to the average energy content of the trajectory, rather than in an absolute sense which would be dominated by the trajectory with u0 = 1. Starting from an initial model formed by balanced truncation, the conjugate gradient algorithm described above with Wolfe conditions defined by c1 = 0.4 and c2 = 0.8 achieved convergence with a gradient magnitude smaller than 10-4 after 88 steps.
In Figure 1a, we see that the resulting reduced-order model trajectories very closely match the trajectories used to find the oblique projection. Moreover, we tested the predictions of the three reduced-order models on 50 impulse-response trajectories with u0 drawn uniformly at random from the interval [0, 1]. The square output prediction errors for each trajectory normalized by the average output energy of the full-order model are shown in Figure 1b. We observe that the POD-based model is poor regardless of the impulse magnitude u0, whereas the balanced reduced-order model performs well when u0 is very close to 0, but poorly when u0 is closer to 1. On the other hand, our optimized reduced-order model yields very accurate predictions for all impulse response magnitudes in the desired range. Furthermore, the reducedorder model we trained to minimize error using two impulse responses has excellent

OPTIMIZING REDUCED-ORDER MODELS

17

y

t Fig. 2. We show the responses of (6.1) and the reduced-order models to input u(t) = sin(t).

predictive performance with different inputs. For instance, Figure 2 shows the predictions of the reduced-order models in response to sinusoidal input u(t) = sin(t) with zero initial condition.

7. Reduction of a High-Dimensional Nonlinear Fluid Flow. In this section we set out to develop a reduced-order model capable of predicting the response of an incompressible jet flow to impulsive disturbances in the proximity of the nozzle. We consider the evolution of an axisymmetric jet flow over the spatial domain  = {(r, z) | r  [0, Lr] , z  [0, Lz]}. Velocities are nondimensionalized by the centerline velocity U0, lengths by the jet diameter D0, and pressure by U02, where  is the fluid density.
Letting q = (u, v) denote the (dimensionless) velocity vector with axial component u and radial component v, and letting p be the (dimensionless) pressure field, we may write the governing equations in cylindrical coordinates as

(7.1) (7.2) (7.3)

u

u u p 1 1  u 2u

= -u - v - +

t

z r z Re

r r

r r

+ z2

v

v v p 1 1  v v 2v

= -u - v - +

t

z r r Re

r r

r r

- r2 + z2

u 1  + (rv) = 0,
z r r

where Re = U0D0/ is the Reynolds number (and  denotes the kinematic viscosity of the fluid). Formulas (7.1) and (7.2) are conservation of momentum statements in the axial and radial directions, respectively, while formula (7.3) is a mass conservation statement. Conservation of mass may be used to eliminate pressure from formulas (7.1) and (7.2), as discussed in Appendix G. We impose a zero-velocity boundary condition at r = Lr, a Neumann outflow boundary condition at z = Lz, and we let the inflow velocity be

(7.4)

1

1

1

u(r, 0) = 1 - tanh

r-

,

2

40

r

where 0 is a dimensionless thickness, which we fix at 0 = 0.0125. The equations of motion are integrated in time using the fractional step method
described in [34] in conjunction with the second-order Adams-Bashforth multistep scheme. The spatial discretization is performed on a fully-staggered grid of size Nz × Nr = 230 × 150 and with Lz = 8 and Lr = 3. If we let the state be composed of the axial and radial velocities at the cell faces, then the state dimension for this flow is 2(Nz × Nr) = 69, 000. All the spatial derivatives are treated with second-order central differences, except for the advective term q · q, which is treated with a thirdorder upwind scheme in order to avoid numerical instabilities. The solver has been

18

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

Energy = yTmym Energy = yTmym





t (a) training data

t (b) testing data

Fig. 3. In panel (a) we show the time history of the energy of the impulse responses in the training data set, with  = 0.1, 0.2, 0.4, 0.6, 0.8, 1.0. In panel (b) we show the analog of panel (a) for the testing data set, with  = 0.15, 0.3, 0.5, 0.7, 0.9. In each trajectory there are a total of 60 equally-spaced data points.

validated against some of the results presented in [46], for which we observed very good qualitative agreement. While the inner product on the state space is given by

(7.5)

f, g = f (r, z)g(r, z) r dr dz,


our observations, y, will be the full velocity field, with the standard Euclidean inner product (i.e., without weighting by r). A detailed derivation of the adjoint of the Navier-Stokes operator required to compute the gradient is presented in Appendix H.

7.1. Results. For the described flow configuration, there exists a convectively unstable steady-state solution, which we will denote Q. In particular, any perturbation q about the steady-state solution will grow while advecting downstream and it will eventually leave the computational domain through the outflow located at z = Lz. During the growth process, nonlinear effects become dominant and lead to the formation of complicated vortical structures. In this section we seek to develop a reduced-order model of the initial growth of these disturbances in response to an impulse, and we consider impulses that enter the radial momentum equation (7.2) through a velocity perturbation localized near r = 1/2 and z = 1; in particular, the perturbation has the form B(r, z)w(t), where

(7.6)

(r - 1/2)2 + (z - 1)2

B(r, z) = exp -

.

0

We simulate the response of the flow to a given impulse w(t) = (t), with   R, by integrating the governing equations (7.1)­(7.3) with initial condition

(7.7)

q(0) = Q + q (0), where q (0) = (0, B).

Here we construct a 50-dimensional reduced-order model to capture the initial response of the flow to impulses with 0.1    1.0 for times t  [0, 15].
We proceed as follows. We generate a training set of M = 6 trajectories corresponding to values  = 0.1, 0.2, 0.4, 0.6, 0.8, 1.0, and from each trajectory we observe L = 60 equally-spaced snapshots of velocity perturbations y = q about the base

OPTIMIZING REDUCED-ORDER MODELS

19

(y^m - ym)T (y^m - ym)/Em (y^m - ym)T (y^m - ym)/Em

t (a) training error

t (b) testing error

Fig. 4. In panel (a) we show the square error across all training trajectories for the optimal reduced-order model and for the POD-based reduced-order model. Panel (b) is the analog of panel (a), except that the error is computed against the testing trajectories.

flow Q. The energy content of the training set is shown in figure 3a. Observe the range of behavior for different values of , reflecting the strong nonlinearity of this flow. Let ym,l denote the lth velocity snapshot in the mth trajectory and let y^m,l denote the corresponding prediction obtained by integrating the reduced-order model from the initial condition q^m,0 = PV,W qm,0. Letting Em denote the average energy along the mth trajectory, we seek to minimize the cost function

(7.8)

J(V, W )

=

1 ML

M -1 m=0

1 Em

L-1
(y^m,l
l=0

- ym,l)T

(y^m,l

- ym,l) + (V, W ),

where  = 10-3. Optimization was carried out using Algorithm 5.1 with a 50dimensional model obtained by POD as the initial guess. The initial  modes were smoothly truncated near the outflow to satisfy the adjoint boundary conditions described in Appendix H.

Remark 7.1. It was advantageous to begin training the model on shorter trajectories and work our way up to the full time horizon.

After obtaining a minimizer, we test the performance of the optimal reduced-order model on a set of M = 5 unseen impulse responses with  = 0.15, 0.3, 0.5, 0.7, 0.9. The energy content of the testing set is shown in figure 3b. The performance of our reduced-order model is shown in figure 4 and it is compared against a 50-dimensional POD/Galerkin reduced-order model. We do not show a comparison against a BPODbased Petrov-Galerkin model because its predictions "blew up" after a few time units. Before proceeding in the analysis of the results, it is worth mentioning that the first 50 POD modes capture approximately 99.6% of the energy of the training data set, as well as approximately 99.6% of the energy of the testing data set. On the other hand, the subspace V we found by optimization captures 99.4% of the energy of both the training and testing sets. Figure 4a shows the error over time across all training trajectories for the optimal reduced-order model and for the POD-based reducedorder model. Figure 4b is the analog of figure 4a, except that the error is computed against the testing data. In both, we can observe that the average error of the optimal reduced-order model is one to two orders of magnitude lower than that of the PODbased model. Moreover, the error curves associated with the optimal model remain

20

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

ground truth

ground truth

r

r

optimized

optimized

r

r

POD

POD

r

r

z (a) vorticity at time t = 10 and  = 0.15

z (b) vorticity at time t = 14.75 and  = 0.15

Fig. 5. In panel (a) we show the predicted vorticity field from the testing trajectory with  = 0.15 at time t = 10.0. From the top we have the ground truth from the full-order model, the prediction given by the optimized model and the prediction given by POD. Panel (b) is the analog of panel (a), at time t = 14.75. The colormap has been saturated to allow for better visualization of the downstream structures.

ground truth

ground truth

r

r

optimized

optimized

r

r

POD

POD

r

r

z (a) vorticity at time t = 10 and  = 0.9

z (b) vorticity at time t = 14.75 and  = 0.9

Fig. 6. Analog of Figure 5 for a higher amplitude  = 0.9.

approximately constant for times t  [2, 15], which suggests that we capture the dynamically-relevant features of the full-order model. This is no surprise, since our framework is designed specifically to develop models that are dynamically accurate and that may therefore be used to predict the time-evolution of the full-order system over some time horizon.
Figures 5a and 5b show the predicted vorticity fields,  × (y^ + Q), at times t = 10 and t = 14.75 from the unseen testing trajectory with initial impulse amplitude  = 0.15. It can be seen from both panels in figure 5 that the prediction from the optimal model closely resembles the ground truth. While the POD-based prediction remains qualitatively close to the ground truth at time t = 10, the prediction at time t = 14.75 is inaccurate. In fact, the POD-based model mistakenly predicts the presence of two vortices at z  2 and z  3 and it also accumulates some phase error on the vortex located near the outflow.
Figures 6a and 6b show the predicted vorticity fields, at times (t = 10 and 14.75)

OPTIMIZING REDUCED-ORDER MODELS

21

from the unseen testing trajectory with initial impulse amplitude  = 0.9. Here, nonlinear effects dominate the dynamics and this is the reason why the POD-based model struggles to even capture the qualitative behavior. The optimized model, on the other hand, captures all instances of vortex shedding and vortex pairing, and it is capable of accurately predicting the locations of the resulting vortical structures.

8. Conclusions. We have introduced a reduced-order modeling approach for large-scale nonlinear dynamical systems based on optimizing oblique projections of the governing equations to minimize prediction error over sampled trajectories. We implemented a provably convergent geometric conjugate gradient algorithm in order to optimize a regularized trajectory prediction error over the product of Grassmann manifolds defining the projection operators. The computational cost to evaluate the gradient is dominated by the cost to assemble the reduced-order model and to evaluate the time derivative of the full-order model at quadrature points along each trajectory. While time-stepping schemes for the full-order model require very fine temporal discretizations to resolve dynamics over a wide range of scales and to remain numerically stable, we may compute the gradient using a much coarser temporal sampling and high-order quadrature rules. Thus, computing the gradient may be orders of magnitude less costly than running a direct simulation when the cost to assemble the reduced-order model is small by comparison.
The method is compared with Proper Orhogonal Decomposition (POD)-based Galerkin projection as well as Petrov-Galerkin projection onto balancing and adjoint modes derived from linearized dynamics about equilibria. We considered a simple three-dimensional system with an important low-energy feature as well as a nonlinear axisymmetric jet flow with 69, 000 state variables. In both cases, the optimized Petrov-Galerkin reduced-order model vastly out-performs the predictions made using the projection-based models obtained by POD and balanced truncation on new trajectories. We argue that this is because POD, while optimal for reconstructing states, ignores important low-energy features that influence the dynamics in the future. On the other hand, model reduction approaches like balanced truncation that rely only on the linearized dynamics can fail to capture features that have important nonlinear interactions far away from equilibria. Our approach is capable of capturing the relevant features needed to predict the nonlinear dynamics on a representative collection of trajectories.
The primary limitation of our approach is that a sufficiently large collection of trajectories must be used to avoid over-fitting. Based on algebraic considerations, the total number of sample data should exceed the dimension 2nr - 2r2 of the product of Grassmann manifolds over which we optimize, where n is the state dimension and r is the dimension of the reduced-order model. In order to avoid over-fitting we suggest exceeding this minimum data requirement by a factor of at least 3. For instance, in the case of our fluid flow example, r is small compared with n and we use 360 snapshots of the state as training data to optimize over subspaces with 2r = 100. When a large number of trajectories are used, it may be advantageous to employ a stochastic gradient descent algorithm [12, 43] with randomized "minibatches" of trajectories. Finally, we performed all computations on a personal computer and did not take advantage of the natural parallel structure of the costly gradient computation step. By taking advantage of this step's parallel structure across trajectories, evaluations at quadrature points, and decomposed spatial domains of the full-order model, we believe that the method can be applied to systems whose state dimensions are much higher than in our jet flow example.

22

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

REFERENCES
[1] P.-A. Absil, R. Mahony, and R. Sepulchre, Optimization algorithms on matrix manifolds, Princeton University Press, 2009.
[2] R. L. Adler, J.-P. Dedieu, J. Y. Margulies, M. Martens, and M. Shub, Newton's method on Riemannian manifolds and a geometric model for the human spine, IMA J. Numer. Anal., 22 (2002), pp. 359­390.
[3] S. Ahuja and C. W. Rowley, Feedback control of unstable steady states of flow past a flat plate using reduced-order estimators, J. Fluid Mech., 645 (2010), pp. 447­478.
[4] A. C. Antoulas, Approximation of large-scale dynamical systems, SIAM, 2005. [5] A. Barbagallo, D. Sipp, and P. J. Schmid, Closed-loop control of an open cavity flow using
reduced-order models, J. Fluid Mech., 641 (2009), p. 1. [6] U. Baur, P. Benner, and L. Feng, Model order reduction for linear and nonlinear systems:
a system-theoretic perspective, Arch. Comput. Meth. Eng., 21 (2014), pp. 331­358. [7] T. Bendokat, R. Zimmermann, and P.-A. Absil, A Grassmann manifold handbook: Basic
geometry and computational aspects, arXiv preprint arXiv:2011.13699, (2020). [8] P. Benner and T. Breiten, Interpolation-based H2-model reduction of bilinear control sys-
tems, SIAM J. Matrix Anal. Appl., 33 (2012), pp. 859­885. [9] P. Benner and T. Breiten, Two-sided projection methods for nonlinear model order reduc-
tion, SIAM J. Sci. Comput., 37 (2015), pp. B239­B260. [10] P. Benner, P. Goyal, and S. Gugercin, H2-quasi-optimal model order reduction for
quadratic-bilinear control systems, SIAM J. Matrix Anal. Appl., 39 (2018), pp. 983­1032. [11] P. Benner, S. Gugercin, and K. Willcox, A survey of projection-based model reduction
methods for parametric dynamical systems, SIAM Rev., 57 (2015), pp. 483­531. [12] S. Bonnabel, Stochastic gradient descent on Riemannian manifolds, IEEE Trans. Automat.
Contr., 58 (2013), pp. 2217­2229. [13] H. Brezis, Functional analysis, Sobolev spaces and partial differential equations, Springer Sci-
ence & Business Media, 2010. [14] G. L. Brown and A. Roshko, On density effects and large structure in turbulent mixing
layers, J. Fluid Mech., 64 (1974), pp. 775­816. [15] J. V. Burke, Line search methods. Lecture notes for MATH 408, Nonlinear Optimization,
University of Washington, published online at https://sites.math.washington.edu/burke/ crs/408/notes/nlp/line.pdf, 2004. [16] J. V. Burke, Continuity and differentiability of solutions. Lecture notes for MATH 555, Linear Analysis, University of Washington, published online at https://sites.math.washington. edu/burke/crs/555/555 notes/continuity.pdf, 2015. [17] Y.-H. Dai and Y. Yuan, A nonlinear conjugate gradient method with a strong global convergence property, SIAM J. Optim., 10 (1999), pp. 177­182. [18] G. Flagg and S. Gugercin, Multipoint Volterra series interpolation and H2 optimal model reduction of bilinear systems, SIAM J. Matrix Anal. Appl., 36 (2015), pp. 549­579. [19] S. Gugercin, A. C. Antoulas, and C. Beattie, H2 model reduction for large-scale linear dynamical systems, SIAM J. Matrix Anal. Appl., 30 (2008), pp. 609­638. [20] V. Guillemin and A. Pollack, Differential topology, vol. 370, American Mathematical Society, 2010. [21] B. Hall, Lie groups, Lie algebras, and representations: an elementary introduction, vol. 222, Springer, 2015. [22] P. Holmes, J. L. Lumley, G. Berkooz, and C. W. Rowley, Turbulence, coherent structures, dynamical systems and symmetry, Cambridge Univ. Press, 2012. [23] W. Huang, K. A. Gallivan, and P.-A. Absil, A Broyden class of quasi-Newton methods for Riemannian optimization, SIAM J. Optim., 25 (2015), pp. 1660­1685. [24] M. Ilak, S. Bagheri, L. Brandt, C. W. Rowley, and D. S. Henningson, Model reduction of the nonlinear complex Ginzburg­Landau equation, SIAM J. Appl. Dyn. Sys., 9 (2010), pp. 1284­1302. [25] S. J. Illingworth, A. S. Morgans, and C. W. Rowley, Feedback control of flow resonances using balanced reduced-order models, J. Sound Vib., 330 (2011), pp. 1567­1581. [26] Y.-L. Jiang and K.-L. Xu, Riemannian modified Polak­Ribi`ere­Polyak conjugate gradient order reduced model by tensor techniques, SIAM J. Matrix Anal. Appl., 41 (2020), pp. 432­ 463. [27] W. G. Kelly and A. C. Peterson, The Theory of Differential Equations, Classical and Qualitative, Pearson Prentice Hall, 2004. [28] J. M. Lee, Introduction to Smooth Manifolds: Second Edition, Springer New York, 2013. [29] K. Lee and K. T. Carlberg, Model reduction of dynamical systems on nonlinear manifolds

OPTIMIZING REDUCED-ORDER MODELS

23

using deep convolutional autoencoders, J. Comput. Phys., 404 (2020). [30] J. L. Lumley, The structure of inhomogeneous turbulent flows, Atmospheric turbulence and
radio wave propagation, (1967). [31] M. Marion and R. Temam, Nonlinear Galerkin methods, SIAM J. Numer. Anal., 26 (1989),
pp. 1139­1157. [32] B. Moore, Principal component analysis in linear systems: Controllability, observability, and
model reduction, IEEE Trans. Automat. Contr., 26 (1981), pp. 17­32. [33] J. Nocedal and S. Wright, Numerical optimization, Springer Science & Business Media,
2006. [34] J. B. Perot, An analysis of the fractional step method, J. Comput. Phys., 108 (1993), pp. 51­
58. [35] J. F. Queiro´, On the interlacing property for singular values and eigenvalues, Linear Algebra
and Its Applications, 97 (1987), pp. 23­28. [36] M. Reed and B. Simon, Methods of modern mathematical physics, Volume I: Functional
Analysis, Academic press, 1980. [37] G. Rega and H. Troger, Dimension reduction of dynamical systems: methods, models, ap-
plications, Nonlin. Dyn., 41 (2005), pp. 1­15. [38] W. Ring and B. Wirth, Optimization methods on Riemannian manifolds and their application
to shape space, SIAM J. Optim., 22 (2012), pp. 596­627. [39] C. W. Rowley, Model reduction for fluids, using balanced proper orthogonal decomposition,
Int. J. Bifurcation Chaos, 15 (2005), pp. 997­1013. [40] C. W. Rowley and S. T. Dawson, Model reduction for flow analysis and control, Ann. Rev.
Fluid Mech., 49 (2017), pp. 387­417. [41] H. Sato, A Dai­Yuan-type Riemannian conjugate gradient method with the weak Wolfe con-
ditions, Comput. Optim. and Appl., 64 (2016), pp. 101­118. [42] H. Sato and T. Iwai, A new, globally convergent Riemannian conjugate gradient method,
Optimization, 64 (2015), pp. 1011­1031. [43] H. Sato, H. Kasai, and B. Mishra, Riemannian stochastic variance reduced gradient algo-
rithm with retraction and vector transport, SIAM J. Optim., 29 (2019), pp. 1444­1472. [44] H. Sato and K. Sato, Riemannian trust-region methods for H2 optimal model reduction, in
2015 54th IEEE Conference on Decision and Control (CDC), IEEE, 2015, pp. 4648­4655. [45] P. J. Schmid and D. S. Henningson, Stability and Transition in Shear Flows, vol. 142,
Springer-Verlag New York, 2001. [46] L. Shaabani-Ardali, D. Sipp, and L. Lesshafft, Vortex pairing in jets as a global Floquet
instability: modal and transient dynamics, J. Fluid Mech., 862 (2019), pp. 951­989. [47] L. Sirovich, Turbulence and the dynamics of coherent structures: Part I: Coherent structures,
Q. Appl. Math., 45 (1987), pp. 561­571. [48] R. C. Thompson, Principal submatrices ix: Interlacing inequalities for singular values of
submatrices, Linear Algebra and its Applications, 5 (1972), pp. 1­12. [49] L. N. Trefethen, A. E. Trefethen, S. C. Reddy, and T. A. Driscoll, Hydrodynamic
stability without eigenvalues, Science, 261 (1993), pp. 578­584. [50] W.-G. Wang and Y.-L. Jiang, H2 optimal model order reduction on the Stiefel manifold for
the MIMO discrete system by the cross Gramian, Math. and Comp. Modeling of Dyn. Sys., 24 (2018), pp. 610­625. [51] P. Wolfe, Convergence conditions for ascent methods, SIAM Rev., 11 (1969), pp. 226­235. [52] K.-L. Xu and Y.-L. Jiang, An unconstrained h2 model order reduction optimisation algorithm based on the Stiefel manifold for bilinear systems, Int. J. Control, 92 (2019), pp. 950­959. [53] Y. Xu and T. Zeng, Fast optimal H2 model reduction algorithms based on Grassmann manifold optimization, International Journal of Numerical Analysis and Modeling, 10 (2013), pp. 972­991. [54] W.-Y. Yan and J. Lam, An approximate approach to H2 optimal model reduction, IEEE Trans. Automat. Contr., 44 (1999), pp. 1341­1358. [55] P. Yang, Y.-L. Jiang, and K.-L. Xu, A trust-region method for h2 model reduction of bilinear systems on the Stiefel manifold, J. Franklin Inst., 356 (2019), pp. 2258­2273. [56] T. Zeng and C. Lu, Two-sided Grassmann manifold algorithm for optimal model reduction, Int. J. Numer. Meth. Eng., 104 (2015), pp. 928­943.

Appendix A. Proof of Theorem 3.3 (Topology of P).
By Lemma 3.1 a function F on Gn,r × Gn,r is continuous if and only if F   is continuous on Rn×r × Rn×r. This allows us to establish the results by working with representatives in Rn×r × Rn×r rather than abstract subspaces in Gn,r × Gn,r.

24

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

To prove that P is open in Gn,r × Gn,r, consider the function

(A.1)

det()2 F  (, ) = det() det() ,

(, )  Rn×r × Rn×r.

The function F is well-defined because the above expression does not depend on the
representatives ,  due to the product rule for determinants. Furthermore, F   is continuous on Rn×r ×Rn×r and so it follows that P is open because it is the pre-image P = F -1((0, )) of the open set (0, ) by Proposition 2.2.
To prove that P is dense in Gn,r × Gn,r, consider a pair of subspaces (V, W )  (Gn,r ×Gn,r)\P and representatives (, )  Rn×r ×Rn×r such that (, ) = (V, W ). Consider the full-sized singular value decomposition

(A.2)

 = U QT

and define the continuously parameterized set of matrices

(A.3)

t =  + t()-1U QT , t  0,

giving rise to a continuously parameterized set of subspaces (V, Wt) = (, t)  Gn,r × Gn,r. We observe that for all t > 0, we have

(A.4)

det(t) = det U QT + tU QT = det(U ) det(Q) det ( + tI) = 0.

Therefore, (V, W0) = (V, W ) / P, but (V, Wt)  P for all t > 0, from which it follows that P is dense in Gn,r × Gn,r.
Since we are working with manifolds, connectedness and path-connectedness are equivalent. In order to prove the connectedness part of Theorem 3.3, we will need the following result:
Lemma A.1. If 1  r  n then Gn,r is connected. If 1  r < n then Rn×r is connected.
Proof. Our proof relies on the well-known result that the general linear group GLn has two connected components, corresponding to matrices with positive and negative determinants [21]. Choose any two subspaces V0, V1  Gn,r and let T0, T1  GLn be chosen such that the first r columns of T0 span V0 and the first r columns of T1 span V1. Recalling that changing the sign of a column changes the sign of the determinant, flip the sign on the first column of T0 if necessary to satisfy sgn det T0 = sgn det T1. Now there exists a continuous path t  At  GLn connecting T0 at t = 0 to T1 at t = 1. Since each At  GLn, its first r columns are linearly independent and span an r-dimensional subspace Vt  Gn,r The path t  Vt is a continuous path connecting V0 at t = 0 to V1 at t = 1.
Essentially the same approach is used to connect any two matrices 0, 1  Rn×r by a continuous path, with the caveat that we must have r < n. Let T0, T1  GLn be matrices whose first r columns are given by 0 and 1 respectively. Since r < n we may choose the sign of the last column of T0 so that sgn det T0 = sgn det T1. Now the first r columns of the matrices Tt  GLn along a continuous path connecting T0 and T1 form a continuous path connecting 0 and 1 in Rn×r.
To prove that P is connected we need only consider the case when r < n since when r = n we obviously have P = Gn,r × Gn,r, which is connected by Lemma A.1. Choose any (V0, W0), (V1, W1)  P and let (0, 0) and (1, 1) be representatives

OPTIMIZING REDUCED-ORDER MODELS

25

of (V0, W0) and (V1, W1) respectively such that 00 = Ir and 11 = Ir. Such representatives may always be found by first choosing any representatives ~ 0, ~ 0 of the subspaces V0, W0 and then letting 0 = ~ 0 and 0 = ~ 0(0~ 0)-1. We may do the same for 1 and 1. In order to construct our path in P, we first consider any continuously parameterized matrices (~ t, ~ t)  Rn×r × Rn×r, 0  t  1 furnished by Lemma A.1 such that (~ 0, ~ 0) = (0, 0) and (~ 1, ~ 1) = (1, 1). Our approach
will be to modify these matrices to avoid singularities. Since t  det (~ t ~ t) is a continuous function, there exists  > 0 such that for
every t  [0, )  (1 - , 1], the matrix ~ t ~ t is invertible. Moreover,  > 0 may be chosen small enough so that

(A.5)

^ t = ~ t(~ t ~ t)-1 t  [0, )  (1 - , 1],

is sufficiently close to ~ t that any convex combination of ^ t and ~ t has linearly independent columns. We observe that on [0, )  (1 - , 1], t  ^ t is continuous and ~ t ^ t = Ir. Furthermore, ^ t agrees with the original 0 at t = 0 and with 1 at t = 1.
Let  : R  [0, 1] be a smooth function such that (t) = 1 for every t  (-, 8/10]  [1 - 8/10, ) and (t) = 0 for every t  [9/10, 1 - 9/10]. We define the continuous set of matrices

(A.6)

t =

~ t (t)^ t + (1 - (t))~ t

t  [9/10, 1 - 9/10] otherwise

for 0  t  1 and we observe that t agrees with the original matrix 0 at t = 0 and with 1 at t = 1. Now let  : R  [0, 1] be a smooth function such that (t) = 1 for every t  (-, 1/10]  [1 - 1/10, ) and (t) = 0 for every t  [2/10, 1 - 2/10]. We now define the continuous set of matrices

(A.7)

t =

t (t)~ t + (1 - (t))t

t  [2/10, 1 - 2/10] otherwise

for 0  t  1 and we observe that t agree with the original matrix 0 at t = 0 and with 1 at t = 1. Finally, we observe that

(A.8)

t t =

t t (t)Ir + (1 - (t))^ t ^ t

t  [2/10, 1 - 2/10] otherwise

for if t  [0, 2/10)  (1 - 2/10, 1] then t = ^ t and ^ t ~ t = Ir. Therefore, t t is a positive-definite matrix for every t  [0, 1] and so (Vt, Wt) = (t, t)  P is a continuous path between (V0, W0)  P and (V1, W1)  P.
Finally, we conclude with

Lemma A.2. The submanifold P  Gn,r × Gn,r is diffeomorphic to

(A.9)

P = P  Rn×n : P 2 = P and rank(P ) = r .

Proof. The map  : P  P defined by

(A.10)

  (, ) = ()-1

is smooth by Lemma 3.1. Moreover,  is injective for if (Vi, Wi)  P, i  {0, 1} are subspace pairs with representatives (i, i)  -1(Vi, Wi) satisfying

(A.11)

0(00)-10 = 1(11)-11,

26

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

then V0 = Range(0) = Range(1) = V1 and W0 = Range(0) = Range(1) = W1.
To show that  is surjective, choose any P  P and consider a slim singular value decomposition P = U Q. It is clear that  is an invertible r × r diagonal matrix and the condition P 2 = P implies that

(A.12)

QU  =   QU = -1.

Therefore,

P

= U (QU )-1Q

for

some

Q, U



n,r
R

such

that

det(QU )

= 0.

Taking

(V, W ) = (U, Q) we obtain (V, W ) = P .

It now remains to show that -1 : P  P is differentiable. Let {ei}ni=1 be an

orthonormal basis for the state space X . If I = {i1, . . . , ir}  {1, . . . , n} is a subset

of r indices, let EI : Rr  X be defined by

(A.13)

EI z = ei1 z1 + · · · + eir zr.

Choose P  P and let I, J  {1, . . . , n} be sets of indices with |I| = |J| = r elements such that Range(P EI ) = Range(P ) and Range(P EJ ) = Range(P ). Here P  refers to the adjoint of P as an operator P : X  X on the state space X . Since  is
bijective, we have

(A.14) P = (Range(P ), Range(P )) = (P EJ ) [(P EI )(P EJ )]-1 (P EI ).

Most importantly, the same sets of indices I and J satisfy the above properties for every P~ in a sufficiently small neighborhood of P in P. The maps

(A.15)

P  P EJ , P  P EI ,

are smooth and so (A.16)

P  (P EJ , P EI )

is the smooth inverse of  over a small neighborhood of P in P. Since such a smooth inverse exists near every P  P it follows that  is a diffeomorphism.
Appendix B. Proof of Proposition 3.4 (Properties of ROM Solutions). Any solution of (2.4) is unique since (2.4) is smooth. This is a trivial consequence of Gr¨onwall's inequality (Corollary 8.62 in [27]). Suppose that x^0 and x^1 are two solutions of (2.4) over the interval [t0, tL-1] at the same (V, W )  P. Since these solutions are are continuous in time, they are contained in some closed ball B¯  X . Since (x, t)  f (x, u(t)) is continuously differentiable by Assumption 2.1, it is LLipschitz in B¯ for some finite L and we have

(B.1)

t
x^0(t) - x^1(t)  PV,W (f (x^0(s), u(s)) - f (x^1(s), u(s))) ds
t0 t
 L PV,W op x^0(s) - x^1(s) ds.
t0

By Gr¨onwall's inequality, it follows that

(B.2)

x^0(t) - x^1(t)  0,

which implies that x^0(t) = x^1(t) for all t  [t0, tL-1].

OPTIMIZING REDUCED-ORDER MODELS

27

Suppose that a solution x^0(t) = x^(t; (V0, W0)) exists for a given (V0, W0)  P. Since x^0 is continuous over the finite interval [t0, tL-1], it is bounded and contained in the open ball

(B.3)

B = x  X : x < sup x^0(t) + 1 .
t[t0 ,tL-1 ]

Moreover, by Assumption 2.1 it follows that every (t, x)  PV,W f (x, u(t)) with (V, W )  P is bounded and Lipschitz on B¯. Therefore, for any (V, W )  P such that
PV,W x0  B, the Picard-Lindelof theorem (Theorem 8.13 in W. G. Kelly A. C. Peter-
son [27]), ensures the reduced-order model (2.4) has a unique solution x^(t; (V, W )) in
B over an interval [t0, ] for some  > t0. Moreover, the extension theorem for ODEs
(Theorem 8.33 [27]) implies that the solution x^(t; (V, W )) of (2.4) exists in B for all
time t  t0, or there is a finite  so that x^(t; (V, W )) remains in B for t  [t0, ) and x^(t; (V, W ))  B as t  -. To be precise, the latter means that x^(t; (V, W )) leaves any compact subset of B as t  -.
For the sake of producing a contradiction, suppose that there is a sequence {(Vk, Wk)} k=1 such that (Vk, Wk)  (V0, W0) and for which the reduced-order model does not have a solution on [t0, tL-1]. Since the map  : (V, W )  PV,W is smooth
by Theorem 3.3, we may assume that each (Vk, Wk) is already in a sufficiently small
neighborhood of (V0, W0) such that PVk,Wk x0  B. Consequently, each reduced-order model solution x^k(t) = x^(t; (Vk, Wk)) exist and remains in B over some maximal interval [t0, k) with t0 < k < tL-1 and x^k(t)  B as t  k-.
To produce a contradiction, we show that x^k(t) remains close to x^0(t) over the
interval [t0, k) for sufficiently large k, which will be at odds with x^k(t)  B as t  k-. For t  [t0, k) we have the following bound on the difference between the trajectories

(B.4)

x^k(t) - x^0(t)  (PVk,Wk - PV0,W0 )x0

t

+

(PVk,Wk - PV0,W0 )f (x^k(s), u(s)) ds

t0

t

+

PV0,W0 f (x^k(s), u(s)) - PV0,W0 f (x^0(s), u(s)) ds.

t0

Let · op denote the induced norm (operator norm) and observe that since (t, x)  f (x, u(t)) is continuously differentiable with respect to x by Assumption 2.1, there are finite constants M and L such that

(B.5) (B.6)

f (x, u(t))  M f (x, u(t)) - f (z, u(t))  L x - z

x  B¯, t  [t0, tL-1] x, z  B¯, t  [t0, tL-1].

Therefore, we have

(B.7)

x^k(t) - x^0(t)  PVk,Wk - PV0,W0 op ( x0 + M (tL-1 - t0))

t

+ L PV0,W0 op

x^k(s) - x^0(s) ds.

t0

Applying Gr¨onwall's inequality (Corollary 8.62 in [27]), we see that

(B.8) x^k(t) - x^0(t)  PVk,Wk - PV0,W0 op ( x0 + M (tL-1 - t0)) eL PV0,W0 opt.

28

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

Since  : (V, W )  PV,W is continuous,

(B.9)

(Vk, Wk)  (V0, W0)  PVk,Wk - PV0,W0 op  0,

and so (B.8) implies that x^k(t) - x^0(t)  0 uniformly over t  [t0, k) as k  . In particular, we may take K sufficiently large so that for any k  K then

(B.10)

1

x^k(t) - x^0(t)

 2

t  [t0, k),

contradicting the fact that x^k(t)  B as t  k-. Therefore, there is an open neighborhood of (V0, W0) in P in which the reduced order model (2.4) has a unique

solution over the time interval [t0, tL-1], which establishes the openness of D in P.

Since D is open in P it follows that there is a set D  Gn,r × Gn,r such that D = D  P. Since P is open in Gn,r × Gn,r by Theorem 3.3, it follows that D is open

in Gn,r × Gn,r since it is a finite intersection of open sets.

Now, let us turn our attention to proving that D = P when f has bounded x-

derivatives. Since the partial derivatives of f with respect to x are bounded, it follows

that for any (V, W )  P there is a constant L such that

(B.11) PV,W f (x1, u(t)) - PV,W f (x2, u(t))  L x1 - x2

x1, x2  Rn, t  R

and so (x, t)  PV,W f (x, t) is Lipschitz in x uniformly over t. A trivial modification of Theorem 7.3 in H. Brezis [13] shows that a solution of the reduced-order model
(2.4) exists on the interval [t0, ). As a consequence, D = P. Now we shall establish the differentiability of x^(t; (V, W )) at each fixed t 
[t0, tL-1] with respect to (V, W )  D. Recall that by Theorem 3.3, the set of rank-r projection matrices P is smoothly diffeomorphic to the 2nr - 2r2 dimensional submanifold P  Gn,r × Gn,r. Let  : R2nr-2r2  U  D be a local parameterization of a an open subset U  D. Letting  : (V, W )  PV,W be the diffeomorphism established by Theorem 3.3, the map P =    is a smooth parameterization of the
open subset (U)  P. It suffices to show that the solution x^(t; (p0)) is continuously differentiable with respect to p0  R2nr-2r2 .
We define the augmented state variable w = (x, p)  Rn × R2nr-2r2 whose dynamics are described by

(B.12)

d w = F (w, t) :=
dt

P (p)f (x, u(t)) 02nr-r2

w(0) = w0.

Clearly, we have w(t; w0) = x^(t; (p0)) when w0 = (P (p0)x0, p0). It is also clear from Assumption 2.1 that F is continuously differentiable with respect to w, and so
by Theorem 8.43 in [27], it follows that w(t; w0) is continuously differentiable with respect to w0. This proves that x^(t; (V, W )) is continuously differentiable with respect to (V, W ) since w0 = (P (p0)x0, p0) is continuously differentiable with respect to p0.
Finally, suppose that {(Vk, Wk)} k=1  D is a sequence approaching (Vk, Wk)  (V0, W0)  P \ D. Denote x^k(t) = x^(t; (Vk, Wk)) and x^0(t) = x^(t; (V0, W0)), and let [t0, R) be the maximum interval of existence for x^0 in an open ball BR  X of radius R > PV0,W0 x0 centered about the origin. Here we have again made use of the extension theorem for solutions of ordinary differential equations (Theorem 8.33 [27]).
It is clear that since (x, t)  f (x, u(t)) is continuously differentiable with respect to
x by Assumption 2.1, it is L-Lipschitz and bounded by M on BR for some finite L and M . Let us suppose that k is sufficiently large so that PVk,Wk x0  BR and

OPTIMIZING REDUCED-ORDER MODELS

29

let [t0, k) denote the maximum interval of existence for x^k(t) in BR. There are two possibilities, either k < R which implies that supt[t0,tL-1] x^k(t)  R, or k  R which implies that x^k(t)  BR for every t  [t0, R). In the second case, the same Gr¨onwall argument we used above shows that that
(B.13) x^k(t) - x^0(t)  PVk,Wk - PV0,W0 op ( x0 + M (tL-1 - t0)) eL PV0,W0 opt
for every t  [t0, R). Since, PVk,Wk  PV0,W0 , we may take k sufficiently large so that the above inequality implies that x^k(t) - x^0(t)  R/2 for every t  [t0, R) when x^k(t)  BR for every t  [t0, R). Since x^0(t)  BR as t  R-, we must have supt[t0,tL-1] x^k(t)  R/2 in the case when x^k(t)  BR for every t  [t0, R). It follows that for sufficiently large k, we always have supt[t0,tL-1] x^k(t)  R/2. Since R was arbitrary, it follows that

(B.14)

sup x^k(t)   as k  .
t[t0 ,tL-1 ]

Furthermore, the sup is actually a max because the trajectories x^k are continuous. This completes the proof of Proposition 3.4.

Appendix C. Regularization and Existence of a Minimizer.

Proof of Theorem 3.5 (Regularization). We begin by showing that (V, W )  + as (V, W )  (V0, W0)  Gn,r × Gn,r. Let 0, 0  -1(V0, W0). By the local
submersion theorem [20], there is an open neighborhood V  Gn,r × Gn,r containing (V0, W0) and an open neighborhood U  Rn×r × Rn×r containing (0, 0) together with local parameterizations  : R2nr  U and  : R2nr-2r2  V of these neighbor-
hoods such that (0, 0) = (0), (V0, W0) = (0), and

(C.1)

(-1    )(x1, . . . , x2nr) = (x1, . . . , x2nr-2r2 ).

Since (Vn, Wn)  (V0, W0) there exist N such that for every n  N , we have (Vn, Wn)  V. Let z(n) = -1(Vn, Wn) be the coordinates of these subspace pairs
for n  N and let us choose the representatives of these subspaces whose coordinates are x(n) = (z(n), 0, . . . , 0)  R2nr, i.e., let (n, n) = (z(n), 0, . . . , 0). It is clear that z(n)  0 as n   and so we have (n, n)  (0, 0) as n   by continuity of
the local parameterizaions. Since the determinant is a continuous function, we have

(C.2)

lim
n

det

(nn)

=

det

(0 0 )

=

0,

lim
n

det

(nn)

=

det

(0 0 )

>

0,

lim
n

det

(nn)

=

det

(0 0 )

>

0

and so it follows that

(C.3)

(Vn, Wn) =   (n, n)   as n  .

Now we seek a minimum of  by first considering the function F : Gn,r × Gn,r  R defined by

(C.4)

det()2 F  (, ) = det() det()

and observing that it is continuous on Gn,r × Gn,r. Since Gn,r is compact, it follows that Gn,r × Gn,r is also compact, and so F attains its maximum. Moreover, if V = W

30

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

then obviously (V, W )  P and choosing the columns of  =  to be an orthonormal basis for V , we find that

(C.5)

det()2 F (V, V ) = det() det() = 1  (V, V ) = - log F (V, V ) = 0.

Consequently, the maximum value of F is at least 1 and so any subspace pair (Vm, Wm) that maximizes F must lie in P = F -1((0, )) and also minimize R = - log F . Since
 is a smooth function on the open set P (see Theorem 3.3), a necessary condition
for (Vm, Wm) to be a minimizer of  is D (Vm, Wm)(, ) = 0 for every (, )  T(Vm,Wm)Gn,r × Gn,r. Or, in terms of representatives, if (m, m)  Rn×r × Rn×r is a representative of subspaces (Vm, Wm) = (m, m)  P that minimize , then for every pair of matrices (X, Y )  Rn×r × Rn×r we have

(C.6) 0 = D(  )(m, m)(X, Y ) = Tr (mm)-1(mX + Xm) + Tr (mm)-1(mY + Y m) - 2 Tr (mm)-1(mX + Y m) .
Applying permutation identities for the trace and collecting terms we have

(C.7) 0 = Tr (mm)-1m - (mm)-1m X + Tr Y  m(mm)-1 - m(mm)-1
for every (X, Y )  Rn×r × Rn×r, which implies that

(C.8) (mm)-1m = (mm)-1m and m(mm)-1 = m(mm)-1.

The above is true only if Range m = Range m; and so a necessary condition for (Vm, Wm) to minimize  over P is that Vm = Wm. But we have already seen that (V, W ) = 0 when V = W , proving that zero is the minimum value of , and the minimum is attained if and only if the subspaces (V, W ) satisfy V = W .
Corollary C.1 (Existence of a Minimizer). Let D be as in Proposition 3.4, and take  > 0. We assume that D is nonempty. Then a minimizer of (2.5) exists in D; that is, there exists a pair of subspaces (Vop, Wop)  D such that

(C.9)

J(Vop, Wop)  J(V, W ), for all (V, W )  D.

Let the set of subspaces defining orthogonal projection operators be denoted

(C.10)

P0 = {(V, W )  P : V = W }

and assume that D  P0 is nonempty. Then, as   , any choice of minimizers, denoted (Vop(), Wop()), approaches D  P0. Furthermore, the corresponding cost, temporarily denoted J(V, W ; ) to emphasize the dependence on , approaches the minimum over D  P0, i.e.,

(C.11)

lim


J

(Vop

(),

Wop

(

);

)  J(V, V ),

for all V  Gn,r.

Note that J(V, V ) does not depend on , since (V, V ) = 0.
Proof. Choose a sequence {(Vn, Wn)} n=1 in D such that

(C.12)

lim J(Vn, Wn) = inf J(V, W ) < .

n

(V,W )D

OPTIMIZING REDUCED-ORDER MODELS

31

Since the Grassmann manifold Gn,r is compact, it follows that Gn,r × Gn,r is compact, and so there exists a convergent subsequence (Vnk , Wnk )  (V0, W0) for some (V0, W0)  Gn,r × Gn,r. We must have (V0, W0)  P; for if not, Theorem 3.5 tells us that (Vnk , Wnk )  + and so J (Vnk , Wnk )  + as k   because Ly  0, contradicting (C.12). Furthermore, (V0, W0)  D, for if not then Proposition 3.4 and Assumption 3.6 imply that J(Vnk , Wnk )  + as k  , contradicting (C.12). The cost function J defined by (2.5) is continuously differentiable on D because the ROM
solution at each sample time (V, W )  x^(ti, (V, W )) is continuously differentiable by Proposition 3.4 and the regularization function defined by (3.8) is smooth. Since J is
continuous on D, we have

(C.13)

inf
(V,W )D

J (V ,

W

)

=

lim
k

J

(Vnk

,

Wnk

)

=

J

(V0,

W0),

proving that (V0, W0) achieves the minimum value of J over D. To prove the second claim about the behavior as   , we begin by observing
that minimizing J over P0 is equivalent to minimizing V  J(V, V ) over Gn,r. By the convention in Assumption 3.6, we have J(V, W ) =  for any (V, W )  P0 \ D. In addition, this minimization does not depend on  because (V, V ) = 0 by Theorem 3.5.
Let us begin by showing that a minimizer of V  J(V, V ) over Gn,r exists. Let {(Vk, Vk)} k=1  D  P0 be a sequence such that

(C.14)

J(Vk, Vk)  inf J(V, V ) <  as k  .
V Gn,r

Since Gn,r is compact, we may pass to a convergent subsequence, still denoted by {(Vk, Vk)} k=1, such that Vk  V0  Gn,r. Clearly, we have (V0, V0)  P0. If (V0, V0) / D then Proposition 3.4 and Assumption 3.6 imply that J(Vk, Vk)  . But this contradicts the fact that the sequence {J(Vk, Vk)} k=1 approaches the infemum of J over P0, which is finite if D  P0 = . Therefore, (V0, V0)  D  P0 and since J is
continuous over D it follows that

(C.15)

inf J (V, V ) = lim J (Vk, Vk) = J (V0, V0),

V Gn,r

k

i.e., (V0, V0) achieves the minimum value of J over P0. Suppose, for the sake of producing a contradiction, that there is an open neigh-
borhood U  D containing D  P0 such that for every  > 0, there exists    such that (Vop(), Wop()) / U . Then Gn,r × Gn,r \ U is a closed subset of Gn,r × Gn,r and hence is compact. By the same argument presented above,  attains its minimum over P \ U, and this value is strictly greater than zero by Theorem 3.5. Consequently, we would have (C.16)
J (V0, V0)  J (Vop(), Wop(); )   min (V, W )   as   ,
(V,W )P\U

contradicting the fact that J(V0, V0) < . Therefore, for every open neighborhood U  D of D  P0, there exists  > 0 such that for every   , we have (Vop(), Wop())  U .
Finally, suppose for the sake of producing a contradiction that there exists  > 0
such that for every  > 0 there exists    such that J(Vop(), Wop(); )  J(V0, V0) - . By continuity of the objective on D, we know that the non-empty set

(C.17)

U = {(V, W )  D : J(V, W ; 0) > J(V0, V0) - }

32

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

is open in D and contains D  P0. And so for every  > 0 we have a    such that

(C.18)

J (Vop(), Wop(); 0)  J (Vop(), Wop(); )  J (V0, V0) - ,

which implies that (Vop(), Wop()) / U , contradicting the fact that

(C.19)

(Vop(), Wop())  D  P0 as   .

Therefore, we conclude that J(Vop(), Wop(); )  J(V0, V0) as   .
Appendix D. Adjoint-Based Gradient and Required Terms. Proof of Theorem 4.1 (Adjoint-Based Gradient). We shall compute the gradients of the component functions

(D.1)

Ji(C, z0) := Ly(g~(z(ti); C) - yi)

and use linear superposition to construct the gradient of (4.2). Consider a small
perturbation z(t) about the trajectory z(t) due to small perturbations of the independent variables C  TC M¯ and z0  Rr governed by the linearized dynamics

(D.2)

d z(t) - F (t)z(t) = S(t)C.
dt

with perturbations to the observables described by

(D.3)

y(t) = H(t)z(t) + T (t)C.

The resulting perturbation of each component of the objective is given by

(D.4) Ji = H(ti)  Ly(g~ (z(ti); C) - yi), z(ti) + T (ti)  Ly(g~ (z(ti); C) - yi), C C ,

and we wish to express its dependence explicitly on the optimization variables. The second term's dependence on C is trivial, so we will focus on revealing the implicit dependence of the first term on the optimization variables. To this end, we denote the first term of (D.4) by i and we construct a signal i(t) so that

(D.5)

ti

i = i(t0), z(t0) +

i(t), S(t)C dt.

t0

This allows us to write the perturbation of the sub-objective in terms of inner products between the gradients and perturbations in each optimization variable:

(D.6)

i = i(t0), z(t0) +

ti
S(t)i(t)dt, C .
t0

To construct i(t), we substitute the linearized dynamics (D.2) into (D.5) and integrate by parts

(D.7)

ti
i = i(t0), z(t0) +
t0
ti
= i(ti), z(ti) +
t0

d

i(t),

z(t) - F (t)z(t) dt

dt

-

d dt

i(t)

-

F

(t)i(t),

z(t)

dt.

OPTIMIZING REDUCED-ORDER MODELS

33

Equating with the first term of (D.4) for all signals z(t), we find that the adjoint variable i(t) must satisfy the sub-objective adjoint dynamics

(D.8)

-

d dt

i(t)

=

F (t)i(t),

i(ti) = H(ti)  Ly(g~ (z(ti); C) - yi).

Finally, by linear superposition we can write the variation of the entire objective in terms of the perturbations to the optimization variables as

(D.9) J¯0 = (t0), z(t0)

tL-1

L-1

+

S(t)(t)dt + T (ti)  Ly(g~ (z(ti); C) - yi), C

t0

i=0

C

using an adjoint variable (t) =

L-1 i=0

[t0,ti](t)i(t)

satisfying

(D.10) - d (t) = F (t)(t), dt

(ti) = lim (t)+H(ti)  Ly(g~ (z(ti); C)-yi),
tt+ i

for i = 0, . . . , L - 2, and (tL-1) = H(tL-1)  Ly(g~ (z(tL-1); C) - yL-1). Proof of Corollary 4.2 (Known Initial Condition). By the chain rule, we have

(D.11) (D.12) (D.13)

 J¯(C)C C

=

 C

J¯0(C, z0)C

+

 z0

J¯0(C,

z0

)

 C

z0(C

)C

= C J¯0(C, z0), C C +

z0 J¯0(C, z0),

 C z0(C)C

=

C J¯0(C, z0) +

 C z0(C)


z0 J¯0(C, z0), C

C

for every C  TC M¯ .
Proof of Proposition 4.4 (Required Terms for Gradient). Our proof of each expression follows directly from the definition of the adjoint of a linear operator between finite-dimensional real inner product spaces. Choosing a pair of vectors v, w  Rr, we have

(D.14)

F (t)v, w =

 f~(z(t), u(t); (, ))v

T
w

z

= v,

 f~(z(t), u(t); (, ))

T
w

,

z

which implies (4.12). In precisely the same way we obtain (4.14). For the next parts, we will need the following:
Lemma D.1. Letting A : Rp  Rq and L : Rq  X be linear operators, we have

(D.15)

(LA) = AT L.

If M : X  Rp is another linear operator, then we have

(D.16)

(AM ) = M AT .

34

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

Proof. Let v  Rp and w  X and observe that

(D.17)

LAv, w X = Av, Lw Rq = v, AT Lw Rp ,

from which we conclude (LA) = AT L. Observing that M  : Rp  X , we use the above result to show

(D.18)

M AT


= AM,

from which we conclude M AT = M AT  = (AM ). Now we consider a vector (X, Y )  T(,)M¯ and a vector w  Rdim y, yielding

(D.19)

 T (t)(X, Y ), w = g(z(t))Xz(t), w
x = Tr z(t)wT  g(z(t))X
x

Applying Lemma D.1, we obtain

(D.20)

T (t)(X, Y ), w = Tr

 g(z(t))


wz(t)T


X

x

= Tr ()-1

 g(z(t))





wz(t)T () X

x

= (X, Y ),

 g(z(t))


wz(t)T (), 0

x

,
(,)

from which we conclude that (4.15) holds for all w  Rdim y. Consider a vector (X, Y )  T(,)M¯ and a vector v  Rr, and observe that

(D.21) S(t)(X, Y ), v

= (Y  - Y  - X) f (z(t), u(t)) +   f (z(t), u(t))Xz(t), v x

= Y  (I - ) f (z(t), u(t)), v - f (z(t), u(t)), Xv

+ z(t), X





f (z(t), u(t)) v .

x

Each term in (D.21) is given by a trace, in particular, (D.22) Y  (I - ) f (z(t), u(t)), v = Tr Y  (I - ) f (z(t), u(t))vT .

(D.23)

f (z(t), u(t)), Xv = Tr Xv (f (z(t), u(t)))T

(D.24)

z(t), X





f (z(t), u(t)) v

= Tr X

 f (z(t), u(t))


vz(t)T

.

x

x

OPTIMIZING REDUCED-ORDER MODELS

35

Substituting back into (D.21) and combining terms on X and Y  we obtain

(D.25) S(t)(X, Y ), v

= Tr X

 f (z(t), u(t))


vz(t)T - v (f (z(t), u(t)))T

x

+ Tr Y  (I - ) f (z(t), u(t))vT .

Recalling that the time derivative of the reduced-order model is given by f~(z(t), u(t)) = f (z(t), u(t)) yields (4.13).

Now, consider a vector (X, Y )  T(,)M¯ and a vector v  Rr, and observe that

(D.26)

 (, ) z0(, )(X, Y ), v

= Y x0 - Y x0 - Xx0, v

= Tr Y  (x0 - x0) vT - Tr Xx0vT

= Tr Y  (x0 - x0) vT - Tr v(x0)T X

= Tr Y  (x0 - x0) vT - Tr Xv(x0)T .

The above is written in terms of the Riemannian metric at (, ) as

(D.27)

 (, ) z0(, )(X, Y ), v

= Tr ()-1Y  (x0 - x0) vT 

- Tr ()-1Xv(x0)T 

= (X, Y ), -v(x0)T , (x0 - x0) vT  (,) ,

which yields (4.16).
Finally, we compute the gradient of the regularization (3.8) by considering a perturbation (X, Y )  T(,)M¯ and writing the resulting perturbation of    as

(D.28) D(  )(, )(X, Y ) = Tr ()-1(X + X) + Tr ()-1(Y + Y )
- 2 Tr ()-1(X + Y ) .

Applying permutation identities for the trace and collecting terms we have
(D.29) D(  )(, )(X, Y ) = 2 Tr ()-1 - ()-1 X + 2 Tr Y  ()-1 - ()-1 ,
yielding
(D.30) D(  )(, )(X, Y ) = 2 Tr ()-1  - ()-1()  X + 2 Tr ()-1  - ()-1()  Y
= 2  - ()-1() , 2  - ()-1() , (X, Y ) (,) . Under the additional assumption that  = Ir, we obtain (4.17). This completes the proof of Proposition 4.4.

36

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

Appendix E. Convergence Guarantees. Here we provide convergence guarantees for the algorithm presented in subsection 5.3 applied to our optimal model reduction problem under modest conditions on the problem's setup. Leveraging the results proved by H. Sato in [41], we obtain general conditions for convergence that we state in Theorem E.1 and specialize to useful classes of systems in Corollary E.7, and Corollary E.8. The algorithm converges in the sense of (E.3), which says that the gradient will eventually become arbitrarily small. Hence, an algorithm whose stopping condition is based on the gradient being sufficiently small will not run forever.
Theorem E.1 (Convergence of Conjugate Gradient Algorithm). Suppose that the functions x  g(x) and (x, t)  f (x, u(t)) describing the full-order model dynamics (2.1) along with their second-order partial derivatives with respect to x are continuous. Let the loss function Ly be twice continuously differentiable, take  > 0, and let D be as in Proposition 3.4. We assume that D contains the initial point (V0, W0) and every (V, W )  P such that J(V, W )  J(V0, W0). Then, at each pk = (Vk, Wk) starting from p0 = (V0, W0), there exists a step size k  0 so that pk+1 = Rpk (kk) satisfies the Wolfe conditions along the search direction k = (k, k)  Tpk P computed by the scaled Riemannian Dai-Yuan conjugate gradient method. Consequently, the cost is non-increasing, i.e.,

(E.1)

J (Vk+1, Wk+1)  J (Vk, Wk) k  0.

Moreover, the step sizes k may always be chosen small enough such that

(E.2)

J (Rpk (tk))  J (V0, W0) t  [0, k].

Let us now assume that there is a subset Dc  D such that Dc is closed in M and contains every (V, W )  P for which J(V, W )  J(V0, W0). If the step sizes are chosen so that Rpk (tk)  Dc for every t  [0, k], which is always possible by (E.2), then the conjugate gradient algorithm converges in the sense that

(E.3)

lim inf
k

 J (Vk, Wk) (Vk,Wk) = 0.

Proof of Theorem E.1. The openness of the set D in P on which the reducedorder model has a unique solution over the desired time interval was established in Proposition 3.4. Now we show that the cost function J given by (2.5) is twice continuously differentiable with respect to the subspaces (V, W ) over the open subset D  P. The following Lemma E.2 shows that the solution for the state of the reducedorder model (2.4) is twice continuously differentiable over D. Combining this with the fact that g and Ly are twice continuously differentiable and that the regularization function  defined by (3.8) is infinitely many times continuously differentiable on P, it follows that the cost function J is twice continuously differentiable over D.
Lemma E.2. The solution x^(t; (V, W )) of the reduced-order model at any t  [t0, tL-1] is twice continuously differentiable with respect to (V, W ) over the open subset D  P.
Proof. Recall that by Theorem 3.3, the set of rank-r projection matrices P is smoothly diffeomorphic to the 2nr - 2r2 dimensional submanifold P  Gn,r × Gn,r. Let  : R2nr-2r2  U  D be a local parameterization of a an open subset U  D. Letting  : (V, W )  PV,W be the diffeomorphism established by Theorem 3.3, the map P =    is a smooth parameterization of the open subset (U)  P.

OPTIMIZING REDUCED-ORDER MODELS

37

Define the augmented state variable w = (x, p)  Rn × R2nr-2r2 whose dynamics are described by

(E.4)

d

P (p)f (x, u(t))

w = F (w, t) := dt

02nr-r2

w(0) = w0.

Clearly, we have w(t; w0) = x^(t; (p0)) when w0 = (P (p0)x0, p0). It is also clear that

F is twice continuously differentiable with respect to w, and so by Theorem 8.43 in

[27], it follows that w(t; w0) is differentiable with respect to w0. Furthermore, the

Jacobian

matrix

w(1)

=

 w(t;w0 ) w0

satisfies

(E.5)

dw dt w(1)

= F (1)(w, w(1), t) :=

F (w, t)

 w

F

(w,

t)w(1)

,

w(1)(0) = w0(1) =

w0 I

by the chain rule. Our smoothness assumptions on f now ensure that F (1) is con-

tinuously differentiable with respect to w and w(1). Applying Theorem 8.43 in [27]

once more we find that w(1)(t; w0(1)) is continuously differentiable with respect to w0(1).

Since

w0

is

an

element

of

w0(1)

it

follows

that

 w(t;w0 ) w0

is

continuosly

differentiable

with

respect to w0 and so w(t; w0) is twice continuously differentiable with respect to w0.

Finally, since w0 = (P (p0)x0, p0) is infinitely many times continuously differen-

tiable with respect to p0, the chain rule shows that x^(t; (p0)) is twice continuously

differentiable with respect to p0. Since  was an arbitrary smooth parameterization

of an open subset of D it follows that x^(t; (V, W )) is twice continuously differentiable

with respect to (V, W ) over D.

Remark E.3. The argument used to prove Lemma E.2 can be iterated as in [16] to prove d-times continuous differentiability of x^(t; (V, W )) with respect to (V, W ) for any integer d  1 as long as (x, t)  f (x, u(t)) has continuous partial derivatives with respect to x up to order d.

We assumed that p0 = (V0, W0)  D, so let us suppose that the current iterate pk = (Vk, Wk), k  1, of the conjugate gradient algorithm also lies in D and satisfies J(pk)  J(pk-1)  ...  J(p0). When  J(pk) = 0 then we are already at a local extremum of the cost function and k = 0 clearly satisfies the Wolfe conditions and yields pk+1 = Rpk (kk) = pk and J (pk+1)  J (pk). On the other hand, Proposition 4.1 in [41] shows that when the gradient  J(pk) = 0 then the conjugate gradient-based search direction is a descent direction, that is

(E.6)

d dt J (Rpk (tk)) t=0 =  J (pk), k pk < 0.

By Lemma 3.1, the retraction Rpk : Tpk M  M defined by (5.6) is infinitely many times continuously differentiable. It follows that the line search function

(E.7)

Jk(t) = J (Rpk (tk))

is twice continuously differentiable over the open set Dk = {t  R : Rpk (tk)  D}. By our assumptions on Dc  D, it is clear that Dk contains the closed set {t  R : Jk(t)  J(V0, W0)} and so Dk contains {t  R : Jk(t)  J(Vk, Wk)}. To prove that there exists k satisfying the Wolfe conditions

(E.8) (E.9)

Jk(k)  Jk(0) + c1kJk(0) Jk(k)  c2Jk(0),

38

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

with 0 < c1 < c2 < 1, we follow the same argument as Lemma 3.1 in J. Nocedal and
S. J. Wright [33]. First we observe that all t  0 such that Jk(t)  Jk(0) + c1tJk(0) automatically has Jk(t)  Jk(0)  J(p0) and so t  Dk. Since J, and hence Jk, is
bounded below, but (t) := Jk(0) + c1tJk(0) is not, the graphs of Jk(t) and (t) must intersect for some t > 0. Let  > 0 be the smallest value such that Jk() = ().
Since Jk(0) < 0 it follows that Jk(t) < (t) for every t  (0, )  Dk. By the mean value theorem, there exists k  (0, ) such that

(E.10)

Jk() - Jk(0) = Jk(k).

It follows that

(E.11)

Jk (k )

=

Jk ()

- 

Jk (0)

=

()

- Jk(0) 

=

c1 Jk (0)



c2 Jk (0),

and so k satisfies the Wolfe conditions. Moreover, we have

(E.12)

J (Rpk (tk)  J (pk) t  [0, k],

and in particular J (pk+1)  J (pk) where the next iterate is pk+1 = Rpk (kk). Therefore we have proven that a Wolfe step satisfying (E.12), and hence (E.2) always exists and that the conjugate gradient algorithm produces a non-increasing sequence of costs J(pk+1)  J(pk) for every k  0.
We now turn our attention to showing that the conjugate gradient algorithm converges in the sense of (E.3). We shall do this by verifying the hypotheses of Theorem 4.2 in [41], which give sufficient conditions for the algorithm to converge in the sense of (E.3). In particular, if there is a Lipschitz constant L such that for every p  P and   TpP with unit magnitude  p = 1 we have

(E.13)

|D(J  Rp)(t) - D(J  Rp)(0)|  Lt, t > 0,

then (E.3) holds. In fact, following the argument in Appendix A of [42], the result of Theorem 4.2 in [41], i.e., (E.3), still holds under the weaker condition that

(E.14)

|D(J  Rpk )(kk)k - D(J  Rpk )(0)k|  Lk

k

2 pk

,

for each of the iterates k  0. Here, we shall prove that there is a constant L such that

(E.15)

d2 dt2 (J  Rp)(t)  L,

t  0 s.t. Rp(t)  Dc

for every p  P and   TpP with  p = 1. Taking p = pk and  = k/ k pk , the assumption Rpk ( k)  Dc for every   [0, k] ensures that Rp(t)  Dc for all t in the interval 0  t  k k pk . Integrating (E.15) over this interval proves (E.14) and therefore (E.3). It now remains to verify (E.15).
As a closed subset of the compact manifold M = Gn,r × Gn,r, the set Dc is compact. Moreover, our assumptions guarantee that Dc contains the entire search path

(E.16)


{Rpk (tk) : t  [0, k]} .
k=0

OPTIMIZING REDUCED-ORDER MODELS

39

Letting  : P  P be the C diffeomorphism established in Theorem 3.3, it is clear
that (Dc) is compact and contained in the relatively open set (D)  P. In order to prove (E.15), it will be easier to lift the problem into Rn×n by working with the equivalent cost function J  -1 on a neighborhood of (D) in Rn×n.
Since (D) is open in P and P is a smooth submanifold of Rn×n by Theorem 3.3, it follows that (D) is a smooth submanifold of Rn×n. The following Lemma E.4 shows that the restriction J  -1 (D) of the lifted cost function to (D) may be extended to a twice continuously differentiable function J~ defined on an open neighborhood U of (D) in Rn×n.
Lemma E.4. Let N be a smooth d-dimensional submanifold of Rk and let h : N  R be a Cr function. Then h may be extended to a Cr function h~ defined on an open neighborhood of N in Rk.
Proof. Let i : N  Rk be the injection of N into Rk. By the local immersion theorem [20] there is an open neighborhood Up  Rk of each point p  N and a C diffeomorphism p : Rk  Up such that (x1, . . . , xd)  (x1, . . . , xd, 0, . . . , 0) is a C parameterization of N  Up. Let  : Rk  Rk denote the projection onto the
leading d-dimensional coordinate subspace (x1, . . . , xk) = (x1, . . . , xd, 0, . . . , 0) and let h~p : Up  R be given by

(E.17)

h~p = h  p    p-1.

Then h~p is a Cr extension of h to the neighborhood Up since p    p-1 is the identity on N  Up.
Covering N by a union U = pN Up of such neighborhoods, we may construct a smooth partition of unity {i} i=1 with the following properties [20]:
1. 0  i(p)  1 for each p  U , 2. each p  U has a neighborhood on which all but finitely many i are identically
zero,

3. 4.

the support of each i is contained in

and

 i=1

i(p)

=

1

for

every

p



U.

a

closed

subset

of

some

Upi ,

The final Cr extension of h to the neighborhood U is constructed by letting

(E.18)


h~ = ih~pi .
i=1

Since each i is C, each h~pi is Cr, and only finitely many i are nonzero on a neighborhood of any p  U , it is easy to see that h~ is Cr on U . Finally, h~pi (p) = h(p)
when p  Upi implies that for any p  N we have

(E.19)



h~(p) = i(p)h~pi (p) =

i(p)h~pi (p) =

i=1

i : pUpi


i(p)
i=1

h(p) = h(p),

and so h~ agrees with h on N . The function J  R may now be written in terms of J~ according to

(E.20)

J  R(V,W )(, ) = J    R¯V,W (¯, ¯) = J  -1    ( + ¯,  + ¯) = J~ ( + ¯) ( + ¯)( + ¯) -1 ( + ¯) ,

40

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

when (, )  -1(V, W ) are representatives. Letting

(E.21)

P (t) := ( + t¯) ( + t¯)( + t¯) -1 ( + t¯),

for any projection subspaces (V, W )  P, tangent vector (, )  T(V,W )P with unit magnitude, and representatives (, )  -1(V, W ), then (E.15) is equivalent to

(E.22)

d2 dt2

J~(P

(t))

L

t  0 s.t. P (t)  (Dc).

Without loss of generality, we shall assume that the chosen representatives are or-

thonormal, i.e.,  =  = Ir. With this choice, the assumption that (, ) 

T(V,W )P has unit magnitude (, ) (V,W ) = 1 implies that the Hilbert-Schmidt norms

of the horizontal lifts satisfy

¯

2 HS

+

¯

2 HS

=

1.

Remark E.5 (Hilbert-Schmidt Norm and Trace). Since we deal with operators

between (finite-dimensional) Hilbert spaces with possibly different inner products, it will be convenient to use Hilbert-Schmidt norms. For instance ¯ is an operator from Rr into the state space X , which has its own norm that may differ from the usual norm on Rn. Since all the operators we deal with here are finite-dimensional, they
are automatically Hilbert-Schmidt operators. If A : H1  H2 is a Hilbert-Schmidt
operator between two Hilbert spaces H1 and H2, then the Hilbert-Schmidt norm is
defined using a generalized notion of trace. In particular, if {ei} is an orthonormal
basis for H1 then

(E.23)

A

2 HS

=

Aei

2 H2

=

ei, AAei =: Tr (AA),

i

i

and this definition is independent of the choice of bases. If B : H2  H3 is another Hilbert-Schmidt operator then we have

(E.24)

AB HS  A HS B HS .

For more details, see Problem 40 in H. Brezis [13] and Section 6.6 in M. Reed and B. Simon [36]. If H1 = H3 and H2 are finite-dimensional, then the trace satisfies the usual adjoint and permutation identities, namely

(E.25)

Tr(BA) = Tr(AB) = Tr(BA) = Tr(AB).

When H1 = Rp and H2 = Rq then the trace defined above is the usual trace and the Hilbert-Schmidt norm is the Frobenius norm.
Differentiating J~(P (t)), and denoting derivates of P (t) with respect to t by P (t), P (t), etc., we shall bound each term of

(E.26)

d2 dt2

J~(P (t))

=

D2

J~(P (t))[P

(t), P

(t)]

+ D J~(P (t))P

(t),

in order to prove (E.22). Since P (t) is assumed to remain in the compact set (Dc) and the derivative D J~(P (t)) and Hessian D2 J~(P (t)) are continuous on U  (Dc), it follows immediately that D J~(P (t)) and D2 J~(P (t)) are bounded. Obviously P (t) is also bounded since it lies in the compact set (Dc).

OPTIMIZING REDUCED-ORDER MODELS

41

It remains to show that P (t) and P (t) are bounded by brute-force differentiation. To simplify our notation, let us define

(E.27)

G(t) = ( + t¯)( + t¯), G(t) = ( + t¯)( + t¯), G,(t) = ( + t¯)( + t¯),

and observe that since we chose  and  to be orthonormal representatives, we have G(0) = G(0) = Ir. The following Lemma E.6 shows that several of the terms that will appear in the expressions for P (t) and P (t) are uniformly bounded.

Lemma E.6. The following bounds hold uniformly over all (V, W )  P, all orthonormal representatives (, )  -1(V, W ), all unit magnitude (, )  T(V,W )P,
and all t  0 such that P (t)  (Dc):

(E.28)

G(t)-1

HS

 r

and

G(t)-1

 HS  r

(E.29)

( + t¯)G(t)-1

HS

 r

and

( + t¯)G(t)-1

 HS  r

(E.30)

( + t¯)G(t)-1( + t¯) HS =

( + t¯)G(t)-1( + t¯)

 HS = r

(E.31)

G,(t)-1 HS  r P (t) HS

(E.32)

( + t¯)G,(t)-1

 HS  r

P (t)

HS

and

G,(t)-1( + t¯)

 HS  r

P (t)

HS.

Proof. Since ¯ is in the horizontal subspace at , we have ¯ = 0 and so G(t) = +t2¯ ¯ is positive-definite. The same holds for G(t) = +t2¯ ¯. Consequently, the eigenvalues i(t) of G(t) are positive and non-decreasing with t,
so

(E.33)

G(t)-1

r

2 HS

=

i=1

1 i(t)2

r1  i=1 i(0)2

=

G(0)-1

2 HS

=

r.

Similarly, we have

G(t)-1

2 HS



r.

By direct calculation, we have

(E.34)

( + t¯)G(t)-1

2 HS

=

Tr

G(t)-1( + t¯)( + t¯)G(t)-1

= Tr

G(t)-1

r1

r1

=



= r.

i=1 i(t) i=1 i(0)

Similarly, we have

( + t¯)G(t)-1

2 HS



r.

By the permutation identity for the trace, we have

(E.35)

( + t¯)G(t)-1( + t¯)

2 HS

=

Tr

( + t¯)G(t)-1( + t¯)

= Tr G(t)G(t)-1 = Tr(Ir) = r.

Similarly, we have

( + t¯)G(t)-1( + t¯)

2 HS

= r.

42

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

To bound G,(t)-1, we observe that

(E.36)

G,(t)-1 = G(t)-1G(t)G,(t)-1G(t)G(t)-1 = G(t)-1( + t¯)P (t)( + t¯)G(t)-1.

By the above arguments, we have

(E.37) G,(t)-1 HS  G(t)-1( + t¯) HS P (t) HS ( + t¯)G(t)-1 HS  r P (t) HS.

Since P (t)  (Dc) and (Dc) is a compact subset of Rn×n, it follows that P (t) HS is bounded and so G,(t)-1 HS is bounded as a consequence.
Using a similar argument, we observe that

(E.38)

G,(t)-1( + t¯) = G(t)-1( + t¯)P (t).

By the bound for G(t)-1( + t¯) proved previously, we conclude that

(E.39) G,(t)-1( + t¯) HS 

G(t)-1( + t¯)

HS

P (t)

 HS  r

P (t)

HS

is bounded. Similarly, we have

( + t¯)G,(t)-1

HS

 r

P (t)

HS.

We take another swig of coffee and differentiate:

(E.40) P (t) = ¯G,(t)-1( + t¯) + ( + t¯)G,(t)-1¯ - ( + t¯)G,(t)-1¯ ( + t¯)G,(t)-1( + t¯)

P (t)
- ( + t¯)G,(t)-1( + t¯) ¯G,(t)-1( + t¯).

P (t)

By Lemma E.6 and the fact that

¯

2 HS

+

¯

2 HS

=

1,

every

term

in

the

above

expression for P (t) is uniformly bounded and so P (t) is uniformly bounded.

Let us now write P (t) as a sum of four terms

(E.41) P (t) = ¯G,(t)-1( + t¯) + ( + t¯)G,(t)-1¯

A(t)

B(t)

- ( + t¯)G,(t)-1¯ P (t) - P (t)¯G,(t)-1( + t¯),

C (t)

D(t)

and show that the derivative of each term is bounded. Fortunately, B(t) and D(t) are what we get when we swap  and  in A(t) and C(t) respectively. We shall only show that A (t) and C (t) are uniformly bounded since the corresponding arguments for B (t) and D (t) are simply obtained by swapping  and  line for line.

(E.42) A (t) = -¯G,(t)-1¯ ( + t¯)G,(t)-1( + t¯)

P (t)
- ¯G,(t)-1( + t¯)¯G,(t)-1( + t¯) + ¯G,(t)-1¯

Again by Lemma E.6 and the fact that

¯

2 HS

+

¯

2 HS

=

1,

every

term

in

the

above expression for A (t) is uniformly bounded and so A (t) is uniformly bounded.

OPTIMIZING REDUCED-ORDER MODELS

43

We now differentiate C(t) and obtain

(E.43)

C (t) = ¯G,(t)-1¯ P (t) + ( + t¯)G,(t)-1¯ P (t) - ( + t¯)G,(t)-1¯ ( + t¯)G,(t)-1¯ P (t) - ( + t¯)G,(t)-1( + t¯) ¯G,(t)-1¯ P (t).

P (t)

Finally, by Lemma E.6, the boundedness of P (t) proved above, and the fact that

¯

2 HS

+

¯

2 HS

=

1,

every

term

in

the

above

expression

for

C (t)

is

uniformly

bounded and so C (t) is uniformly bounded. Repeating the symmetric arguments for

B (t) and D (t) we finally conclude that P (t) is uniformly bounded. We have now

established that every term in (E.26) is uniformly bounded and so (E.22) holds for

some fixed L, completing the proof of Theorem E.1.

While the conditions in Theorem E.1 appear abstract, they actually encompass two very important special cases. In Corollary E.7, below, we show that when the fullorder model has bounded first derivatives with respect to the state variables, then the conjugate gradient algorithm always converges. For instance, Corollary E.7 implies that the algorithm always converges when the governing equations are linear.

Corollary E.7. Suppose that f , g, and Ly are as in Theorem E.1 and  > 0

and

suppose

that

 x

f

(x,

u(t))

is

bounded.

If

there

is

a

finite constant

C



(V0, W0)/

so that the step sizes k satisfy the Wolfe conditions and (Rpk (tk))  C for every

t  [0, k], then the conjugate gradient algorithm converges in the sense of (E.3).

Proof. We need only verify the assumptions in Theorem E.1: namely, that there
is a subset Dc  D that is closed in M and contains every (V, W )  P for which J(V, W )  J(V0, W0). By Proposition 3.4, our assumption that the Jocobian of the full-order model is bounded implies that D = P. It is also clear that Dc = {(V, W )  P : (V, W )  C} is closed in M. To see this, suppose that {pk} k=1  Dc is a sequence such that pk  p  M. Then Theorem 3.5 implies that p  P, for if not then (pk)  , which contradicts (pk)  C. Since  is continuous on P, it follows that (p) = limk (pk)  C and so p  Dc.
Finally, if C  (V0, W0)/, then any (V, W )  P with J(V, W )  J(V0, W0) must have

(E.44)

1

1

(V, W )



J(V, W ) 



 J (V0, W0)



C

and so (V, W )  Dc. Furthermore, the property that Rpk (tk)  Dc for every t  [0, k] is satisfied
automatically by our assumption, so Theorem E.1 implies that (E.3) holds, and the algorithm converges.
On the other hand, many important systems, such as the discretized NavierStokes equations have quadratic nonlinearities resulting in unbounded first derivatives with respect to the state variables. In such cases, a poorly chosen projection-based reduced-order model (2.4) may have states that blow up in finite time. As long as Assumption 3.6 holds (i.e., any finite-time blow-up also results in an unbounded cost J), then the following Corollary E.8 shows that the conjugate gradient algorithm will converge.
Corollary E.8. Suppose that f , g, and Ly are as in Theorem E.1 and  > 0. If there is a finite constant C  J(V0, W0) so that the step sizes satisfy the Wolfe

44

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

conditions and J(Rpk (tk))  C for every t  [0, k], then the conjugate gradient algorithm converges in the sense of (E.3).
Proof. As we have seen in the proof of Proposition 3.4, the smoothness of f ensures that if a solution of the reduced-order model (2.4) exists, it must be unique. Moreover, if a solution of the reduced-order model does not exist over [t0, tL-1], then the solution x^(t; (V, W )) can be defined over some maximal interval [t0, ) with t0 <  < tL-1 and x^(t; (V, W ))   as t  -. Consequently, if the reducedorder model does not have a solution for (V, W )  P over [t0, tL-1] then J(V, W ) = . Therefore, the set D can be identified with the set over which the cost function is finite, i.e.,

(E.45)

D = {(V, W )  P : J(V, W ) < }.

We shall show that the set

(E.46)

Dc = {(V, W )  P : J(V, W )  C}

is closed in M, which will complete the proof since (E.45) implies that Dc  D and C > J (V0, W0). Suppose that {(V~k, W~ k)} k=1  Dc is a sequence such that (V~k, W~ k)  (V~ , W~ )  M. Then it is clear that (V~ , W~ )  P, for if it were not then Theorem 3.5 would give (V~ , W~ )   and so J(V~ , W~ )  . Moreover, if (V~ , W~ )  P \ D then by Proposition 3.4 we would have

(E.47)

max x^(t; (V~k, W~ k))   as k  ,
t[t0 ,tL-1 ]

which implies that J(V~k, W~ k)   by Assumption 3.6. This contradicts the assumption that J(V~k, W~ k)  C for every k. Therefore, the limit point (V~ , W~ )  D. In Proposition 3.4 we showed that J is differentiable, and hence continuous on D and so

(E.48)

J(V~ , W~ ) = lim J(V~k, W~ k)  C.
k

Therefore, (V~ , W~ )  Dc and we conclude that Dc is closed in M. Applying Theorem E.1 completes the proof.

Appendix F. Auxiliary Proofs and Results.
Proof of Proposition 2.2 (Subspaces Defining Oblique Projections). Suppose that w  W is nonzero and w  V . Since Range  = W and Range  = V , there exists z  Rr such that w = z and

(F.1)

0 = z, x = zT x x  Rr.

It follows that z  Range () and so det () = 0. Similarly, suppose that v  V and v  W . Then there exists z  Rr such that v = z and xT z = 0 for every x  Rr. Hence z  Null () and so det () = 0.
On the other hand, if det () = 0 then there exists nonzero n  Null () and nonzero z  Range (). It follows that n  V and z  W are nonzero vectors satisfying n  W and z  V because for every x  Rr we have 0 = xT n = x, n and 0 = zT x = z, x . This proves that the first three statements
are equivalent. Suppose that the first three statement hold, then x^ = ()-1x satisfies

(F.2)

w, x^ = ()-1w, x = w, x

w  W

OPTIMIZING REDUCED-ORDER MODELS

45

because any w  W can be written as w = z for some z  Rr and ()-1w = ()-1z = z = w. Moreover, if there is another x^ satisfying the desired condition then x^ - x^  V is orthogonal to W ; hence x^ = x^ by the first statement.
Finally suppose that there exists a nonzero element w  W that is orthogonal to V . For this w, we have w, w = 0 and w, x^ = 0 for every x^  V , contradicting the fourth statement when x = w. Even if we have an x  Rn for which there exists x^  V satisfying w, x = w, x^ for every w  W , there is a nonzero v  V that is orthogonal to W , any multiple of which can be added to x^, contradicting uniqueness of x^.
Proposition F.1 (Bound on Projection Operators). The operator norm of PV,W for (V, W )  P is bounded by the regularization function (V, W ) according to

(F.3)

PV,W op =

sup

PV,W v X  e(V,W )/2.

vX : v X 1

Proof. Let (, )  -1(V, W ) be orthonormal representatives of (V, W )  P, i.e.,  =  = Ir. By the interlacing properties of singular values [35, 48], it follows that the lth singular values of  and  satisfy l()  l() = 1, for
each 1  l  r. Therefore, by (3.3) and (3.8) we have

(F.4)

PV,W

op 

 op ()-1 op  op =

()-1

op =

1 r ( )



1

r i=1

i()

=

1 |det()|

=

e(V,W )/2.

Appendix G. The role of pressure in incompressible flows. As mentioned in the body of the paper in section 7, the pressure may be removed entirely from the Navier-Stokes formulation (7.1)­(7.3).
Let us write equations (7.1)­(7.3) in compact form as

(G.1)

I 0

0 0

 t

q p

=

F D

-G 0

q p

+

g(q) 0

,

where q = (u, v), D is the divergence operator, G is the gradient operator, F contains the vector Laplacian and g(q) contains the nonlinear terms in the momentum equations (7.1) and (7.2). Taking the divergence of the first row of (G.1), and using Dq = 0 (by the second row of (G.1)) along with DF q = F Dq = 0, we obtain a Poisson equation

(G.2)

DG p = Dg(q),
L

where L is the scalar Laplacian operator. Often, instead of prescribing pressure boundary conditions at the physical boundaries of the spatial domain, a unique solution to (G.2) is instead computed by fixing the pressure and the pressure gradient at some location (r0, z0) in physical space [34]. That is, in cylindrical coordinates

(G.3)

p p

p= = =0 z r

at (r0, z0)  ,

46

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

where (r0, z0) may be chosen arbitrarily. This approach is particularly convenient in numerical methods based on the finite volume or finite difference discretization of the spatial dimensions. The pressure may thus be written as

(G.4)

p = L-1Dg(q),

and consequently (G.1) may be reduced to

(G.5)

 q = F q + g(q) - GL-1Dg(q) = f (q), t

which is in the form of (2.1). In practice, in order to compute the action of f on some vector field q, we proceed as follows:
1. compute (q) = F q + g(q),
2. compute the pressure by solving Lp = D(q), and finally 3. compute f (q) = (q) - Gp.

Appendix H. Derivation of the adjoint of the Navier-Stokes equation. In this appendix we derive the adjoint of the Navier-Stokes equation linearized about a steady solution Q = (U, V ), which satisfies the boundary conditions described in section 7. We will work in cylindrical coordinates, and the adjoint equation will be derived with respect to the inner product

(H.1)

f, g = f (r, z)g(r, z) r dr dz,


where  = {(r, z)| r  [0, Lr], z  [0, Lz]} is the spatial domain. We let q = (u, v) be the two-dimensional velocity with axial component u and
radial component v, and we let p be the pressure, as in section 7. For a given Reynolds number Re, the linearized Navier-Stokes equation and the continuity equation then read,

u

U U u u p 1 1  u 2u

(H.2) = -u - v - U - V - +

t

z r z r z Re

r r

r r

+ z2

v

V V

v

v p 1 1  v

v 2v

(H.3) = -u - v - U - V - +

t

z r z r r Re

r r

r r

- r2 + z2

(H.4)

u 1  + (rv) = 0,
z r r

with velocity boundary conditions

(H.5) (H.6) (H.7)

u =v=0
r u=v=0
u v = =0
z z

at r = 0 at r = Lr, z = 0 at z = Lz.

As discussed in appendix Appendix G, in order to uniquely determine the pressure field, it suffices to fix the pressure and the pressure gradients at some location (r0, z0) in physical space, rather than specifying pressure boundary conditions at the boundaries of the physical domain. We therefore let

(H.8)

p p

p= = =0 z r

at (r0, z0)  .

OPTIMIZING REDUCED-ORDER MODELS

47

Compactly, the equations of motion (H.2)-(H.4) may be written as

(H.9)

I 0

0 0

 t

q p

=

L D

-G 0

q p

.

N

Letting q = (u, v) and p denote the adjoint velocity field and adjoint pressure field, we seek an operator N  such that

(H.10)

(q, p), N (q, p) = N (q, p), (q, p) .

Using the inner product defined in (H.1), we have

(q, p), N (q, p) =



u

U U u u p 1 -u - v - U - V - +

z r z r z Re

1 r r

u r
r

2u + z2

+

v

V V v v p 1 -u - v - U - V - +
z r z r r Re

1 r r

v r
r

v 2v - r2 + z2

+

p

u 1  + (rv)

r dr dz.

z r r

Integrating by parts twice with respect to r and with respect to z, and using the fact that the steady-state solution Q = (U, V ) satisfies the continuity equation (H.4), we obtain



u

-u U

- v V

u +U

u +V

p -

+

1

z

z

z

r z Re

1 r r

u r
r

2u + z2

+

v

-u U - v V

v

v p

+U +V - +

1

r

r

z

r r Re

1 r r

v r
r

v 2v - r2 + z2

+

p u + 1  (rv) z r r

r dr dz + I1 + I2 = N (q, p), (q, p) ,

where I1 and I2 are boundary integrals arising from the integration by parts and they are given by

I1 =
1 Re

- ruV u + 1 ur u - ur u

Re

r

r

vr v

-

v vr

+ rpv

r=Lr
dz,

r

r

r=0

- rvV v - rvp+

and

I2 =
1 Re

- uU u - up + 1

u

u

-

u u

Re z z

v v

-

v v

+ pu

z=Lz
r dr.

z z

z=0

- vUv+

Using the fact that V (r = 0) = V (r = Lr) = 0, the boundary integrals I1 and I2

48

S. E. OTTO, A. PADOVAN, AND C. W. ROWLEY

vanish if the adjoint fields satisfy the following boundary conditions:

(H.11) (H.12) (H.13)

u = v = 0 r u = v = 0

u = vU +

1

v = p -

1

u =0

Re z

Re z

at r = 0 at r = Lr, z = 0 at z = Lz.

A noteworthy difference between the forward and adjoint formulations, is the fact that a pressure boundary condition at the outflow z = Lz now arises in the adjoint formulation, while in the forward formulation we had prescribed the constraints (H.8) at (r0, z0)  . As a concluding remark, the pressure may be removed from both the forward and adjoint formulations in a similar fashion as described in appendix Appendix G. The pressure Poisson equation arising in the forward formulation may be solved with the constraints in (H.8), while the Poisson equation arising in the adjoint formulation may be solved using the pressure boundary condition in (H.13).

