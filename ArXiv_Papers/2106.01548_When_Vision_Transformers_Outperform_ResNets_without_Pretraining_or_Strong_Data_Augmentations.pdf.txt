When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations

arXiv:2106.01548v1 [cs.CV] 3 Jun 2021

Xiangning Chen1,2

Cho-Jui Hsieh2

Boqing Gong1

1Google Research

2UCLA

Abstract
Vision Transformers (ViTs) and MLPs signal further efforts on replacing handwired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pretraining and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rate). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3% and +11.0% top-1 accuracy on ImageNet for ViT-B/16 and MixerB/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pretraining or strong data augmentations. They also possess more perceptive attention maps.

1 Introduction
Transformers [56] have become the de-facto model of choice in natural language processing (NLP) [19, 44]. In computer vision, there has recently been a surge of interest in end-to-end Transformers [1, 2, 5, 20, 22, 38, 55] and MLPs [37, 40, 53, 54], prompting the efforts to replace hand-wired features or inductive biases with general-purpose neural architectures powered by datadriven training. We envision these efforts may lead to a unified knowledge base that produces versatile representations for different data modalities, simplifying the inference and deployment of deep learning models in various application scenarios.
Despite the appealing potential of moving toward general-purpose neural architectures, the lack of convolution-like inductive bias also challenges the training of vision Transformers (ViTs) and MLPs. When trained on ImageNet [18] with the conventional Inception-style data preprocessing [51], Transformers "yield modest accuracies of a few percentage points below ResNets of comparable size" [20]. To boost the performance, existing works resort to large-scale pre-training [1, 2, 20] and repeated strong data augmentations [55], resulting in excessive demands of data, computing, and sophisticated tuning of many hyper-parameters. For instance, Dosovitskiy et al. [20] pre-train ViTs using 304M labeled images, and Touvron et al. [55] repeatedly stack four strong image augmentations.
Work done as a student researcher at Google.
Preprint. Under review.

In this paper, we show ViTs can outperform ResNets [24] of even bigger sizes in both accuracy and various forms of robustness by using a principled optimizer, without the need for large-scale pre-training or strong data augmentations. MLP-Mixers [53] also become on par with ResNets.
We first study the architectures fully trained on ImageNet from the lens of loss landscapes and draw the following findings. First, visualization and Hessian matrices of the loss landscapes reveal that Transformers and MLP-Mixers converge at extremely sharp local minima, whose largest principal curvatures are almost an order of magnitude bigger than ResNets. Such effect accumulates when the gradients backpropagate from the last layer to the first, and the initial embedding layer suffers the largest eigenvalue of the corresponding sub-diagonal Hessian. Second, the networks all have very small training errors, and MLP-Mixers are more prone to overfitting than ViTs of more parameters (probably because of the difference in self-attention). Third, ViTs and MLP-Mixers have worse "trainabilities" than ResNets following the neural tangent kernel analyses [58].
We conjecture that the convolution-induced translation equivariance and locality help ResNets escape from bad local minima when trained on visual data. However, we need improved learning algorithms to prevent them from happening to the convolution-free ViTs and and MLP-Mixers. The first-order optimizers (e.g., SGD [41] and Adam [32]) only seek the model parameters that minimize the training error. They dismiss the higher-order information such as flatness that correlates with the generalization [10, 29, 30, 33, 48].
The above study and reasoning lead us to the recently proposed sharpness-aware minimizer (SAM) [23] that explicitly smooths the loss geometry during model training. SAM strives to find a solution whose entire neighborhood has low losses rather than focus on any singleton point. We show that the resultant models exhibit smoother loss landscapes, and their generalization capabilities improve tremendously across different tasks including supervised, adversarial, contrastive, and transfer learning (e.g., +5.3% and +11.0% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). The enhanced ViTs achieve better accuracy and robustness than ResNets of similar and bigger sizes when trained from scratch on ImageNet, without large-scale pretraining or strong data augmentations.
By analyzing some intrinsic model properties, we find the models after SAM reduce the Hessian eigenvalues by activating sparser neurons (on ImageNet), especially in the first few layers. The weight norms increase, implying the commonly used weight decay may not be an effective regularization alone. A side observation is that, unlike ResNets and MLP-Mixers, ViTs have extremely sparse active neurons (less than 5% for most layers), revealing the redundancy of input image patches and the capacity for network pruning. Finally, we note that ViTs' performance gain also translates to plausible attention maps containing more perspicuous information about semantic segmentation than those trained without the sharpness-aware optimization.
2 Background and Related Work
We briefly review ViTs, MLP-Mixers, and some related works in this section.
Dosovitskiy et al. [20] show that a pure Transformer architecture [56] can achieve state-of-the-art accuracy on image classification by pretraining it on large datasets such as ImageNet-21k [18] and JFT-300M [50]. Their vision Transformer (ViT) is a stack of residual blocks, each containing a multi-head self-attention, layer normalization [3], and a MLP layer. ViT first embeds an input image x  RH×W ×C into a sequence of features z  RN×D by applying a linear projection over N nonoverlapping image patches xp  RN×(P 2·C), where D is the feature dimension, P is the patch resolution, and N = HW/P 2 is the sequence length. The self-attention layers in ViT are global and do not possess the locality and translation equivariance of convolutions. ViT is compatible with the popular architectures in NLP [19, 44] and, similar to its NLP counterparts, requires pretraining over massive datasets [1, 2, 20] and repeated strong data augmentation [55]. Some works specialize the ViT architectures for the visual data [5, 22, 38, 62].
More recent works find that the self-attention in ViT is not vital for performance, resulting in several architectures exclusively based on MLPs [37, 40, 53, 54]. Here we take MLP-Mixer [53] as an example. MLP-Mixer shares the same input layer as ViT; namely, it partitions an image into a sequence of nonoverlapping patches/tokens. It then alternates between token and channel MLPs, where the former allows feature fusion from different spatial locations.
2

(a) ResNet

(b) ViT

(c) Mixer

(d) ViT-SAM

(e) Mixer-SAM

Figure 1: Cross-entropy loss landscapes of ResNet-152, ViT-B/16, and Mixer-B/16. ViT and MLP-

Mixer converge to sharper regions than ResNet when trained on ImageNet with the basic Inception-

style preprocessing. SAM, a sharpness-aware optimizer, significantly smooths the landscapes.

Table 1: Number of parameters, NTK condition number , Hessian dominate eigenvalue max, accuracy on ImageNet, and accuracy/robustness on ImageNet-C. ViT and MLP-Mixer suffer divergent  and converge to sharp regions of big max; SAM rescues that and leads to better generalization.

ResNet-50

ResNet-152

ViT-B/16

ViT-B/16SAM

Mixer-B/16

Mixer-B/16SAM

#Params NTK  Hessian max
ImageNet (%) ImageNet-C (%)

25M 2801.6 122.9
76.0 44.6

60M 2801.6 179.8
78.5 50.0

87M

4205.3

738.8

20.9

74.6

79.9

46.6

56.5

59M

14468.0

1644.4

22.5

66.4

77.4

33.8

48.8

We focus on ViTs and MLP-Mixers in this paper. We denote by "S" and "B" the small and base model sizes, respectively, and by an integer the image patch resolution. For instance, ViT-B/16 is the base ViT model taking as input a sequence of 16 × 16 patches. Appendices contain more details.

3 ViTs and MLP-Mixers Converge to Sharp Local Minima
The current training recipe of ViTs, MLP-Mixers, and related convolution-free architectures relies heavily on massive pretraining [1, 2, 20] or a bag of strong data augmentations [16, 17, 53, 55, 63, 65]. It highly demands data and computing, and leads to many hyper-parameters to tune. Existing works report that ViTs yield inferior accuracy to the ConvNets of similar size and throughput when trained from scratch on ImageNet without the combination of those advanced data augmentations, despite using various regularization techniques (e.g., large weight decay, Dropout [49], etc.). For instance, ViT-B/16 [20] gives rise to 74.6% top-1 accuracy on the ImageNet validation set (224 image resolution), compared with 78.5% of ResNet-152 [24]. Mixer-B/16 [53] performs even worse (66.4%). There also exists a large gap between ViTs and ResNets in robustness tests (e.g., against 19 corruptions in ImageNet-C [26]).
Moreover, Chen et al. [15] find that the gradients can spike and cause a sudden accuracy dip when training ViTs, and Touvron et al. [55] report the training is sensitive to initialization and hyperparameters. These all point to optimization problems. In this paper, we investigate the loss landscapes of ViTs and MLP-Mixers to understand them from the optimization perspective, intending to reduce their dependency on the large-scale pretraining or strong data augmentations.
ViTs and MLP-Mixers converge to extremely sharp local minima. It has been extensively studied that the convergence to a flat region whose curvature is small benefits the generalization of neural networks [10, 13, 29, 30, 33, 48, 64]. Following [36], we plot the loss landscapes at convergence when ResNets, ViTs, and MLP-Mixers are trained from scratch on ImageNet with the basic Inceptionstyle preprocessing [51] (see Appendices for details). As shown in Figures 1(a) to 1(c), ViTs and MLP-Mixers converge to much sharper regions than ResNets. In Table 1, we further validate the results by computing the dominate Hessian eigenvalue max, which is a mathematical evaluation of the landscape curvature. The max values of ViT and MLP-Mixer are orders of magnitude larger than that of ResNet, and MLP-Mixer suffers the largest curvature among the three species (see Section 4.4 for a detailed analysis).
Small training errors. This convergence to sharp regions coincides with the training dynamics shown in Figure 2 (left). Although Mixer-B/16 has fewer parameters than ViT-B/16 (59M vs. 87M), it has a smaller training error but much worse test accuracy, implying that using the cross-token MLP to learn the interplay across image patches is more prone to overfitting than ViTs' self-attention

3

Figure 2: Left and Middle: ImageNet training error and validation accuracy vs. iteration for ViTs and MLP-Mixers with different patch sizes, vanilla SGD or SAM, and basic pre-processing vs. repeated augmentations. Right: Percentage of activated neurons at each block of MLP-Mixers.
mechanism whose behavior is restricted by a softmax. Such a difference probably explains that it is easier for MLP-Mixers to get stuck in sharp local minima.
ViTs and MLP-Mixers have worse trainability. Furthermore, we discover that ViTs and MLPMixers suffer poor trainability, defined as the effectiveness of a network to be optimized by gradient descent [8, 47, 58]. Xiao et al. [58] show that the trainability of a neural network can be characterized by the condition number of the associated neural tangent kernel (NTK), (x, x ) = J(x)J(x )T , where J is the Jacobian matrix. Denoting by 1  · · ·  m the eigenvalues of NTK train, the smallest eigenvalue m converges exponentially at a rate given by the condition number  = 1/m. If  diverges then the network will become untrainable [12, 58]. As shown in Table 1,  is pretty stable for ResNets, echoing previous results that ResNets enjoy superior trainability regardless of the depth [36, 60]. However, we observe that the condition number diverges when it comes to ViT and MLP-Mixer, confirming that the training of ViTs desires extra care [15, 55].

4 A Principled Optimizer for Convolution-Free Vision Architectures

The commonly used first-order optimizers (e.g., SGD [41], Adam [32]) only seek to minimize the training loss Ltrain(w). They usually dismiss the higher-order information such as curvature that correlates with the generalization [10, 21, 30]. However, the objective Ltrain for deep neural networks are highly non-convex, making it easy to reach near-zero training error but high generalization error Ltest during evaluation, let alone their robustness when the test sets have different distributions [26, 28]. ViTs and MLPs amplify such drawbacks of first-order optimizers due to the lack of inductive bias for visual data, resulting in excessively sharp loss landscapes and poor generalization, as shown in the previous section. We hypothesize that smoothing the loss landscapes at convergence can significantly improve the generalization ability of those convolution-free architectures, leading us to the recently proposed sharpness-aware minimizer (SAM) [23] that explicitly avoids sharp minima.

4.1 SAM: Overview

Intuitively, SAM [23] seeks to find the parameter w whose entire neighbours have low training loss Ltrain by formulating a minimax objective:

min max Ltrain(w + ),

(1)

w

2 

where  is the size of the neighbourhood ball. Without loss of generality, here we use l2 norm for its strong empirical results [23] and omit the regularization term for simplicity. Since the exact solution
of the inner maximization = arg max 2 Ltrain(w + ) is hard to obtain, they employ an efficient first-order approximation:

^(w) = arg max Ltrain(w) + T wLtrain(w) = wLtrain(w)/ wLtrain(w) 2. (2)
2 

Under the l2 norm, ^(w) is simply a scaled gradient of the current weight w. After computing ^, SAM updates w based on the sharpness-aware gradient wLtrain(w)|w+^(w).

4.2 Sharpness-aware optimization substantially improves ViTs and MLP-Mixers
We train ViTs and MLP-Mixers with no large-scale pretraining or strong data augmentation. We directly apply SAM to the original ImageNet training pipeline of ViTs [20] without changing any

4

Table 2: Accuracy and robustness of ResNets, ViTs, and MLP-Mixers trained from scratch on ImageNet with SAM (improvement over the models trained using vanilla SGD is shown in the parentheses). We use the Inception-style preprocessing (with resolution 224) rather than a combination of strong data augmentations. ViTs achieve better accuracy and robustness than ResNets of similar size and throughput (calculated following [53]), and MLP-Mixers become on par with ResNets.

Model

#params

Throughput (img/sec/core)

ImageNet

Real

V2

ImageNet-R ImageNet-C

ResNet-50-SAM ResNet-101-SAM ResNet-152-SAM ResNet-50x2-SAM ResNet-101x2-SAM ResNet-152x2-SAM

25M 44M 60M 98M 173M 236M

ViT-S/32-SAM

23M

ViT-S/16-SAM

22M

ViT-S/14-SAM

22M

ViT-S/8-SAM

22M

ViT-B/32-SAM

88M

ViT-B/16-SAM

87M

Mixer-S/32-SAM

19M

Mixer-S/16-SAM

18M

Mixer-S/8-SAM

20M

Mixer-B/32-SAM

60M

Mixer-B/16-SAM

59M

Mixer-B/8-SAM

64M

2161 1334 935 891 519 356
6888 2043 1234 333 2805 863
11401 4005 1498 4209 1390 466

ResNet

76.7 (+0.7) 78.6 (+0.8) 79.3 (+0.8) 79.6 (+1.5) 80.9 (+2.4) 81.1 (+1.8)

83.1 (+0.7) 84.8 (+0.9) 84.9 (+0.7) 85.3 (+1.6) 86.4 (+2.4) 86.4 (+1.9)

Vision Transformer

70.5 (+2.1) 78.1 (+3.7) 78.8 (+4.0) 81.3 (+5.3) 73.6 (+4.1) 79.9 (+5.3)

77.5 (+2.3) 84.1 (+3.7) 84.8 (+4.5) 86.7 (+5.5) 80.3 (+5.1) 85.2 (+5.4)

MLP-Mixer

66.7 (+2.8) 72.9 (+4.1) 75.9 (+5.7) 72.4 (+9.9) 77.4 (+11.0) 79.0 (+10.4)

73.8 (+3.5) 79.8 (+4.7) 82.5 (+6.3) 79.0 (+10.9) 83.5 (+11.4) 84.4 (+10.1)

64.6 (+1.0) 66.7 (+1.4) 67.3 (+1.0) 67.5 (+1.7) 69.1 (+2.8) 69.6 (+2.3)
56.9 (+2.6) 65.6 (+3.9) 67.2 (+5.2) 70.4 (+6.2) 60.0 (+4.7) 67.5 (+6.2)
52.4 (+2.9) 58.9 (+4.1) 62.3 (+6.2) 58.0 (+10.4) 63.9 (+13.1) 65.5 (+11.6)

23.3 (+1.1) 25.9 (+1.5) 25.7 (+0.4) 26.0 (+2.9) 27.8 (+3.2) 28.1 (+2.8)
21.4 (+2.4) 24.7 (+4.7) 24.4 (+4.7) 25.3 (+6.1) 24.0 (+4.1) 26.4 (+6.3)
18.6 (+2.7) 20.1 (+4.2) 20.5 (+5.1) 22.8 (+8.2) 24.7 (+10.2) 23.5 (+9.2)

46.5 (+1.9) 51.3 (+2.8) 52.2 (+2.2) 50.7 (+3.9) 54.0 (+4.7) 55.0 (+4.2)
46.2 (+2.9) 53.0 (+6.5) 54.2 (+7.0) 55.6 (+8.5) 50.7 (+6.7) 56.5 (+9.9)
39.3 (+4.1) 42.0 (+6.4) 42.4 (+7.8) 46.2 (12.4) 48.8 (+15.0) 48.9 (+16.9)

hyperparameters. The pipeline employs the basic Inception-style preprocessing [51]. The original training setup of MLP-Mixers [53] includes a combination of strong data augmentations; we replace it with the same Inception-style preprocessing for a fair comparison. Note that we perform grid search for the learning rate, weight decay, Dropout, and stochastic depth before applying SAM. Please refer to Appendices for more details.
Smoother regions around the local minima. Thanks to SAM, both ViTs and MLP-Mixers converge at much smoother regions, as shown in Figures 1(d) and 1(e). The curvature measurement, i.e., the largest eigenvalue max of the Hessian matrix, also decreases to a small value (see Table 1).
Higher accuracy. What comes along is tremendously improved generalization performance. On the ImageNet validation set, SAM boosts the top-1 accuracy of ViT-B/16 from 74.6% to 79.9%, and Mixer-B/16 from 66.4% to 77.4%. For comparison, the improvement on a similarly sized ResNet-152 is 0.8%. Empirically, the degree of improvement negatively correlates with the level of inductive biases built into the architecture. ResNets with inherent translation equivalence and locality benefit less from landscape smoothing than the attention-based ViTs. MLP-Mixers gain the most from the smoothed loss geometry. Moreover, SAM brings larger improvements to the models of larger capacity (e.g., +4.1% for Mixer-S/16 vs. +11.0% for Mixer-B/16) and longer patch sequence (e.g., +2.1% for ViT-S/32 vs. +5.3% for ViT-S/8). See Table 2 for more results and Section 4.3 for more discussions.
Better robustness. We also evaluate the models' robustness using ImageNet-R [28] and ImageNetC [26] and find even bigger impacts of the smoothed loss landscapes. On ImageNet-C, which corrupts images by noise, bad weather, blur, etc., we report the average accuracy against 19 corruptions across five severity. As shown in Tables 1 and 2, the accuracies of ViT-B/16 and Mixer-B/16 increase by 9.9% and 15.0%, respectively, after SAM smooths their converged local regions.

4.3 ViTs outperform ResNets without pretraining or strong data augmentations
The performance of a model architecture is often conflated with the training strategies [4], where data augmentations play a key role [14, 16, 17, 59, 65]. However, the design of data augmentations requires substantial domain expertise and may not translate between images and videos, for instance. Thanks to the principled sharpness-aware optimizer, we can remove the advanced augmentations and focus on the architecture itself (with the basic Inception-style preprocessing).
When trained from scratch on ImageNet with SAM, ViTs outperform ResNets of similar and greater sizes (also comparable throughput at inference) regarding both clean accuracy (on ImageNet [18], ImageNet-ReaL [6], and ImageNet V2 [45]) and robustness (on ImageNet-R [28] and ImageNet-

5

Table 3: Dominant eigenvalue max of the sub-diagonal Hessians for different network components, and norm of the model parameter w and the post-activation ak of block k. Each ViT block consists of a MSA and a MLP, and MLP-Mixer alternates between a token MLP a channel MLP. Shallower layers have larger max. SAM smooths every component.

Model
ViT-B/16 ViT-B/16-SAM Mixer-B/16 Mixer-B/16-SAM

Embedding
300.4 3.8
1042.3 18.2

max of diagonal blocks of Hessian MSA/Token MLP MLP/Channel MLP Block1 Block6

179.8 8.5

281.4 9.6

44.4 32.4

1.7

1.7

95.8

417.9

239.3 41.2

1.4

9.5

4.0

1.1

Block12
26.9 1.5
5.1 0.3

Whole
738.8 20.9
1644.4 22.5

w2
269.3 353.8 197.6 389.9

a1 2
104.9 117.0 96.7 110.9

a6 2
104.3 120.3 135.1 176.0

a12 2
138.1 97.2 74.9 216.1

C [26]). ViT-B/16 achieves 79.9%, 26.4%, and 56.6% top-1 accuracy on ImageNet, ImageNet-R, and ImageNet-C, while the counterpart numbers for ResNet-152 are 79.3%, 25.7%, and 52.2%, respectively (see Table 2). The gaps between ViTs and ResNets are even wider for small architectures. ViT-S/16 outperforms a similarly sized ResNet-50 by 1.4% on ImageNet, and 6.5% on ImageNet-C. SAM also significantly improves MLP-Mixers' results.

4.4 Intrinsic changes after SAM

We take a deeper look into the models to understand how they intrinsically change to reduce the Hessian' eigenvalue max and what the changes imply in addition to the enhanced generalization.

Smoother loss landscapes for every network component. In Table 3, we break down the Hessian of the whole architecture into small diagonal blocks of Hessians concerning each set of parameters, attempting to analyze what specific components cause the blowing up of max in the models trained without SAM. We observe that shallower layers have larger Hessian eigenvalues max, and the first linear embedding layer incurs the sharpest geometry. This agrees with the finding in [15] that spiking gradients happen early in the embedding layer. Additionally, the multi-head self-attention (MSA) in ViTs and the Token MLPs in MLP-Mixers, both of which mix information across spatial locations, have comparably lower max than the other network components. SAM consistently reduces the max of all network blocks.

We can gain insights into the above findings by the recursive formulation of Hessian matrices for MLPs [7]. Let hk and ak be the pre-activation and post-activation values for layer k, respectively. They satisfy hk = Wkak-1 and ak = fk(hk), where Wk is the weight matrix and fk is the activation function (GELU [27] in MLP-Mixers). Here we omit the bias term for simplicity. The diagonal block of Hessian matrix Hk with respect to Wk can be recursively calculated as:

Hk = (ak-1aTk-1)  Hk, Hk = BkWkT+1Hk+1Wk+1Bk + Dk,

(3)

L

Bk = diag(fk(hk)),

Dk = diag(fk (hk) ak ),

(4)

where  is the Kronecker product, Hk is the pre-activation Hessian for layer k, and L is the objective function. Therefore, the Hessian norm accumulates as the recursive formulation backpropagates to shallow layers, explaining why the first block has much larger max than the last block in Table 3.

Greater weight norms. After applying SAM, we find that the norm of the post-activation value ak-1 and the weight Wk+1 become even bigger (see Table 3), indicating that the commonly used weight decay may not effectively regularize ViTs and MLP-Mixers.

Sparser active neurons in MLP-Mixers. Given the recursive formulation (3) to (4), we identify another intrinsic measure of MLP-Mixers that contribute to the Hessian: the number of activated neurons. Indeed, Bk is determined by the activated neurons whose values are greater than zero, since the first-order derivative of GELU becomes much smaller when the input is negative. As a result, the number of active GELU neurons is directly connected to the Hessian norm. Figure 2 (right) shows the proportion of activated neurons for each block, counted using 10% of the ImageNet training set. We can see that SAM greatly reduces the proportion of activated neurons for the first few layers, pushing them to much sparser states. This result also suggests the potential redundancy of image patches.

ViTs' active neurons are highly sparse. Although Equations (3) and (4) only involve MLPs, we still observe a decrease of activated neurons in the first layer of ViTs (but not as significant as in MLP-Mixers). More interestingly, we find that the proportion of activated neurons in ViT is much

6

Figure 3: Raw images (Left) and attention maps of ViT-S/16 with (Right) and without (Middle) sharpness-aware optimization. ViT-S/16 with less sharp local optimum contains perceptive segmentation information in its attention maps.
smaller than that in ResNets or MLP-Mixers -- less than 5% neurons have values greater than zero for most ViT layers. In other words, ViTs offer a huge potential for network pruning. This sparsity may also explain why one Transformer can handle multi-modality signals (vision, text, and audio) [1].
More perceptive attention maps in ViTs. We visualize ViT-S/16's attention map of the classification token averaged over the last multi-head attentions in Figure 3 following Caron et al. [9]. Interestingly, the ViT model optimized with SAM can encode plausible segmentation information, giving rise to better interpretability than the one trained via the conventional SGD optimization.
Higher training errors. As shown in Figure 2 (Left), ViT-B/16 with SAM has a higher training error than the one trained with vanilla SGD. Such regularization effect also takes place when we use strong data augmentations in training, which forces the network to explicitly learn priors like rotation and translation equivariance in RandAugment [17] and linear interpolation in mixup [65]. However, the augmentations are sensitive to different training settings (Section 5.2) and result in highly noisy loss curves (Figure 2 (Middle)).
5 Comparison Experiments and Ablation Studies
Section 4.3 shows that ViTs outperform ResNets when trained from scratch on ImageNet with the basic Inception-style preprocessing for both clean accuracy and robustness. In this section, we provide a more comprehensive study about SAM's effect on various vision models and under different training setups (e.g., varying the amount of training data, cross-entropy loss vs. contrastive loss, strong data augmentations, and transfer learning).
5.1 SAM enables ViTs and MLP-Mixers to scale down and up with the training set size
Previous studies scale up training data to show massive pretraining trumps inductive biases [20, 53]. Here we show SAM further enables ViTs and MLP-Mixers to handle small-scale training data well. We randomly sample 1/4 and 1/2 images from each ImageNet class to compose two smaller-scale training sets, i.e., i1k (1/4) and i1k (1/2) in Figure 4 with 320,291 and 640,583 images, respectively. We also include ImageNet-21k to pretrain the models with SAM, followed by fine-tuning on ImageNet1k without SAM. The ImageNet validation set remains intact.
As expected, fewer training examples amplify the drawback of ViTs and MLP-Mixers' lack of the convolutional inductive bias -- their accuracies decline much faster than ResNets' (see the top panel in Figure 4 and the corresponding numbers in Table 4). When trained with 1/4 of the ImageNe training images, ViT-B/16 has top-1 accuracy 52.4%, Mixer-B/16 gives 37.2%, but ResNet-152 maintains as high as 68.0%.
However, SAM can drastically rescue ViTs and MLP-Mixers' performance decrease on smaller training sets. Figure 4 (bottom) shows that the improvement brought by SAM over vanilla SGD
7

Table 4: Data augmentation, SAM, and their combination applied to different model architectures trained on ImageNet and its subsets.

Training Set
ImageNet i1k (1/2) i1k (1/4) i1k (1/10)

#Images
1,281,167 640,583 320,291 128,116

ResNet-152 Vanilla SAM
78.5 79.3 74.2 75.6 68.0 70.3 54.6 57.1

Vanilla
74.6 64.9 52.4 32.8

ViT-B/16 SAM AUG
79.9 79.6 75.4 73.1 66.8 63.2 46.1 38.5

SAM + AUG
81.5
75.8 65.6 45.7

Vanilla
66.4 53.9 37.2 21.0

Mixer-B/16 SAM AUG
77.4 76.5 71.0 70.4 62.8 61.0 43.5 43.0

SAM + AUG
78.1
73.1 65.8 51.0

training is proportional to the number of training images. When trained on i1k (1/4), it boosts ViTB/16 and Mixer-B/16 by 14.4% and 25.6%, escalating their results to 66.8% and 62.8%, respectively. It also tells that ViT-B/16-SAM matches the performance of ResNet-152-SAM even with only 1/2 ImageNet training data.

5.2 SAM complements strong augmentation and is more robust to different training settings

Following the training pipeline in [53], we also study how SAM interplays with the strong data augmentations of mixup [65] (with probability 0.5) and RandAugment [17] (with two layers and magnitude 15). Table 4 shows the effects of the augmentations, SAM, and their combination on ImageNet and three subsets of training images. SAM benefits ViT-B/16 and Mixer-B/16 more than the strong data augmentations, especially when the training set is small. When the training set contains only 1/10 of ImageNet training images, SAM outperforms data augmentations by 7.6% for ViT-B/16. Besides, SAM and the strong data augmentations are complementary for most test cases.

The results demonstrate SAM is more robust to the change of training settings than the combination of strong augmentation methods. Figure 2 (middle) plots the training loss when using strong augmentations. It is very noisy, implying the difficulty of tuning their hyperparameters. In comparison, SAM is a principled optimizer that introduces only one additional hyperparameter . Appendices report the experiments of tuning , which does not complicate other hyper-parameters.
5.3 SAM complements contrastive learning

Figure 4: ImageNet validation accuracy (Top) and improvement (Bottom) brought by SAM on different training sets.

In addition to data augmentations and large-scale pretraining, another notable way of improving a neural model's generalization is (supervised) contrastive learning [9, 11, 25, 31]. We couple SAM with the supervised contrastive learning [31] for 350 epochs, followed by fine-tuning the classification head by 90 epochs for both ViT-S/16 and ViT-B/16. Please see the Appendices for more implementation details. Compared to the training procedure without SAM, we find considerable performance gain thanks to SAM's smoothing of the contrastive loss geometry, improving the ImageNet top-1 accuracy of ViT-S/16 from 77.0% to 78.1%, and ViT-B/16 from 77.4% to 80.0%.

5.4 When ViTs and MLP-Mixers meet both SAM and adversarial training

Interestingly, SAM and adversarial training are both minimax problems except that SAM's inner maximization is with respect to the network weights, while the latter concerns about the input for defending contrived attack [39, 57]. Moreover, similar to SAM, Shafahi et al. [46] suggest that adversarial training can flatten and smooth the loss landscape. In light of these connections, we study ViTs and MLP-Mixers under the adversarial training framework. To incorporate SAM, we formulate a three-level objective:

min max max Ltrain(w + , x + , y),

(5)

w Ssam Sadv

where Ssam and Sadv denote the allowed perturbation norm balls for the model parameter w and input image x, respectively. Note that we can simultaneously obtain the gradients for computing

8

Table 5: Comparison under the adversarial training framework on ImageNet (numbers in the parentheses denote the improvement over the standard adversarial training without SAM). With similar model size and throughput, ViTs-SAM can still outperform ResNets-SAM for clean accuracy and adversarial robustness.

Model

#params

Throughput (img/sec/core)

ImageNet

Real

V2

PGD-10 ImageNet-R ImageNet-C

ResNet-50-SAM

25M

ResNet-101-SAM 44M

ResNet-152-SAM 60M

2161 1334 935

ResNet

70.1 (-0.7) 73.6 (-0.4) 75.1 (-0.4)

77.9 (-0.3) 81.0 (+0.1) 82.3 (+0.2)

56.6 (-0.8) 60.4 (-0.6) 62.2 (-0.4)

Vision Transformer

54.1 (+0.9) 58.8 (+1.4) 61.0 (+1.8)

27.0 (+0.9) 29.5 (+0.6) 30.8 (+1.4)

42.7 (-0.1) 46.9 (+0.3) 49.1 (+0.6)

ViT-S/16-SAM

22M

ViT-B/32-SAM

88M

ViT-B/16-SAM

87M

Mixer-S/16-SAM 18M Mixer-B/32-SAM 60M Mixer-B/16-SAM 59M

2043 2805 863
4005 4209 1390

73.2 (+1.2) 69.9 (+3.0) 76.7 (+3.9)

80.7 (+1.7) 76.9 (+3.4) 82.9 (+4.1)

MLP-Mixer

67.1 (+2.2) 74.5 (+2.3) 69.3 (+9.1) 76.4 (+10.2) 73.9 (+11.1) 80.8 (+11.8)

60.2 (+1.4) 55.7 (+2.5) 63.6 (+4.3)
52.8 (+2.5) 54.7 (+9.4) 60.2 (+11.9)

58.0 (+5.2) 54.0 (+6.4) 62.0 (+7.7)
50.1 (+4.1) 54.5 (+13.9) 59.8 (+17.3)

28.4 (+2.4) 26.0 (+3.0) 30.0 (+4.9)
22.9 (+2.6) 26.3 (+8.0) 29.0 (+10.5)

47.5 (+1.6) 46.4 (+3.0) 51.4 (+5.0)
37.9 (+2.5) 43.7 (+8.8) 45.9 (+12.5)

Table 6: Accuracy on downstream tasks of the models pretrained on ImageNet. SAM improves ViTs and MLP-Mixers' transferabilities to the tasks. ViTs transfer better than ResNets of similar sizes.

%
CIFAR-10 CIFAR-100 Flowers Pets
Average

ResNet50-SAM
97.4 85.2 90.0 91.6
91.1

ResNet152-SAM
98.2 87.8 91.1 93.3
92.6

ViT-S/16
97.6 85.7 86.4 90.4
90.0

ViT-S/16SAM
98.2 87.6 91.5 92.9
92.6

ViT-B/16
98.1 87.6 88.5 91.9
91.5

ViT-B/16SAM
98.6 89.1 91.8 93.1
93.2

Mixer-S/16
94.1 77.9 83.3 86.1
85.4

Mixer-S/16SAM
96.1 82.4 87.9 88.7
88.8

Mixer-B/16
95.4 80.0 82.8 86.1
86.1

Mixer-B/16SAM
97.8 86.4 90.0 92.5
91.7

and  by backpropagation only once. To lower the training cost, we use fast adversarial training [57] with the l norm for , and the maximum per-pixel change is set as 2/255.
Table 5 evaluates the models' clean accuracy, real-world robustness, and adversarial robustness (under 10-step PGD attack [39]). It is clear that the landscape smoothing significantly improves the convolution-free architectures for both clean and adversarial accuracy. However, we observe a slight accuracy decrease on clean images for ResNets despite gain for robustness. Similar to our previous observations, ViTs surpass similar-size ResNets when adversarially trained on ImageNet with the basic Inception-style preprocessing for both clean accuracy and adversarial robustness.
5.5 ViTs and MLP-Mixers with smoother loss geometry transfer better to downstream tasks
Finally, we study the role of smoothed loss geometry in transfer learning. We select four datasets to test ViTs and MLP-Mixers' transferabilities: CIFAR-10/100 [35], Oxford-IIIT Pets [43], and Oxford Flowers-102 [42]. We fine-tune all the models with image resolution 224 using vanilla SGD. For comparison, we also include ResNet-50-SAM and ResNet-152-SAM in the experiments. Table 6 summarizes the results, which confirm that the enhanced models also perform better after fine-tuning and that MLP-Mixers gain the most from the sharpness-aware optimization.
6 Conclusion and Discussion
This paper presents a detailed analysis of the convolution-free ViTs and MLP-Mixers from the lens of the loss landscape geometry, intending to reduce the models' dependency on massive pretraining and/or strong data augmentations. We arrive at the sharpness-aware minimizer (SAM) after observing sharp local minima of the converged models. By explicitly regularizing the loss geometry through SAM, the models enjoy much flatter loss landscapes and improved generalization regarding accuracy and robustness. The resultant ViT models outperform ResNets of comparable size and throughput when learned with no pretraining or strong augmentations. Further investigation reveals that the smoothed loss landscapes attribute to much sparser activated neurons in the first few layers. Moreover, ViTs after SAM offer perceptive attention maps.
Future work will focus on the following limitations of the work. The update to is approximated up to the first order by one step only, and it may be improved by considering multiple steps or higher

9

orders of updates. Besides SGD for the network weights, SAM incurs another round of forward and backward propagations to update . It is desired to reduce the computation cost. Finally, we hope SAM can help develop advanced neural architectures that are efficient in data and computation. Potential negative societal impacts are mainly concerned with the applications of convolution-free architectures, whose impacts may be related to this work.
References
[1] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. arXiv preprint arXiv:2104.11178, 2021.
[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic´, and Cordelia Schmid. Vivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021.
[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[4] Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies, 2021.
[5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095, 2021.
[6] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.
[7] Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for deep learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 557­565. PMLR, 06­11 Aug 2017. URL http://proceedings.mlr.press/v70/ botev17a.html.
[8] Rebekka Burkholz and Alina Dubatovka. Initialization of relus for dynamical isometry. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ d9731321ef4e063ebbee79298fa36f56-Paper.pdf.
[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers, 2021.
[10] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. In International Conference on Learning Representations, 2017.
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1597­1607. PMLR, 13­18 Jul 2020. URL http://proceedings.mlr.press/v119/chen20j.html.
[12] Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four GPU hours: A theoretically inspired perspective. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Cnon5ezMHtu.
[13] Xiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-based regularization. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1554­1565. PMLR, 13­18 Jul 2020. URL http://proceedings.mlr.press/v119/chen20f.html.
10

[14] Xiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh, and Boqing Gong. Robust and accurate object detection via adversarial learning, 2021.
[15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers, 2021.
[16] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
[17] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical automated data augmentation with a reduced search space. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 3008­3017, 2020. doi: 10.1109/CVPRW50498.2020.00359.
[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248­255, 2009. doi: 10.1109/CVPR.2009.5206848.
[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
[21] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Gal Elidan, Kristian Kersting, and Alexander T. Ihler, editors, Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017. AUAI Press, 2017. URL http://auai.org/uai2017/proceedings/papers/173. pdf.
[22] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227, 2021.
[23] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=6Tm1mposlrM.
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770­778, 2016. doi: 10.1109/CVPR.2016.90.
[25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
[26] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJz6tiCqYm.
[27] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2020.
[28] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization, 2020.
11

[29] Stanislaw Jastrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amost Storkey. On the relation between the sharpest directions of DNN loss and the SGD step length. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=SkgEaj05t7.
[30] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=H1oyRlYgg.
[31] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18661­18673. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf.
[32] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
[33] Bobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape local minima? In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2698­2707. PMLR, 10­15 Jul 2018. URL http://proceedings.mlr.press/v80/ kleinberg18a.html.
[34] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision ­ ECCV 2020, pages 491­507, Cham, 2020. Springer International Publishing. ISBN 978-3-03058558-7.
[35] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
[36] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/ file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf.
[37] Hanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint arXiv:2105.08050, 2021.
[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.
[39] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
[40] Luke Melas-Kyriazi. Do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet. arXiv preprint arXiv:2105.02723, 2021.
[41] Y. Nesterov. A method for solving the convex programming problem with convergence rate o(1/k2). Proceedings of the USSR Academy of Sciences, 269:543­547, 1983.
[42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing, pages 722­729, 2008. doi: 10.1109/ICVGIP.2008.47.
[43] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3498­3505, 2012. doi: 10.1109/CVPR.2012.6248092.
12

[44] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.
[45] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389­5400. PMLR, 2019.
[46] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 7503cfacd12053d309b6bed5c89de212-Paper.pdf.
[47] Yeonjong Shin and George Em Karniadakis. Trainability of relu networks and data-dependent initialization. Journal of Machine Learning for Modeling and Computing, 1(1):39­74, 2020. ISSN 2689-3967.
[48] Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BJij4yg0Z.
[49] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):1929­1958, 2014. URL http://jmlr.org/papers/ v15/srivastava14a.html.
[50] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 843­852, 2017. doi: 10.1109/ICCV.2017.97.
[51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818­2826, 2016. doi: 10.1109/CVPR.2016.308.
[52] T. Tieleman and G. Hinton. Lecture 6.5--RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
[53] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021.
[54] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jégou. Resmlp: Feedforward networks for image classification with data-efficient training, 2021.
[55] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention, 2021.
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
[57] Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=BJx040EFvH.
[58] Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generalization in deep neural networks. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10462­10472. PMLR, 13­18 Jul 2020. URL http:// proceedings.mlr.press/v119/xiao20b.html.
13

[59] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V. Le. Adversarial examples improve image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
[60] Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 81c650caac28cdefce4de5ddc18befa0-Paper.pdf.
[61] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Syx4wnEtvH.
[62] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet, 2021.
[63] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. Cutmix: Regularization strategy to train strong classifiers with localizable features. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6022­6031, 2019. doi: 10.1109/ICCV.2019.00612.
[64] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Understanding and robustifying differentiable architecture search. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=H1gDNyrKDS.
[65] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=r1Ddp1-Rb.
14

Appendices
A Architectures
Table 7 specifies the ViT [20, 56] and MLP-Mixer [53] architectures used in this paper. "S" and "B" denote the small and base model scales following [20, 53, 55], followed by the size of each image patch. For instance, "B/16" means the model of base scale with non-overlapping image patches of resolution 16 × 16. We use the input resolution 224 × 224 throughout the paper. Following Tolstikhin et al. [53], we sweep the batch sizes in {32, 64, . . . , 8192} on TPU-v3 and report the highest throughput for each model.

Table 7: Specifications of the ViT and MLP-Mixer architectures used in this paper. We train all the architectures with image resolution 224 × 224.

Model
ViT-S/32 ViT-S/16 ViT-S/14 ViT-S/8 ViT-B/32 ViT-B/16
Mixer-S/32 Mixer-S/16 Mixer-S/8 Mixer-B/32 Mixer-B/16 Mixer-B/8

#params
23M 22M 22M 22M 88M 87M
19M 18M 20M 60M 59M 64M

Throughput (img/sec/core)
6888 2043 1234 333 2805 863
11401 4005 1498 4209 1390 466

Patch Resolution
32 × 32 16 × 16 14 × 14 8×8 32 × 32 16 × 16
32 × 32 16 × 16 8×8 32 × 32 16 × 16 8×8

Sequence Length
49 196 256 784 49 196
49 196 784 49 196 784

Hidden Size
384 384 384 384 768 768
512 512 512 768 768 768

#heads
6 6 6 6 12 12
­ ­ ­ ­ ­ ­

#layers
12 12 12 12 12 12
8 8 8 12 12 12

Token MLP Dimension
­ ­ ­ ­ ­ ­
256 256 256 384 384 384

Channel MLP Dimension
­ ­ ­ ­ ­ ­
2048 2048 2048 3072 3072 3072

B Visualization
B.1 Loss landscape
We use the "filter normalization" method [36] to visualize the loss function curvature shown in Figure 1 in the main text. For a fair comparison, we use the cross-entropy loss when plotting the landscapes for all architectures, although the original training objective is the sigmoid loss for ViTs and MLP-Mixers. Note that their sigmoid loss geometry is even sharper. We equally sample 2,500 points on the 2D projection space and compute the losses using 10% of the ImageNet training images (i.e., the i1k (1/10) subset in the main text) [11] to save computation.
B.2 Attention map
The visualization of the ViT's attention maps (Figure 3 in the main text) follows [9]. We average the self-attention scores of the "classification token" from the last MSA layer to obtain a matrix A  RH/P ×W/P , where H, W , P are the image height, image width, and the patch resolution, respectively. Then we upsample A to the image shape H × W before generating the figure.
C Hessian Eigenvalue
The Hessian matrix requires second-order derivative, so we compute the Hessian (and all the subdiagonal Hessian) max using 10% of the ImageNet training images (i.e., i1k (1/10)) via power iteration 2, where we use 100 iterations to ensure its convergence.
D NTK Condition Number
We compute the neural tangent kernel with batch size 48 and average over the i1k (1/10) subset. Notice that the computation is based on the architecture at initialization without training. As the activation plays an important role when computing NTK -- we find that smoother activation functions
2https://en.wikipedia.org/wiki/Power_iteration
15

Table 8: Hyperparameters for training from scratch on ImageNet with basic Inception-style preprocessing and 224 × 224 image resolution.

ResNet

ViT

MLP-Mixer

Data augmentation Input resolution Batch size Epoch Warmup steps
Peak learning rate Learning rate decay Optimizer SGD Momentum Adam (1, 2) Weight decay Dropout rate Stochastic depth Gradient clipping

Inception-style

224 × 224

4096

90

300

5K

10K

0.1

×

batch size 256

cosine

3e-3 cosine

SGD

AdamW

0.9

­

­

(0.9, 0.009)

1e-3

0.3

0.0

0.1

­

­

­

1.0

300 10K
3e-3 linear AdamW
­ (0.9, 0.009)
0.3 0.0 0.1 1.0

enjoy smaller condition numbers, we replace the GELU in ViT and MLP-Mixer with ReLU for a fair comparison with ResNet.
E Training Details
Except for the experiments in Section 5.2 (SAM combined with strong data augmentations) and Section 5.3 (contrastive learning), we train all the models from scratch on ImageNet with the basic Inception-style preprocessing [51], i.e., a random image crop and a horizontal flip with probability 50%. Please see Table 8 for the detailed training settings. We simply follow the original training settings of ResNet and ViT [20, 34]. For MLP-Mixer, we remove the strong augmentations in its original training pipeline and perform a grid search over the learning rate in {0.003, 0.001}, weight decay in {0.3, 0.1, 0.03}, Dropout rate in {0.1, 0.0}, and stochastic depth in {0.1, 0.0}. Note that training for 90 epochs is enough for ResNets to converge, and longer schedule brings almost no effect. For all the experiments, we use 128 TPU-v3 cores (2 per chip), resulting in 32 images per core. The SAM computation for ^ is conducted on each core independently.
E.1 Perturbation strength in SAM
Different architecture species favor different strengths of perturbation . We perform a grid search over  and report the best results -- Table 9 reports the corresponding strengths used in our ImageNet experiments. Besides, we show the results when varying  in Table 10. Similar to [23], we also find that a relative small   [0.02, 0.05] works the best for ResNets. However, larger  gives rise to the best results for ViTs and MLP-Mixers. We also observe that architectures with larger capacities and longer input sequences prefer stronger perturbation strengths. Interestingly, the choice of  coincides with our previous observations. Since MLP-Mixers suffer the sharpest landscapes, they need the largest perturbation strength. As strong augmentations and contrastive learning already improve generalization, the suitable  becomes significantly smaller. Note that we do not re-tune any other hyperparameters when using SAM.
E.2 Training on ImageNet subsets
In Section 5.1, we train the models on ImageNet subsets, and the hyperparameters have to be adjusted accordingly. We simply change the batch size to maintain similar total iterations and keep all other settings the same, i.e., 2048 for i1k (1/2), 1024 for i1k (1/4), and 512 for i1k (1/10). We do not scale the learning rate as we find the scaling harms the performance.
16

Table 9: The SAM perturbation strength  for training on ImageNet. ViTs and MLP-Mixers favor larger  than ResNets does. Larger models with longer patch sequences need stronger strengths.

Model

Task

SAM 

ResNet

ResNet-50-SAM ResNet-101-SAM ResNet-152-SAM ResNet-50x2-SAM ResNet-101x2-SAM ResNet-152x2-SAM ResNet-50-SAM ResNet-101-SAM ResNet-152-SAM

supervised

0.02

supervised

0.05

supervised

0.02

supervised

0.05

supervised

0.05

supervised

0.05

adversarial

0.05

adversarial

0.05

adversarial

0.05

ViT

ViT-S/32-SAM ViT-S/16-SAM ViT-S/14-SAM ViT-S/8-SAM ViT-B/32-SAM ViT-B/16-SAM ViT-B/16-AUG-SAM ViT-S/16-SAM ViT-B/32-SAM ViT-B/16-SAM ViT-S/16-SAM ViT-B/16-SAM

supervised

0.05

supervised

0.1

supervised

0.1

supervised

0.15

supervised

0.15

supervised

0.2

supervised

0.05

adversarial

0.1

adversarial

0.1

adversarial

0.1

supervised contrastive 0.02

supervised contrastive 0.02

MLP-Mixer

Mixer-S/32-SAM

supervised

0.1

Mixer-S/16-SAM

supervised

0.15

Mixer-S/8-SAM

supervised

0.2

Mixer-B/32-SAM

supervised

0.35

Mixer-B/16-SAM

supervised

0.6

Mixer-B/8-SAM

supervised

0.6

Mixer-B/16-AUG-SAM

supervised

0.2

Mixer-S/16-SAM

adversarial

0.05

Mixer-B/32-SAM

adversarial

0.25

Mixer-B/16-SAM

adversarial

0.25

Table 10: ImageNet top-1 accuracy (%) of ViT-B/16 and Mixer-B/16 when trained from scratch with different perturbation strength  in SAM.

SAM 

0.0 0.05 0.1 0.2 0.25 0.35 0.4 0.5 0.6 0.65

ViT-B/16 74.6 77.5 78.8 79.9 79.3 ­ ­ ­ ­ ­ Mixer-B/16 66.4 69.5 ­ ­ 74.1 74.7 75.6 76.9 77.4 77.1

E.3 Training with strong augmentations
We tune the learning rate and regularizations when using strong augmentations (mixup with probability 0.5, RandAugment with two layers and magnitude 15) in Section 5.2 following [53]. For ViT, we use 1e-3 peak learning rate, 0.1 weight decay, 0.1 Dropout, and 0.1 stochastic depth; For MLP-Mixer, those hyperparameters are exactly the same as [53], peak learning rate as 1e-3, weight decay as 0.1, Dropout as 0.0, and stochastic depth as 0.1. Other settings are unchanged (Table 8).

E.4 Contrastive learning

In Section 5.3, we train ViTs under the supervised contrastive learning framework [31]. We take the

classification token output from the last layer as the encoded representation and retain the structures of

the projection and classification heads [31]. We employ a batch size 2048 without memory bank [25]

and use AutoAugment [16] with strength 1.0 following [31]. For the 350-epoch pretraining stage, the

contrastive loss temperature is set as 0.1, and we use the LAMB optimizer [61] with learning rate

0.001

×

batch size 256

along

with

a

cosine

decay

schedule.

For

the

second

stage,

we

train

the

classification

17

Table 11: Hyperparameters for downstream tasks. All models are fine-tuned with 224×224 resolution, a batch size of 512, cosine learning rate decay, no weight decay, and grad clipping at global norm 1.

Dataset

Total steps Warmup steps

Base LR

CIFAR-10

10K

CIFAR-100

10K

Flowers

500

Pets

500

500

500 100

{0.001, 0.003, 0.01, 0.03}

100

head for 90 epochs via a RMSProp optimizer [52] with base learning rate 0.05 and exponential decay. The weight decays are set as 0.3 and 1e-6 for the first and second stages, respectively. We use a small SAM perturbation strength under the contrastive learning framework  = 0.02.
E.5 Adversarial learning
We use the fast adversarial training [57] (FGSM with random start) with the l norm and maximum per-pixel change 2/255 during training. All the hyperparameters remain the same as the vanilla supervised training. When evaluating the adversarial robustness, we use the PGD attack [39] with the same maximum per-pixel change 2/255. The total number of attack steps is 10, and the step size is 0.25/255.
E.6 Fine-tuning on downstream tasks
We use image resolution 224 × 224 during fine-tuning on downstream tasks, other settings exactly follow [20, 53] (see Table 11). Note that we do not employ SAM during fine-tuning. We perform a grid search over the base learning rates on small sub-splits of the training sets (10% for Flowers and Pets, 2% for CIFAR-10/100). After that, we fine-tune on the entire training sets and report the results on the respective test sets.
F Longer Schedule of Vanilla SGD
Since SAM needs another forward and backward propagation to compute ^, its training overhead is  2× of the vanilla baseline. We also experiment with 2× schedule vanilla training (600 epochs). We observe that training longer brings no effect on both clean accuracy and robustness, indicating that the current 300 training epochs for ViTs and MLP-Mixers are enough for them to converge.

18

