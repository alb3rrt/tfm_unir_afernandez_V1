FedHybrid: A Hybrid Primal-Dual Algorithm Framework for Federated Optimization

arXiv:2106.01279v1 [math.OC] 2 Jun 2021

Xiaochun Niu Department of Industrial Engineering and Management Sciences
Northwestern University xiaochunniu2024@u.northwestern.edu
Ermin Wei Department of Industrial Engineering and Management Sciences
Northwestern University ermin.wei@northwestern.edu

Abstract
We consider a multi-agent consensus optimization problem over a server-client (federated) network, where all clients are connected to a central server. Current distributed algorithms fail to capture the heterogeneity in clients' local computation capacities. Motivated by the generalized Method of Multipliers in centralized optimization, we derive an approximate Newton-type primal-dual method with a practical distributed implementation by utilizing the server-client topology. Then we propose a new primal-dual algorithm framework FedHybrid that allows different clients to perform various types of updates. Specifically, each client can choose to perform either gradient-type or Newton-type updates. We propose a novel analysis framework for primal-dual methods and obtain a linear convergence rate of FedHybrid for strongly convex functions and a sublinear convergence rate for general convex functions, regardless of clients' choices of gradient-type or Newton-type updates. Numerical studies are provided to demonstrate the efficacy of our method in practice. To the best of our knowledge, this is the first hybrid algorithmic framework allowing heterogeneous local updates for distributed consensus optimization with a provable convergence and rate guarantee.

1 Introduction

The problem of optimizing an objective function by employing a distributed procedure over a network
where both data collection and model training is pushed to massive edge clients has gained significant
attention recently. This is motivated by the large-scale nature of many modern big-data problems such as multi-vehicle and multi-robot networks [6, 32], machine learning [9, 25], and especially federated learning [11]. In such problems, a set of n clients are connected to a central node (server), where each client has access only to its local data. We refer to such a model as a server-client (federated) network. The goal is to learn a shared model over all the data in the network without exchanging local data due to privacy issues or communication limitations [11]. Formally, if we define fi : Rd  R as local objective function corresponding to client i, the system-wide problem can be represented as

n

min

fi().

Rd i=1

(1.1)

For instance, in a supervised learning setting, consider an empirical risk minimization problem, the local objective function fi represents expected loss over the local data distribution of client i.

Preprint. Under review.

To develop a distributed computation method where each client only has access to local information
and can only communicate with the central server, we first decouple the computation of individual client by introducing local copies of the decision variable. We index the central server as the 0th node and denote by x0, xi  Rd the local copies of  kept at the central server and each client i  [n],
respectively. Problem 1.1 is reformulated as the following consensus optimization problem [3, 18],

n
min f (x) = fi(xi) s.t. W x = 0,
x i=1

(1.2)

where x = (x1 , · · · , xn)  Rnd and x = (x0 , x )  R(n+1)d are concatenations of local variables,
f : R(n+1)d  R is the aggregate function and W = (1n, -In)  Id  Rnd×(n+1)d is the adjacency
matrix of the server-client network, where the ith row of W x = 0 represents the consensus constraint
x0 = xi and enforces the equivalence of the two problem. For notational simplicity, we define
f : Rnd  R, f (x) = f (x). Also, throughout the paper, we differentiate whether a variable includes
the server's decision or clients' only by using a tilde.

While there is a proliferating literature on developing distributed optimization methods to solve the above problem, most existing works are either gradient-type methods [10, 18, 21] or Newton-type methods [8, 20, 27, 31], where the clients have no flexibility to choose the types of updates. In stark comparison, due to the drastically varying capabilities of storage, computation, and communication among clients caused by hardware, network connectivity, and battery power, distributed systems in practice may involve severely heterogeneous clients [7, 13]. The desire to provide hybrid methods for networks involving heterogeneous clients is even more pronounced in the federated learning setting [11­13]. As a result, it is imperative to provide a flexible and efficient hybrid algorithm framework to tolerate heterogeneous clients and utilize the heterogeneous structure. To the best of our knowledge, however, algorithms with heterogeneous clients setting have not been considered before.

To incorporate heterogeneous clients, we propose a hybrid primal-dual algorithm framework named FedHybrid, which is a distributed method for server-client (federated) network allowing some clients to do first-order updates while others use second-order information. Specifically, motivated by the generalized Method of Multipliers (MM) in centralized optimization [2, 24], we first introduce a gradient-type primal-dual approximation of MM with a practical distributed implementation. Second, for a speedup, we propose a Newton-type approximation of MM, where we derive a distributed approximated dual Hessian utilizing the server-client network. Then we propose FedHybrid as a combination of gradient-type and Newton-type primal-dual methods. In specific, those clients with higher computational capabilities and/or cheaper cost to perform computation can implement Newtontype updates locally, while other clients can adopt much simpler gradient-type updates. Finally, we propose a novel analysis framework for primal-dual algorithms and obtain a last iterate linear (Q-linear) convergence rate of FedHybrid for strongly convex objective functions and an average iterate sublinear convergence rate for general convex objective functions. Numerical experiments on both synthetic data and real-life data are conducted to demonstrate the efficacy of our method.

Contributions. Our contribution is threefold. First, we propose a Newton-type primal-dual method with a practical distributed implementation by approximating primal and dual Hessian utilizing the server-client network. Second, we propose FedHybrid as a primal-dual method combining both gradient-type and Newton-type updates for heterogeneous clients. Finally, we propose a novel analysis framework and obtain a last iterate linear convergence rate of FedHybrid for strongly convex objective functions and an average iterate sublinear convergence for general convex objective functions. To the best of our knowledge, this is the first hybrid algorithmic framework allowing heterogeneous local updates for distributed consensus optimization with a provable convergence guarantee.

Related Work. Our work is closely related to the growing literature on distributed algorithms for solving Problem 1.2. We outline various lines of researches as follows. We start by reviewing the literature studying general network topology. Primal iterative methods, including distributed (sub)-gradient descent (DGD) and related methods [18, 21], have updates in the form of some linear combinations of a gradient descent step with respect to its local objective function and a weighted average with local neighbors. A related work is [23], where the authors derive a DGD based method with the inclusion of first and second order update in continuous time setting, which cannot be directly translated to a discrete time update. While they provide an asymptotic convergence guarantee, their work lacks convergence rate analysis. There are econd-order methods including Network Newton

2

[16] and Distributed Newton method [26]. Also, dual decomposition based methods include ADMM [5, 28], ESOM [17], and CoCoA[22]. While none of these works consider a mixture of first and second order updates, ESOM with an approximated primal-dual framework is closely related to our work. The main difference there is ESOM uses only gradient-type update for the dual variables. Since we utilize the server-client topology, FedHybrid allows dual updates to also mimic a Newton step.
Another line of research focuses on the server-client network or the federated learning setting. Existing works include primal first-order methods like FedAvg [14, 15], FedProx[12] and FedAC [29]. There are also primal-dual methods like FedPD [30] that utilizing first-order updates in both primal and dual space. Such methods suffer from slow convergence due to their first-order nature. Some second-order methods have been proposed. Examples include DANE [20], DiSCO [31], GIANT [27], and DINGO [8]. However, most of these methods require the assumption that each client has access to IID local data, which is not realistic in practice, especially in federated learning settings. With the primal-dual framework, FedHybrid can handle non-IID data distributions among clients and enjoy the speedup brought by second-order updates.
Notations. For any integer m, we denote by [m] = {1, · · · , m}, Im  Rm×m the identity matrix,
and 1m = (1, · · · , 1)  Rm. Also, we denote by  the Kronecker product.

2 Preliminaries

In this section, we review the generalized Method of Multipliers (MM) in centralized optimization that is derived by formulating a dual problem based on augmented Lagrangian [2]. The MM method helps to motivate our derivation of FedHybrid. Before presenting the methods, we first introduce a standard condition on local objective functions that we will assume to hold throughout the paper.
Assumption 2.1. For all i  [n], the local function fi(·) is twice continuously differentiable and the eigenvalues of the Hessian 2fi(·) are bounded by constants 0  mi  i < , that is, miI 2fi(·) iI.

For all i  [n], the above upper bound on the Hessian implies that the gradient fi is i-Lipschitz continuous and the lower bound implies that the function fi is convex. In particular, the function fi is mi-strongly convex if mi > 0.

Dual Problem. Now we introduce a dual problem of Problem 1.2 based on augmented Lagrangian.

To do so, we introduce dual variables  = (1 , · · · , n)  Rnd with i  Rd associated with the

ith constraint x0 = xi and define the augmented Lagrangian function L(x, ) of Problem 1.2 as

µ

L(x, ) = f (x) +  W x + x W W x,

(2.1)

2

where µ > 0 is a constant. We remark that the matrix W W is the graph Laplacian of the server-client

network. The augmentation term µx W W x/2 is zero if x is feasible to Problem 1.2; otherwise, it

is positive and serves as a penalty for the violation of the consensus constraint. Also, we note that for

any feasible x, the augmented Lagrangian and the regular Lagrangian attain the same value. Then the

dual problem is defined as follows,

max g(), where g() = min L(x, ).

Rnd

xR(n+1)d

(2.2)

We will refer to g : Rnd  R as the dual function. For any   Rnd, we define x() 

argminx L(x, ) as an optimal point of the inner problem in Problem 2.2. Following from Assumption 2.1 and Slater's condition, the strong duality holds [4]. Thus, x() is an optimal solution of Problem 1.2, where  is an optimal solution of Problem 2.2.

Generalized MM. We introduce the generalized MM [2, 24] that uses the augmented Lagrangian

function L defined in (2.1) to solve Problem 1.2 as follows. At each iteration k,

x(k)  argmin L(x, k), k+1 = U (x(k), k),
x

(2.3)

where U : R(n+1)d×nd  R is a general dual update formula with the property that  = U (x(), ). We give some popular choices for U as follows,

U1(x(k), k) = k + 1g(k),

(2.4)

U2(x(k), k) = k - 2 2g(k) -1 g(k),

(2.5)

3

where 1, 2 > 0 are stepsizes. We remark that (2.4) and (2.5) correspond to gradient ascent method and Newton's method with respect to the dual function defined in 2.2, respectively. The following lemma gives the explicit forms of g() and 2g() used in (2.4) and (2.5). Lemma 2.2 ([24, 26]). Under Assumption 2.1, the gradient and the Hessian of the dual function defined in Problem (2.2) are given by
g() = W x(), 2g() = -W 2xxL(x(), ) -1 W .
We remark that the primal update in (2.3) is computationally expensive due to the requirement of an exact solution to the inner minimization problem and it cannot be readily implemented in a distributed manner due to the nonseparable augmentation term µx W W x/2.

3 Algorithm

In this section, to reduce the computational complexity and obtain an update in a distributed manner, we first derive a gradient-type approximation of MM, which recovers the Arrow-Hurwicz-Uzawa method [1]. This is a special case of FedHybrid when all clients perform gradient-type updates. Then for a speedup, we propose a Newton-type approximation of MM utilizing the server-client topology, which leads to another special case of FedHybrid with all clients performing Newton-type updates. Finally, we combine the gradient-type and Newton-type updates to provide our FedHybrid method, which allows a mixture of first and second order updates by various clients and hence is able to provide flexibility to handle heterogeneity in the network.

3.1 Gradient-type Approximation of MM

To develop a first-order method based on the dual gradient ascent update in (2.4), we need to compute g(k) = W x() by Lemma 2.2. However, the computation of an exact minimizer x(k) can
be computationally expensive. Thus, we approximate it by taking a primal gradient descent step on the primal variable and use the obtained xk as an approximation. This leads to the following
gradient-type primal-dual algorithm. At each iteration k,

xk+1 = xk - 1xL(xk, k), k+1 = k + 1W xk,

(3.1)

where 1, 1 > 0 are primal and dual stepsizes, respectively. This recovers the Arrow-HurwiczUzawa method [1], which is a special case of FedHybrid when all clients perform gradient-type updates. While this gradient-type approximation lead to simple distributed implementation, it suffers from slow convergence due to its first-order nature. This motivates us to consider Newton's method for a speedup.

3.2 Newton-type Approximation of MM

We now derive a Newton-type approximation of the generalized MM under the server-client topology.

Primal Update. We consider a Newton's step as an approximation of the primal update in (2.3). Note that the primal Hessian 2xxL(xk, k) = 2f (xk) + µW W is nonseparable due to the graph Laplacian W W , which makes the computation of the exact Hessian inverse intractable in a decentralized setting. Thus, we approximate the graph Laplacian W W by its block diagonal part E = diag{nId, Id, · · · , Id}  R(n+1)d×(n+1)d. By using H = 2f (x) + µE to approximate the primal Hessian 2xxL(x, ), we obtain the following Newton-type primal update. At each iteration k,

xk+1 = xk - (Hk)-1xL(xk, k),

(3.2)

where Hk = 2f (xk) + µE . In particular, the server update takes the following form,

xk0+1

=

1 n

xki

-

1 µn

ki .

i[n]

i[n]

(3.3)

We remark that xk0+1 can also be viewed as a penalized average of the primal decision variables {xki }i[n], where the second term in (3.3) serves as the penalty.

4

Dual Update. We define the exact dual Newton update as k, that is, k satisfies the Newton update formula 2g(k)k = g(k). Note that the explicit form of the Hessian 2g(k) given in Lemma 2.2 is nonseparable, making it difficult to compute k in a distributed manner. Thus,
to obtain a Newton-type dual update in the form of (2.5) in a distributed scheme, we introduce the following lemma to provide an approximation of k, whose proof is deferred to Appendix.
Lemma 3.1. Consider the exact dual Newton update k defined as above, it holds that

W k = 2xxL(xk, k)(xk - yk  1n+1),

where yk =

i[n] 2fi(xki ) -1 i[n] 2fi(xki )xki is the Hessian weighted average of the

primal decision variables.

If we substitute x0k+1 defined in (3.3) and Hk defined above as approximations of yk and
2xxL(xk, k), respectively, we obtain an approximation k of k satisfying W k =
Hk(xk - xk0+1  1n+1). Then following from the structure of the adjacency matrix W , we have

k = HkW xk,

(3.4)

where Hk = 2f (xk) + µInd  Rnd×nd, which is a submatrix of Hk, corresponding to the components related to the clients. Thus, using the dual Newton's formula in (2.5) with the approximation in (3.4), we obtain a Newton-type dual update

k+1 = k + 2HkW xk.

(3.5)

Now, by combining (3.2) and (3.5), we obtain our Newton-type approximation of MM as follows. At each iteration k, we have

xk+1 = xk - (Hk)-1xL(xk, k), k+1 = k + 2HkW xk,

(3.6)

where Hk = 2f (xk) + µInd and Hk = diag{µnId, Hk}.

3.3 FedHybrid to Handle System Heterogeneity

Since different clients in the network have different computation capacities, we consider combining gradient-type method in (3.1) and Newton-type method in (3.6) to provide a hybrid update framework. Specifically, all clients in the network can choose to perform gradienttype or Newton-type updates based on their computation capacities. For notational convenience, we denote by J1 = {i  [n] : client i performs gradient-type updates} and J2 = {i  [n] : client i performs Newton-type updates}. Based on such choices of different update types, we propose our hybrid updates as follow. At each iteration k, we have

xk+1 = xk - A(Dk)-1xL(xk, k), k+1 = k + BDkW xk,

(3.7)

where stepsize matrices A = diag{a0, a1, · · · , an}  Id  R(n+1)d×(n+1)d and B = diag{b1, · · · , bn}  Id  Rnd×nd with personalized stepsizes ai, bi > 0, and update matrices
Dk = diag{Id, Dk}  R(n+1)d×(n+1)d, and Dk = diag{D1k, · · · , Dnk }  Rnd×nd. Here Dik = I if i  J1 while Dik = 2fi(xi) + µI if i  J2. We remark that the updates in (3.7) generalize the updates in both (3.1) and (3.6). On the one extreme, if J1 = [n] and J2 = , the updates in (3.7) recovers the gradient-type updates in (3.1); on the other extreme, if J1 =  and J2 = [n], the updates in (3.7) recovers the Newton-type updates in (3.6). Based on (3.7), we propose our
FedHybrid method with a distributed scheme in Algorithm 1.

Our FedHybrid in Algorithm 1 consists of three steps: gradient-type updates, Newton-type updates, and consensus update. Specifically, gradient-type (Lines 4­7) and Newton-type updates (Lines 8­11) follow from (3.7) by extracting the corresponding block. In the consensus update (Line 12), we choose the stepsize a0 = 1/(µn) in A of (3.7) and replace the primal and dual decision variables xk and k by their updated counterpart xk+1 and k+1. Then, by substitution, we obtain that

xk0+1

=

1 n

xki +1

-

1 µn

ki +1,

i[n]

i[n]

(3.8)

which corresponds to the consensus update in Line 12.

5

Algorithm 1 FedHybrid: Hybrid Primal-dual Algorithm for distributed consensus optimization

1: Input: Initialization x00, x0i , 0i  Rd for all i  [n], index sets J1 and J2, penalty parameter µ, and stepsizes ai, bi  R+ for all i  [n].

2: for k = 1, . . . , K - 1 do

3: Server sends xk0 to all clients in the network;

4: for each client i  J1 parallel do

5:

xki +1 = xki - ai(fi(xki ) - ki + µ(xki - xk0 ));

6:

ki +1 = ki + bi(xk0 - xki );

7:

Send xki +1 and ki +1 to the server;

Gradient-type updates

8: for each client i  J2 parallel do

Newton-type updates

9:

xki +1 = xki - ai(2fi(xi) + µId)-1(fi(xki ) - ki + µ(xki - xk0 ));

10:

ki +1 = ki + bi(2fi(xi) + µId)(xk0 - xki );

11:

Send xki +1 and ki +1 to the server;

12:

Server updates: xk0+1 = i[n] xki +1/n - i[n] ki +1/(µn).

Consensus update

4 Convergence Analysis

This section presents the convergence results for FedHybrid in Algorithm 1, regardless of clients' choices of gradient-type or Newton-type updates. In Section 4.1, we show that FedHybrid converges to the optimal solution at a linear rate if the local objective fi(·) is strongly convex for all i  [n]. In Section 4.2, we show that FedHybrid converges to a stationary point at a sublinear rate if fi(·) is general convex for all i  [n]. Before presenting the results, we first introduce our performance
metric and a reformulation of the augmented Lagrangian L.
Performance Metric. We define the primal tracking error and the dual optimality gap as follows,

kx = L(xk, k) - L(x(k), k), k = g() - g(k),

(4.1)

where  is the optimal solution to the dual problem in (2.2). Here kx quantifies how close the augmented Lagrangian function at xk is from the optimal value of the inner problem given k. In Section 4.3, we will present a novel proof framework to show the convergence of both k and kx. We remark that under Assumption 2.1, the convergence of k ensures that the dual variable sequence {k}k[K] converges to the optimal solution  and the convergence of kx ensures that the primal variable sequence {xk}k[K] converges to x(), where x() is the optimal solution
of the original problem in (1.2) due to the strong duality.

Reformulation of L Based on Consensus Update. With slight abuse of notations, the consensus update (3.8) in Algorithm 1 can be written as xk0 = x0(xk, k), where x0 : Rnd × Rnd  R such that
x0(x, ) = (1n  Id) (x/n - /(µn)) for any x,   Rnd. By substituting x0 = x0(x, ) in the
augmented Lagrangian function L defined in (2.1), we have L(x, ) = L(x0(x, ), x, ). Motivated
by this, we define L : Rnd × Rnd  R such that L(x, ) = L(x0(x, ), x, ), which can be shown to be equivalent. See Appendix for details. For convenience, we use L in the convergence analysis.

4.1 Convergence of FedHybrid for Strongly Convex Function
In this section, we assume that the local objective function fi(·) is strongly convex for all i  [n], i.e., mi > 0 in Assumption 2.1. In the following theorem, we show that FedHybrid converges to the optimal solution at a linear rate under this case.
Theorem 4.1. For any given µ > 0, under Assumption 2.1 with mi > 0 for all i  [n], we suppose that the stepsizes {ai, bi}i[n] satisfy
ai  1/(22µ/9 + 2 ), bi  min{µ/9, (m + µ)2/21}, for i  J1, ai  (mi + µ)/(22µ/9 + 2 ), bi  min{µ/9, (m + µ)2/21}/( i + µ), for i  J2, (4.2)

6

where m = mini[n]{mi}, = maxi[n]{ i}, and  = min{miniJ1 {ai}, miniJ2 {ai/( i + µ)}}. Then for all k = 0, 1, · · · , K - 1, the iterates from FedHybrid satisfy
k+1  (1 - ) k,
where  = min 3/(13m + 13µ), m/2 with  = min{miniJ1 {bi}, miniJ2 {bi(mi + µ)}}, and k = 13k + kx. Here k and kx are defined in (4.1).
Remark 4.2. Theorem 4.1 provides a last iterate linear (Q-linear) rate of convergence of the error term k, which combines the dual optimality gap k and the primal tracking error kx. This ensures that FedHybrid converges linearly to the optimal solution, regardless of clients' choices of gradienttype or Newton-type updates. Also, in FedHybrid, each client can choose not only the type of updates based on their computation capacities, but also personalized stepsizes related to the properties of their local objective functions, which provides flexibility to handle heterogeneity in practice.

4.2 Convergence for general convex functions

In this section, we assume that the local objective function fi(·) is general convex for all i  [n], that is, mi  0 in Assumption 2.1. In the following theorem, we show that FedHybrid converges to a stationary point at a sublinear rate under this case.
Theorem 4.3. For any given µ > 0, under Assumption 2.1 with mi  0 for all i  [n], we suppose that the stepsizes satisfy (4.2). Then for all K  1, the iterates from FedHybrid satisfy

1 K-1 K

 4

xL(xk, k)

2 3 + 2

g(k) 2

0 ,
K

k=0

where the constants 0 = 130 + 0x, and m, , ,  are defined in Theorem 4.1.

Theorem 4.3 shows the average iterates convergence of norms of both dual gradient and primal gradient for general convex functions, regardless of clients' choices of different types of updates.

4.3 Proof Sketch

In this section, we provide a novel proof framework for analyzing primal-dual algorithms. We apply such analysis to FedHybrid and obtain the convergence results in above theorems. We highlight that proofs of both theorems rely on similar ideas of tackling coupled inequalities of kx and k.
Due to the coupled nature of primal and dual updates in the algorithm, our main idea for analyzing primal-dual methods is to upper bound the dual optimality gap k and the primal tracking error kx through coupled inequalities. We remark that the dual function g(·) is mg-strongly concave with g-Lipschitz continuous gradient and the function L(·, ) is m-strongly concave with L-Lipschitz continuous partial gradient with respect to x for any fixed . In particular, if m = 0, L(·, ) is general convex. See Appendix for details. We decompose our proof framework into the following three steps.
Step 1. We derive an upper bound of dual optimality gap k+1 with an alternative primal tracking error xL(xk, k) 2 in the following lemma, which quantifies how close k+1 is from the dual optimal .
Lemma 4.4. Under Assumption 2.1, given a constant µ > 0, we suppose that the stepsizes satisfy (4.2). It holds for all k = 0, 1, · · · , K - 1 that

k+1  k -

1 2 - g

g (k )

2
BDk +

1 2 + g

 (m + µ)2

xL(xk, k)

2
,

where  = max{maxiJ1 {bi}, maxiJ2 {bi( i + µ)}} and g is the Lipschitz constant of g(·).
We remark that the derivation of Lemma 4.4 relies on the Lipschitz gradient property of the dual function g(·). See Appendix for a detailed proof of Lemma 4.4.

7

Step 2. We provide an upper bound of kx+1 involving the dual optimality gap k as follows. We remark that the primal tracking error at iteration k + 1 consists of the following three parts.

kx+1 = L(xk+1, k+1) - L(xk+1, k) + L(xk+1, k) - L(x(k), k)

increase due to dual update

updated primal tracking error

+ L(x(k), k) - L(x(k+1), k+1) .

difference between dual optimality gaps

We can upper bound the updated primal tracking error by kx using the Lipschitz property of xL(x, ) with respect to x, and upper bound the other two parts using dual information. As a results, we obtain the following lemma, whose proof is deferred to Appendix.
Lemma 4.5. Under Assumption 2.1, given a constant µ > 0, we suppose that the stepsizes satisfy (4.2). It holds for all k = 0, 1, · · · , K - 1 that

kx+1 kx + 

g (k )

2
BDk -

xL(xk, k)

2 Pk

+

k

-

k+1,

where  = 3 + 22/µ2 + /µ and P k = A(Dk)-1 - ( + L/2)A2(Dk)-2 - /(m + µ)2I,  is defined in Lemma 4.4 and L is the Lipschitz constant of xL(x, ) with respect to x.

Step 3. We combine the above coupled inequalities to provide convergence results in Theorems 4.1 and 4.3. Using a weighted sum of the two inequalities in Lemmas 4.4 and 4.5, and combining the strong-convexity of L(·, ) and the strong-concavity of g(·), we conclude the proof of Theorem 4.1. Moreover, by telescoping the two inequalities in Lemmas 4.4 and 4.5, we conclude the proof of Theorem 4.3. See Appendix for a detailed proof of the theorems.

5 Numerical Experiments
In this section, we present experimental results for FedHybrid on some convex distributed optimization problems. In particular, we consider least squares problems and binary classification problems in a server-client network, both over synthetic and real-life datasets.
Experimental Setup. We evaluate all algorithms on four setups with non-IID data partitioning, where in each setup, there are n clients with d-dimensional decision variables: (1) Linear regression on a synthetic dataset: n = 20, d = 30. (2) Linear regression on non-IID partitioned Boston housing prices dataset: n = 8, d = 14. (3) Regularized logistic regression on a synthetic dataset: n = 20, d = 30. (4) Regularized logistic regression on non-IID partitioned mushroom dataset: n = 8, d = 99. For synthetic datasets, we generate data following different distributions across each client. For real datasets, we distribute the data to each client according to the values of the responses. In all setups, the local dataset sizes are randomly sampled. See Appendix for a detailed description of the setup.
Compared Methods. We focus on comparing the following algorithms designed for the server-client network: Federated Averaging (FedAvg) [15], Distributed Self-Concordant Optimization (DiSCO) [31], and FedHybrid with number of K clients performing Newton-type updates while all others doing gradient-type updates (FedH-K). We remark that for FedH-K, we have |J2| = K. In particular, FedH-G and FedH-N are short for FedHybrid with all clients performing gradient-type and Newton-type updates, respectively.
We remark that FedAvg is the baseline of first-order method for the server-client network. For a fair comparison, we consider the non-stochastic version of FedAvg, that is, at each iteration, all agents in the network take a full gradient descent step. Among second-order methods, DiSCO is an inexact damped Newton method that performs well in practice. We choose DiSCO instead of DANE [20], AIDE [19] or GIANT [27] as a baseline second-order method since the other methods require the assumption that each client has access to IID sampled data points, which is generally not realistic in practice. For each method, we tune parameters using grid search in the range [10-4, 10] and stepsizes in the range [10-4, 1] and choose the best one that minimizes the optimality gap.
Experimental Results. As shown in Figure 1, with some clients in the network performing Newtontype updates, FedHybrid improves the overall training speed a lot and outperforms the baseline method FedAvg consistently. See Appendix for more results.

8

Optimality gap f(x0k) f * log of loss function log(f(x0k))

5 4

FedAvg

DiSCO

FedH-N FedH-16

1

FedH-4

FedAvg DiSCO FedH-G FedH-N FedH-16

3

0

FedH-14

2

1

1

2

0 0 10 20 30 40 50 60 70 80 iteration, k

0

5

10 iterat1io5n, k 20

25

30

(a) Least squares on synthetic dataset (setup (1)). (b) Logistic regression on Mushroom dataset (setup (4)).

Figure 1: Comparison of convergence of different algorithms

In traditional distributed optimization algorithms, all clients are performing the same updates. The complexity of the method is determined by the client equipped with the worst computation hardware. While in our proposed FedHybrid framework, since for some parts of the clients, efficient Newtontype updates are involved, the overall system enjoys a faster convergence speed compared to systems running gradient-type methods only. Thus we can maximally leverage the parallel heterogeneous computation capabilities in this setting.
In particular, if all clients in the network perform Newton-type updates, our second-order FedH-N method achieves a comparable convergence performance or outperforms DiSCO. The reason is that DiSCO uses a preconditioning matrix, which performs the best only when the clients' data are IID.
Relation between convergence rate and the number of Newton-type clients. As shown in Figure 2, as the number of clients that perform Newton-type updates increases, the convergence rate of FedHybrid becomes faster. This suggests that those clients with higher computational capabilities and/or cheaper cost to perform computation can choose to implement Newton-type updates locally to help speedup the overall training speed of the system.

14

5000

FedH-G

FedH-N FedH-6

12

4000

FedH-4 FedH-2 10

3000

8 2000

FedH-G FedH-N FedH-12 FedH-8 FedH-4

6 1000

0

4

0 5 10 15iterat2io0n, k 25 30 35 40

0

20

40

60

80

100

iteration, k

(a) Least squares on Boston dataset (setup (2)). (b) Logistic regression on synthetic dataset (setup (3)).

Figure 2: Comparison of convergence rate with different number of Newton-type clients.

Optimality gap f(x0k) f * Loss function f(x0k)

6 Conclusion
This paper proposes FedHybrid, a distributed hybrid primal-dual algorithm framework that allows clients to perform either gradient-type or Newton-type updates based on their computation capabilities. We provide a novel analysis framework for primal-dual algorithms and obtain a linear convergence result of FedHybrid for strongly convex functions. Numerical studies are also provided to demonstrate the efficacy of FedHybrid in practice. Possible future directions include the extension of FedHybrid to time-variying graphs and/or stochastic settings.
9

References
[1] K. Arrow and L. Hurwicz. H. uzawa--studies in nonlinear programming, 1958.
[2] D. P. Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press, 2014.
[3] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and distributed computation: numerical methods, volume 23. Prentice hall Englewood Cliffs, NJ, 1989.
[4] S. Boyd, S. P. Boyd, and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
[5] S. Boyd, N. Parikh, and E. Chu. Distributed optimization and statistical learning via the alternating direction method of multipliers. Now Publishers Inc, 2011.
[6] Y. Cao, W. Yu, W. Ren, and G. Chen. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, 9(1):427­438, 2012.
[7] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.
[8] R. Crane and F. Roosta. Dingo: Distributed newton-type method for gradient-norm optimization. arXiv preprint arXiv:1901.05134, 2019.
[9] J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3): 592­606, 2011.
[10] D. Jakovetic´, J. Xavier, and J. M. Moura. Fast distributed gradient methods. IEEE Transactions on Automatic Control, 59(5):1131­1146, 2014.
[11] J. Konecny`, H. B. McMahan, D. Ramage, and P. Richtárik. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.
[12] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
[13] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith. Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37(3):50­60, 2020.
[14] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.
[15] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273­1282. PMLR, 2017.
[16] A. Mokhtari, Q. Ling, and A. Ribeiro. Network newton. In 2014 48th Asilomar Conference on Signals, Systems and Computers, pages 1621­1625, 2014. doi: 10.1109/ACSSC.2014.7094740.
[17] A. Mokhtari, W. Shi, Q. Ling, and A. Ribeiro. A decentralized second-order method with exact linear convergence rate for consensus optimization. IEEE Transactions on Signal and Information Processing over Networks, 2(4):507­522, 2016.
[18] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48­61, 2009.
[19] S. J. Reddi, J. Konecny`, P. Richtárik, B. Póczós, and A. Smola. Aide: Fast and communication efficient distributed optimization. arXiv preprint arXiv:1608.06879, 2016.
[20] O. Shamir, N. Srebro, and T. Zhang. Communication-efficient distributed optimization using an approximate newton-type method. In International conference on machine learning, pages 1000­1008. PMLR, 2014.
[21] W. Shi, Q. Ling, G. Wu, and W. Yin. Extra: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944­966, 2015.
[22] V. Smith, S. Forte, M. Chenxin, M. Takác, M. I. Jordan, and M. Jaggi. Cocoa: A general framework for communication-efficient distributed optimization. Journal of Machine Learning Research, 18:230, 2018.
10

[23] C. Sun, M. Ye, and G. Hu. Distributed optimization for two types of heterogeneous multiagent systems. IEEE Transactions on Neural Networks and Learning Systems, 32(3):1314­1324, 2021. doi: 10.1109/TNNLS.2020.2984584.
[24] R. A. Tapia. Diagonalized multiplier methods and quasi-newton methods for constrained optimization. Journal of Optimization Theory and Applications, 22(2):135­194, 1977.
[25] K. I. Tsianos, S. Lawlor, and M. G. Rabbat. Consensus-based distributed optimization: Practical issues and applications in large-scale machine learning. In 2012 50th annual allerton conference on communication, control, and computing (allerton), pages 1543­1550. IEEE, 2012.
[26] R. Tutunov, H. Bou-Ammar, and A. Jadbabaie. Distributed newton method for large-scale consensus optimization. IEEE Transactions on Automatic Control, 64(10):3983­3994, 2019.
[27] S. Wang, F. Roosta, P. Xu, and M. W. Mahoney. Giant: Globally improved approximate newton method for distributed optimization. Advances in Neural Information Processing Systems, 31: 2332­2342, 2018.
[28] Y. Wang, W. Yin, and J. Zeng. Global convergence of admm in nonconvex nonsmooth optimization. Journal of Scientific Computing, 78(1):29­63, 2019.
[29] H. Yuan and T. Ma. Federated accelerated stochastic gradient descent. arXiv preprint arXiv:2006.08950, 2020.
[30] X. Zhang, M. Hong, S. Dhople, W. Yin, and Y. Liu. Fedpd: A federated learning framework with optimal rates and adaptivity to non-iid data. arXiv preprint arXiv:2005.11418, 2020.
[31] Y. Zhang and X. Lin. Disco: Distributed optimization for self-concordant empirical loss. In International conference on machine learning, pages 362­370. PMLR, 2015.
[32] K. Zhou and S. I. Roumeliotis. Multirobot active target tracking with combinations of relative observations. IEEE Transactions on Robotics, 27(4):678­695, 2011.
11

