Parameter-Efficient Neural Question Answering Models via Graph-Enriched Document Representations

Louis Castricato Georgia Tech Georgia USA
lcastricato3@gatech.edu

Stephen Fitz Keio University
Tokyo Japan stephenf@keio.jp

Won Young Shin University of Toronto
Ontario Canada

arXiv:2106.00851v1 [cs.CL] 1 Jun 2021

Abstract
As the computational footprint of modern NLP systems grows, it becomes increasingly important to arrive at more efficient models. We show that by employing graph convolutional document representation, we can arrive at a question answering system that performs comparably to, and in some cases exceeds the SOTA solutions, while using less than 5% of their resources in terms of trainable parameters. As it currently stands, a major issue in applying GCNs to NLP is document representation. In this paper, we show that a GCN enriched document representation greatly improves the results seen in HotPotQA, even when using a trivial topology. Our model (gQA), performs admirably when compared to the current SOTA, and requires little to no preprocessing. In [28], the authors suggest that graph networks are not necessary for good performance in multi-hop QA. In this paper, we suggest that large language models are not necessary for good performance by showing a na¨ive implementation of a GCN performs comparably to SoTA models based on pretrained language models.
1 Introduction
In the past decade, Deep Learning (DL) approaches have dominated the field of Artificial Intelligence (AI) across a wide a array of domains. Recent developments in neural information processing methods sparked a revolution in Natural Language Processing (NLP), which resembles advances made in Computer Vision (CV) at the beginning of this new chapter of AI development. This progress was made possible through increasingly more advanced representation methods of natural language inputs. Initially shallow pre-training of early model layers became standard in NLP research through methods such as word2vec [20]. Subsequent progress followed trends similar to those in CV, which naturally led to pre-training of multiple layers of abstraction. These

1

advancements resulted in progressively deeper hierarchical language representations, such as those derived using self-attention mechanisms in transformer-based architectures [29]. Currently SOTA NLP systems use representations derived from pre-training of entire language models on large quantities of raw text, and often involve billions of parameters. Neural language modelling methods came to prominence in recent years due to the development of techniques such as BERT [6]. The success of neural network based Machine Learning (ML) models, especially those involving very deep architectures, can be attributed to their ability to derive informative embeddings of raw data into submanifolds of real vector spaces. The common idea behind these developments is that we can learn syntax and semantics of natural languages by training a DL model in a self-supervised fashion on a corpus of raw text. The general language representations of words in vector spaces induced by modern neural NLP models can be transferred to other domain-specific architectures and further tuned for downstream tasks. One could argue that this transfer learning benefit, as well as straightforward applicability to cascaded and multi-task learning, is the most exciting recent development in representation learning research in the context of natural language data.
However, in many applications of practical interest in the area of NLP, a need arises to represent language on a supra-lexical level. In this paper we concern ourselves with the evaluation of modern representation methods on document level. Some canonical tasks requiring such coarser embedding methods include document classification, question answering, and summarization.
Traditionally, document embedding methods combined token-level embeddings by mapping them to a single vector through a variety of models (e.g. vector averaging and word2vec inspired methods [15, 12, 10, 1, 22, 14, 10, 3, 19], attention mechanisms [29, 6, 26, 24, 25], convolutional architectures [18], or gated recurrent units [11, 4]).
1.1 Arbitrary Topologies
A Document Topology is the adjacency matrix that determines relationships between parts of the document (usually sentences or paragraphs). For instance, consider a news article like those found in CNN/Dailymail [27]. Within this dataset, the document topology resembles a sparse diagonal matrix, implying that the sentences are meant to be read sequentially in the order they appear. A family of neural embedding models taking graph topology into account can be grouped under the label of graph neural networks (GNN). In recent years multiple instantiations of this idea appeared in literature [2, 9, 7, 16, 5, 13].
As an example of a downstream task which benefits from GCN encoding consider question answering on the HotPotQA Dataset, [31]. This is a multi document question answering dataset where documents may or may not be related to the question. Additionally, some questions may require inferring from "multiple hops" within documents to reach a conclusive answer.
1.2 Advantages to non-na¨ive representation.
A common theme in contemporary multi-doc QA can be defined as figuring out the topology of a set of inference hops. Consider approaches like HGN and DFQN [8, 30] where the topology is constructed by following relevant corefences to bridge paragraphs and documents together. While they both report above baseline performance, the preprocessing step is nontrivial as it requires ranking paragraphs using large pretrained language models. By comparison, we require no such step.
2

Figure 1: Diagram of the gQA model. Sentences are encoded disjointly, and then enriched with an embedding of the question. Global self attention is then applied to all documents at a word level, which is then summed to the query enrichment of the words. A third BiLSTM is intialized to these summed embeddings. The hidden state of this BiLSTM becomes the sentence embedding. This sentence embedding is fed to a GGNN with our document topology. A fourth BiLSTM is initialized to the output of the GGNN and fed the output of the third BiLSTM as input. The (output, hidden state) tuple is then fed to the decoder, which is a series of BiLSTM+Linear Classifier, followed by concating onto the word and sentence embeddings. Finally, answer type is determined by feeding concatenaded sentence embeddings to a GGNN, pooling the output via a weighted average, and then feeding the pooled output to a linear classifier.
Methods closer to ours, like QFE and KGNN [21, 32] which possess identical decoders and encoders, albeit they do not encode sentences disjointly, similarly use multihop reasoning and do not require large language models. Thus these four will be our main points of comparison.
In the following section, we introduce gQA our test model for multi-document question answering. The goal of this project wasn't to beat SOTA results on QA, but rather to evaluate the performance of naive represnetations for multi-doc QA.
However, to our surprise, introducing graphical representation on document topology to a baseline QA approach greatly improved performance, resulting in a model that performs competitively with SOTA systems, while avoiding costly pre-training of over-parametrized language models. This led us to the realization that a graph based representation can produce more sustainable multi-doc QA systems by significantly reducing model complexity, and hence lowering the cost of model training.
3

2 Model
In the design of gQA, we aimed for a model that would work over arbitrary input topologies for multi-document question answering.
2.1 Architecture
2.2 Separate Encoding of Sentences
Inspired by [23], we introduce individual sentence representations. Given that sentences in Wikipedia, and particularly HotPotQA, can be viewed as "supporting facts" for questions rather than outlining some long detailed chronology it made sense to consider these sentences as individual nodes in a larger concept graph.
In GraphIE [23], an identical approach is utilized by considering twitter accounts as individual nodes and performing NER over the entire graph.
2.3 Encoder
By contrast, when compared to GraphIE [23], we consider the GNN as well as the"decoder" BiLSTM as part of our encoder. That is to say, we encode the sentences disjointly, perform inter-sentence enrichment via the Gated Graph Neural Network (GGNN) [17], and then intra-sentence enrichment by initializing a BiLSTM to the enriched sentence embeddings and feeding the word embeddings as input. GNN's or GCN's are models that capture graphical information via message passing between the nodes of a graph. Gated GNNs incorporate gating mechanisms that can learn to allow, block, or forget information that attempts to pass through the cells in GGNNs.
The propagation and output model for the GGNN is described by the following set of equations. 1
h(v1) = [xv , 0] a(vt) = Av [h(1t-1) ...h(Nt-1) ] + b zvt = (W za(vt) + U zh(vt-1)) rvt = (W ra(vt) + U rh(vt-1)) h(vt) = tanh(W a(vt) + U (rvt h(vt-1))) h(vt) = (1 - zvt ) h(vt-1) + zvt h(vt)
In the formulae above, A denotes the adjacency matrix of our GGNN, h(vt) denotes the hidden state of vertex v at time t, and xv denotes the vector input to vertex v. Finally, W, U, b are all learnable parameters.
In order to ablate the contribution of document representation, we wanted this to be a na¨ive implementation of GGNNs on top of the baseline HotPotQA. For this purpose, we introduce a Bi-Attention and Self-Attention layer preceding the GNN. Both Bi-Attention and Self-Attention are done over the entire set of documents. Bi-Attention is conditioned on the question. Excluding the GNN, our encoder is identical to that presented in the baseline of HotPotQA [31].
1A soft attention mechanism is applied with (i(hTv, xv)), which in this case is the sentence level attention. h(vT) and xv are concatenated and input into neural networks i() and j(), which outputs real-valued vectors. The tanh(·) function can be replaced with an identity function.
4

For the first layer of our encoder, we used a BiLSTM + CharCNN, with GloVe embeddings.
2.4 Decoder
The head utilized is nearly identical to that presented in HotPotQA's baseline, with the exclusion of how "answer type" is computed.
2.4.1 Supporting Sentences Given the output of the post-GGNN BiLSTM encoder, the sentence embeddings are fed to a linear classification layer in order to determine the supporting sentences.
The contextualized word embeddings and contextualized sentence embeddings for the supporting sentences are then concatenated onto hword and hsent respectively.
2.4.2 Answer Start Given (hword, hsent) from the supporting sentence computation, a sentence level BiLSTM is initialized to hsent and given hword as input.
The word level output of this BiLSTM is fed to a linear classification layer, and the BiLSTM output is concatenated onto the latent word and sentence embeddings.
2.4.3 Answer End Answer end is computed identically to answer start, including the output being concatenated onto the latent embeddings.
2.4.4 Answer Type A GGNN is fed hsent as input, which is then fed to a sentence level attention pooling layer and eventually another linear classification layer.

3 Preprocessing

We define the adjacency matrix used in our GGNNs by the following formula

M

=

1 (B

+

()(S

-

I)

+

L)T (B

+

()(S

-

I)

+

L)

2

Where B is a rotated diagonal matrix, (S - I) is the regularization parameter 2, and L is an adj. matrix that connects the last sentence of every document to the first sentence of every other.

2S - I refers to the cosine similarity matrix with a zero'ed diagonal. During training,  = 0.5 but during testing  = 0.05.  represents the only hyperparameter search conducted.

5

Model gQA-XS
gQA-S HGN
DFGN QFE
KGNN Baseline
C2F-R

EM 62.91 68.47 66.07 56.31 53.86 45.60 44.60 67.98

F1 72.48 76.85 79.36 69.69 68.06 59.02 59.02 81.24

Params 2.42M 9.04M  125M  110M
N/A N/A 960.1K  667M

Base-LM None None
RoBERTa-B BERT-B None None None
RoBERTa-L

Table 1: Comparison of gQA to the baseline and others. At the time of our experiments, C2F Reader was SOTA.

4 Results
Results of our sustainable QA model are compared to top models at the time of writing. Our model performs at SoTA level while utilizing less than 5%, and near SoTA (still beating some of the top models), with less than 1% of trainable parameters. gQA-XS and gQA-S use 100-dim and 200-dim word embeddings respectively. Notice that gQA-S is comparable to HGN with respect to answer exact match and answer F1. Training gQA-S takes about 12hrs on a GTX 1080. We used a 90/5/5 split over 90400 examples found in the distractor training set. We did not test on the dev set, nor did we submit to the HotPotQA leaderboards.

5 Discussion
We have presented a minimal extension to the original baseline of HotPotQA that performs comparably to SOTA with minimal preprocessing and minimal hyper parameterization. The model presented here has the ability to easily transfer to single-doc QA, as the only difference would be the adjacency matrix. This might be explored in future work. Source code and pretrained models will be released upon publication.
A question remains about the performance of gQA-S when enriched with a large language model, for instance RoBERTa, thus creating gQA-L.
Furthermore an improved pooling mechanism, like multi-head attention, would significantly aid in this direction. We have not included it here, since our purpose was to investigate na¨ive extension to the baseline model in order to gauge the contribution of graph representation.

References
[1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016. A simple but tough-to-beat baseline for sentence embeddings.
[2] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral networks and locally connected networks on graphs.
[3] Minmin Chen. 2017. Efficient vector representation for documents through corruption. arXiv preprint arXiv:1707.02377.

6

[4] Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.
[5] Michae¨l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering.
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
[7] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Ala´n Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pages 2224­2232.
[8] Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. 2019. Hierarchical graph network for multi-hop question answering.
[9] Mikael Henaff, Joan Bruna, and Yann LeCun. 2015. Deep convolutional networks on graph-structured data.
[10] Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. arXiv preprint arXiv:1602.03483.
[11] Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735­1780.
[12] Tom Kenter, Alexey Borisov, and Maarten De Rijke. 2016. Siamese cbow: Optimizing word embeddings for sentence representations. arXiv preprint arXiv:1606.04640.
[13] Thomas N. Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks.
[14] Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294­3302.
[15] Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International conference on machine learning, pages 1188­1196.
[16] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated graph sequence neural networks.
[17] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2017. Gated graph sequence neural networks. arXiv:1511.05493.
[18] Chundi Liu, Shunan Zhao, and Maksims Volkovs. 2017. Learning document embeddings with cnns. arXiv preprint arXiv:1711.04168.
[19] Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations. arXiv preprint arXiv:1803.02893.
7

[20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111­3119.
[21] Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and Junji Tomita. 2019. Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction.
[22] Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. arXiv preprint arXiv:1703.02507.
[23] Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, and Regina Barzilay. 2018. Graphie: A graph-based framework for information extraction.
[24] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf.
[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.
[26] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.
[27] Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks.
[28] Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, and Guoping Hu. 2020. Is graph structure necessary for multi-hop reasoning?
[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998­6008.
[30] Yunxuan Xiao, Yanru Qu, Lin Qiu, Hao Zhou, Lei Li, Weinan Zhang, and Yong Yu. 2019. Dynamically fused graph network for multi-hop reasoning.
[31] Zhilin Yang, Peng Qi, and Saizheng Zhang. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
[32] Deming Ye, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, and Maosong Sun. 2019. Multi-paragraph reasoning with knowledge-enhanced graph neural network.
8

