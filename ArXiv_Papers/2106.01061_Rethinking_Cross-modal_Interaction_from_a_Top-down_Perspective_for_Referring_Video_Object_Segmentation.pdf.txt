Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation
Chen Liang1,3, Yu Wu3,4, Tianfei Zhou2, Wenguan Wang2, Zongxin Yang3,4, Yunchao Wei4 and Yi Yang1
1Zhejiang University 2ETH Zurich 3Baidu Research 4University of Technology Sydney leonnnop@gmail.com, wenguanwang.ai@gmail.com, yangyics@zju.edu.cn

arXiv:2106.01061v1 [cs.CV] 2 Jun 2021

Abstract
Referring video object segmentation (RVOS) aims to segment video objects with the guidance of natural language reference. Previous methods typically tackle RVOS through directly grounding linguistic reference over the image lattice. Such bottom-up strategy fails to explore object-level cues, easily leading to inferior results. In this work, we instead put forward a two-stage, top-down RVOS solution. First, an exhaustive set of object tracklets is constructed by propagating object masks detected from several sampled frames to the entire video. Second, a Transformer-based tracklet-language grounding module is proposed, which models instance-level visual relations and cross-modal interactions simultaneously and efficiently. Our model ranks 1st place on CVPR2021 Referring Youtube-VOS challenge.
1. Introduction
Referring video object segmentation (RVOS) targets at segmenting video objects referred by given language expressions. RVOS is a challenging task as it requires not only comprehensive understanding the semantics within individual modalities, but also pixel-level cross-modal reasoning. Existing RVOS models [17, 9] typically work in a bottomup fashion (Fig. 1-(a)), i.e., perform grid-level alignment between visual and linguistic modalities. Thus they lack explicit knowledge about visual objects, leading to unreliable cross-modal reasoning and inaccurate segmentation.
In this work, we rethink RVOS from a top-down perspective (Fig. 1-(b)), by comprehensively exploring cross-object relations and conducting object-level cross-modal grounding. With a similar spirit of [12], our approach mainly consists of two stages: object tracklet generation and trackletlanguage grounding. In the first stage, we generate a set of high-quality object tracklets from input videos. Then, in the second stage, we ground the reference over the detected tracklets and select the best-matched one as the final output.
More specifically, in the object tracklet generation stage, a lot of object candidate masks are first generated by apply-

Query: A skateboard with a person on it on the sidewalk

(a)

(b)

Figure 1: An illustration of our motivation. Previous bottom-up methods (a) perform cross-modal interaction at grid level, and fail to capture crucial object-level relations as top-down approach (b).
ing instance segmentation over several sampled key frames. We further propagate the detected candidate masks to the whole video sequence, and generate an exhaustive set of object tracklets. After that, a tracklet-NMS mechanism is designed to remove redundant tracklets and select the highquality ones as candidates for language-guided segmentation. In the tracklet-language grounding stage, we build a Transformer-based grounding module. Benefiting from the powerful self-attention computation within the Transformer blocks, the within-modal relations among objects and intermodal interactions between tracklets and language can be comprehensively and efficiently modeled.
Our model ranked 1st place in 3rd Large-scale Video Object Segmentation Challenge (CVPR2021): Referring Video Object Segmentation track [1], with an overall J &F of 61.4% and 60.7% on test-dev and testchallenge, respectively.
2. Related Work
RVOS Datasets. The task of RVOS is proposed in [9], which mainly focuses on actor behavior understanding within an limited predefined action categories. Recently, Seo et al. introduced a new large-scale dataset [17], i.e., Refer-Youtube-VOS (RVOS-D), derived from YoutubeVOS [24]. RVOS-D provides more complex language descriptions from a broader object categories (90 categories) within relatively longer video sequences ( 6 s). Thus it poses more challenges for RVOS methods. Referring Youtube-VOS challenge [1] is built upon RVOS-D [17].

1

Tracklet

Instance Segmentation Mask
...

Key frame

Video sequence

K NK

12 11

Tracklet-NMS

Propagation Module

Instance-level Feature
Extraction

...

E Visual

Referred tracklet

...

F Grounding Module

Gnd

E Linguistic

Instance Segmentation Module
Ik
...

Query: A tiger walking near a person
{I t }
...

Figure 2: Pipeline of our proposed method, which contains two major stages, i.e., object tracklet generation (left column) and tracklet-language grounding (right column).
RVOS Methods. Current studies in the filed of RVOS are made mainly around the theme of building effective multimodal feature representations. Existing methods typically make use of dynamic convolutions [21, 4] to adaptively generate convolutional filters that better respond to the referent, or leverage cross-modal attention [22, 16] to compute the correlations among input visual and linguistic embeddings. However, these methods only approach RVOS on the grid level, ignoring the importance of object-level visual cues.

3. Methodology
Overview. Given a video sequence I = {It RW ×H×3}Tt=1 with T frames, and corresponding referring expression Q = {ql}Ll=1 with L words, a set of referred object segmentation masks {St {0, 1}W ×H }Tt=1 is requested for RVOS. As illustrated in Fig. 2, we design a two-down approach with two stages for object tracklet generation and tracklet-language grounding, respectively. In the first stage, we construct a comprehensive set of object candidate tracklets from I and propose a sequence-nms module for reducing redundant candidates. In the second stage, the referred target is selected from the tracklets under the guidance of Q. Object Tracklet Construction. We first uniformly sample K frames from I. For each key frame Ik, a set of mask candidates, i.e., Ok, are generated through an image instance segmentation model F Seg:

Ok = {Onk }Nn=k1 = F Seg(Ik),

(1)

where Nk refers to the number of the candidates in Ik, and, for each mask candidate Onk  Ok, we have Onk  {0, 1}W ×H . Then, a video mask propagation model F Prop is applied for each Onk  Ok, to forward and backward prop-
agate the mask to the entire video and get corresponding object tracklet kn:

kn = F Prop(Onk , I)  {0, 1}T ×W ×H .

(2)

Thus each tracklet is a sequence of masks, i.e., kn = {Mnk,t  {0, 1}W ×H }Tt=1, corresponds to the object candidate Onk in key frame Ik. And we define T k = {kn}Nn=k 1 as the set of all the tracklets generated from Ok.
Based on above strategy, we generate a lot of tracklets, i.e., {T k}k, from the K key frames. This ensures that we can generate a complete object candidate set that covers object instances in I as many as possible, without the disturbance from object occlusion, and move-in/-out. We denote the set of all generated candidate tracklets as T = k{1···K}T k. Tracklet-NMS. As we sample several key frames, there exist a lot of similar tracklets that correspond to the same object instance. This would bring an extra challenge to the following tracklet-language grounding process. Inspired by [13], we introduce a tracklet-level NMS process that eliminates redundant candidates in T efficiently. We first define tracklet-IoU that measures the similarity between two tracklets, i.e., p, q  T :

tracklet-IoU(p, q) =

T t=1 T t=1

|Mpt |Mpt

 

Mqt| , Mqt|

(3)

where p = {Mqt}Tt=1, and q = {Mqt}Tt=1. Each tracklet  is also assigned with a score, defined as the product of the confidence score of Onk (obtained from F Seg) and the mask propagation probability (obtained from F Prop), aver-

aged over all the T frames. Based on the tracklet score

and tracklet-IoU, traditional NMS algorithms [5, 6] is con-

ducted. As at most 5 objects might be requested in our con-

cerned experimental setting, we keep at most P = 10 track-

lets with highest scores for each video after NMS. We refer the final tracklet set as T^ = {p}Pp=1.
Tracklet-Language Grounding. We adopt per-frame reference grounding to determine the referred object from T^ .
Each frame It and linguistic input Q are first fed into single-

modality encoders, i.e., separately for within-modal feature

extraction:

It = E Visual(It)



w×h×D
R

,

(4)

Q = E Linguistic(Q)



L×D
R

,

where It and Q are extracted visual and linguistic features,
respectively. For each tracklet p  T^ , we extracted its corresponding
feature at frame It through:

pt

= AvgPool(It  Mpt)



D
R

,

(5)

where Mpt  {0, 1}W ×H refers to the candidate mask of tracklet p in frame It, and  denotes hadamard product. Note that the rescaling process for feature dimension align-
ment is omitted. Given concatenated embeddings t = [1t, · · · , Pt ] 
RP ×D for the candidate tracklets T^ = {p}Pp=1 at

2

1. A person is showing his skate board skills on the road 2. A skateboard with a person on it on the sidewalk

1. A leopard is biting a neck of a injured deer 3. A deer is down and attacked by a leopard 2. A leopard with partial visibility at the left is watching another leopard

Figure 3: Representative visual results on RVOS-D test-challenge set. Each referent and the corresponding textual description are highlighted in the same color.

Team

J &F 

J

F

leonnnop (Ours) nowherespyfly seonguk wangluting Merci1

61.4 (+6.6) 54.8 48.9 48.5 44.9

60.0 (+6.3) 53.7 47.0 47.1 43.9

62.7 (+6.7) 56.0 50.8 49.9 45.9

Table 1: Benchmarking results on the test-dev set of Referring Youtube-VOS challenge.

frame It, and the linguistic representation Q, we propose a Transformer-based [20] grounding module F Gnd for
tracklet-language grounding:

{stp}Pp=1 = F Gnd([t + ev, Q + el],

(6)

where stp  [0, 1] is the grounding score of tracklet p in It. Here ev  RP×D and el  RL×D are learnable modal
embeddings. Due to the self-attention mechanism in the
Transformer, the interactions among different object track-
lets and between different modalities are comprehensively
captured, leading to promising grounding performance.
The final grounding score for each p is given as: sp = Mean(s1p, · · · , sTp ). The the segments {St}Tt=1 are {Mpt}Tt=1, where p = arg maxp(s1, · · · , sP ).

4. Experiment

Challenge Dataset and Evaluation Metrics. We test our model on Referring Youtube-VOS challenge [1], which is built upon the recently released RVOS-D dataset [17]. The challenge dataset has 3,978 videos with about 15K language reference sentences in total; 3,471 videos are released with annotations for training. The rest videos are split into 202/305 for constructing test-dev/test-challenge sets, whose annotations are preserved for benchmarking. We use standard metrics, i.e., region similarity J and contour accuracy F, for evaluation. Detailed Network Architecture. We employ two instance segmentation models, i.e., HTC [2] and CondInst [19], for implementing F Seg in Eq. 1. At each key frame Ik, the mask candidates Ok are a combination of all proposals generated from the two models. The mask propagation model F Prop (Eq. 2) is implemented as CFBI+ [25]. We uniformly sample K = 7 key frames for each video sequence. For tracklet-language grounding, we implement the visual encoder EVisual as ResNet-101 [7] initialized from ImageNetpretrained weights and linguistic encoder ELinguistic as a standard BERTBASE model. The grounding module F Gnd is

Team

J &F 

J

F

leonnnop (Ours) nowherespyfly feng915912132 Merci1 wangluting

60.7 (+11.3) 49.4 48.2 41.2 40.7

59.4 (+11.0) 48.4 47.4 40.6 39.5

62.0 (+11.7) 50.3 49.0 41.8 41.8

Table 2: Benchmarking results on test-challenge set of Referring Youtube-VOS challenge.

Model

J &F  J  F 

Image-level Baseline +Video-level Propagation F Prop (Eq. 2) +Transformer-based Grounding F Gnd (Eq. 6)
+Sequence-NMS (Eq. 3) & Model Ensemble

40.9 40.5 41.3 49.2 47.5 50.9 56.4 54.8 58.1 61.4 60.0 62.7

Table 3: Ablation study of essential components on test-dev.

a 4-layer Transformer [20] with 12 heads in each layer, followed by a 2-layer MLP and a softmax layer for probability prediction. Input sentences are split by the WordPiece tokenizer [23] as in [3]. Both the hidden dimensions of Transformer and feature channel of within-modal representations are set to 768, i.e., D = 768. Training Detail. For F Seg, HTC is trained on COCO [14] without finetuning. CondInst is pretrained on COCO and finetuned on the training split of RVOS-D with the standard training setting in [19] for about 15,000 steps. The propagation module F Prop, i.e., CFBI+, is pretrained on COCO and finetuned over training split of VOS [24] track as a standard training setting in semi-supervised VOS task (see [25] for more details). For tracklet-language grounding module (Eqs. 4- 6), we pretrain it using the data from RefCOCO [26], RefCOCOg [26] and RefCOCO+ [15] for about 20 epochs. We use Adam [10] as the optimizer with a learning rate of 4e-5, batch size of 48 and weight decay of 1e-4. The module is further finetuned on the training split of RVOS-D for five epochs with a learning rate of 1e-5. Model Ensemble. Model ensemble is also used in our final submission. We build five models with different implementations of the visual encoder EVisual, i.e., ResNet101 [7], HRNet [18] and ResNeSt101 [27], and linguistic encoder ELinguistic, i.e., DebertaBASE [8] and BartBASE [11]. We use different hyperparameter settings to further promote model performance and simply average grounding probabilities from different models for final prediction. Results on RVOS Challenge. Table 1 and Table 2 show the ranking result of top teams in test-dev and test-challenge sets respectively. Our approach achieves the best performance on both the two sets across

3

all the metrics and outperforms 2nd best team with a large margin, i.e., 11.3% in terms of overall J &F on test-challenge. Fig. 3 shows qualitative results of our proposed model on test-challenge. With the effective top-down model design, our approach generates robust predictions even in challenging scenes, e.g., semantically similar instances, inconspicuous referent, complex linguistic description, etc. Ablation Study. We start our ablation study with a simple image-level grounding pipeline (1st row in Table 3), which only contains an image instance segmentation module F Seg (Eq. 1) for image-level object candidates generation and implements the grounding module F Gnd (Eq. 6) as a na¨ive feature similarity operation without tracklet construction F Prop (Eq. 2). Then we progressively add essential modules (2nd4th rows in Table 3). With the fully exploration of intra- and inter-modal interactions, consistent performance improvements can be achieved.
Acknowledgements This work was sponsored by the Fundamental Research Funds for the Central Universities.
References
[1] The 3rd large-scale video object segmentation challenge. https://youtube- vos.org/challenge/2021/. 1, 3
[2] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In CVPR, 2019. 3
[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2018. 3
[4] Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek. Actor and action video segmentation from a sentence. In CVPR, 2018. 2
[5] Ross Girshick. Fast r-cnn. In ICCV, 2015. 2
[6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 2
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 3
[8] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In ICLR, 2021. 3
[9] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In ACCV, 2018. 1
[10] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014. 3
[11] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence

pre-training for natural language generation, translation, and comprehension. In ACL, 2020. 3 [12] Chen Liang, Yu Wu, Yawei Luo, and Yi Yang. Clawcranenet: Leveraging object-level relation for text-based video segmentation. arXiv preprint arXiv:2103.10702, 2021. 1 [13] Huaijia Lin, Ruizheng Wu, Shu Liu, Jiangbo Lu, and Jiaya Jia. Video instance segmentation with a propose-reduce paradigm. arXiv preprint arXiv:2103.13746, 2021. 2 [14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 3 [15] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016. 3 [16] Ke Ning, Lingxi Xie, Fei Wu, and Qi Tian. Polar relative positional encoding for video-language segmentation. In IJCAI, 2020. 2 [17] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with a large-scale benchmark. In ECCV, 2020. 1, 3 [18] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In CVPR, 2019. 3 [19] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In ECCV, 2020. 3 [20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 3 [21] Hao Wang, Cheng Deng, Fan Ma, and Yi Yang. Context modulated dynamic networks for actor and action video segmentation with language queries. In AAAI, 2020. 2 [22] Hao Wang, Cheng Deng, Junchi Yan, and Dacheng Tao. Asymmetric cross-guided attention network for actor and action video segmentation from natural language query. In ICCV, 2019. 2 [23] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. 3 [24] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. Youtube-vos: Sequence-to-sequence video object segmentation. In ECCV, 2018. 1, 3 [25] Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative video object segmentation by foreground-background integration. In ECCV, 2020. 3 [26] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In ECCV, 2016. 3 [27] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020. 3

4

