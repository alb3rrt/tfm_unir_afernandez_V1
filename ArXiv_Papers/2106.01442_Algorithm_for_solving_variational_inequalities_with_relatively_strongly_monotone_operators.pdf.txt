Algorithm for solving variational inequalities with relatively strongly monotone operators
Alexander A. Titov Moscow Institute of Physics and Technology, Moscow, Russia

arXiv:2106.01442v2 [math.OC] 8 Jun 2021

Abstract. Basing on some recently proposed methods for solving variational inequalities with non-smooth operators, we propose an analogue of the Mirror Prox method for the corresponding class of problems under the assumption of relative smoothness and relative strong monotonicity of the operator.
Keywords: Relative Boundedness. Variational Inequality. Relative strong monotonicity. Relative Smoothness. Relative strong convexity.

Introduction

Recently [1] there were proposed some numerical methods for solving saddle point problems and variational inequalities with simplified requirements for the smoothness conditions of functionals. By simplified smoothness conditions we mean, generally speaking, non-smooth operators, which do not satisfy the Lipschitz condition, but satisfy some of its weakened versions. We base on the novel work [2], in which the authors successfully transferred the ideology of gradient methods to the case of relatively strongly convex and relatively smooth objective. We continue developing similar ideas and consider variational inequalities with relatively smooth and relatively strongly monotone operators. Such operators naturally arise when considering the relatively strongly convex saddle point problem and reducing it to the variational inequality [3].
Let E be some finite-dimensional vector space and E be its dual. For a fixed norm . on E define the corresponding dual norm ·  as follows:

 = max { , x },
x 1
where , x denotes the value of the linear function   E at the point x  E. Let X  E be a compact set. Consider relatively smooth, relatively strongly convex, and relatively strongly
monotone operator g(x) : X  E, satisfying

1. Inexactness

g(y), x - y  g(y), x - y + 

(1)

2. Relative strong monotonicity

g(y), x - y + g(x), y - x + µV (x, y)  

(2)

2

Alexander A. Titov

3. Relative smoothness

g(y) - g(z), x - z  LV (x, z) + LV (z, y) + 

(3)

Definition 1 (Minty Variational Inequality). For a given operator g(x) : X  R we need to find a vector x  X, such that

g(x), x - x  0, x  X.

(4)

We also need to choose a prox-function d(x), which is continuously differentiable and 1-strongly convex on X, and the corresponding Bregman distance, defined as follows:

V (y, x) = d(y) - d(x) - d(x), y - x , x, y  X.

(5)

Consider the following Mirror Prox algorithm [2].

Algorithm 1 Universal Mirror Prox (UMP)
Require:  > 0,  > 0, x0  X, initial guess L0 > 0, prox-setup: d(x), V (x, z). 1: Set k = 0, z0 = arg minuQ d(u). 2: repeat 3: Find the smaller ik  0 :

g(zk), zk+1 - zk  g(wk), zk+1 - wk + g(zk), wk - zk

(6)

+ Lk+1(V (wk, zk) + V (zk+1, wk)) + 

4: where Lk+1 = 2ik-1Lk and

wk = arg min{ g(zk), x - zk + Lk+1V (x, zk)}

(7)

x

zk+1 = arg min{ g(wk), x - wk + Lk+1V (x, zk)}

(8)

x

5: until

SN

=

N -1 k=0

1 Lk+1



max V (x, x0)
xX


(9)

Ensure: zk

Theorem 1. It is well-known [2], that for the output of the Mirror Prox algorithm the following inequality takes place:

g(x), zk - x

 - 1 N-1 g(wk), x - wk

SN k=0

Lk+1

 2LV (x, z0) , N

moreover, the total number of iterations does not exceed

2L max V (x0, x)

N = xX

.



Title Suppressed Due to Excessive Length

3

Lemma 1. For the described operator g and Mirror Prox algorithm there takes place the following -decreasing of Bregman divergence:

V (x, zN )  V (x, z0) + SN .

(10)

Proof. It is known ([2], proof of Theorem 4.8), that k  0

- g(wk), u - wk  Lk+1V (u, zk) - Lk+1V (u, zk+1) + 

(11)

Due to the relatively strong monotonicity of the operator g, we can consider, that the solution to weak variational inequality is also a strong solution:

- g(x), wk - x  0,

(12)

which, due to (1), leads to

- g(x), wk - x  ,

(13)

and

0  - g(x), x - wk +   - g(wk), x - wk 

(14)

 Lk+1V (x, zk) - Lk+1V (x, zk+1) + .

So, there takes place the following -decreasing of the Bregman divergence:

V

(x,

zk+1)



V

(x,

zk)

+

 Lk+1

k.

(15)

Thus,

V

(x,

zN )



V

(x,

z0)

+



N i=1

1 Li

.

(16)

Let us now consider the following algorithm, which is in fact the restarted version of the considered Mirror Prox algorithm.

Algorithm 2 Restarted Universal Mirror Prox (Restarted UMP).

Require:

 > 0,

µ > 0,



:

d(x) 

 2

x  Q :

x  1; x0, R0

:

x0 - x 2 R02.

1: Set p = 0, d0(x) = R02d

x-x0 R0

.

2: repeat

3: Set xp+1 as the output of UMP for monotone case with prox-function dp(·) and

stopping criterion

k-1 i=0

Mi-1



 µ

.

4:

Set

Rp2+1

=

R20 2(p+1) µSNp

-

 µ

.

5:

Set dp+1(x)  Rp2+1d

x-xp+1 Rp+1

.

6: Set p = p + 1.

7: until p > log2

2R20 

.

Ensure: xp.

4

Alexander A. Titov

Theorem 2. Consider relatively smooth and relatively strongly monotone operator g, satisfying (1)-(3). Then the restarted version of Mirror Prox algorithm produces the point xp, such that

V

(x,

xp)





+

 µ

1 + 2L µ

,

moreover, the total number of iterations does not exceed

N

=

2L µ

log2

R02 . 

Proof. Due to relatively strong monotonicity of the operator:

µV (x, zp+1)  - g(x), zp+1-x - g(zp+1), x-zp+1  - g(zp+1), x-zp+1

(17)

Let us use induction to show the fulfillment of the theorem. Consider p = 0.

After

no

more

than

N0

=

2L µ

iterations

of

Mirror

Prox

algorithm

we

get:

- g(x), wN0-1 -x
x1

 2 - 1 N0-1 g(wk), x - wk

SN0 k=0

Lk+1



R02 2SN0

-

:=

R12

(18)

Let us assume, that this inequality takes place for some p > 0:

- 2 + µ Np-1 V (x, wk)  -2 - 1 Np-1

SNp k=0

Lk+1

SNp k=0

and prove, that the same holds for p + 1.

g(wk), x - wk Lk+1

 Rp2 -  2SNp (19)

Np-1 V (x, wk)

k=0

Lk+1

V

x, wNp-1

· SNp

-

 LNp

-

+  LNp LNp-1

- . . .- SNp =

Thus,

= SNp V

x, wNp-1

- Np - (Np - 1) - 2 - 

LNp

LNp-1

L2 L1

-2 + µ Np-1 V (x, wk)

SNp k=0

Lk+1

-2 + µV x, wNp-1 -

 Np + Np - 1 + . . . + 2 + 1

SNp LNp LNp-1

L2 L1

So,

V (x, xp+1)

Rp2 2µSNp

+



+ Np µ

which ends the proof of the theorem.

 µV (x, wNp-1) - (2 + Np)

Rp2 2µSNp

+

 µ

1

+

2L µ

,

Title Suppressed Due to Excessive Length

5

References

1. Titov, A., Stonyakin, F., Alkousa, M., and Gasnikov, A. (2021). Algorithms for solving variational inequalities and saddle point problems with some generalizations of Lipschitz property for operators. arXiv preprint arXiv:2103.00961.
2. Stonyakin, F., Tyurin, A., Gasnikov, A., Dvurechensky, P., Agafonov, A., Dvinskikh, D., ... and Piskunova, V. (2020). Inexact relative smoothness and strong convexity for optimization and variational inequalities by inexact model. arXiv preprint arXiv:2001.09013.
3. Lu, H., Freund, R. M., and Nesterov, Y. (2018). Relatively smooth convex optimization by first-order methods, and applications. SIAM Journal on Optimization, 28(1), 333-354.
4. Cohen, M. B., Sidford, A., and Tian, K. (2020). Relative Lipschitzness in Extragradient Methods and a Direct Recipe for Acceleration. arXiv preprint arXiv:2011.06572.

