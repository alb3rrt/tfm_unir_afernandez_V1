arXiv:2106.00387v1 [cs.LG] 1 Jun 2021

Decision Concept Lattice vs. Decision Trees and Random Forests
Egor Dudyrev1[0000-0002-2144-3308] Sergei O. Kuznetsov1[0000-0003-3284-9001]
National Research University Higher School of Economics, Moscow, Russia
Abstract. Decision trees and their ensembles are very popular models of supervised machine learning. In this paper we merge the ideas underlying decision trees, their ensembles and FCA by proposing a new supervised machine learning model which can be constructed in polynomial time and is applicable for both classification and regression problems. Specifically, we first propose a polynomial-time algorithm for constructing a part of the concept lattice that is based on a decision tree. Second, we describe a prediction scheme based on a concept lattice for solving both classification and regression tasks with prediction quality comparable to that of state-of-the-art models.
Keywords: Concept Lattice · Decision Trees · Random Forest.
1 Introduction
In this work we propose an approach to combining the ideas based on concept lattices and decision trees, which are extensively used in practical machine learning (ML), in order to create a new ML model which generates good classifiers and regressors in polynomial time.
Formal Concept Analysis (FCA) is a mathematically-founded theory well suited for developing models of knowledge discovery and data mining [18], [11], [8]. One of the serious obstacles to the broad use of FCA for knowledge discovery is that the number of formal concepts (i.e. patterns found in a data) can grow exponentially in the size of the data [15]. Sofia algorithm [7] offers a solution to this problem by constructing only a limited amount of most stable concepts.
Learning decision trees (DT) [5] is one of the most popular supervised machine learning approaches. Most famous methods based on ensembles of decision trees ­ aimed at increasing the accuracy of a single tree ­ are random forest (RF) [6] and gradient boosting over decision trees [9]. Both algorithms are considered among the best in terms of accuracy [17].
There are a number of papers which highlight the connection between the concept lattice and the decision tree. The work [4] states that a decision tree can be induced from a concept lattice. In [16] the author compares the ways the concept lattice and the decision tree can be used for supervised learning. Finally, in [12] the authors provide a deep mathematical explanation on the connection between the concept lattice and the decision tree.

2

E. Dudyrev, S. O. Kuznetsov

In this paper we develop the previous work in a more practical way. We show that the decision tree (and its ensembles) can induce a subset of concepts of the concept lattice. We propose a polynomial-time algorithm to construct a supervised machine learning model based on a concept lattice with prediction quality comparable to that of the state-of-the-art models.

2 Basic Definitions
For standard definitions of FCA and decision trees we refer the reader to [10] and [5], respectively.
In what follows we describe algorithms for binary attributes, numerical data can be processed by means of interval pattern structures or can be scaled to binary contexts [13].

3 Construct a Concept Lattice via a set of Decision Trees

Definition 1 (Classification rule). Let M be a set of attributes of a context K and Y be a set of "target" values. A pair (, y^),   M, y^  Y is a classification rule where  is called a premise and y^ is a target prediction.
Applied to object g  G it can be interpreted as "if the description of g falls under the premise , then object g should have the target value y^" or "if   g  y^".
In the case of classification task Y can be represented either as a set {0, 1}: Y = {y  {0, 1}}|iG=|1 or a set of probabilities of a positive class: Y = {y  [0, 1]}|iG=|1. In the case of regression task target value Y is a set of real valued numbers: Y = {y  R}|iG=|1.
We can define a decision tree DT as a partially ordered set (poset) of classification rules:

DT  {(, y^) |   M, y^  Y }

(1)

where by the order of classification rules we mean the inclusion order on their

premises:

(1, y^1 )  (2, y^2 )  1  2

(2)

Here we assume that a decision tree is a binary tree, i.e. its node is either a leaf (has no children) or has exactly 2 children nodes.
The other property of a decision tree is that each premise of its classification rules describes its own unique subset of objects:

(1, y^1 )  DT , (2, y^2 )  DT : 1 = 2

(3)

These simple properties result in an idea that 1) we can construct a concept lattice by closing premises of a decision tree, 2) join semilattice of such concept lattice is isomorphic to a decision tree.

Decision Lattice

3

Proposition 1. Let K = (G, M, I) be a formal context, L(K) be a lattice of the
context K. A subset of formal concepts LDT (K) forming a lattice can be derived from the decision tree DT (K) constructed from the same context as:

LDT = {(, ) | (, y^)  DT (K)}  {(M , M )}

(4)

Proposition 2. Join-semilattice of a concept lattice LDT is isomorphic to the decision tree DT .

Proof. Given two classification rules (1, y^1), (1, y^1 )  DT let us consider two cases:
1. 1  2  (1, 1)  (2, 2) 2. 1  2, 2  1  m  M : m  1, ¬m  2
 (1, 1)  (2, 2), (2, 2)  (1, 1)
Thus the formal concepts from the join-semilattice of LDT possess the same partial order as the classification rules from DT .

Since we can construct a concept lattice from a decision tree and there is a union operation for concept lattices then we can construct a concept lattice which will correspond to a "union" of a number of independent decision trees (i.e. a random forest).

Proposition 3. Let K = (G, M, I) be a formal context, L(K) be a lattice of the context K. A subset of formal concepts LRF (K) of the concept lattice L(K) form-
ing a lattice can be obtained from a random forest, i.e. from a set of m decision trees constructed on subsets of a formal context DTi(Ki), i = 1, ..., m, Ki  K:

m

LRF (K) = LDTi (Ki)

(5)

i=1

The size of the lattice LRF is close to the size of the underlying random
forest RF : |LRF |  |RF |  O(mG log(G)), where m is the number of trees in
RF [3]. According to [2] the time complexity of constructing a decision tree is O(M G2 log(G)). Several algorithms for constructing decision trees and random forests are implemented in various libraries and frameworks like Sci-kit learn1 , H2O2 , Rapids3 . The latter is even adapted to be run on GPU.
Thus, our lattice construction algorithm has two steps:

1. Construct a random forest RF 2. Use random forest RF to construct a concept lattice LRF (by eq. 3)
Both strong and weak side of this algorithm is that it relies on a supervised machine learning model, so it can be applied only if target labels Y are given. In addition, the result set of concepts may not be optimal w.r.t. any concept interestingness measure [14]. Though it is natural to suppose that such set of concepts should be reasonable for supervised machine learning tasks.

1https://scikit-learn.org/stable/modules/ensemble.html#random-forests 2http://h2o-release.s3.amazonaws.com/h2o/master/1752/docs-website/datascience/rf.html 3https://docs.rapids.ai/api/cuml/stable/api.html#random-forest

4

E. Dudyrev, S. O. Kuznetsov

4 Decision Lattice

Given a formal concept (A, B) we can use its intent B as a premise of a classification rule (B, y^B).
The target prediction y^B of such classification rule (B, y^B) can be estimated via an aggregation function over the set {yg | g  A}. In what follows we use the average aggregation function:

y^B

=

1 |A|

yg

(6)

gA

Let us define a decision lattice (DL) as a poset of classification rules.

Definition 2. Let M be a set of attributes of a formal context K and Y be a set of target values. Then a poset of classification rules is called a decision lattice DL if a premise of each classification rule of DL describes its own unique subset of objects (similar to DT in equation 3)

Decision lattice DL can be constructed from a concept lattice L as follows:

DL = {(B, y^B) | (A, B)  L}

(7)

where y^B can be computed in various ways (we use the equation 6).
To get a final prediction y^g for an object g a decision tree DT firstly selects all the classification rules DT g describing the object g. Then it uses the target prediction of the maximal classification rule from DT g

DT g = {(, y^)  DT |   g}

(8)

DTmg ax = {(, y^)  DT g | (1, y^1 )  DT g :   1}

(9)

y^g = y^, (, y^)  DTmg ax

(10)

We use the same algorithm to get a final prediction y^g for an object g by a decision lattice DL. The only difference is that when the subset DTmg ax always contains only one classification rule a subset DLgmax may contain many. In this case we average the predictions of maximal classification rules DLgmax:

y^g

=

1 |DLgmax|

y^
(,y^ )DLgmax

(11)

Let us consider the fruit context K = (G, M, I) and fruit label Y presented in Table 1. We want to compare the way decision lattice makes an estimation of the label ymango of object mango when this object is included in the train or the test context.
Figure 1 represents decision lattices constructed upon fruit context with (on the left) and without (in the center) mango object. In both cases we show only the classification rules which cover (describe) mango object.

Decision Lattice

5

objects G

attributes M

label Y

firm smooth color

form

fruit

yellow green blue white round oval cubic

apple

X

X

X

1

grapefruit

X

X

1

kiwi

X

X

1

plum

X

X

X

1

toy cube X X

X

X

0

egg

XX

X

X

0

tennis ball

X

X

0

mango

X

X

X

1

Table 1. Fruit context and fruit labels

The left picture represents a decision lattice with 8 classification rules and 1 single maximal classification rule: ("color is green & form is oval & smooth", 1). Therefore we use this classification rule to predict the target label of mango.
The picture in the center shows a decision lattice with 6 classification rules and 2 maximal classification rules: ("color is green & form is oval", 1), ("form is oval & smooth", 1/2). We average the target predictions of these classification rules to get a final prediction of 3/4 as shown in the picture on the right.

Deci ion Lattice covering object "mango"
from train data
:  0y : 5/8

: color_i _green : form_i _oval : mooth

3

2

1

y : 2/3

y : 3/4

y : 3/5

6y : 1/2

5y : 2/2

4y : 2/3

7y : 1/1

Decci oi"oenrLinagttice object "mango" from te t data
:  0y : 47

: color_i _green : form_i _o"al

3

2

y : 12

y : 23

: mooth 1 y : 24

5y : 11

4y : 12

w aD"eecrai gioend Lpartetdicicetion of object "mango" from te t data
:  0y : 47

: color_i _green : form_i _o"al : mooth

3

2

1

y : 12

y : 23

y : 24

5y : 11

4y : 12

6y : 34

Fig. 1. Example of prediction of mango object

5 Experiments
We compare our decision lattice (DL) approach with the most popular machine learning models on real world datasets. One can reproduce the results by running Jupyter notebooks stored on GitHub [1]. Decision lattice models are implemented in open-source Python library for FCA which is called FCApy and located in the same GitHub repository.
We use ensembles of 5 and 10 decision trees to construct decision lattice models DL RF 5 and DL RF 10, respectively. The same ensemble of 5 decision trees is used by random forest models RF 5. Thus, we can compare prediction

6

E. Dudyrev, S. O. Kuznetsov

qualities of DL RF 5 and RF 5 based on the same set of decision trees (and, consequently, classification rules).
The non-FCA model we use for the comparison are decision tree (DT), random forest (RF) and gradient boosting (GB) from sci-kit learn library, gradient boostings from LightGBM (LGBM), XGBoost (XGB), CatBoost (CB) libraries.
We also test Sofia algorithm [7] as a polynomial-time approach to construct a decision lattice DL Sofia. We compute only 100 of most stable concepts by Sofia algorithm because of its time inefficiency.
Metadata of the datasets is given in Table 2.

boston calhouse diabetes mean delta

Dataset name

Task type # Instances # Attrsibutes

train test train test train test train test

adult 4

Bin. class.

48842

14 model

amazon 5

Bin. class.

bank 6

Bin. class.

breast 7

Bin. class.

heart 8

Bin. class.

kick 9

Bin. class.

mammographic 10 Bin. class.

seismic 11

Bin. class.

boston 12

Regression

calhouse 13

Regression

diabetes 14

Regression

32770 45211
569 303 72984 961 2584 506 20640 442

10 DL RF 5 17 DL RF 10 32 DL Sofia 75
DT 34 RF 5
6 RF 19 GB 14 LGBM
8 CB 10

0.02 0.06 0.01 0.07 0.29 0.20 0.00 0.05 0.05 0.01 0.04 0.00 0.05 0.00 0.04 0.01 0.02 0.00

0.14 0.05 0.13 0.04
0.00 0.09 0.14 0.04 0.06 0.02 0.17 0.02 0.13 0.00 0.13 0.00

0.05 0.00 0.01 0.01 0.40 0.11 0.00 0.12 0.15 0.02 0.12 0.00 0.16 0.00 0.11 0.00 0.06 0.00

0.07 0.05 0.35 0.00 0.12 0.07 0.13 0.09 0.07

0.04 0.04 0.16 0.09 0.03 0.01 0.01 0.01 0.00

Table 2. Description

best result 0.00 0.14 0.00 0.21 0.00 0.31 0.00 0.22

of the datasets

Table 3. Weighted Average Percent-

age Error (best model delta)

For each dataset we use 5-fold cross-validation. We compute F1-score to measure the predictive quality of classification and weighted average percentage error (WAPE) to that of regression. In Tables 4­3 we show the difference between the metric value of the model and the best obtained metric value among all methods.
As can be seen from Tables 4­3 DL RF model does not always show the best result among all the tested models, though its prediction quality is comparable to the state-of-the-art.

4https://archive.ics.uci.edu/ml/datasets/Adult 5https://www.kaggle.com/c/amazon-employee-access-challenge/data 6https://archive.ics.uci.edu/ml/datasets/bank+marketing 7https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) 8https://archive.ics.uci.edu/ml/datasets/heart+Disease 9https://www.kaggle.com/c/DontGetKicked/data?select=training.csv 10http://archive.ics.uci.edu/ml/datasets/mammographic+mass 11https://archive.ics.uci.edu/ml/datasets/seismic-bumps 12https://archive.ics.uci.edu/ml/machine-learning-databases/housing 13https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset 14https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset

Decision Lattice

7

DL Sofia model shows the worst results. There may be 2 reasons for this. First, it uses only a hundred of concepts. Second, we use Sofia algorithm to find one of the most stable concepts, but not the ones which minimize the loss.
Figure 2 shows the time needed to construct a lattice by the sets of 5 (DL RF 5) and 10 (DL RF 10) decision trees and by Sofia algorithm (DL Sof ia). The lattice can be constructed in a time linear in the number of objects in the given data.

model

adult

amazon bank

breast

heart

kick

mamm. seismic

mean delta

train test train test train test train test train test train test train test train test train test

DL RF 5 DL RF 10 DL Sofia DT RF 5 RF GB LGBM CB

-0.35 -0.06 -0.01 -0.00 -0.41 -0.16 -0.01 -0.01 -0.03 -0.02 -0.59 -0.03 -0.03 -0.02 -0.24 -0.15

-0.33 -0.05 -0.01 -0.00 -0.37 -0.14 -0.00 -0.00 -0.01 0.00 -0.58 -0.03 -0.01 -0.02 -0.13 -0.15

-1.00 -0.95 -0.33 -0.27

-0.87 -0.72 -1.00 -0.15

0.00 -0.10 -0.00 -0.01 0.00 -0.24 0.00 -0.06 0.00 -0.15 0.00 -0.05 -0.00 -0.08 0.00 0.00

-0.35 -0.05 -0.01 -0.00 -0.41 -0.12 -0.01 -0.01 -0.05 -0.07 -0.60 -0.02 -0.04 -0.02 -0.36 -0.07

-0.00 -0.04 0.00 0.00 0.00 -0.11 0.00 0.00 0.00 -0.00 -0.00 -0.01 0.00 -0.03 -0.00 -0.12

-0.36 -0.02 -0.01 -0.00 -0.47 0.00 0.00 -0.01 -0.07 -0.01 -0.62 -0.00 -0.07 0.00 -0.42 -0.09

-0.31 -0.00 -0.01 -0.00 -0.32 -0.04 0.00 -0.02 0.00 -0.03 -0.60 -0.00 -0.03 -0.02 -0.00 -0.11

-0.31 0.00 -0.01 -0.00 -0.31 -0.05 0.00 -0.01 -0.01 -0.01 -0.59 0.00 -0.04 -0.01 -0.33 -0.13

-0.21 -0.05 -0.18 -0.05 -0.80 -0.52 -0.00 -0.09 -0.23 -0.05 -0.00 -0.04 -0.25 -0.02 -0.16 -0.03 -0.20 -0.02

best result 1.00 0.65 0.98 0.97 1.00 0.48 1.00 0.95 1.00 0.76 1.00 0.35 0.95 0.81 1.00 0.15 0.99 0.64

Table 4. F1 score (best model delta)

Time (minutes)

14 Time to construct a concept lattice

12

DL_RF_10

10

8 DL_Sofia 6
4

DL_RF_5

2

0

0 10 20 30 40 50 60 Number of objects (thous.)

Fig. 2. Time needed to construct a lattice

6 Conclusions
In this paper we have introduced a new concept-based method to classification and regression. The proposed method constructs concept-based classifiers obtained with decision trees and random forests. This method is quite efficient and

8

E. Dudyrev, S. O. Kuznetsov

can be used for big datasets. We have shown that our approach is non-inferior to the predictive quality of the state-of-the-art competitors.
In the future work we plan to extend the algorithm for constructing decision trees in the case of data given by pattern structures.

Acknowledgments
The work of Sergei O. Kuznetsov on the paper was carried out at St. Petersburg Department of Steklov Mathematical Institute of Russian Academy of Science and supported by the Russian Science Foundation grant no. 17-11-01276

References
1. Experiments source code, https://github.com/EgorDudyrev/FCApy/tree/main/notebooks/DecisionLattice_eval 2. Sci-kit learn description of decision trees, https://scikit-learn.org/stable/modules/tree.html 3. Sci-kit learn description of random forest, https://scikit-learn.org/stable/modules/ensemble.html#parameter 4. Belohlavek, R., De Baets, B., Outrata, J., Vychodil, V.: Inducing decision trees
via concept lattices. (01 2007) 5. Breiman, L., Friedman, J., Stone, C., Olshen, R.: Classification and Regression
Trees. Taylor & Francis (1984) 6. Breiman, L.: Random forests. Machine Learning (10 2001) 7. Buzmakov, A., Kuznetsov, S., Napoli, A.: Sofia: How to make fca polynomial? In:
FCA4AI@IJCAI (2015) 8. Buzmakov, A., Egho, E., Jay, N., Kuznetsov, S., Napoli, A., Ra¨issi, C.: Fca and
pattern structures for mining care trajectories. CEUR Workshop Proceedings 1058 (08 2013) 9. Drucker, H., Cortes, C.: Boosting decision trees. vol. 8, pp. 479­485 (01 1995) 10. Ganter, B., Wille, R.: Formal Concept Analysis: Mathematical Foundations. Springer Berlin Heidelberg (1999) 11. Kaytoue, M., Kuznetsov, S., Napoli, A., Duplessis, S.: Mining gene expression data with pattern structures in formal concept analysis. Inf. Sci. 181, 1989­2001 (05 2011) 12. Krause, T., Lumpe, L., Schmidt, S.: A link between pattern structures and random forests. In: CLA (2020) 13. Kuznetsov, S.: Pattern structures for analyzing complex data. vol. 5908, pp. 33­44 (12 2009) 14. Kuznetsov, S., Makhalova, T.: On interestingness measures of formal concepts. Information Sciences 442 (11 2016) 15. Kuznetsov, S., Obiedkov, S.: Comparing performance of algorithms for generating concept lattices. J. Exp. Theor. Artif. Intell. 14, 189­216 (04 2002) 16. Kuznetsov, S.O.: Machine learning and formal concept analysis. In: Eklund, P. (ed.) Concept Lattices. pp. 287­312. Springer Berlin Heidelberg, Berlin, Heidelberg (2004) 17. Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A.V., Gulin, A.: Catboost: unbiased boosting with categorical features (2019) 18. Wille, R.: In: Formal Concept Analysis. pp. 314­339. Springer Berlin Heidelberg, Berlin, Heidelberg (2009)

