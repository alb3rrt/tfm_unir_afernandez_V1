arXiv:2106.01092v1 [cs.LG] 2 Jun 2021

Statistical optimality conditions for compressive ensembles
Henry W.J. Reeve and Ata Kab´an
June 3, 2021
Abstract
We present a framework for the theoretical analysis of ensembles of low-complexity empirical risk minimisers trained on independent random compressions of high-dimensional data. First we introduce a general distribution-dependent upper-bound on the excess risk, framed in terms of a natural notion of compressibility. This bound is independent of the dimension of the original data representation, and explains the in-built regularisation effect of the compressive approach. We then instantiate this general bound to classification and regression tasks, considering Johnson-Lindenstrauss mappings as the compression scheme. For each of these tasks, our strategy is to develop a tight upper bound on the compressibility function, and by doing so we discover distributional conditions of geometric nature under which the compressive algorithm attains minimax-optimal rates up to at most poly-logarithmic factors. In the case of compressive classification, this is achieved with a mild geometric margin condition along with a flexible moment condition that is significantly more general than the assumption of bounded domain. In the case of regression with strongly convex smooth loss functions we find that compressive regression is capable of exploiting spectral decay with near-optimal guarantees. In addition, a key ingredient for our central upper bound is a high probability uniform upper bound on the integrated deviation of dependent empirical processes, which may be of independent interest.
1 Introduction
Compressive learning aims to make use of inexpensive dimensionality reduction or sketching methods to overcome the curse of dimensionality in statistical learning. The term was coined by Calderbank et al. (2009) in analogy with compressive sensing (CS) (Donoho, 2006; Cand`es and Tao, 2006), which established sparsity conditions under which high dimensional signals are recoverable from their low dimensional linear random projection. The spectacular advances of CS provide data acquisition devices that directly collect random projections of the data without storing the original (Duarte et al., 2008), and most recently, dedicated photonic computing hardware became available that can perform random projection in a massively parallel fashion (Saade et al., 2016; Gupta et al., 2019). Such technologies open new doors for dealing with massive high dimensional data sets, and inspire new research in areas as diverse as numerical analysis (Halko et al., 2011), statistical methodology (Heinze et al., 2016; Cannings and Samworth, 2017; Tian and Feng, 2021), pattern recognition (Reboredo et al., 2016), clustering (Boutsidis et al., 2015; Biau et al., 2008; Meintrup et al., 2019), optimisation (Pilanci and Wainwright, 2015; Pilanci and Wainwright, 2016, 2017; Derezinski et al., 2020), search based software engineering (Nair et al., 2016), imaging (Lustig et al., 2007; Ye, 2019; Palmer et al., 2015; Bentley et al., 2019), medical research (Peressutti et al., 2015), neuroscience
1

(Arriaga et al., 2015), and computer vision (Jiao et al., 2019). The interested reader may also refer to recent surveys (Gibson et al., 2020), (Cannings, 2020), and references therein.
Whilst the theory of compressive sensing and random projection based dimensionality reduction is well understood, the use of these methods in machine learning raises important questions about a theoretical understanding of the risk of compressive learning.
Firstly, the goal in statistical learning is very different from both compressive sensing and dimensionality reduction, as we do not aim to recover, or even to approximate the seen data, instead we aim to produce accurate predictions on unseen data. This motivates the search for sufficient conditions for controlling the excess risk of compressive ensembles as a function of natural geometric characteristics of the problem.
The setting we consider is analogous to that of compressive sensing in that the data features are only available in compressive form ­ that is, the data features undergo compression before being fed to a learning algorithm. We do not impose any other regularisation to make the high dimensional problem learnable from a finite sample, and are interested in conditions on the unknown data distribution under which this compression alone makes the resulting ensemble of empirical risk minimisers nearly minimax optimal.
Previous work in this setting provided upper bounds on the error for several compressive learning machines ­ most often under (a combination of) existing assumptions from statistical learning theory (e.g. large margin, norm constraints, generative model assumptions) or from compressed sensing (e.g. sparse representation, low complexity feature space), or seek to interpret bounds in these terms ­ for compressive classification (Arriaga and Vempala, 1999; Balcan et al., 2006; Calderbank et al., 2009; Durrant and Kab´an, 2010; Reboredo et al., 2013; Renna et al., 2016), compressive regression (Maillard and Munos, 2012; Fard et al., 2012; Kab´an, 2014; Gian-Andrea Thanei, 2017; Slawski, 2018), and other learning tasks. The work of Chen and Li (2014) explicitly studied the rate of convergence of the excess risk for compressive regularised kernel-based learning in a reproducing Hilbert space with the least square and hinge losses, obtaining upper bounds with a rate of order n-1/4. However, the statistical optimality of these results, e.g. in the minimax sense, has not been established. Reeve and Brown (2017) obtained the minimax-optimal rates for the compressive knearest neighbour algorithm in cost-sensitive classification where the data support was assumed to have a manifold structure. The assumptions and analysis are very different from those of the present work. Optimality results have also been obtained in other problem settings, where the compression is used purely to speed up a statistically optimal predictor in a way that preserves its optimality ­ for instance Yang et al. (2017) give matching upper and lower bounds for compressive kernel ridge regression with compression applied to the kernel matrix.
Secondly, ensembles of compressive learners trained on independent random compressions of the high dimensional data have been found to increase performance in practice, as computations can be run in parallel, and random variations are reduced. However, a bottleneck for the theoretical understanding of such ensembles is that, unlike traditional ensembles, which combine predictors from the same function class, here each predictor added to the ensemble corresponds to a new random projection of the data. Previous work in machine learning (Durrant and Kab´an, 2015) analysed in detail a special case of compressive Fisher Linear Discriminant ensemble, and found a desirable implicit regularisation effect, which prevents overfitting. However, the analytic methods used there are specific to a particular generative model and it is not clear whether a similar effect occurs more generally. Subsequent work in statistics considered a more general approach in terms of the base learners employed (Cannings and Samworth, 2017; Slawski, 2018), and provided upper bounds on the excess risk of random projection ensembles in expectation w.r.t. the training set.
2

Bounds in expectation have also been given very recently for axis-aligned random subspace ensembles (Tian and Feng, 2021). However, we are interested in high probability bounds to reveal more information about the worst case behaviour for the excess risk, subject to a failure probability. In another line of research, recent work by Lopes (2020) determined the asymptotic speed of convergence as the number of predictors in the ensemble grows. While this is informative for very large ensembles, we are interested in non-asymptotic guarantees for ensembles of any given finite size.
1.1 Overview of contributions
In this paper we introduce a general framework for the theoretical study of compressive learning. This framework facilitates the discovery of new conditions, specific to the learning task, which allow favourable convergence rates for compressive learning. The algorithmic approach we consider throughout this study consists of ensembles of any number of empirical predictors made of compressive empirical risk minimisers (ERM) that are trained in parallel on independent randomised compressions of data sets of arbitrarily many dimensions, with their predictions combined by an averaging-type operation. This simple procedure is presented in Algorithm 1.
Below we summarise our main results.
· We introduce the concept of a compressibility function, which quantifies the average excess loss incurred by working with a low-complexity function of compressed features and plays a key role in guiding the analysis of compressive learning.
· We give a general distribution-dependent high probability upper-bound on the excess risk of ERM ensembles of arbitrary size, composed of low-complexity predictors (Theorem 2). Our bound contains three terms: (1) A statistical error that decays with the sample size at a rate depending upon the Bernstein-Tsybakov noise exponent, (2) A term which converges to zero as the size of our ensemble grows and (3) The compressibility function. This reveals an implicit regularisation effect that is exhibited by a wide variety of randomised heterogeneous ensembles.
The general form of our upper bound and compressibility function allow us to study specific learning problems in a unified framework. Specifically, we are interested in the following question: Under what natural conditions can the compressive ERM ensemble attain minimax-optimal rates of convergence with respect to the sample size? To approach this question we restrict attention to Johnson-Lindenstrauss mappings as the compression scheme, which makes it feasible to control the compressibility function, and we demonstrate our approach by instantiating our general upper bound in two fundamental classic learning tasks.
· In the case of compressive classification we find that very mild conditions of geometric nature suffice for a high probability upper bound on the excess error (Theorem 3). These conditions include a flexible geometric margin condition that differs significantly from previously considered margin assumptions, along with a flexible moment condition that allows for distributions supported on an unbounded domain. For a wide range of parameters we find better rates than the previous upper bounds implied by (Chen and Li, 2014) (albeit in a different setting) even in the absence of favourable Tsybakov-margin. In fact, in Theorem 5 we show that the upper bound for compressive ensembles given by Theorem 3 is minimax optimal up to logarithmic factors.
3

· In the case of regression with strongly convex loss functions we find that Johnson-Lindenstrauss compressors are capable of exploiting spectral decay with near-minimax optimal guarantees (Theorems 6-8). Our high probability guarantee highlights the role of spectral decay in attaining near-optimality. These results complement recent findings by Slawski (2018) which give an expectation bound for compressive OLS with fixed design.
· Our general upper bound builds on a high probability uniform bound on the integrated deviation for dependent empirical processes that allows to exploit local Rademacher complexities (Theorem 9), which may be of independent interest.
Our approach provides a framework that places a computationally attractive and empirically successful algorithmic scheme on solid theoretical foundations. Our framework can be extended and used to unearth novel conditions which help gaining more understanding in specific compressive learning problems.

2 Problem setting
We shall consider supervised learning. Suppose we have complete separable metric spaces (X , dX ), (Y, dY ), (V, dV ), where X is a feature space, Y is a target space, and V is a prediction space, which may or may not equal Y. In typical applications (X , dX ), (Y, dY ), (V, dV ) will be subsets of Euclidean space with their respective Euclidean norms. However, the additional level of generality in this section comes at no expense. We shall assume that there is an unknown Borel probability distribution P over random variables (X, Y ), where X takes values in X , and Y takes values in Y. The quality of a prediction for a given target is quantified through a loss function L : V ×Y  [0, ).
Given a pair of measurable spaces Z and W, we let M(Z, W) denote the set of measurable functions g : Z  W. For brevity we let M(Z) denote M(Z, R) and for each b > 0 let Mb(Z) := M(Z, [-b, b]), where both R and [-b, b] are endowed with the Borel sigma algebra. The goal of the learner is to obtain   M(X , V) such that the corresponding risk

RL,P () := E(X,Y )P [L((X), Y )] = L((x), y)dP (x, y)

is as low as possible. Given a Borel probability distribution P we let   P  M(X , V) denote the Bayes optimal predictor, satisfying

  argminM(X ,V) {RL,P ()} .

(1)

For simplicity, we shall assume throughout that V is a compact metric space and L is continuous in its first argument, which ensures that a Bayes optimal predictor  exists (Proposition 6), although it need not be unique. We view the Bayes optimal predictor   M(X , V) as the mapping we
would select if we knew the distribution P .
Of course, in practice the learner does not have direct access to the distribution P . Instead,
the learner selects   M(X , V) based upon a sample D := {(Xj, Yj)}j[n], where (Xj, Yj)  P are independent copies of (X, Y ). Whilst the true risk RL,P () cannot be directly observed, the learner does have access to the empirical risk R^L,D(, D) := n-1 · j[n] L((Xj ), Yj ). Given   M(X , V), we write

EL,P () := RL,P () - RL,P () = RL,P () - inf {RL,P ()}

(2)

M(X ,V)

4

for the excess risk. For a positive integer m  N we shall use the notation [m] := {1, · · · , m}. We also define the notation log+(x) := max{log(x), 1}, where log is the natural logarithm.

2.1 Learning from compressive data sketches
In this work we consider a high-dimensional setting where the dimensionality of the feature space X is arbitrarily large, and working directly with the features themselves becomes computationally and statistically prohibitive. Instead, we work with randomised compressions of the feature representation of the data, for instance via random projections.
Given k  N we let Ak  M(X , Rk) be a set of random feature mappings. Given a data sample D := {(Xj, Yj)}j[n] and a mapping A  Ak, we define the corresponding compressed sample A(D) := {(A(Xj), Yj)}j[n]. Let k be a probability distribution on the set of random feature mappings Ak. Given m  N, we take m random projections A1, · · · , Am that are independent and identically distributed with each Ai  k, and consider A1(D), · · · , Am(D), that is m random projections of the data. We let Fk  M(Rk, V) be a set of functions on the transformed feature space Rk. We shall view Fk as being of relatively small capacity in a sense that will be made precise in Section 3.1 (Assumption 4). Examples will include sets of linear classifiers on Rk. For each i  [m], we shall choose f^i in Fk based on the compressed sample Ai(D) by minimising R^L,D (fi, Ai(D)) over f  Fk. At test time, the set of predictions {f^i(Ai(x))}i[m] is combined into a voting ensemble through a function Ens:

^(x)  Ens f^i(Ai(x))

.

i[m]

The appropriate combination rule Ens depends on the learning task. For example, in the case of classification with the zero-one loss we advocate taking the modal average, and in the case of regression with a squared loss we advocate taking the mean average. The reasons for this will become clear shortly in Section 3.1. The pseudo-code for the procedure that we study in the remainder of this paper is described in Algorithm 1.

Algorithm 1: Compressive ensemble empirical risk minimisers Input : A data sample D, a number of projections m, a distribution over random compressors k, a loss function L and a low-dimensional function class Fk.
for i  [m] do
Sample Ai  k;
Compute Ai(D) := {(Ai(Xj), Yj )}j[n]; Choose f^i  Fk to minimise R^L,D (f, Ai(D)); end

Combine ^(x) := Ens f^i(Ai(x))

;

i[m]

Output : Compressive ensemble predictor ^.

Minimising the empirical risk can often be a challenging optimisation problem (Feldman et al., 2012), and, in general, an exact minimiser need not even exist. In this work, we focus on the statistical challenge of learning with compressive ensembles and assume that the optimisation error is dominated by the statistical error (see Section 3.2 for details).

5

We also remark that strictly speaking ^  M((X × Y)n × Am k × X , V), since it implicitly depends upon both the random sample D, which takes values in (X × Y)n, and the sequence of random projections (Ai)i[m], which takes values in Am k . However, we typically view ^ as a random element of M(X , V), suppressing the dependence upon D and (Ai)i[m] for notational convenience.

3 General framework and main upper bound
This section presents the main assumptions we employ throughout of this work, along with some illustrative examples. We also introduce a notion of compressibility, which will allow us to state our main result in general terms in the next section, before studying specific instances.

3.1 Initial assumptions

We begin with two standard assumptions on the loss function.

Assumption 1 (Bounded loss function). We shall assume that the loss function L : V ×Y  [0, ) is bounded by some constant b  1, so for all v  V and y  Y we have L(v, y)  [0, b].

Assumption 2 (Lipschitz loss function). We shall assume that V is a compact metric space with
metric dV and there exists Lip  1 such that |L(v0, y) - L(v1, y)|  Lip·dV (v0, v1) for all v0, v1  V and y  Y.

Typically we have V  R, in which case we can take dV to be the standard metric defined by dV (v0, v1) = |v0 - v1|.
The following assumption will connect the excess risk of the ensemble with the average excess risk of its members. The function Ens is a measurable map of the form Ens : Vm  V, so Ens  M (Vm, V). We can extend Ens  M (Vm, V) to a map Ens : M(X , V)m  M(X , V) in a point-
wise fashion by defining Ens {i}i[m] (x) = Ens {i(x)}i[m] , for any {i}i[m]  M(X , V)m
and x  X .

Assumption 3 (Quasi-convexity). We shall say that L satisfies the quasi-convexity assumption with constant qc  1 and averaging function Ens if

EL,P Ens {i}i[m]



qc m

EL,P (i, P )

i[m]

for all Borel probability distributions P on X × Y and all {i}i[m]  M(X , V)m.

The nomenclature comes from the following consequence of Jensen's inequality (Lemma 1). However the benefit of the quasi-convexity assumption is that it applies also to the 0-1 loss (Lemma 24).

Lemma 1. Suppose that V is a vector space and L is convex in its first argument. Then Assumption

3

holds

with

qc

=

1

and

Ens((vi )i[m] )

=

1 m

i[m] vi for (vi)i[m]  V m.

Next, we make precise the idea that the set of functions on the low dimensional space Fk  M Rk, V is of low capacity, through the following assumption. First, recall the notion of covering numbers. Given a set T with metric dT and  > 0, the -covering number N (T, dT, ) is the

6

cardinality of the smallest subset T~  T such that for every t  T there exists some t~  T~ with

dT(t~, t)  .

Given any n  N

and

any

u1:n

= {uj}j[n]

 (Xk)n

the

empirical

2

metric

dV
u1:n

on

Fk is defined for f0, f1  Fk by

dV
u1:n

(f0

,

f1)

:=

1 n

dV (f0(uj ), f1(uj))2.

j[n]

Assumption 4 (Covering number condition). We shall say that Fk  M Rk, V satisfies the

logarithmic covering number assumption n, k  N with n > k, u1:n  Rk n,  > 0,

with

constant

Ccn



1

and

bound





1

if

for

every

N

Fk

,

dV
u1:n

,





n

Ccn k
.



Finally, we shall make use of the Bernstein-Tsybakov condition, which has been key to obtaining fast rates in the statistical learning literature Tsybakov (2004b).

Assumption 5 (Bernstein-Tsybakov condition). We shall say that L and P satisfy the BernsteinTsybakov condition with exponent   [0, 1] and constant CB  1 if

E(X,Y )P {L ((X), Y ) - L (P(X), Y )}2  CB · EL,P () ,

for all   M (X , V).
Here P  M (X , V) denotes a Bayes optimal predictor satisfying (1). Note that the BernsteinTsybakov condition is not necessarily restrictive, since it always holds with  = 0 and CB = b2. However, faster rates than n-1/2 are obtainable whenever Assumption 5 holds with  > 0.
Example 1 (Binary classification with the zero-one loss). Take V = Y = {-1, +1} and the zero-one loss function L0,1(v, y) = 1 {v = y} for (v, y)  V×Y, with the class Fk0,1 = u  sgn (w · u - t) : w  Rk, t  R  M Rk, {-1, +1} .
Example 2 (Bounded regression with the squared loss). Take V = Y  [-, +] for some  > 0, and consider the squared loss function Lsqr(v, y) = (v - y)2 for (v, y)  V × Y, with the class Fkbl = u  max (min (w · u - t, ) , -) : w  Rk, t  R  M Rk .
Example 3 (Conditional probability estimation with the Kullback-Leibler loss). Take Y = {0, 1}, define a mapping  : R  [0, 1] by (a) = ea/(1 + ea), and consider the Kullback Leibler divergence kl(v, y) = y log(y/v) + (1 - y) log((1 - y)/(1 - v)) for (v, y)  [0, 1] × Y. Take V = [-, ] for some  > 0, and consider the Kullback Leibler loss function Lkl := kl  , acting on the class of functions Fkbl.
In Appendix A we verify that Examples 1-3 satisfy Assumptions 1-5. We remark that Example 1 satisfies the quasi-convexity condition (Assumption 3) with the modal average (i.e. majority voting), Examples 2 and 3 satisfy the same condition with the arithmetic average of {i}i[m]  Fkbl, and in the case of Example 3 this corresponds to the product of experts combination (Hinton, 2002) of the nonlinear probabilistic outputs {(i)}i[m].

7

3.2 Compressibility
Our main upper bound on the excess risk of Algorithm 1 in the next section will be expressed in terms of a compressibility function P : N  [0, ), defined for each k  N as the expected approximation error of the compressive class Fk:

P (k) := EAk

inf {EL,P (f  A)}
f Fk

.

The compressibility function P quantifies the average amount of loss incurred by predicting with

the best member of the class Fk with compressed inputs A(x), rather than the Bayes-optimal

predictor P . In order to focus on the statistical aspects of the problem we shall assume that the

optimisation

error

O

:=

1 m

m i=1(R^L,D(f^i, Ai(D)) - inffFk {R^L,D(f, Ai(D))}) is dominated by the

compressibility term (k). The functional form of the compressibility function is specific to the

randomisation scheme employed, and the learning task. Examples will be given in Sections 4.1 and

4.2.

3.3 Main upper bound

With our framework in place, we give a general upper bound on the worst case excess risk of the empirical predictor returned by Algorithm 1.

Theorem 2. Suppose that Assumptions 1, 2, 3, 4 and 5 hold with parameters b, , Lip, qc, Ccn,

CB > 1,   [0, 1]. Take n, k, m  N and let ^n,k,m denote the compressive ensemble predictor from

Algorithm 1 with a sample size n, a projection dimension k, an ensemble size m. There exists a

constant (Ai)i[m]

C2km1,,

depending only upon b, , Lip, qc, Ccn, CB, the following holds with probability at least 1 -

, ,

such

that,

given

D



P n

and

EL,P

^n,k,m

 C2

P (k) +

k · log+(n) + log+(1/) n

1

2-

+

log+(1/) m

.

Theorem 2 provides generalisation guarantees in arbitrary learning problems which are independent of the dimensionality of the original feature space. The bound consists of three terms. The first term P (k) corresponds to the amount of accuracy lost by working on a k-dimensional compression of the original problem. The second term corresponds to the statistical difficulty of learning in the k-dimensional setting. The first two terms are in tension: By decreasing k we can reduce the statistical difficulty of our k-dimensional problem. However, this reduction in statistical error comes at the expense of an increase in the compressibility term P (k). The reduction in statistical error for low k shows that compressive ERM ensembles in Algorithm 1 perform in-built regularisation effect to guard against overfitting. However, for well-behaved distributions with compressible structure the term P (k) can be made small with modest values of k yielding efficient dimension independent rates. We shall discuss examples of compressible structure for specific learning problems in Sections 4.1 and 4.2.
The third term in the bound corresponds to the error contribution from working with a finite ensemble size m. Note that the bound is non-asymptotic and holds for any finite ensemble size m, and one may set m to be of order n. The excess risk guarantee improves as the ensemble size m grows, at a speed that matches the asymptotically optimal rate m-1 determined in recent work of Lopes (2020).

8

The proof of Theorem 2 is given in Section 5. The main bottleneck is to prevent the excess risk probabilities of individual ensemble members from accumulating with the ensemble size. To achieve this, the starting point is a uniform upper bound on the integrated deviations of dependent empirical processes, which might find applications elsewhere.
So far we have left unspecified the randomised dimensionality compression scheme to be used. Indeed the general result presented in this section could potentially be applied to any independently randomised ensemble, including random coordinate projections (Ho, 1998; Tian and Feng, 2021), and various sketching methods (Cormode, 2017; Cannings, 2020). For the bound to be useful, we need to be able to control the compressibility function P (k). In the next section we instantiate the general bound presented in this section to classification and regression problems, considering low-distortion compressions, i.e. random projections. Such low-distortion compressions permit bounding the compressibility term to yield generalisation guarantees for ensembles of any size, even for a singleton. In contrast, coordinate projections are known to require a sufficiently large ensemble.
4 Near-minimax optimality conditions for compressive classification and regression with convex losses
In this section we instantiate the general bound of the previous section (Theorem 2) in two fundamental learning problems. Throughout this section, X will be a separable Hilbert space of arbitrary dimension. We shall consider low-distortion Johnson-Lindenstrauss mappings which make it feasible to control the compressibility in terms of distributional conditions of geometric nature.
Assumption 6 (Johnson-Lindenstrauss property). We shall say that (k)kN satisfies the Johnson Lindenstrauss property with constant CJL  1 if given any set {x1, · · · , xq}  X of cardinality q, any   (0, 1),   (0, 1) and k  CJL log(q/) · -2 we have
k A  Ak : j, j  [q] (1 - ) xj - xj 2  A(xj) - A(xj ) 2  (1 + ) xj - xj 2  1 - .
There are many examples of Johnson-Lindenstrauss (JL) mappings, and the results of this section hold for any of these. In particular, all subgaussian linear maps satisfy JL (Matousek, 2008) - a class that includes the Gaussian random projection (Dasgupta and Gupta, 2003), as well as computation-friendly bit-flip based transforms (Achlioptas, 2003). For practical implementation, typically X = Rd is taken, although a separable Hilbert space is sufficient in theory (Biau et al., 2008, Theorem 3.1.).
Moreover, Assumption 6 is also satisfied by certain structured random matrices that enable efficient computation of the compressive mapping, most notably the Fast Johnson-Lindenstrauss transform (Ailon and Chazelle, 2006), and random matrices that exploit sparse matrix multiplications (Kane and Nelson, 2014). The quest for developing efficient Johnson-Lindenstrauss transforms is currently an active research area; some constructions require a slightly larger target dimension in exchange of greater savings in computation time (Freksen and Larsen, 2020). However, the target dimension of order log(q)-2 is known to be optimal in that any transform that satisfies Assumption 6 uniformly over any set of q points must have a target dimension of this order (Larsen and Nelson, 2017).
9

4.1 Compressive classification with a geometric margin condition
Consider the classification setting discussed in Example 1. Take V = Y = {-1, +1} and the zeroone loss function L0,1(v, y) = 1 {v = y}. Given a Borel probability distribution P on X × Y we let  : X  [0, 1] denote the regression function defined by (x) := E(X,Y )P [Y = 1|X] and let PX denote the marginal distribution over X where (X, Y )  P. We let · denote the Euclidean norm on a Hilbert space X . The low complexity function class of interest in this section is Fk = {z  sgn(wz - t) : w  Rk, t  R}  M1(Rk).
We introduce three distributional assumptions. These capture benign characteristics of the problem that allow a tight bound on the compressibility (k). Our first assumption is a geometric margin condition.
Assumption 7 (Geometric margin condition). We shall say that a distribution P on X × {0, 1} satisfies the geometric margin condition with exponent  > 0, constant CG  1 and approximation error   [0, 1], if X is a separable Hilbert space and there exists (w, t)  X × R with w = 1 with the following properties:
(i) The linear classifier  : X  {-1, +1} defined by (x) := sgn(wx - t) for x  X , has excess error EL,P ()  .
(ii) Letting S := {x  X : |wx - t|  } we have S |2(x) - 1| dPX (x)  CG ·  for each  > 0.
The reason we call Assumption 7 a geometric margin condition is that it involves the set of points S near the -decision boundary S0 := {x  X : wx = t}. However, observe that Assumption 7 differs in an essential way from previously considered geometric margin conditions in compressive classification, e.g. by Balcan et al. (2006); Arriaga and Vempala (1999) and others, which were inherited directly from traditional data-space classification theory. In particular, assumption 7 does not require the classes to be separable. Assumption 7 requires that there is a linear classifier  with low excess error , for which there is not too much mass close to its decision boundary S0. Moreover, this mass is weighted by |2(·) - 1| so that very little penalty is incurred for difficult to classify points near the decision boundary. Our next assumption controls the tails of our marginal distribution.
Assumption 8 (Moment condition). We say that P satisfies the moment condition with exponent   (0, ) and constant CM  1 if

|2(x) - 1| dPX (x)  CM · s-
x >s

for all s > 0.

Assumption 8 is a significant relaxation of the assumption of bounded support often utilised
within the classification literature. We refer to Assumption 8 as a moment condition since it holds whenever 2EXPX [ X ]  CM, by Markov's inequality. Finally we shall make use of the classification form of the Tsybakov noise condition (Mammen and Tsybakov, 1999). This will ensure
the Bernstein-Tsybakov condition (Assumption 5) is satisfied.

Assumption 9 (Noise condition). We say that P satisfies the Tsybakov noise condition with expo-

nent   [0, 1)

and

constant

CT

1

if

PX (x  X

:

|2(x)

-

1|



)



CT

·



 1-

for

all

  (0, 1).

10

Tysbakov et al. have shown that Assumption 9 implies Assumption 5 with a suitable choice of CB depending upon  and CT (Tsybakov, 2004b, Proposition 1). We now introduce a class of distributions with compressible structure.

Definition 1 (Compressive classification measure class). Given parameters  = ((, CG), (, CM), (, CT)) where CT, CG, CM  1,   [0, 1), ,   (0, ) we let P0,1(, ) denote the set of all Borel probability P on X × Y which satisfy Assumption 7 with parameters (, CG, ), Assumption 8 with parameters (, CM), and Assumption 9 with parameters (, CT).

Theorem 3 (Compressive classification upper bound). Let L0,1 be the zero-one loss function and
take distributional parameters  = ((, CG), (, CM), (, CT)) where CB, CG, CM  1,   [0, 1), ,   (0, ). Suppose further that our random projection measures (k)kN satisfy the Johnson Lindenstrauss property with constant CJL  1. There exists a constant C3  1, depending only on  and CJL, such that for any n, k, m  N,   [0, 1],   (0, 1), and P  P0,1(, ), then the following holds with probability at least 1 -  over D  P n and (Ai)i[m]  km,

EL0,1,P ^n,k,m  C3

log+(k)

 2( +)
+

k · log+(n) + log+(1/)

1
2- + log(1/) + 

.

k

n

m

2( +)
Moreover, if k =  n/log+(n)  2(+)+(2-) then with probability at least 1 -  we have,

EL0,1,P ^n,k,m  C3



1

log+(n) n

2( +)+ (2-)
+

log+(1/) n

2-

+

log(1/) m

+



.

Theorem 3 highlights the dependence of the worst case excess risk on the geometric exponents  and  in combination with the statistical Bernstein-Tsybakov exponent . Theorem 5 below implies that the rate is minimax optimal up to logaritimic factors.
A direct comparison with the rate previously obtained for compressive learning by Chen and Li (2014) would be difficult to make, as their assumptions are very different in flavour from ours, and they consider regularised models in the reduced space whereas our result highlights the regularisation effect of the compressive approach itself. However, whenever  >  + , the rate in Theorem 3 is always (i.e. even if  = 0) no worse than the rate n-1/4 found previously in the analysis of Chen and Li (2014), which did not exploit geometric properties of the distribution.
Theorem 3 is a consequence of Theorem 2 combined with the following proposition.

Proposition 1 (Compressiblity of linear classification). Let L0,1 be the zero-one loss function and take distributional parameters  = ((, CG), (, CM), (, CT)) where CB, CG, CM  1,   [0, 1], ,   (0, ). Suppose further that our random projection measures (k)kN satisfy the Johnson Lindenstrauss property with constant CJL  1. There exists a constant C~3  1, depending only on , CJL, such that for all k  N, and all P  P0,1(, ),



P(k)  C~3

log+(k) k

2( +)
+ .

(3)

To prove Proposition 1 we begin with the following consequence of Assumption 6.

Lemma 4. Suppose that Assumption 6 holds. Take k  N, w  X with w = 1, t  R,   (0, 1] and s  [1, ). Given x  X with |wx - t|   and x 2  s we have,

k A  Ak : sgn(A(w)A(x) - t) = sgn(wx - t)  3e-2k/(4CJLs2).

(4)

11

Proof. Take  := /(2s) and define an event Ex,k  Ak by

Ex,k :=

A  Ak : (1 - ) · w - z 2  A(w) - A(z)  (1 + ) · w - z 2 .

z{±x/ x }

Take A  Ex,k and suppose that wx - t > 0. Taking x~ = x/ x 2,

A(w)A(x) =

x 4

A(w) + A (x~)

2 2

-

A(w) - A(x~)

2 2



x 4

(1 - )

w + x~

2 2

-

(1

+

)

w - x~

2 2

 wx -  · x > t +  -  · s  t,

so sgn(A(w)A(x) - t) = sgn(wx - t). By symmetry we also have sgn(A(w)A(x) - t) = sgn(wx - t) when A  Ex,k and wx - t < 0. Moreover, by Assumption 6 we have k(Ex,k)  1 - 3e-2k/CJL so result follows.

Next we deduce Proposition 1 from Lemma 4.
Proof of Proposition 1. Take (w, t)  X × R with w = 1 with the properties guaranteed by Assumption 7 and let (x) := sgn(wx - t) be the associated linear classifier. Take   (0, 1] and s  [1, ) and let U := x  Bs(0)\S : (x) = (x) . By Assumptions 7 and 8, for any   M(X , V), we have

EL0,1,P ()  |2(x) - 1| · 1 {(x) = (x)} dPX (x) +

|2(x) - 1|PX (x)

U

X \U

 PX ({x  U : (x) = (x)}) + CG ·  + CM · s- + .

Next for each A  Ak let fA  Fk denote the map fA(z) = sgn(A(w)z - t). Hence, by Lemma 4 for each x  U we have k({A  Ak : fA  A(x) = (x)})  3e-2k/(4CJLs2). Thus, by Fubini's theorem we have

(k) 

EL0,1,P (fA  A) dk(A)

Ak

 PX ({x  U : fA  A(x) = (x)})dk(A) + CG ·  + CM · s- + 
Ak
 3e-2k/(4CJLs2) + CG ·  + CM · s- + 

To complete the


proof we take  = 1  (4CJL log(3k)/k) 2(+)

and s = -/

to yield the

required

bound.

Our next result (Theorem 5) is a minimax lower bound for the class of distributions introduced in Definition 1. In conjunction with Theorem 3, this result shows that compressive ensembles achieve the minimax optimal rate, up to logarithmic factors.

12

Theorem 5 (Minimax lower bound). Let L0,1 be the zero-one loss function and take  = ((, CT), (CG, ), (CM, )) where CB, CM  1, CG  2/2,   [0, 1), ,   (0, ). Given n  N, let X be a Hilbert space of dimension at least n, and take   [0, 1]. There exists a constant c5 > 0 depending only upon , such that for any empirical classifier ^ : (X ×Y)n ×X  Y, there exists a distribution P  P0,1(, )
with

EDP [E n L0,1,P (^)]  c5 ·

n +  -

2(+

 )+ (2-)

.

(5)

The proof of Theorem 5 is given in Section 6. The proof involves the construction of a family of distributions within the class which are, simultaneously, sufficiently close that they are hard to tell apart based on a sample of size n, and sufficiently far apart that failing to do so must incur a large error.

4.2 Compressive regression with a fast decaying covariance spectrum

We now turn our attention to strongly convex losses such as the squared loss and logistic loss
discussed earlier in the Examples 2-3. Throughout the section we take X to be a separable Hilbert space, Y = R, V = [-, ], and consider the bounded linear function class Fk  Fkbl =
u  max (min (w · u - t, ) , -) : w  Rk, t  R  M Rk . A function  : [a, b]  R is said to be H-strongly convex on [a, b] if

((1

-

t)v0

+

tv1)



(1

-

t)(v0)

+

t(v1)

-

H 2

·

t(1

-

t)

·

(v0

-

v1)2,

(6)

for all v0, v1  [a, b] and t  [0, 1]. We say that  : [a, b]  R is H-strongly mid-point convex on [a, b] if (6) holds for all v0, v1  [a, b] and t = 1/2. Note that any twice differentiable function  with strictly positive second derivative   H is H-strongly convex (Lemma 29), and hence
H-strongly mid-point convex.

Assumption 10 (Strongly Convex loss). We shall say that the loss function L : [-, ]×Y  [0, b] is strongly mid-point convex with constant H > 0 if for all y  Y, the function v  L(v, y) is Hstrongly mid-point convex on [-, ].

Lemma 28 shows that any loss function satisfying Assumption 10 also satisfies the BernsteinTsybakov condition with exponent  = 1, and constant CB = 42Lip/H. We shall introduce two further distributional assumptions. Firstly, Assumption 11 concerns the existence of a linear predic-
tor of bounded norm with low excess error. Secondly, Assumption 12 requires that the eigenvalues
of the underlying covariance matrix decay at an exponential rate.

Assumption 11 (Linear approximation condition). We shall say that P on X × Y satisfies the linear approximation condition with loss L : [-, ] × Y  [0, b] and constants Wmax  1,   [0, 1] if X is a Hilbert space and there exists (w, t)  X × R with and w 2  Wmax such that the predictor  : X  R defined by (x) := min{, max{-, wx + t}} has excess risk EL,P ()  .
Furthermore, we assume a fast decay on the eigen-spectrum of the covariance operator of the marginal distribution.

Assumption 12 (Spectral decay condition). We say that P satisfies the spectral decay condition with constant Csp  1, decay   (0, 1) if PX has covariance operator  with singular values r()  Csp · r for all r  N.

13

We define the following class of distributions.

Definition 2 (Compressive regression measure class). Given a bounded loss function L : [-, ] × Y  [0, b] and parameters  = (Wmax, Csp, )  [1, ) × ([1, ) × (0, 1)) we let PL(, ) denote the set of all Borel probability distributions P on X × Y, for which Assumption 11 is satisfied with loss function L and parameters (Wmax, ) and Assumption 12 is satisfied with parameters (Csp, ).

Theorem 6 (Compressive regression upper bound). Let L : [-, ] × Y  [0, b] be a loss function satisfying Assumptions 1, 2, 10 with parameters  := (, b, Lip, H)  [1, )2 × (0, ) and take distributional parameters  = (Wmax, Csp, )  [1, )2 × (0, 1) and   [0, 1]. Suppose further that (k)kN satisfy the Johnson Lindenstrauss property with constant CJL  1. There exist constants C6  1, c6  (0, 1) depending only on ,  and CJL, such that for any n, k, m  N,   [0, 1] and   (0, 1), for any probability distribution P  PL(, ), with probability at least 1 - 

EL,P

^n,k,m

 C6

exp(-c6

·

k)

+

k

·

log+(n) + n

log+(1/)

+

log(1/) m

+



.

Moreover, with k = log+(n) with probability at least 1 -  we have

EL,P ^n,k,m  C6

log2+(n)

+ log+(1/) n

+

log(1/) m

+



.

(7)

Theorem 6 complements recent work by Slawski (2018), which gives an expectation bound for compressive ordinary least squares (OLS) regression in the fixed design setting and highlights a parallel with Principal Component Regression (PCR) through the role of spectral decay. Our result shows that a similar effect holds in the in random design setting, for a larger class of loss functions, and our guarantees hold with high probability rather than just in expectation w.r.t. the training sample. We should mention that guarantees that do not require a spectral decay condition are known for a form of compressive ridge-regularised kernel regression (Yang et al., 2017). However, a comparison would be difficult as the setting is very different. In particular, the goal in that work was to preserve the optimality of a ridge-regularised model when compressing the kernel for computational speedup, whereas our analysis is aimed to bring out the regularisation effect of a compressive feature representation itself, highlighting its ability to transform an arbitrary high dimensional problem that is not learnable from a small sample into a low dimensional problem that is nearly optimally learnable. Theorem 6 is a consequence of Theorem 2 combined with Lemma 28 and Proposition 2.

Proposition 2 (Compressibility of generalised linear regression). Take , Lip  [1, ), CJL  [1, ) and  = (Wmax, Csp, )  [1, )2 × (0, 1). Let L : [-, ]× Y  [0, ) be a loss function satisfying Assumption 2 with parameter Lip. Suppose that (k)kN satisfy the Johnson Lindenstrauss property with constant CJL  1. There exist constants C~6, c~6 > 0 depending only on , Lip,  and CJL such that for all k  N,   [0, 1], and all P  PL(, ),

P (k)  C~6 · exp(-c~6 · k) + .

(8)

To prove Proposition 2 we use the following consequence of (Slawski, 2018, Theorem 1). For completeness, we give a short alternative proof of Lemma 7 in Appendix E.

14

Lemma 7 (Excess risk of compressive OLS with fixed design). Given q, d, k  N, take a vector w  Rd, a d × q matrix X and let A a random k × d matrix satisfying Assumption 6 with constant CJL. There exists a constant cS  (0, 1), that depends only on CJL, such that for any r < min{q, k} the following holds with probability at least 1 - (24r + 2q)e-cSk,

inf
wRk

wAX - wX

2 2



18

w

2 2

j (XX ).
jr+1

(9)

Proof of Proposition 2. Fix q, r  N, to be specified later. For each A  Ak define A : X q  R by

A(x1:q) := sup
f Fk

(f
X



A(x)

-

(x))2dPX (x)

-

1 q

q
(f
=1



A(x)

-

 (x ))2

for x1:q = (x)[q]  X q. Define subsets Erd and Esp  X q by

Erd := x1:q  X q :
  Esp := x1:q  X q : 

A(x1:q)dk(A)  482
Ak

Ccn · k q

1
· log+2

j
jr+1

1 q

q

xx

=1





3

j () .

jr+1



22q k

We shall use the probabilistic method to show that Erd  Esp = . By Lemma 12 with r = (2)4, for each A  Ak we have

R^

x  (f  A(x) - (x))2 fFk , x1:q  232

Ccn · k q

1
· log+2

q k

.

(10)

Hence, by symmetrization (eg. (Mohri et al., 2012, Chapter 3)) for each A  Ak we have

A(x1:q)d(PX )q(x1:n)  2 R^

Xq

Xq

x  (f  A(x) - (x))2 fFk , x1:q d(PX )q(x1:n).

By applying Fubini's theorem and combining with (10) we have

Xq

A(x1:q)dk(A) d(PX )q(x1:n)  242
Ak

Ccn · k q

·

1
log+2

q k

.

Hence, by Markov's inequality we have (PX )q(Erd)  2/3. Similarly, by Markov's inequality, we also have (PX )q(Esp)  2/3. Combining these two bounds we conclude that Erd  Esp = . For the remainder of the proof we fix x1:q = (x)[q]  Erd  Esp. Let  : X  Rd be a linear map which is isometric on the d-dimensional linear subspace spanned by {x1, . . . , xq, w} and let X  Rd×q be the matrix with columns {q-1/2 · (x1), . . . , q-1/2 · (xq)}. It follows that given a random projection A : X  Rk with A  k satisfying Assumption 6 with constant CJL, the induced random projection A : Rd  Rk also satisfies Assumption 6 with constant CJL. Hence, noting

15

that z  min{, max{-, z}} is 1-Lipschitz and applying Lemma 7 we see that with probability at least 1 - (24r + 2q)e-cSk over A  k we have

inf
f Fk

1 q

q
(f
=1



A(x)

-

 (x ))2



inf
wRk,tR

1 q

q =1

(wAx + t) - (wx + t) 2

 inf 1 q wRk q

wAx - wx 2

=1

= inf 1 q wRk q =1

wA(x) - (w)(x) 2

= inf wAX - (w)X 2
wRk

 18 (w) 2

j XX

jr+1

= 18 w 2

j

jr+1

1 q

q

xx

=1

 54Wm2 ax

j () ,

jr+1

where we have used x1:q  Esp in the final inequality. Hence, by combining with x1:q  Erd we deduce that

inf
Ak f Fk

1/2
(f  A(x) - (x))2 dPX (x) dk(A)
X


Ak

inf
f Fk

1 q

q
(f  A(x) - (x))2

=1

+ A(x1:q)

1/2
dk (A)



 54Wm2 ax

j () + (2)2{24r + 2q}e-cSk + 482



jr+1

Ccn · k q

·

1
log+2

1/2

q k

.

Taking q = ecSk/2 and r := cSk/(2 log 24) we see that there exists C~6  1 and c~6  (0, 1), both depending only upon , Lip,  and CJL such that

1/2

Lip

inf

(f  A(x) - (x))2 dPX (x)

Ak f Fk

X

dk(A)  C~6 exp(-c~6k),

(11)

By the Lipschitz property of the loss function combined with Jensen's inequality we have

EL,P () 

L((x), y) - L((x), y)dP (x, y) + EL,P ()

X ×Y

 Lip ·

1/2

((x) - (x))2 dPX (x)

+ .

X

Hence, by applying the bound (11) we obtain P (k)  C~6 exp(-c~6k) + , as required.

16

Finally, we show that the upper bound in Theorem 6 is minimax-optimal up to logarithmic factors (Theorem 8) for non-trivial loss functions. More precisely, we require the following additional non-degeneracy condition.

Assumption 13 (Non-degenerate loss functions). We shall say that the loss function L : V × Y  [0, b] is -non-degenerate if there exists y0, y1  Y such that

inf
vV

{L(v,

y0)

+

L(v,

y1)}



inf {L(v,
vV

y0)}

+

inf {L(v,
vV

y1)}

+

2.

(12)

We emphasise that Assumption 13 will always hold, for an appropriately chosen  > 0, on any interesting learning problems in our setting. Indeed, if Assumption 13 is violated for some loss function L : V × Y  [0, b] where V is compact and L satisfies Assumptions 2 and 10 then any function satisfying (x)  yY arg infvV L(v, y) will be simultaneously Bayes optimal for all distributions P on X × Y. Note in particular that the squared loss Lsqr : [-, ]2  [0, (2)2] satisfies Assumption 13 with  = 2 and the Kullback-Leibler loss Lkl : [-, ]2  [0, log(1 + e)] satisfies Assumption 13 with  = log(2/(1 + e-)).

Theorem 8 (Minimax lower bound). Let L : [-, ] × Y  [0, b] be a loss function satisfying
Assumptions 1, 2 and 13 with parameter   (0, ). Let X be Hilbert space containing a non-zero element, and take  = (Wmax, Csp, )  [1, )2 × (0, 1) with Wmax  2 · -1/2 and   [0, 1]. There exists a constant c8 > 0 depending only upon  and b such that for any n  N , and any empirical predictor ^ : (X × Y)n × X  Y, there exists a distribution P  PL(, ) such that

EDP n [EL,P(^)]  c8 · n-1 +  .

Theorem 8 shows that the upper bound achieved by compressive regression in Theorem 6 is minimax optimal up to logarithmic factors. A proof of Theorem 8 is given in Section 6.

5 Proof of the general upper bound
The first stage of the proof is to establish a high probability uniform upper bound on the integrated deviations of dependent empirical processes where the integration is with respect to the distribution over random projections (Section 5.1, Theorem 9). This result may be of independent interest, and is therefore stated in a more general setting.
The second stage of the proof of Theorem 2 is based on applying Theorem 9 to the ensemble of compressive ERMs that each act on a random mapping of the input space (Section 5.2).
5.1 A concentration inequality for the integrated empirical processes
In this section we shall derive a concentration inequality for sequences of functions (g)   G where each G  M(Z, R) is a function class. Later we will apply this result to settings
in which each function class is associated with a random projection. We begin by recalling the concept of empirical Rademacher complexity. Let's suppose G 
M(Z, R) is a function class. We shall say that G is separable if it is separable with respect to the topology of pointwise convergence, so there exists a countable subset G  G with the property that for any g  G there exists a sequence (g)N such that lim g(z) = g(z) for all z  Z. Given

17

a separable function class G  M(Z, R) and a sequence z1:n = {zj}j[n]  Zn, the corresponding empirical Rademacher complexity is defined by





R^

(G,

z1:n)

:=

E
1:n

sup
gG

1 n

j[n]

j

·

 g(zj) 


,

where the expectation is taken of independent Rademacher random variables 1:n  {-1, +1}n.

Note that the assumption that G  M(Z, R) is seperable ensures that the supremum supgG

1 n

j[n] j · g(zj ) =

supgG

1 n

j[n] j · g(zj) for a countable subset G  G. Consequently this supremum is a mea-

surable function and has a well-defined expectation (Boucheron et al., 2013, Chapter 11). Given a

probability measure P on Z and a function g  M(Z, R) we let P (g) = gdP . Given a sequence z1:n = {zj}j[n]  Zn we define an empirical probability measure P^z1:n by

P^z1:n (g)

:=

1 n

g(zj ).

j[n]

In particular, given a random sequence D = {Zj}j[n] where Zj are independent Z-valued random variables, P^D is a random probability measure.

Theorem 9 (Local Rademacher concentration inequality for integrated deviation). Suppose we

have a set  along with a probability measure  on . For each   , we have a separable class

of r

functions G  M1(Z) and a function  : Zn  (z1:n, r) is non-decreasing, r  (z1:n,

× (0, ) r)/ r is

 (0, ) such that for each non-increasing, and for all

z1:n r>

 0

Zn, and

z1:n = {zj}j[n]  Zn,

R^ g  G : P^z1:n (g2 )  r , z1:n   (z1:n, r).

For each z1:n  Zn we choose (z1:n)  (0, ) so that (z1:n, (z1:n)) = (z1:n). Suppose we have a sequence of independent random variables D = {Zj}j[n] with common distribution P . Given any   (0, 1), with probability at least 1 -  over D the following holds for all sequences
(g)   G,

P^D(g) - P (g) d() 


P (g2 )d() · C^n,(D) + C^n,(D),


where C^n,(D) = 650 · (D)d() + 152 log(4 log(n)/)/n.
Theorem 9 may be viewed as a generalisation of concentration inequalities in the literature which provide similar high probability bounds for the special case in which  is a singleton (Massart, 2000; Massart et al., 2006; Koltchinskii, 2006; Bartlett et al., 2005; Boucheron et al., 2013). Theorem 9 builds upon these results and implies that the tail of the integrated deviations of infinitely many (possibly dependent) processes are of the same order of those for a single process. In other words, the failure probabilities of the individual concentration guarantees do not accumulate despite there is arbitrary dependence among them. This will translate into desirable learning guarantees for randomised ensembles that use an averaging-type combination rule.

18

Note that the high probability bound in Theorem 9 cannot be immediately deduced from the special case in which  is a singleton, as integrating both sides would require a union bound over all   , which would blow up the failure probability. On the other hand, such an approach would allow us to obtain an expectation bound by Fubini's theorem. To prove Theorem 9 we can apply this idea to the logarithm of the moment generating function to show that the tail of the integrated process is not much larger than the tail of the individual processes (Lemma 10). In order to apply this idea to obtain a local bound we must first decouple the -dependency from the function dependent terms in our bound which requires Lemma 11.
Lemma 10 ( Ensemble tail bound). Suppose that we have a sequence of real-valued random variable {X()} such that for some constant  > 0 and all   ,   (0, 1) we have P (X() >  · log(1/))  . Given any probability measure  on  we have

P X()d() > 2 · log(2/)  .

Proof. Given any   (0, 1/) and    we have,

E [exp( · X())] =


P
0

X ()

>

log(t) 

dt =

 0

max{1,

t-

1 ·

}dt

=

1

1 -

·

.

By Jensen's inequality we have E exp( · X()d())  E [exp( · X())] d()  1/(1-·). Hence, for each t > 0,   (0, 1/) we have

P

X()d() > t  E exp  ·

X ()d ()

· e-·t

=

1

e-·t -·



.

The lemma follows by taking t = 2 · log(2/) and  = 1/(2).

Lemma 11. Let (n) =

21+q n

:

q



{0, · · ·

, log2(n) - 1}

.

Given any x  [0, 1] and y  0 we

have

min
(n)



·

x

+

y 

 max

3xy,

3y,

4 n

.

Proof. We consider three cases.

1. If x  y then with  = n-1 · 2log2(n)  (n) we have   (1/2, 1], so  · x + y/  3y.

2.

If y

y/x  2/n then with  = 2 · n-1  (n) we use  2 · n-1 so y/  2/n. Hence,  · x + y/  4/n.

x



1

to

infer



·x



2 · n-1

and

3. 3Ifxyy./x  (2/n, 1] then there exists some   (n) with  < y/x  2. Hence, ·x+ y/ 

We can now complete the proof of Theorem 9.

19

Proof of Theorem 9. We begin by fixing  > 0. By considering a special case in which  is a singleton (Theorem 34, Appendix B) we see that for each    and   (0, 1), with probability at least 1 -  over D, the following holds for all g  G,

P^D(g) - P (g)  2 ·

P (g2 ) ·

72

·

 (D)

+

2

log(4

log(n)/) n

+

132

·

 (D)

+

30

log(4 log(n)/) n

  · P (g2 ) +

132

+

72 

· (D) +

30

+

2 

·

log(4

log(n)/) n

.

(13)

We define random variables for each    by

X() := sup
g G

P^D(g) - P (g) -  · P (g2 )

-

132

+

72 

· (D) -

30

+

2 

·

log(4

log(n)) n

.

By (13) we have P

X() >

30

+

2 

·

log(1/) n

  for all   (0, 1). Thus, by Lemma 10 the

following holds for all   (0, 1),

P

X()d() >

60

+

4 

·

log(2/) n

 .

Hence, for each   (0, 1) the following holds with probability at least 1 - ,

sup
(g )  G

P^D(g) - P (g) d() -  · P (g2 )d()





 sup P^D(g) - P (g) -  · P (g2 ) d()
 gG



132

+

72 

· (D)d() +


60

+

4 

·

log(4

log(n)/) n

.

Thus, given any   (0, 1) and  > 0 the following holds with probability at least 1 - , for all (g)   G,

P^D(g) - P (g) d()   ·




P (g2 )d()

+

1 

·

72 ·

 (D)d ( )

+

4

log(4

log(n)/) n

+ 132 ·

 (D)d ( )

+

60

log(4

log(n)/) n

.

(14)

Now take (n) =

21+q n

:q



{0, · · ·

, log2(n) - 1}

as in Lemma 11. Note that (n) has cardinal-

ity no more that log2(n)  2 log(n). By (14) combined with Lemma 11 we see that with probability

20

at least 1 - 2 log(n) · , the following holds for all (g)   G,

P^D(g) - P (g) d()  min



(n)

·



P (g2 )d()

+

1 

·

72 ·

 (D)d ( )

+

4 log(4 log(n)/) n

+ 132 ·

 (D)d ( )

+

60 log(4 log(n)/) n

6

P (g2 )d() · 18 ·


 (D)d ( )

+

log(4 log(n)/) n

+ 348 ·

 (D)d ( )

+

72

log(4

log(n)/) n

.

Taking /(2 log(n)) in place of  we see that the following holds with probability at least 1 -  over D, for all (g)   G,

P^D(g) - P (g) d()  6


P (g2 )d() · 18 ·


 (D)d ( )

+

2

log(4

log(n)/) n

+ 348 ·

 (D)d ( )

+

152

log(4 log(n)/) n

.

5.2 High probability bound on the ensemble error of compressive empirical risk minimisers

The second stage of the proof of Theorem 2 is to establish Proposition 3 below, which gives a high
probability upper bound on the ensemble error of compressive empirical risk minimisers. First, we require some additional notation. Given a mapping A  Ak and a Borel probability distribution P on X × Y, we define an associated function class

GA := {(x, y)  L (f (A(x)), y) - L ((x), y) : f  Fk}  M (X × Y, R) .

We shall also refer to data-dependent elements of GA which are random elements of GA which implicitly depend upon the data D = ((Xj, Yj))j[n]. We also define the compressibility of a finite ensemble of size m  N as the 1 -  upper quantile of the approximation error of the ensemble given
by

P,,m(k) := inf

¯  R : 1
Am k

1 m

m i=1

inf {EL,P
f Fk

(f



Ai)}



¯

dk(A1) . . . dk(Am)  

.

When the distribution P is clear from context we shall denote P,,m(k) by ,m(k).

Proposition 3 (Ensemble error of compressive ERMs). Suppose that Assumptions 1, 2, 4 and 5

hold with parameters b, , Lip, Ccn, CB  1,   [0, 1]. Take n  N, k  N and   (0, 1), and for each A  Ak let g^A be a data-dependent element of GA. Then with probability at least 1 - 2 we have

1 m

m

P (g^Ai )



1
16 (CBCn,k, ) 2-

+

2 m

m

i=1

i=1

P^D (g^Ai

)

-

inf
gGAi

P^D (g )

+ 4 (b · Cn,k, + ,m(k)) ,

where Cn,k, = (4000Ccnk) · log+ (Lipn) + 152 · log+(4 log(n)/) · n-1.

21

We prove Proposition 3 via Lemmas 12 and 13 below.

Lemma 12 (Local Rademacher complexity bound ). Suppose that Assumptions 2 and 4 hold with parameters , Lip, Ccn. Then for each A  Ak, GA is separable. Moreover, given a distribution P on X × Y, along with r > 0 and z1:n  Zn we have

R^

g  GA : P^D(g2)  r , z1:n  2

Ccn

·k n

·

r

·

1
log+2

Lipn kr

.

The proof of Lemma 12 is standard and is contained in Appendix D for completeness. We shall now deduce the following consequence of Lemma 12 combined with Theorem 9.

Lemma 13. Suppose that we are in the setting of Proposition 3. Then with probability at least

1 -  the following holds for all (gAi )m i=1 

m i=1

GAi ,P ,

1m m

P^D(gAi ) - P (gAi ) 

i=1

CB · Cn,k, ·

1 m

m
P (gAi )


+ b · Cn,k,.

i=1

(15)

Proof. We introduce function classes GA for each A  Ak by GA ,P := {b-1 · g}gGA,P so that each function class GA ,P  M1(Z), where Z = X × Y, which will allow us to apply Theorem 9. Let  : (0, )  (0, ) denote the function defined by

(r) := 2

Ccn

·k n

·

r

·

1
log+2

Lipn r

.

Observe that r  (r) is non-decreasing, r  (r)/r is non-increasing and by Lemma 12 we have

R^ g  GA : P^D{(g)2}  r , z1:n = b-1 · R^ g  GA : P^D(g2)  b2 · r , z1:n  (r),

for all r > 0. Choose  so that () =  and observe that   (6Ccnk) · n-1 · log+ (Lipn). Note that the samples Zj = (Xj, Yj) are assumed to be independent and identically distributed. Moreover, random projections (Ai)m i=1 are independent from the data D = ((Xj, Yj))nj=1 and so sam-
ples Zj = (Xj, Yj) are also independent and identically distributed with respect to the conditional distribution. Hence, by Theorem 9 we have,





P

sup

(gA i )m i=1 

m i=1

GA i

1 m

m i=1

P^D(gA i ) - P (gA i )

-

1 m

m

P

i=1

(gA i )2





 · Cn,k,

>

Cn,k,

(Ai)m i=1

 .



Hence, by the law of total expectation with probability at least 1 -  the following holds for all

(gAi )m i=1 

m i=1

GAi

1m m

P^D(gAi ) - P (gAi ) 

i=1

1 m

m

P {(gAi )2} · Cn,k, + b · Cn,k,.

i=1

(16)

Thus, to complete the proof it suffices to deduce (15) from (16). Given any function g  GA  Mb(Z) with A  Ak there exists   M(X , V) with g(x, y) = L ((x), y) - L (P(x), y) for (x, y)  X × Y.

22

Since L and P satisfy the Bernstein condition with parameters , CB we see by (16) with probability

at least 1 -  the following holds for all (gAi )m i=1 

m i=1

GAi

,

1m m

P^D(gAi ) - P (gAi ) dk(A) 

i=1



1 m

m

P (gA2 i ) · Cn,k, + b · Cn,k,

i=1

(CB · Cn,k,) ·

1 m

m
P (gAi )


+ b · Cn,k,.

i=1

(17)

where we used the Bernstein-Tsybakov condition and Jensen's inequality combined with the con-

vexity

of

z



z

1 

.

To complete the proof of Proposition 3 we also require the following elementary lemma, which

will be used to rearrange the bound into an additive form.

 Lemma 14. Given x, c, d > 0 satisfying x  cx + d and   (1, 2] we have x  4

( - 1)-2 ·

1
c 2- +  · d.

We are now ready to prove Proposition 3.

Proof of Proposition 3. Fix  > 0 and for each A  Ak choose gA  GA so that P (gA )  infgGA {P (g)} + . We define a pair of events E1, and E2 by

E1, :=

1 m

m i=1

inf
gGAi

P (g)



P,,m(k)

+



,



E2

:=

1 m

m i=1

P^D(gAi ) - P (gAi )



CB · Cn,k, ·

1 m

m

P (gAi )

i=1





 + b · Cn,k, .



where the intersection is over all (gAi)m i=1 

m i=1

GAi

.

We

claim

that

P(E1,)  1 - .

Indeed,

by

the definition of P,,m(k) with probability at least 1 -  we have

,m(k) + 



1 m

m i=1

inf {EL,P
f Fk

(f

 Ai)}



1 m

m i=1

inf {RL,P (f
f Fk

 Ai) - RL,P ()}

 1 m inf m i=1 f Fk

{L (f (Ai(x)), y)

-

L ((x), y)} dP (x, y)

=

1 m

m i=1

inf
gGAi

P (g).

In addition, by Lemma 13 we have P E2 > 1-. Thus, by the union bound we have P(E1, E2) >

23

1-

2.

We

write

O

:=

1 m

m i=1

P^D(g^Ai )

-

inf gGAi

P^D(g),

so

on

the

event

E2

we

have

1m m

P (g^Ai ) - P (gA i )

i=1



1 m

m

P^D(g^Ai ) - P^D(gA i )

i=1

+

1 m

m

P^D(g^Ai ) - P (g^Ai )

+

1 m

m

P^D(gA i ) - P (gA i )

i=1

i=1

O+

(CB · Cn,k,) ·

1 m

m
P (g^Ai )


+

i=1

(CB · Cn,k,) ·

1 m

m
P

gA i

i=1


+ b · 2Cn,k,

2

(CB · Cn,k,) ·

1 m

m
P (g^Ai )


+ b · 2Cn,k, + O.

i=1

Note

also

that

on

the

event

E1,

we

have

1 m

2. Hence, on the event E1,  E2 we have

m i=1

P

(gA i

)



1 m

m i=1

inf gGAi

{P

(g)}

+





,m(k)

+

1 m

m

P (g^Ai )  2

i=1

2

(CB · Cn,k,) · (CB · Cn,k,) ·

1 m

m
P (g^Ai )



+

O

+

2b

·

Cn,k,

+

1 m

m
P (gA i )

i=1

i=1

1 m

m
P (g^Ai )


+ O + 2b · Cn,k, + ,m(k) + 2.

i=1

Hence, by Lemma 14 with  = 2 we see that on the event E1,  E2 we have

1 m

m

1
P (g^Ai )  4 (4CBCn,k,) 2- + 2

O + 2b · Cn,k,

+ 2,m(k) + 4.

i=1

Since P(E1,  E2) > 1 - 2 it follows that with probability at least 1 - 2 we have

1 m

m

P (g^Ai )



1
16 (CBCn,k, ) 2-

+ 2O + 4b · Cn,k,

+ 2,m(k) + 4.

i=1

Letting   0 and noting that O  ,m(k) gives the required result.

5.3 Completing the proof of Theorem 2
Before completing the proof we apply Bennet's inequality to bound ,m(k) in terms of (k).
Lemma 15. Given any m  N,   (0, 1) we have ,m(k)  2(k) + 3b log(1/)/(2m).
Proof. The sequence of random variables (inffFk {EL,P (f  Ai)})i[m] are independent, bounded by b, have expectation (k), and variance no larger than b · (k). Hence, the result follows from Bennett's inequality (Boucheron et al., 2013, Chapter 2).

24

Proof of Theorem 2. By Assumption 3 and Proposition 3 we have,

EL,P

^n,k,m



qc m

EL,P f^i  Ai

i[m]

=

qc m

P (g^Ai )

i[m]

1
 qc 16 (CBCn,k,) 2- + 4b · Cn,k, + 4,m(k) ,

(18) (19)

where Cn, (D) = (4000Ccnk) · log+ (Lipn) + 152 · log+(4 log(n)/) · n-1. By combining the above bound with Lemma 15, the result follows.

6 Proofs of minimax lower bounds

This section proves the fundamental limits achievable for the distributional classes described in Definitions 1 and 2. We shall begin by proving Theorem 8 in Section 6.1 before moving onto the proof of Theorem 5, which is given in Section 6.2. A key component in proving Theorem 8 is Lemma 19, which will also be applied in the proof of Theorem 5. To prove our minimax lower bounds (Theorems 5 and 8), we shall construct finite families of distributions for each n  N, such that (i) all distributions in the family satisfy the required conditions of the relevant distributional class, (ii) the distributions must be similar enough that they are difficult to identify based on an i.i.d. sample of size n, and (iii) they must be different enough so that failing to identify the generating distribution incurs a high excess risk.
Before presenting the proofs we first recall some useful terminology and results from the literature on minimax rates. For a comprehensive introduction see Tsybakov (2004a). Given a convex function f : R  R with f (1) = 0 and a pair of distributions Q0 and Q1 on a common measurable space (Z, B) the f -divergence between Q0 and Q1 is defined by

Df (Q0, Q1) := f
Z

dQ0 dQ1

dQ1,

when Q0 is absolutely continuous with respect to Q1 and Df (Q0, Q1) =  otherwise. The 2divergence 2(Q0, Q1) is the f -diverence with f (z) := (z - 1)2, the Kullback­Leibler divergence KL(Q0, Q1) is the f -divergence with f (z) := z log z. We shall also use the total variation distance, defined by

TV(Q0, Q1) := sup |Q0(A) - Q1(A)|.
AB
The total variation, Kullback­Leibler and 2-divergence are related as follows. Lemma 16. Given distributions Q0 and Q1 on (Z, B) we have
2 · TV2(Q0, Q1)  KL(Q0, Q1)  2(Q0, Q1). Proof. This follows from (Tsybakov, 2004a, Lemma 2.5 (i) combined with Lemma 2.7).

We can use the following relationships to upper bound the divergence of products.

25

Lemma 17. Given distributions Q0 and Q1 on (Z, B) we have

KL Q0 n, Q1 n = n · KL(Q0, Q1)

&

TV Q0 n, Q1 n  n · TV(Q0, Q1).

Proof. See (Tsybakov, 2004b, Section 2.4) for the result involving KL. See (Sendler et al., 1975, Lemma 2.1) for the total-variation inequality.

Given q  N we let   q := {-1, 1}q denote the q-dimensional binary hyper-cube and let dH denote the associated Hamming distance i.e.

dH(, ) :=

1{j = j },

j[q]

for  = (j)j[q],  = (j )j[q]  q. We shall leverage Assouad's lemma (Lemma 18), which is widely used for deriving minimax lower bounds Tsybakov (2004a); Wainwright (2019).

Lemma 18 (Assouad's lemma). Let (Z, BZ ) be a measurable space and {Q}q a collection of probability measures on (Z, B) such that TV(Q, Q )  1/2 for all ,   q with dH(, ) = 1. Then, given any BZ -measurable mapping ^ : Z  q we have

max
q

EQ

[dH(^, )]



q 4

.

Proof. See e.g. (Tsybakov, 2004a, Theorem 2.12 (ii)).

6.1 Proof of the regression lower bound (Theorem 8)

We will begin by proving the simpler of the two lower bounds (Theorem 8). The core of the proof is
Lemma 19, which will also be applied in the proof of Theorem 5. Lemma 19 shows that given any distribution P0, any empirical predictor ^ and any mixing proportion , we can generate a mixture distribution P := (1 - ) · P0 +  · P1 with respect to which P incurs an excess error that is at least a constant multiple of the mixing proportion .

Lemma 19 (Mixture lower bound). Let L : V × Y  [0, ) be a loss function satisfying Assump-
tions 2 and 13 where V is a compact metric space. Fix a distribution P0 on X ×Y and take   (0, 1]. Choose  > 0 and y0,y1  Y so that (12) holds. Given n  N, suppose that X contains q  2n distinct points {x1, . . . , xq}  X such that P(X,Y )P0(X = xj) = 0 for each j  [q]. Given any empirical predictor ^ : (X × Y)n × X  Y, there exists a distribution P1 on X × Y such that;

(a) The distribution P1 is supported on {x1, . . . , xq} × {y0, y1}  X × Y;

(b) For all x  {x1, . . . , xq} there exists y  {y0, y1} with P1(Y = y|X = x) = 1;

(c)

If

we

let

P

:= (1 - ) · P0 +  · P1

then

EDP n [EL,P(^)] 

 4

.

Proof. We begin by constructing a family of measures (P1,) where  = {-1, 1}q as follows. For each  = (j )j[q]   we let

1

P1,({(xj , y)}) =

q
0

if



=

j +1 2

otherwise.

(20)

26

Note that P1, satisfies (a) for each   . In addition, if ,    satisfy dH(, ) = 1 then

TV(P , P )



1 q

.

Hence,

if

we

define

a

family

of

mixture

distributions

(P )

by

P

:=

(1 - ) ·

P0 +  · P1, then given any ,    with dH(, ) = 1 by Lemma 17 we have

TV

(P)n, (P )n

 n · TV (P, P )  n

· TV (P1,, P1, ) 

n q



1 2

.

(21)

Moreover, given j  [q],  = (j )j[q]  , and letting j = (j + 1)/2 we have,

P(X,Y )P (Y = yj |X = xj ) = P(X,Y )P1, (Y = yj |X = xj ) = 1,

since P(X,Y )P0 (X = xj) = 0. Hence, (b) holds for P and L (xj ), yj = infvV L v, yj where we have used Assumption 2 combined with the fact that V is compact.
To prove (c) we define an estimator ^ : (X × Y)n   by

^j := sgn L(^(xj ), y0) - inf L(v, y0) - L(^(xj), y1) - inf L(v, y1) ,

vV

vV

and letting ^ = (^j)j[q], which implicitly depends upon the sample. It follows that for each j  [q] with ^j = j and j = (j + 1)/2 we have

L(^(xj

),

yj

)

-

inf
vV

L(v,

yj

)



L(^(xj

),

y1-j

)

-

inf
vV

L(v,

y1-j

).

(22)

By Assumption 12 combined with (22) we deduce that for j  [q] with ^j = j and j = (j + 1)/2

L(^(xj ),

yj

)

-

L((xj ),

yj

)

=

L(^(xj ),

yj

)

-

inf
vV

L(v,

yj

)



.

Hence, for each    we have

EL,P (^)



 q

L(^(xj ), yj ) - L(P  (xj ), yj )



 q

·

dH(^, ).

j[q]

By Assoud's lemma (Lemma 18) combined with (21) there exists at least one    with

ED(P)n [EL,P(^)]



 q

· ED(P)n [dH(^, )]



 4

,

as required.

Lemma 20. Let L : [-, ] × Y  [0, b] be a loss function satisfying Assumptions 1, 2 and 13 and

take  = (Wmax, Csp, )  [1, )2 × (0, 1) with Wmax  2 · -1/2 and   [0, 1]. Let X be a Hilbert

space containing a unit vector e1 and a zero vector 0X . Suppose that P is a distribution on X ×Y with marginal PX on X satisfying both (i) PX ({0X ,  ·e1})  1-/b and (ii) PX ({(t )·e1}t[0,1]) = 1

. Then P  PL(, ).

Pt-r}o1}o/f2t.h·|eFnir(fsotrchx·eo1o)s-{e0ttX|,:=2··e(01-}X1)w/2e

[-,

]

and

w

:=

-1/2

·

(

 (

·

e1)

-

t)

·

e1

 Wmax. Moreover, if we define (x) := min{

have (x) = (x). Hence, by (i) we have

so that w 2 = , max{-, wx+

 EL,P ()  b · PX (X \{0X ,  · e1})  .

27

Thus, P satisfies Assumption 11 with parameters (Wmax, ). Second, it follows from (ii) that the covariance operator  corresponding to PX has singular
values 1()    Csp ·  and r() = 0 for r  N\{1}. Thus, Assumption 12 also holds. By combining these two conclusions we see that P  PL(, ).

Proof of Theorem 8. We begin by choosing y0  Y and defining a distribution P0 on X × Y so that

P0(A) = 1{(0X , y0)  A} for Borel sets A  X × Y. Note also that since X is a Hilbert space

containing a non-zero element we may choose a unit vector e1  X . We now consider two cases.

First {x1, . . . ,

xsuq}ppos{e(tthat)· e1}nt-(10,1>].

0. Take  := /b, q := 2n, and choose By Lemma 19 there exists a distribution

a set of q distinct P1 with marginal

points (P1)X

supported on {x1, . . . , xq} such that P = (1 - )P0 + P1 satisfies

EDP n [EL,P(^)]



 4

=

 4b



 8b

· (n-1

+ ).

Moreover, since (i) PX ({0X }) = 1 - /b and (ii) PX ({0X }  {x1, . . . , xq}) = 1, it follows from Lemma 20 that P  PL(, ).
Now suppose that  < n-1. We apply Lemma 19 once again with  = 1/(2n), q = 1 and x1 := -1/2 · e1 to obtain a a distribution P1 with marginal (P1)X supported on {x1} such that the mixture P = (1 - )P0 + P1 satisfies

EDP n [EL,P(^)]



 4

=

 8n



 16

· (n-1

+ ).

Moreover, since PX ({0X , -1/2 · e1}) = 1 it follows from Lemma 20 that P  PL(, ).

6.2 Proof of the classification lower bound (Theorem 5)

In this section we prove Theorem 5. In order to apply Assoud's lemma (Lemma 18) we shall first

construct a parameterised family of distributions. We then establish sufficient conditions for these

distributions to belong to the class P0,1(, ). First suppose that n  N\{1}. Given q  {1,

.

..

,

n

-

1},

r



[1, q],

v



(0,

1),





(0,

1/2)

we

shall define a family of measures (P ) on X × Y indexed by  = {-1, 1}q. Our construction

wr.ilWl beegwinillbryeqfiurisrtecrhoosinqgtao

set of q ensure

"difficult to classify" points in X , with total mass v and norm that the points can be classified with sufficiently large margin.

More precisely, let {e0, e1, . . . , eq}  X be a collection of q + 1 orthonormal vectors. We let x0 := e0 and for   [q] let x := r · e. All of our measures P  will share a common marginal distribution µ

on X supported on {x0, x1, . . . , xq} and defined by

v

µ({x})  µq,r,v({x}) :=

q
1-v

if   [q] if  = 0.

(23)

For each  = ()[q]   we define an associated regression function  : X  by

 (x )



q,r,v, (x )

:=

1

+· 2

 ,

(24)

for each   [q] and (x) = 1 for x / {x1, . . . , xq}. The family of measures (P ) can now be defined by taking

P ({x, y})



Pq,r,v,({x, y})

:=

µ({x})

·



(x)

1+y 2

·

{1 -



(x)}

1-y 2

,

(25)

28

for all (x, y)  X × {-1, +1}.

Lemma

21.

Suppose

that

 = ((, CT), (CG, ), (CM, ))

where

CB,

CM

 1,

CG



2 2

,

  [0, 1],

,   (0, ) and take   [0, 1]. Then for each   ,

(a)

P  satisfies Assumption 7 with parameters (CG, , ) provided that  · v  CG

 r/ 2q

;

(b) P  satisfies Assumption 8 with parameters (CM, ) provided that  · v  CM · r-;

(c)

P

satisfies

Assumption

9

with

parameters

(CT, )

provided

that

v



CT

·

 1-

.

Proof. We fix (a) Suppose 

·v=(C)G·[q]r/2qand.

consider each Define w 

assumption in

w()

:=

1 2

turn. e0 + 1q

[q]  · e  X , take

t := 0 and let (x) := sgn(wx - t) for x  X . Observe that w 2 = 1. Moreover, it follows

from (24) that

P  (x) = sgn(2(x) - 1) =  = sgn(wx + t) = (x),
for each   [q]. Similarly, P  (x0) = 1 = (x0). Since µ = (P )X is supported {x0, x1, . . . , xq} it follows that EL,P () = 0  . To complete the proof we must take S := {x  X : |wx - t|  } and show that

|2(x) - 1| dPX (x)  CG ·  ,

(26)

S

for each  > 0. Observe that |wx0 - t| = 2-1/2 and |wx - t| = r(2q)-1/2  2-1/2 for   [q]. We shall consider three cases. If (i)   (0, r(2q)-1/2) then S  supp(µ) =  so (26) holds. If (ii)   [r · (2q)-1/2, 2-1/2) then S  supp(µ) = {x1, . . . , xq} and so

|2(x) - 1| dPX (x) = µ({x}) · |2(x) - 1| = v ·   CG ·

S

[q]

r 2q


 CG ·  ,

as

required.

Finally,

if

(iii)





2-1/2

then

(26)

follows

from

the

assumption

that

CG



2

 2

.

Hence,

Assumption 7 holds with parameters (CG, , ).

(b) Suppose  · v  CM · r-. To prove the claim it suffices to show that for all s  (0, ),

|2(x) - 1| dPX (x)  CM · s-

(27)

x >s

Recall that supp(µ) = supp{(P )X } = {x0, x1, . . . , xq} and note that x0 = 1 and x = r  1 for   [q]. As before, we consider three cases. If (i) s  (0, 1] then (27) follows from CM  1. If (ii) s  (1, r) then

|2(x) - 1| dPX (x) = µ({x}) · |2(x) - 1| = v ·   CM · r-  CM · s-,

x >s

[q]

as required. Finally, if (iii) s  [r, ) then µ({x  X : x > s}) = 0 so (27) holds.

29

(c)

Now

suppose

that

v



CT

·



 1-

.

Observe

that

|2(x0) - 1|

=

1

and

|2(x) - 1|

=



for





[q].

To prove the claim it suffices to show that for all   (0, 1) we have

µ ({x



X

:

|2(x) - 1|



})



CT

·



 1-

.

(28)

We consider two cases. If   (0, ) then {x  supp(µ) : |2(x) - 1|  } =  so (28) holds. On the other hand, if   [, 1) then

µ ({x



X

:

|2(x) - 1| 

})

=

µ({x1, . . . , xq})

=

v



CT

·  1-



CT

·



 1-

.

Lemma 22. Given ,   q with dH(, ) = 1 we have 2(P , P  )  242v/q, and hence TV{(P )n, (P  )n}  1/2 whenever 25n2v  q.

Proof. Choose 0  [q] so that 0 = dH(, ) = 1 there is exactly one such

 0 0

where  = ()[q] and  = ( )[q].  [q]. Observe that by construction µ =

Note (P )X

that since = (P  )X

is supported on {x0, x1, . . . , xq} and (x) =  (x) for x  supp{(P  )X }\{x0}. In addition, we

have

P

 ({x,

y})

=

µ({x})

·

 (x)

1+y 2

·

{1

-





(x)}

1-y 2

for all x  X

and

y  {-1, 1}.

It follows that

P ({(x, y)}) = P  ({(x, y)}) for all x  supp{(P  )X }\{x0}. In addition, since   (0, 1/2), we

have

P ({(x0 , 1)}) P  ({(x0 , 1)})

-

1

2
=

(x0 )  (x0 )

-

1

2
=

(1 +  · 0 ) - (1 +  · 0 ) 1 +  · 0

2
 242,

and similarly

P  ({(x0 ,-1)}) P  ({(x0 ,-1)})

-

1

2
 242.

Hence,

2 P , P 

=
X ×Y

dP  dP 

-1
(x,y)

2
dP  (x, y)

q
=
=0 y{-1,1}

P ({(x, y)}) P  ({(x, y)})

-

1

2
P  ({(x, y)})

=
y{-1,1}

P ({(x0 , y)}) P  ({(x0 , y)})

-

1

2
P  ({(x0 , y)})

 242

µ({x0 }) ·





(x0

)

1+y 2

·

{1

-



(x0

)}

1-y 2

y{-1,1}

=

242v q

.

Hence, by Lemmas 16 and 17 we have

2 · TV2{(P )n, (P  )n}  KL{(P )n, (P  )n}  n · KL P , P   n · 2

P , P 



24n2v q

.

Consequently, TV{(P )n, (P  )n}  1/2 provided 25n2v  q.

30

Lemma 23. Given a classifier  : X  {-1, 1} and  = ()[q]   we have

EL0,1,P  ()



v q

1 {(x) = } .

[q]

Proof. Observe that for each   [q] we have µ({x}) = v/q, (x) = (1 +  · )/2 and hence P  has Bayes classifier P  (x) = . Thus, we may lower bound the excess error by

EL0,1,P  () =

X

|2(x)

-

1|

· 1{(x)

=

P  (x)}



v q

1 {(x)
[q]

=

} .

To prove Theorem 5 we shall consider three cases. The first case we consider will occur when the
small approximation error  and large sample size n regime in which the lower bound is attained by the family (Pq,r,v,), for an appropriate choice of q,r, v, . The lower bound in the second case in which both the approximation error  and the sample size n are small will be deduced from the
first. In the third case, where the approximation error  is large, we will deduce the lower bound
from Lemma 19.

Proof

of

Theorem

5.

First suppose that

n-

 2(+ )+

(2-)



and

6{2(+)+(2-)} 2(+)+(2-)

n  n0  n0(, , ) := max 1 + 2

 (2-)

,2

 (1-)

.

Now define q  N, r > 0 and v,   (0, 1) by

q

:=

(2 n) 5

2( +) 2( +)+ (2-)

,


r := q 2(+) ,

v

:=

q , -

  2( +)

 := q . -

 (1-) 2( +)

Note that since n  n0 we have q < n, r  [1, q] and   (0, 1/2). Hence, we may apply the

construction outlined that by construction

in (23), (a)  · v

(=24)(,r/(25q))tocoCnGstrur/cto2uqrf,am(bi)lyo·fvdi=strrib-utionsCM(P·r)-a. ndNo(tce)

v

=

 1-



CT

·

 1-

.

Hence,

by

Lemma

21

we

have

{P }



P0,1(, ).

Moreover,

25n2v



q

so by Lemma 22 we have TV{(P )n, (P  )n}  1/2 for all ,   q with dH(, ) = 1. Next

we associate to our empirical classifier ^ : (X × Y)n × X  Y a random binary ^ = (^)[q], taking

values in , by ^ = ^(x) for all   [q]. By Assouad's lemma (Lemma 18) there exists some







with

E(P  )n

[dH(^, )]



q 4

.

Thus,

by

Lemma

23,

E(P  )n

EL0,1,P  ()



v q

· E(P  )n

1 {(x) = }

=

v q

· E(P  )n

dH(^, )

[q]



v 4

=

1 4

· q-

 2( +)

 2 ( ) · n , -

5+

 2( +)

-

 2(+ )+

(2-)

provided

n  n0

and

n-

 2(+ )+

(2-)

 .

Moreover, any

empirical

classifier

^ : (X

× Y)n × X



Y based on a sample of size n  {1, . . . , n0} may be viewed as a special case of an empirical

31

classifier with a sample size n0 which disregards a fraction of the data. Hence, for all n  N with

n-

2(+

 )+

(2-)



there

exists

P

 P0,1(, )

with

E(P )n

EL0,1,P ()

 2 ( ) · (n · n) -

5+

 2( +)

0

-

2(+

 )+

(2-)

 2 ( ) · n · {n + }. -

6+

 2( +)

-

 2(+ )+

(2-)

0

-

 2(+ )+ (2-)

(29)

Now

suppose

that

n-

 2(+ )+ (2-)

< .

Note that the zero-one loss L0,1 satisfies Assumption

13 with y0 = -1, y1 = 1 and  = 1/2. Let P0 denote the distribution on X × Y defined by

P0(A) := 1{(0X , 1)  A} for Borel sets A  X × Y. Note also that since X is a Hilbert space of

dimension

at

least

n



1

we may

choose

a

unit

vector

e1



X.

Now

set

x

:=

 q

· e1

for





[q].

Thus,

by Lemma 19 there exists a distribution P1 supported on {x1, . . . , xq} × {-1, +1}, with regression

function P1 (x) = P(X,Y )P1 (Y = 1|X = x)  {0, 1} for all x  {x1, . . . , x1} such that the mixture

P := (1 - ) · P0 +  · P1 satisfies

EDP n [EL,P(^)]



 8

>

2-4

·

n +  -

 2(+ )+ (2-)

.

Note that P ({(0, 1)}) =  so Assumption 7 holds with w = 0X and t = 1. Moreover, Assumption

8 also holds since PX is supported on {0X }  {x}[q]  {x  X : x }. Finally Assumption 9

holds since P (x) = P(X,Y )P (Y = 1|X = x)  {0, 1} for all x  supp(PX ). By combining with

(29)

we

see

that

(5)

holds

with

c5

:=

2 ( ) -

6+

 2( +)

·

n-

 2(+ )+ (2-)

0

.

7 Conclusions
We presented a general analytic framework for the theoretical study of compressive learning with arbitrary size ensembles, on arbitrary high dimensional data sets. This yields high probability risk guarantees that are able to take advantage of both statistical and geometric structure in the underlying data distribution. We demonstrated in our framework that new conditions can be unearthed specifically for compressive learning, which differ from those of both compressive sensing and traditional learning, and which lead to a better understanding of compressive learning. In particular, we found some distributional characteristics under which the worst-case excess risk of voting compressive ERMs nearly attain the minimax-optimal rate w.r.t. the sample size. To our knowledge, these are the first statistical optimality guarantees for compressive ERM learning machines and ensembles thereof. In addition, a key ingredient in the proof of our general upper bound may find applications in other areas.
Our results shed light on the question of when and why compressive learning with JohnsonLindenstrauss compressors works well, and provides new insights that may eventually inform data pre-processing and approximate algorithm design in future work. Several questions remain for future research: How to make practical use of the theoretically optimal values of k? How to extend the analysis to other data sketching schemes? What other predictor classes enjoy similar guarantees?

Acknowledgment
Both authors were funded by EPSRC grant EP/P004245/1 "FORGING: Fortuitous Geometries and Compressive Learning" at the University of Birmingham, where a significant proportion of this project was undertaken.

32

A Verifying assumptions for the examples
In this section we verify the properties asserted in Section 2.

A.1 Bounded Lipschitz loss functions

Proposition 4. Examples 1-3 satisfy Assumptions 1 and 2, as follows:

1. The zero-one loss L0,1 satisfies Assumption 1 with b = 1 and Assumption 2 with the standard metric dV (v0, v1) = |v0 - v1| and Lipschitz constant Lip = 1/2.
2. The squared loss Lsqr satisfies Assumption 1 with b = 42 and Assumption 2 with the standard metric and Lip = 4.

3. The Kullback Leibler loss function Lkl satisfies Assumption 1 with b =  + log(2), and Assumption 2 with the standard metric on [-, ] and Lip = 1.

Proof of Proposition 4. 1. Since L0,1(v, y)  {0, 1}, Assumption 1 with b = 1 is immediate. Moreover, if L0,1(v0, y) = L0,1(v1, y) then |v0 -v1| = 2 so Assumption 2 holds with Lip = 1/2.

2.

If

v, y

 [-, ]

then

|v - y|  2

so

Lsqr(v, y) = (v - y)2

 42.

Moreover,

 v

(Lsqr(v, y)) =

2(v - y) so by the mean value theorem Lsqr is 4-Lipschitz.

3. Given a  V = [-, ],

Lkl(a, y)

=

y

log

y (a)

+

(1

-

y) log

1

1-y - (a)



log

y2 (a)

+

(1 - y)2 1 - (a)

(30)

 max

log

1 (a)

,

log

1

1 - (a)

 max{log(1 + exp(-a)), log(1 + exp(a))}

(31)

  + log(2).

(32)

This confirms Assumption 1 with b =  + log(2). Furthermore,

Lkl(a, y)

=

y

log

y (a)

+

(1

-

y) log

1-y 1 - (a)

=

-[ya

-

log(1

+

exp(a))]

(33)

= log(1 + exp(-(2y - 1)a))

(34)

so the first derivative

is bounded

as

|

 a

Lkl

(a,

y

)|

=

|y

-

exp(a) 1+exp(a)

|



1.

By the mean value

theorem, this confirms Assumption 2 with Lip = 1 w.r.t. the standard metric on [-, ].

A.2 Quasi-convex loss functions
Lemma 24. Examples 1-3 satisfy Assumption 3 for any distribution P : 1. The zero-one loss L0,1 satisfies Assumption 3 with qc = 2, and Ens equal to the modal average: Ens({i(x)}i[m]) = sgn( i[m] i(x)); 2. The squared loss Lsqr satisfies Assumption 3 with qc = 1, and Ens equal to the arithmetic average of the bounded-linear outputs;
33

3. The Kullback Leibler loss Lkl satisfies Assumption 3 with qc = 1, and Ens equal to the arithmetic average of the bounded-linear outputs.
Proof. To prove the first claim we take (x) = P[Y = 1|X = x] and PX (A) = P(X  A). So, (x) = 1((x)  1/2) · 2 - 1 and we can write

RL,P (h) - RL,P () = |(x) - 1/2| · 1 {h(x) = (x)} dPX (x).

(35)

Given {i}i[m]  M(X , V)m, if for some x  X , Ens {i}i[m] (x) = (x), where Ens denotes the modal average, then we have i(x) = (x) for at least m/2 values of i  [m]. Thus,

1

Ens {i}i[m] (x) = (x)



2 m

·

1 {i(x) = } .

i[m]

Integrating over PX and substituting into (35) proves the claim. Since Lsqr is convex the second result is a consequence of Lemma 1. Similarly, the function
Lkl(a, y) = kl((a), y) is convex in a. Hence the result again follows from Lemma 1.
We note that for the Lkl, the arithmetic average is equivalent to the product-of-experts combination of the nonlinear probabilistic outputs.

A.3 Covering number assumptions
Let us recall some useful terminology and foundational results on psuedo-dimension which may be found in Anthony and Bartlett (2009).
Definition 3 (Pseudo-shattering). Take F  M(X , R). A set x1:n = {xj}j[n]  X n is said to be pseudo-shattered by F if there exists real numbers r1:n = {rj}j[n] such that for every 1:n = {j }j[n]  {-1, 1}n there exists f1:n  F with sgn (f1:n(xj ) - rj ) = j for every j  [n].
The pseudo-dimension is the natural analogue of the VC dimension for real-valued functions.
Definition 4 (Pseudo-dimension). Take F  M(X , R). The pseudo-dimension of F , denoted by PDim(F ), refers to the maximum cardinality n  N of a set x1:n  X n which is pseudo-shattered by F . If there are sets of arbitrarily large cardinality which are pseudo-shattered by F then we say that F has infinite pseudo-dimension PDim(F ) = .
We use the following well known results.
Theorem 25. Let Flinear  M Rk, R be the set of affine functions
Flinear := u  w · u - t : w  Rk, t  R .
Then Flinear has pseudo-dimension PDim (Flinear) = k + 1.
Proof. See (Anthony and Bartlett, 2009, Theorem 11.6).
Theorem 26. Suppose that F  M (X , R) and let g  M (R, R) be a non-decreasing function. Then the set g (F ) := {g  f : f  F } has pseudo-dimension PDim (g (F ))  PDim (F ).

34

Proof. See (Anthony and Bartlett, 2009, Theorem 11.3).

Given any function class F  M(X , R) and any x1:n = {xj}j[n]  X n, the empirical  metric

d
x1:n

on

F

is

defined

for

f0,

f1



F

by

d
x1:n

(f0

,

f1)

:=

max
j[n]

{|f0(xj )

-

f1(xj )|}

.

Theorem 27. Suppose  > 0, d  N and F  M (X ) has pseudo-dimension PDim (F )  d. Then for all n  d, x1:n  X n and  > 0,

log

N

F

,

d
x1:n

,



 d · log+

n ·d

+2 .

Proof. By (Anthony and Bartlett, 2009, Theorem 12.2) for   [0, 2) we have

log

N

F

,

d
x1:n

,



 d · log

2en ·d

 d · log+

n ·d

+2 .

For   2 the claim follows immediately from F  M(X ) so N

F , d , 
x1:n

 1.

We now verify Assumption 4 for the function classes used in our examples, Fk0,1 (as in Example 1) and Fkbl (as in Examples 2-3). We note that other nonlinear transformations of the linear function class may also satisfy this assumption provided that the nonlinearity only changes the log covering number by a constant factor.

Proposition 5. Examples 1-3 satisfy Assumption 4. More precisely:

1. The class Fk0,1 = u  sgn (w · u - t) : w  Rk, t  R  M Rk, {-1, +1} satisfies Assumption 4 with constant Ccn = 4 and bound  = 1.

2. The class Fkbl = u  max {min {w · u - t, } , -} : w  Rk, t  R  M Rk satisfies Assumption 4 with constant Ccn = 4 and bound .

Proof of Proposition 5. 1. By Theorem 25, PDim (Flinear) = k + 1. Moreover, the mapping

t  sgn(t) is non-decreasing, so by Theorem 26, PDim Fk0,1  k + 1. Note that for all u1:n,

and any f0, f1 we have Theorem 27, for every

dV
u1:n

(f0

,

n, k  N

f1)



d
u1:n

(f0

,

f1),

with n > k, u1:n 

where (Rk )n ,

dV
u1:n

is

the

and  > 0,

empirical

2

metric.

By

log

N

Fk0,1

,

dV
u1:n

,



 log

N

Fk0,1

,

d
u1:n

,



 (k + 1) ·

log+

n  · (k + 1)

+2

 4k · log+

n ·k

.

2. Since PDim (Flinear) = k + 1 and the mapping t  max{min{t, }, -} is non-decreasing,

Theorem 26 implies PDim u1:n  Rk n and  > 0,

Fkbl

 k + 1. By Theorem 27 for every n, k  N with n > k,

log

N

Fkbl

,

d
u1:n

,



 (k + 1) · log+

n  · (k + 1)

+2

 4k · log+

n ·k

.

(36)

Hence,

since

dV
u1:n



d
u1:n

we

have

log

N

Fkbl

,

dV
u1:n

,



 4k · log+

n ·k

.

35

A.4 Bernstein-Tsybakov condition

We now consider Assumption 5. For example 1, the 0-1 loss L0,1 (example 1), the Tsybakov noise condition implies the Bernstein condition (Tsybakov, 2004b, Proposition 1). For the squared loss and the Kullback-Leibler loss we utilise the following lemma.

Lemma 28. Suppose that L : [-, ] × Y  [0, b] satisfies Assumptions 2 and 10. Then satisfy the Bernstein-Tsybakov condition with exponent  = 1 and constant CB = 42Lip/H.
Proof. Choose   M(X , V) and define 1/2  M(X , V) by 1/2(x) = ((x) + (x))/2 for x  X . By Assumption 10 we have

((x)

-

(x))2



4 H

{L((x), y) - L((x), y)} + 2

L

1/2(x), y

- L((x), y)

for all x  X and y  Y. Hence, by applying Assumption 2 we have

as required.

{L ((x), y) - L ((x), y)}2 dP (x, y)  2Lip ((x) - (x))2 dPX (x)

 42Lip H

EL,P () - 2EL,P (1/2)



42Lip H

EL,P

(),

Lemma 29. A twice differentiable function  : [a, b]  R with second derivative   H is H-strongly convex on [a, b].

Proof. Take v0, v1  [a, b] with v0  v1, let vt := tv0 + (1 - t)v1 and  := v1 - v0. By a second order Taylor expansion we have at xt we have

(v0)



(vt)

+

(vt)(v0

-

vt)

+

H 2

(v0

-

vt )2

=

(vt)

-

(vt)t

+

H 2

2t2

(v1)



(vt)

+

(vt)(v1

-

vt)

+

H 2

(v1

-

vt )2

=

(vt)

+

(vt)(1

-

t)

+

H 2

2

(1

-

t)2.

By rearranging we have (vt)  (1 - t)(v0) + t(v1) - (H/2) · t(1 - t) · 2, as required.

Corollary 30. The square loss Lsqr and the Kullback Leibler loss Lkl satisfy Assumption 5 with exponent  = 1.

Proof. By Lemmas 28 and 29 it suffices to show that both u  Lsqr(u, y) and Lkl(u, y) are twice

differentiable with a uniform lower bound of H.

For the square loss we have

2 u2

{Lsqr

(u,

y)}



2

and for the Kullback-Leibler loss we have

2 u2

{Lkl(u,

y)}

=

exp(u) (1 + exp(u))2



(1

exp() + exp())2

,

for all u  [-, ] and y  {0, 1}, as required.

36

B Local Rademacher concentration inequality

In this section we give a local Rademacher concentration inequality for the deviation of an empirical process from its expectation (Theorem 34), which is used in the proof of Theorem 9. In our context, it corresponds to the special case of Theorem 9 where  consists of a single point. We note that several similar results are available in the literature Massart (2000); Massart et al. (2006); Koltchinskii (2006); Bartlett et al. (2005); Boucheron et al. (2013) which highlight the role of variance in obtaining tight concentration guarantees. However, we for completeness we provide a proof Theorem 34 which is well-suited to our setting. We shall begin by recalling several useful results required for the proof. The first is Bartlett et al's variance dependent Rademacher bound.
Theorem 31 (Bartlett et al. (2005)). Suppose we have a class of functions G  M1(Z). Suppose Z is a random variable taking values in Z with distribution P and take v := supgG{ (g - gdP )2dP }. Given n  N we let D = {Zj}j[n] be a sequence of independent random variables with distribution P . Given any   (0, 1), the following holds with probability at least 1 -  over D we have

sup
gG

P^D(g) - P (g)

 6 · R^ (G, D) + 2

v

log(1/) n

+

11

log(1/) n

.

We shall also use a variant of Dudley's inequality which allows us to control the Rademacher complexity of a function class in terms of its covering numbers Dudley (1967). Given a function class G  M(Z, R) and a sequence z1:n = {zj}j[n]  Zn we define a data dependent metric dz1:n
on G by dz1:n(g0, g1) = P^z1:n (g0 - g1)2 , for g0, g1  G. The following refinement of Dudley's
inequality due to Srebro and Sridharan Srebro and Sridharan (2010).

Theorem 32 (Dudley (1967); Srebro and Sridharan (2010)). Suppose we have a function class

G  M(Z, R) and a sequence z1:n = {zj}j[n]  Zn, we have the following bound,



R^

(G,

z1:n )



inf
>0

 4


+



supgG

P^z1:n (g2)





log N (G, dz1:n , )  .

n



We also utilize Talagrand's contraction inequality Ledoux and Talagrand (2013).

Lemma 33 (Ledoux and Talagrand (2013)). Suppose that  : R  R is a L-Lipschitz function and G  M(Z) is a function class. Then for any z1:n  Zn we have R^ ({  g : g  G} , z1:n)  L · R^ (G, z1:n).

With these results in hand we give the following concentration inequality which adapts ideas from Massart (2000); Massart et al. (2006); Koltchinskii (2006); Bartlett et al. (2005); Boucheron et al. (2013) to our setting.

Theorem 34 (Local Rademacher concentration inequality). Suppose we have a countable class of functions G  M1(Z) and a function  : Zn × (0, )  (0, ) such that for each z1:n  Zn, r  (z1:n, r) is a non-decreasing function such that r  (z1:n, r)/ r is non-increasing and for all r > 0 and z1:n = {zj}j[n]  Zn,





R^

 g





G

:

1 n

g2(zj )
j[n]



 r , z1:n




(z1:n, r).

37

For each z1:n  Zn we choose (z1:n)  (0, ) so that (z1:n, (z1:n))) = (z1:n). Suppose we have a sequence of independent random variables D = {Zj}j[n] with common distribution P .
Given any   (0, 1), the following holds with probability at least 1 -  over D for all g  G,

P^D(g) - P (g)  2 ·

P (g2) ·

72

·

(D)

+

2 log(4 log(n)/) n

+

132

·

(D)

+

30 log(4 log(n)/) n

.

Before the proof we recall the following elementary lemma. Lemma 35. Suppose that x, A, B > 0 satisfy x  Ax + B. Then x  A2 + 2B.

Proof of Theorem 34. For each k  {1, · · · , log2(n)} we let vk = 2k+1/n. We let G1 = {g  G :

P (g2)  v1} and for each k  {2, · · · , log2(n)} we let Gk = {g  G : vk-1 < P (g2)  vk}. Observe

that vlog2(n)  1 and since G  M1(Z) we have P (g2)  1 for all g  G, so G 

log2(n) k=1

Gk

.

Fix

k  {1, · · · , log2(n)}. By Theorem 31 we see that with probability at least 1 - , for all g  Gk

we have

P^D(g) - P (g)  6 · R^ (Gk, D) + 2

vk

log(1/) n

+

11

log(1/) n

.

(37)

Let Gk2 = {g2 : g  Gk}. Note also that z  z2 is 2-Lipschitz so by Lemma 33 we have R^ Gk2, D  2R^ (Gk, D). Note also that for g  Gk, P ((g2)2)  P (g2)  vk, since g is bounded by 1, so applying Theorem 31 once again, we see that with probability at least 1 - , for all g  Gk we have

P^D(g2) - P (g2)  12 · R^ (Gk, D) + 2

vk

log(1/) n

+

11

log(1/) n

.

(38)

Thus, by the union bound (37) and (38) both hold for all k  {1, · · · , log2(n)} with probability at least 1 - 2 log2(n) ·   1 - 4 log(n) · . Observe that for all k  {1, · · · , log2(n)} and g  Gk, P (g2)  vk, so given (38) we have

P^D(g2)  vk + 12 · R^ (Gk, D) + 2

vk

log(1/) n

+

11

log(1/) n

.

Let uk := vk + 12 · R^ (Gk, D) + 2 vk log(1/)/n + 11 log(1/)/n. We deduce that Gk  {g  G : P^D(g2)  uk} and so





R^ (Gk, D)



R^

 g





G

:

1 n

g2(zj )
j[n]



 uk , z1:n




 (D, uk) .

(39)

Plugging back into the previous inequality gives

We claim that

uk  vk + 12 · (D, uk) + 2

vk

log(1/) n

+

11

log(1/) n

.

uk  vk + 12 ·

(D) · uk + 2

vk

log(1/) n

+

11

log(1/) n

.

(40)

38

Indeed, either combine (D, (D, (D))/

uk(D(D)))=(D),((DiDn,)uwkwh)ii/cthh uctahks,eewftahhcietchctlhabaiymt (3((49D0)),yrhi)eo/llddssr,tohisre

uk > (D). If the latter holds we non-increasing implies (D) =
claim (40). Hence, in either case

(40) holds. Now by plugging in the definition for uk and subtracting vk + 2 vk log(1/)/n +

11 log(1/)/n from both sides we obtain

R^ (Gk, D) 

(D) ·

vk + 12 · R^ (Gk, D) + 2

vk

log(1/) n

+

11

log(1/) n



(12(D)) · R^ (Gk, D) +

(D) ·

2vk

+

12

log(1/) n

.

By Lemma 35 this implies

R^ (Gk, D)  12 · (D) + 2

(D) ·

2vk

+

12

log(1/) n

.

Hence, by (37) with probability at least 1-4 log(n)· the following holds for all k  {0, ·log2(n)-1} and g  Gk we have

P^D(g) - P (g)  6 · R^ (Gk, D) + 2

vk

log(1/) n

+

11 log(1/) n

 72 · (D) + 12

(D) ·

2vk

+

12 log(1/) n

+2

vk

log(1/) n

+

11 log(1/) n

 72 · (D) + 12

(D) ·

max

8 n

,

4P

(g2

)

+

12

log(1/) n

+2

max

8 n

,

4P

(g

2

)

n

·

log(1/)

+

11

log(1/) n

 72 · (D) + 24

(D) ·

P (g2)

+

5 log(1/) n

+4

P (g2)

· log(1/) n

+

17

log(1/) n

 2·

P (g2) ·

72

·

(D)

+

2

log(1/) n

+

132

·

(D)

+

30

log(1/) n

.

By noting that G 

log2(n)-1 k=0

Gk

and

taking

/4 log(n)

in

place

of



in

the

above

bound,

the

conclusion of the theorem follows.

C The existence of the Bayes optimal predictor

In this section we consider the existence of Bayes optimal predictor. Recall that, given a loss function L : V ×Y  [0, b] and a Borel probability distribution P on X ×Y, a function   L,P  M(X , V) is said to be a Bayes optimal predictor if it satisfies

RL,P () = inf {RL,P ()} .
M(X ,V)

(41)

39

In this section we shall show that a Bayes optimal predictor exists under mild conditions (see Proposition 6). We require the measurable maximum theorem.
Definition 5 (Carath´eodory function). A function g : X × V  R is said to be a Carath´eodory function if:
· For all x  X , the map u  g(x, u) is a continuous function of u  V,
· For all u  V, the map x  g(x, u) is a measurable function of x  X .
We shall utilise the following simplified version of the measurable maximum theorem.
Lemma 36 (Measurable maximum theorem). Suppose that X is a measurable space and V is a compact metric space. Let g : X × V  R be a Carath´eodory function. Define m : X  R by m(x) := supuV {g(x, u)}. Then mg is a measurable function and there exists a measurable function h : X  V with the property that g(x, h(x)) = m(x) for all x  X .
Proof. This is a special case of (Aliprantis and Border, 2013, Theorem 18.19).
We also utilise regular conditional distributions.
Lemma 37 (Regular conditional distributions). Let P be a Borel probability measure on the product space X × Y, where X and Y are complete separable metric spaces. Then there exists a family (P x)xX is a family of Borel probability measures on Y such that for all measurable functions h : X × Y  R with |h(x, y)|dP (x, y) < , the mapping x  Y h(x, y)dP x(y) is measurable and satisfies

h(x, y)dP (x, y) =

h(x, y)dP x(y) dPX (x).

(42)

X ×Y

XY

Proof. Apply (Klenke, 2013, Theorem 8.37) combined with the fact that both X and Y are complete separable metric spaces, and so X ×Y is separable and completely meterizable, and hence Borel.

We shall refer to (P x)xX as the regular conditional distribution with respect to X. We can now
give a proposition which gives sufficient conditions for the existence of a Bayes optimal predictor .

Proposition 6. Suppose that V is a compact metric space, P is a Borel probability distribution on
X × Y and L : V × Y  [0, b] is a bounded loss function which is continuous in its first argument. Then there exists a Bayes optimal predictor   M(X , V). Moreover, any Bayes optimal predictor   M(X , V) satisfies

(x)  argminuV

L(u, y)dP x(y)

(43)

Y

for PX almost every x  X .

Proof. We begin by applying Theorem 37 to obtain a regular conditional distribution (P x)xX such that given any h : X × Y  R with |h(x, y)|dP (x, y) <  the map x  Y h(x, y)dP x(y) is

40

measurable and satisfies (42). In particular, given any   M(X , V) the map (x, y)  L((x), y) is a bounded measurable map and so x  Y L((x), y)dP x(y) is a measurable map satisfying,

RL,P () =

L((x), y)dP (x, y) =

L((x), y)dP x(y) dPX (x).

(44)

X ×Y

XY

Now define a function g : X × V  R by g(x, u) := - Y L(u, y)dP x(y). Observe that for each u  V, x  g(x, u) is measurable. Moreover, for each y  Y, the function u  -L(u, y) is
continuous and so by the dominated convergence theorem it follows that u  g(x, u) is continuous for each x  X . Hence, we have confirmed that g is a Carath´eodory function. Since V is compact we can apply Theorem 36 to see that there exists a function  : X  V with the property that g(x, (x)) = supuV {g(x, u)} for all x  X . Hence, for all x  X , we have Y L((x), y)dP x(y)  infuV { Y L(u, y)dP x(y)} so (43) holds. By (44) it follows that  satisfies (41) and so is a Bayes optimal predictor. Suppose on the other hand that ~  M(X , V) is such
that ~(x) / arg infuV Y L(u, y)dP x(y) on a set A  X of positive PX measure. Then by (44) it follows that RL,P (~) > RL,P () and so ~ is not a Bayes optimal predictor.

D Proof of Lemma 12

To prove Lemma 12 we require the following elementary lemma.

Lemma 38. Given any , T > 0 we have



1

log+2

0

T 

1
d   · log+2

T 



+

 2

.

Proof. First assume that T /  e, so we have



1

log+2

0

T 



d =

log

1 2

0

·

log

1 2

T 

1

d =  ·

log

1 2

0

T ·z

dz

T 

1

+

log

1 2

(1/z

)dz

0

=·

log

1 2

T 



+

 2

,

where we use the fact that

1 0

log

1 2

(1/z

)dz

=

/2

by

a

change

of

variables.

On

the

other

hand,

if

T / < e we have



1

log+2

0

T 

T

d =

e

log

1 2

0

T 

d +

 

-

T e



=T ·

log

1 2

T

+  + - T =+ T · 

e

T /e

2

e

e2





<·

1+

 2

1
=  · log+2

T 

+

 2

,

where we have applied the previous inequality with T /e in place of .

41

Proof Lemma 12. u1:n = {uj}j[n]

For the  Rk n

purpose of the proof where uj = A(xj ) 

define GA(r) := Rk. Given any

{g g0,

 g1

GA : P^z1:n (g2)  GA we may

 r} and choose f0,

f1  Fk such that g0(z) = L(f0(A(x)), y) - L((x), y) and g1(z) = L(f1(A(x)), y) - L((x), y)

for all z = (x, y)  X × Y. By Assumption 2 we have

dz1:n (g0, g1) := P^z1:n ((g0 - g1)2) =

1 n

(L (f0(uj), yj) - L (f0(uj), yj))2

j[n]

 Lip ·

1 n

dV

(f0(uj ), f1(uj ))2

=

Lip

·

dV
u1:n

(f0

,

f1).

j[n]

Hence, by Assumption 4 for all  > 0 we have

log (N (GA(r), dz1:n , ))  log (N (GA, dz1:n , ))  log

N

Fk ,

dV
u1:n

,

 Lip

 Ccn · k · log+

Lipn k

.

Note also that supgGA(r){

P^z1:n (g2)}



 r.

Hence, by Dudley's inequality (Theorem 32) we

have





r
R^ (GA(A), z1:n) 
0

log N (G, dz1:n , ) d  n

Ccn · n

k

·



0

r

1

log+2

Lipn k

d



Ccn · k · r · n

1
log+2

Lipn kr

+

 2

2

Ccn

·k n

·

r

·

1
log+2

Lipn kr

,

where the penultimate inequality follows from Lemma 38.

E Proof of Lemma 7

Before proving Lemma 7 we recall the following useful result due to Baraniuk et al. (2009).
Lemma 39. (Baraniuk et al., 2008, Lemma 5.1) Let A  Rk×d be a matrix that satisfies Assumption 6 with constant CJL  1. There exists a constant cB > 0 depending only on CJL such that for any r-dimensional linear subspace U  Rd with r < k and   (0, 1), the following holds with probability at least 1 - (12/)re-cBk,

(1 - ) x 2  Ax 2  (1 + ) x 2,

(45)

for all x  U.

Proof of Lemma 7. Let m  min{q, d} denote be the rank of X and take a singular value decomposition X = U V T so that U  Rd×m,   Rm×m and V  Rq×m with  diagonal and U U = V V = Im. Next let r := min{r, m} and partition U = [u1, . . . , um] column-wise into

42

matrix Ur  Rd×r , consisting of the first r columns of U and U~r  Rd×(m-r) consisting of the remaining m - r columns (so U~r may be empty). Similarly, let r  Rr×r be the upper diagonal block of  containing the top r singular values, and ~ r  R(m-r)×(m-r) the lower (possibly empty) diagonal block. In particular, we have U  = [Urr, U~r~ r]. Next we define a pair of events Esp and Eta by
Esp := { Urz 2  2 AUrz 2 for all z  Rr } , 
Eta := Au 2  2 for   {r + 1, . . . , m} .

By Lemma 39 Esp holds with probability at least 1 - 24re-cBk/2. Moreover, by Assumption 6 Eta holds with probability at least 1 - (q - r + 1)e-k/4CJL. Hence, by the union bound it suffices to work on the event Esp  Eta and show that (9) holds. Now on the event Esp the k × r matrix AUr is of rank r with singular values at least 1/2, so (AUr)+ spec  2 and (AUr)+(AUr) = Ir is the r × r identity matrix (M + denotes the Moore-Penrose inverse of M ). In addition, on the event Eta writing (~ r,j )j[m-r] diagonal elements of ~ r, given any z = (zj )j[m-r]  Rm-r we have

m

AU~r~ rz 2 = A

~ r,jzj uj

j =r +1

=

m
~ r,jzj Auj

 2

2

j =r +1

2

m
~ 2r,j zj2
j =r +1

  2z2

m

~ 2r,j

 2

z

j =r +1

j (XX ),
jr+1

so

AU~r~ r

2 spec



 2

m j=r+1

j

(XX

).

Hence,

choosing

w



w(A, r)

:=

{w Ur (AUr )+ } ,

inf
w~Rk

w~AX - wX

2 2





=

= 

(wA - w)X

2 2

=

(wA - w)U 

2 2

(wA - w)Urr

2 2

+

(wA - w)U~r~ r

2 2

wUr(AUr)+(AUr)r - wUrr

2 2

+

(wA - w)U~r~ r

2 2

w(Ur(AUr)+A - Id)U~r~ r

2 2

w

2 2

·

(Ur(AUr)+A - Id)U~r~r

2 spec

2

w

2 2

·

Ur(AUr)+AU~r~ r

2 spec

+

r+1(XX)

2

w

2 2

·

(AUr )+

2 spec

·

AU~r~ r

2 spec

+

r+1 (XX )

m

 18

w

2 2

j (XX ),

j=r+1

as required.

References
Achlioptas, D. (2003). Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences, 66(4):671­687.

43

Ailon, N. and Chazelle, B. (2006). Approximate nearest neighbors and the fast JohnsonLindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 557­563.
Aliprantis, C. D. and Border, K. (2013). Infinite Dimensional Analysis: A Hitchhiker's Guide. Springer-Verlag Berlin and Heidelberg GmbH & Company KG.
Anthony, M. and Bartlett, P. L. (2009). Neural network learning: Theoretical foundations. Cambridge University Press.
Arriaga, R. I., Rutter, D., Cakmak, M., and Vempala, S. S. (2015). Visual categorization with random projection. Neural Computation, 27(10):2132­2147.
Arriaga, R. I. and Vempala, S. (1999). An algorithmic theory of learning: Robust concepts and random projection. In 40th Annual Symposium on Foundations of Computer Science (FOCS), pages 616­623.
Baraniuk, R., Davenport, M., DeVore, R., and Wakin, M. (2008). A simple proof of the restricted isometry property for random matrices. Constructive Approximations, 28:253­263.
Baraniuk, R., Davenport, M., DeVore, R., and Wakin, M. (2009). Random projections of smooth manifolds. Foundations of computational mathematics, 9(1):51­77.
Bartlett, P. L., Bousquet, O., and Mendelson, S. (2005). Local Rademacher complexities. The Annals of Statistics, 33(4):1497­1537.
Bentley, A., Rowe, J. E., and Dehghani, H. (2019). Single pixel hyperspectral bioluminescence tomography based on compressive sensing. Biomed. Opt. Express, 10(11):5549­5564.
Biau, G., Devroye, L., and Lugosi, G. (2008). On the performance of clustering in Hilbert spaces. IEEE Transactions on Information Theory, 54(2):781­790.
Boucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: A nonasymptotic theory of independence. Oxford university press.
Boutsidis, C., Zouzias, A., Mahoney, M. W., and Drineas, P. (2015). Randomized dimensionality reduction for k-means clustering. IEEE Trans. Inf. Theory, 61(2):1045­1062.
Balcan, M. F., Blum, A., and Vempala, S. (2006). Kernels as features: On kernels, margins, and low-dimensional mappings. Machine Learning, 65:79­94.
Calderbank, R., Jafarpour, S., and Schapire, R. (2009). Compressed learning: Universal sparse dimensionality reduction and learning in the measurement domain. Technical Report, Rice University.
Cand`es, E. J. and Tao, T. (2006). Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Trans. Inf. Theory, 52(12):5406­5425.
Cannings, T. I. (2020). Random projections: Data perturbation for classification problems. Wiley interdisciplinary reviews., pages 1­15.
44

Cannings, T. I. and Samworth, R. (2017). Random-projection ensemble classification. Royal Statistical Society B, 79:959­1035.
Chen, D.-R. and Li, H. (2014). Convergence rates of learning algorithms by random projection. Applied and Computational Harmonic Analysis, 37(1):36 ­ 51.
Cormode, G. (2017). Data sketching. Queue, 15(2):49­67.
Dasgupta, S. and Gupta, A. (2003). An elementary proof of a theorem of Johnson and Lindenstrauss. Random Structures & Algorithms, 22(1):60­65.
Derezinski, M., Liang, F. T., Liao, Z., and Mahoney, M. W. (2020). Precise expressions for random projections: Low-rank approximation and randomized newton. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 18272­18283. Curran Associates, Inc.
Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289­ 1306.
Duarte, M. F., Davenport, M. A., Takhar, D., Laska, J. N., Sun, T., Kelly, K. F., and Baraniuk, R. G. (2008). Single-pixel imaging via compressive sampling. IEEE Signal Processing Magazine, 25(2):83­91.
Dudley, R. M. (1967). The sizes of compact subsets of Hilbert space and continuity of Gaussian processes. Journal of Functional Analysis, 1(3):290­330.
Durrant, R. J. and Kab´an, A. (2010). Compressed fisher linear discriminant analysis: Classification of randomly projected data. In Proc. 16th ACM SIGKDD international conference on Knowledge Discovery and Data Mining (KDD), pages 1119­1128.
Durrant, R. J. and Kab´an, A. (2015). Random projections as regularizers: learning a linear discriminant from fewer observations than dimensions. Machine Learning, 99(2):257­286.
Fard, M. M., Grinberg, Y., Pineau, J., and Precup, D. (2012). Compressed least-squares regression on sparse spaces. In AAAI.
Feldman, V., Guruswami, V., Raghavendra, P., and Wu, Y. (2012). Agnostic learning of monomials by halfspaces is hard. SIAM Journal on Computing, 41(6):1558­1590.
Freksen, C. B. and Larsen, K. G. (2020). On using Toeplitz and circulant matrices for Johnson­Lindenstrauss transforms. Algorithmica, 82:338­354.
Gian-Andrea Thanei, Christina Heinze, N. M. (2017). Random projections for large-scale regression. In S., S. A., editor, Big and Complex Data Analysis. Contributions to Statistics. Springer, Cham.
Gibson, G. M., Johnson, S. D., and Padgett, M. J. (2020). Single-pixel imaging 12 years on: a review. Optics Express, 28(19):28190­28208.
Gupta, S., Gribonval, R., Daudet, L., and Dokmani´c, I. (2019). Don't take it lightly: Phasing optical random projections with unknown operators. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems 32, pages 14855­14865. Curran Associates, Inc.
45

Halko, N., Martinsson, P.-G., and Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev, 53(2):217­ 288.
Heinze, C., McWilliams, B., and Meinshausen, N. (2016). Dual-loco: Distributing statistical estimation using random projections. In Gretton, A. and Robert, C. C., editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 875­883, Cadiz, Spain. PMLR.
Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771­1800.
Ho, T. K. (1998). The random subspace method for constructing decision forests. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(8):832­844.
Jiao, S., Feng, J., Gao, Y., Lei, T., Xie, Z., and Yuan, X. (2019). Optical machine learning with incoherent light and a single-pixel detector. Opt. Lett., 44(21):5186­5189.
Kab´an, A. (2014). New bounds on compressive linear least squares regression. In Kaski, S. and Corander, J., editors, Proceedings of Machine Learning Research, volume 33, pages 448­456, Reykjavik, Iceland. PMLR.
Kane, D. M. and Nelson, J. (2014). Sparser Johnson-Lindenstrauss transforms. Journal of the ACM (JACM), 61(1):1­23.
Klenke, A. (2013). Probability theory: a comprehensive course. Springer Science & Business Media.
Koltchinskii, V. (2006). Local rademacher complexities and oracle inequalities in risk minimization. The Annals of Statistics, 34(6):2593­2656.
Larsen, K. G. and Nelson, J. (2017). Optimality of the Johnson-Lindenstrauss lemma. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 633­638.
Ledoux, M. and Talagrand, M. (2013). Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media.
Lopes, M. E. (2020). Estimating a sharp convergence bound for randomized ensembles. Journal of Statistical Planning and Inference, 204:35­44.
Lustig, M., Donoho, D., and Pauly, J. (2007). Sparse MRI: The application of compressed sensing for rapid mr imaging. Magn. Reson. Med., 58(6):1182­95.
Maillard, O.-A. and Munos, R. (2012). Linear regression with random projections. Journal of Machine Learning Research, 13(89):2735­2772.
Mammen, E. and Tsybakov, A. B. (1999). Smooth discrimination analysis. Ann. Statist., 27(6):1808­1829.
Massart, P. (2000). Some applications of concentration inequalities to statistics. In Annales de la Facult´e des sciences de Toulouse: Math´ematiques, volume 9, pages 245­303.
46

Massart, P., N´ed´elec, E´ ., et al. (2006). Risk bounds for statistical learning. The Annals of Statistics, 34(5):2326­2366.
Matousek, J. (2008). On variants of the Johnson-Lindenstrauss lemma. Random Structures & Algorithms, 33(2):142­156.
Meintrup, S., Munteanu, A., and Rohde, D. (2019). Random projections and sampling algorithms for clustering of high-dimensional polygonal curves. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.
Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2012). Foundations of machine learning. MIT press.
Nair, V., Menzies, T., and Chen, J. (2016). An (accidental) exploration of alternatives to evolutionary algorithms for SBSE. In Sarro, F. and Deb, K., editors, Search Based Software Engineering, pages 96­111, Cham. Springer International Publishing.
Palmer, A. D., Bunch, J., and Styles, I. B. (2015). The use of random projections for the analysis of mass spectrometry imaging data. J. Am. Soc. Mass Spectrom., 26(2):315­322.
Peressutti, D., Bai, W., Jackson, T., Sohal, M., Rinaldi, A., Rueckert, D., and King, A. (2015). Prospective identification of crt super responders using a motion atlas and random projection ensemble learning. In Navab, N., Hornegger, J., Wells, W. M., and Frangi, A. F., editors, Medical Image Computing and Computer-Assisted Intervention ­ MICCAI 2015, pages 493­500, Cham. Springer International Publishing.
Pilanci, M. and Wainwright, M. J. (2015). Randomized sketches of convex programs with sharp guarantees. IEEE Transactions on Information Theory, 61(9):5096­5115.
Pilanci, M. and Wainwright, M. J. (2016). Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares. Journal of Machine Learning Research, 17(53):1­ 38.
Pilanci, M. and Wainwright, M. J. (2017). Newton sketch: A near linear-time optimization algorithm with linear quadratic convergence. SIAM Journal on Optimization, 27:205­245.
Reboredo, H., Renna, F., Calderbank, A. R., and Rodrigues, M. R. D. (2013). Compressive classification. In 2013 IEEE International Symposium on Information Theory, pages 674­678.
Reboredo, H., Renna, F., Calderbank, A. R., and Rodrigues, M. R. D. (2016). Bounds on the number of measurements for reliable compressive classification. IEEE Trans. Signal Processing, 64(22):5778­5793.
Reeve, H. W. J. and Brown, G. (2017). Minimax rates for cost-sensitive learning on manifolds with approximate nearest neighbours. In Hanneke, S. and Reyzin, L., editors, Proceedings of Machine Learning Research, volume 76, pages 11­56.
Renna, F., Wang, L., Yuan, X., Yang, J., Reeves, G., Calderbank, R., Carin, L., and Rodrigues, M. R. D. (2016). Classification and reconstruction of high-dimensional signals from low-dimensional features in the presence of side information. IEEE Transactions on Information Theory, 62(11):6459­6492.
47

Saade, A., Caltagirone, F., Carron, I., Daudet, L., Dremeau, A., Gigan, S., and Krzakala, F. (2016). Random projections through multiple optical scattering: Approximating kernels at the speed of light. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2016, Shanghai, China, March 20-25, 2016, pages 6215­6219. IEEE.
Sendler, W. et al. (1975). A note on the proof of the zero-one law of Blum and Pathak. The Annals of Probability, 3(6):1055­1058.
Slawski, M. (2018). On principal components regression, random projections, and column subsampling. Electron. J. Statist., 12(2):3673­3712.
Srebro, N. and Sridharan, K. (2010). Note on refined Dudley integral covering number bound. Unpublished results. http://ttic. uchicago. edu/karthik/dudley. pdf.
Tian, Y. and Feng, Y. (2021). RaSE: Random subspace ensemble classification. Journal of Machine Learning Research, 22(45):1­93.
Tsybakov, A. B. (2004a). Introduction a l'estimation non-param´etrique (Introduction to nonparametric estimation). Math´ematiques & Applications (paris). 41.
Tsybakov, A. B. (2004b). Optimal aggregation of classifiers in statistical learning. Annals of Statistics, 32(1):135­166.
Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press.
Yang, Y., Pilanci, M., and Wainwright, M. J. (2017). Randomized sketches for kernels: Fast and optimal nonparametric regression. Ann. Statist., 45(3):991­1023.
Ye, J. C. (2019). Compressed sensing MRI: a review from signal processing perspective. BMC Biomedical Engineering, 1(8).
48

