arXiv:2106.00308v1 [cs.IT] 1 Jun 2021

Fast Splitting Algorithms for Sparsity-Constrained and Noisy
Group Testing
Eric Price, Jonathan Scarlett, and Nelvin Tan
Abstract In group testing, the goal is to identify a subset of defective items within a larger set of items based on tests whose outcomes indicate whether at least one defective item is present. This problem is relevant in areas such as medical testing, DNA sequencing, communication protocols, and many more. In this paper, we study (i) a sparsity-constrained version of the problem, in which the testing procedure is subjected to one of the following two constraints: items are finitely divisible and thus may participate in at most  tests; or tests are size-constrained to pool no more than  items per test; and (ii) a noisy version of the problem, where each test outcome is independently flipped with some constant probability. Under each of these settings, considering the for-each recovery guarantee with asymptotically vanishing error probability, we introduce a fast splitting algorithm and establish its near-optimality not only in terms of the number of tests, but also in terms of the decoding time. While the most basic formulations of our algorithms require (n) storage for each algorithm, we also provide low-storage variants based on hashing, with similar recovery guarantees.
1 Introduction
In the group testing problem, the goal is to identify a small subset S of defective items of size k within a larger set of items of size n, based on a number T of tests. This problem is relevant in areas such as medical testing, DNA sequencing, and communication protocols [2, Sec. 1.7], and more recently, utility in testing for COVID-19 [20, 38].
E. Price is with the Department of Computer Science, University of Texas at Austin (e-mail: ecprice@cs.utexas.edu). J. Scarlett is with the Department of Computer Science, the Department of Mathematics, and the Institute of Data Science, National University of Singapore (e-mail: scarlett@comp.nus.edu.sg). N. Tan is is with the Department of Computer Science, National University of Singapore (e-mail: nelvintan@u.nus.edu).
E. Price was supported in part by NSF Award CCF-1751040 (CAREER). J. Scarlett was supported by an NUS Early Career Research Award.
1

In this paper, we present algorithms for sparsity-constrained (bounded tests-per-item or items-per-test) and noisy variants of group testing with a near-optimal sublinear decoding time, building on techniques recently proposed for the unconstrained noiseless group testing problem [11, 32]. These extensions come with new challenges presented by the infeasibility of the designs in [11, 32] in the sparsity-constrained setting, and the need to handle both false positive and false negative tests in the noisy setting.

1.1 Problem Setup
Let n denote the number of items, which we label as {1, . . . , n}. Let S  {1, . . . , n} denote the fixed set of defective items, and let k = |S| be the number of defective items. To avoid cumbersome notation, we present our algorithms in a form that uses k directly; however, the analysis goes through unchanged when an upper bound k¯  k is used instead, and k¯ replaces k in the number of tests and decoding time.
We are interested in asymptotic scaling regimes in which n is large and k is comparatively small, and thus assume that k = o(n) throughout. We let T = T (n) be the number of tests performed. In the noiseless setting, the i-th test takes the form

Y (i) =

Xj(i),

(1)

jS

where the test vector X(i) = X1(i), . . . , Xn(i)  {0, 1}n indicates which items are are included in the test, and Y (i)  {0, 1} is the resulting observation, indicating whether at least one defective item was included in

the test. The goal of group testing is to design a sequence of tests X(1), . . . , X(T ), with T ideally as small as

possible, such that the outcomes can be used to reliably recover the defective set S with probability close to

one, while ideally also having a low-complexity decoding procedure. We focus on the non-adaptive setting,

in which all tests X(1), . . . , X(T ) must be designed prior to observing any outcomes.

We consider the for-each recovery guarantee; specifically, we seek to develop a randomized algorithm

that, for any fixed defective set S of cardinality k, produces an estimate S such that the error probability

Pe := P S = S

is asymptotically vanishing as n  . For all of our algorithms, only the tests

X (i)

T i=1

will be randomized, and the decoding procedure will be deterministic given the test outcomes.

Notation. Throughout the paper, the function log(·) has base e, and we make use of Bachmann-Landau

asymptotic notation (i.e., O, o, , , ), as well as the notation O(·), which omits poly-logarithmic factors

in its argument.

2

1.1.1 Sparsity-Constrained Setting
In the sparsity-constrained group testing problem [16], the testing procedure is subjected to one of two constraints:
· Items are finitely divisible and thus may participate in at most  = o(log n) tests;
· Tests are size-constrained and thus contain no more than  = o(n/k) items per test.
For instance, in the classical application of testing blood samples for a given disease [15], the -divisible items constraint may arise when there are limitations on the volume of blood provided by each individual, and the -sized test constraint may arise when there are limitations on the number of samples that the machine can accept, or on the number that can be mixed together while avoiding undesirable dilution effects.
It is well known that if each test comprises of (n/k) items, then (min{n, k log n}) tests suffice for group testing algorithms with asymptotically vanishing error probability [9, 1, 33, 27]. Moreover, this scaling is known to be optimal [4]. Hence, the parameter regime of primary interest in the size-constrained setting is  = o(n/k). By a similar argument, the parameter regime of primary interest in the finitely divisible setting is  = o(log n).

1.1.2 Noisy Setting Generalizing (1), we consider the following widely-adopted symmetric noise model:

Y (i) =

Xj(i)  Z,

(2)

jS

where Z  Bernoulli(p) for some p  (0, 1/2), and  denotes modulo-2 addition. While the symmetry assumption may appear to be restrictive, our results and analysis will hold with essentially no change under any non-symmetric random noise model where 0  1 flips and 1  0 flips both have probability at most p.
Throughout the paper, we will focus separately on the sparsity-constrained aspects and noisy aspects. While their joint treatment is also of interest, it was shown in [16] that for finitely divisible items, if the tests are subject to random noise of the form in (2), then the error probability is bounded away from zero regardless of the total number of tests in the finitely-divisible setting with  = o(log k). Thus, at least in most scaling regimes of interest, handling noise and finite-divisibility constraints simultaneously would require changing the noise model and/or the recovery criteria, and we make no attempt to do so. On the other hand, for noisy size-constrained tests, schemes that attain asymptotically vanishing error probability do indeed exist [16]. We still focus on the size-constrained and noisy aspects separately for clarity of exposition, but the two can be combined using our techniques in a straightforward manner, as we briefly discuss in Appendix D.

3

1.1.3 Mathematical and Computational Assumptions
Throughout the paper, we assume a word-RAM model of computation; for instance, with n items and T tests, it takes O(1) time to read a single integer in {1, . . . , n} from memory, perform arithmetic operations on such integers, fetch a single test outcome indexed by {1, . . . , T } and so on.
For simplicity of notation, we assume throughout the analysis that k, n, and  are powers of two. Our algorithm only requires an upper bound on the number of defectives, and hence, any other value of k can simply be rounded up to a power of two. In addition, the total number of items n can be increased to a power of two by adding "dummy" non-defective items, and  can be rounded down without impacting our final scaling laws (we do not seek to characterize the precise constants).
1.2 Related Work
While extensive works have studied the number of tests for various group testing strategies (see [2] for a survey), relatively fewer have sought efficient poly(k log n) decoding time. For the standard noiseless group testing problem, the most relevant existing results come from two recent concurrent works [11, 32], which showed that there exists a non-adaptive group testing algorithm that succeeds with O(k log n) tests and has O(k log n) decoding time. We build on these splitting techniques in this paper (see Figures 1, 3, and 4 below).
For noiseless sparsity-constrained group testing, the most relevant existing results are summarized in Table 1. Our algorithm for finitely divisible items matches that of the COMP algorithm1 in the number of tests when  = (1) (and comes close more generally), while having much lower decoding time. Furthermore, our algorithm for size-constrained tests uses an order-optimal O(n/) number of tests, and has matching O(n/) decoding time.
For noisy non-adaptive group testing under the noise model in (2), the most relevant existing results are summarized in Table 2. Under (n)-decoding time, we note that the references shown are only illustrative examples, and that several additional works also exist with O(k log n) scaling, e.g., [30, 34, 18]. More relevant to our work is the fundamental limitation that the works attaining O(k log n) scaling only attain a quadratic or worse dependence in k in the decoding time (or (n)). On the other hand, GROTESQUE and SAFFRON attain k poly(log n) decoding time, but fail to attain order-optimality in the number of tests.
In a distinct but related line of works, the for-all recovery guarantee (i.e., zero error probability) was considered [10, 25, 31, 22, 11], with typical results for the unconstrained setting requiring O(k2 log n) tests and poly(k log n) decoding time. In particular:
1The COMP algorithm simply labels any item in an negative test as non-defective, and all other items as defective.
4

-sized tests -divis. items

Reference
Lower Bound [36, 17] Gandikota et al. [16]
COMP [16] DD [17]
This Paper Lower Bound [16, 17]
Gandikota et al. [16]
COMP & DD [16, 17] This Paper

Number of tests

 k max

k,

n k

1/

O k2

n k2

1/

O(kn1/ )

1

O k max

k,

n k



O(kn1/ )



n 

O max

n 

log

,

k2 log

n k2

O

n 

O

n 

Decoding time

­

O k2 log

n k2

(n)

(n) O(kn1/ )

­

Construction
­ Explicit Randomized Randomized Randomized
­

O(T )

Explicit

(n)

O

n 

Randomized Randomized

Table 1: Overview of noiseless non-adaptive sparsity-constrained group testing results under the for-each

guarantee.

For entries containing O(·) notation, the results correspond to

1 poly(log n)

error probability, but

more general variants are also available. A construction is said to be explicit if its test matrix can be

computed deterministically in poly(n) time; the results shown for explicit constructions additionally require k = O( n).

· In the finitely divisible setting, [22] gives a lower bound of 

min

2k

k

n, k n -1+k -1+k

and an algorithm

k
that requires O min n, kn -1+k tests and runs in poly(k) + O(T ) time in the case of -divisible

items,

and

a

lower

bound

of



k

n 

and

an

algorithm

that

requires

T

=

O

k

n 

tests and runs in

poly(k) + O(T ) time in the case of -sized tests.

· In a setting with adversarial noise, recovery guarantees were given in [11, Thms. 3.8 and 3.9] with a constraint on the number of false positive tests or false negative tests. It was left open how to handle both false positives and false negatives simultaneously.

Under all variants of the group testing problem that we consider, the stronger for-all guarantee comes at the price of requiring considerably more tests. Thus, the two types of guarantee are both of significant interest but not directly comparable, and we omit direct comparisons.
Finally, we briefly mention that studies of sublinear-time decoding are prevalent in related problems such as sparse recovery [14, 19, 5, 26] and the heavy hitters problem [13, 12, 28]. While algorithms for such settings typically do not transfer directly to the group testing problem, we detail one relatively direct approach for the noisy setting in Appendix D, and contrast it with our own. In addition, we note that our work builds primarily on [11, 32], which in turn built on tree-based algorithms such as [13, 26].

1.3 Summary of Results

Here we informally summarize our main results, formally stated in Theorems 3, 1, and 2.

·

Finitely

divisible

items:

A

special

case of

our

result states

that

for any n

=

1 poly(log

n)

,

there

exists

5

Reference
Lower Bound [30] Inan et al. [23]
Inan et al. (fast) [24] NCOMP & NDD [9, 35]
GROTESQUE [7] SAFFRON [29] BMC [6]
This Paper

Number of tests



k log

n k

O(k log n)

O(k log n)

O(k log n)

O(k · log k · log n) O(k · log k · log n)
O(k log n)

O(k log n)

Decoding time

­

(n)

O k3 · log k + k log n

(n) O k(log n + log2 k)

O(k · log k · log n)

O(k2 · log k · log n)

O

k

log

n k

1+

Construction
­ Explicit Explicit Randomized Randomized Randomized Randomized
Randomized

Table 2: Overview of noisy non-adaptive group testing results under the for-each guarantee and the noise model in (2). A construction is said to be explicit if its test matrix can be computed deterministically in poly(n) time, and in the final row, is an arbitrarily small positive constant.

a non-adaptive group testing algorithm that succeeds with probability 1-O(n) using O kn1/ tests and O kn1/ decoding time provided that  = (1). The case of finite  will also be handled with only slightly worse scaling laws, and we will specify the precise dependence on n, without resorting to O(·) notation.

· Size-constrained tests: For any  > 0, there exists a non-adaptive group testing algorithm that succeeds with probability 1 - O n- using O n/ tests and O n/ decoding time.

· Noisy setting: For any parameters t = O(1) and  (1/t, 1), there exists a non-adaptive group testing

algorithm that succeeds with probability 1-O

k

log

n k

1- t

using O(k log n) tests and O

k

log

n k

1+

decoding time.

We observe that in the sparsity-constrained setting, our decoding time matches the number of tests, whereas previous algorithms using the same number of tests incurred (n) decoding time. Similarly, in the noisy setting, we significantly improve on the best previous known decoding time among any algorithm using an order-optimal O(k log n) number of tests. Specifically, [6] incurred a quadratic dependence on k, whereas we incur a near-linear dependence.
While our focus is on the number of tests and decoding time, another important practical consideration is the storage required. Naively, the algorithms attaining the above results require (n) storage. However, in Appendix E, we discuss storage reductions via hashing, attaining identical results with sublinear storage in the size-constrained and noisy settings, and similar (but slightly weaker) results in the finitely divisible setting.

6

2 Algorithm for Finitely Divisible Items
Our algorithm (both here and in subsequent sections) resembles the non-adaptive binary splitting approach of [11, 32]. At a high level, we form large groups of items and recursively split them into smaller sub-groups, then randomly place groups into tests. The decoder works down the resulting tree (see Figure 1), eliminating groups that are believed to be defective based on the test outcomes, while recursively handling all remaining groups.
We highlight the following differences compared to the binary splitting approach [11, 32]:
· We use a shorter tree of height   . This is because a given item is placed in a single test at each level, so the assumption  = o(log n) prohibits us from having O(log n) levels. We consider    so that the remaining budget can be used at the final level, and we later optimize  to minimize the number of tests.
· In view of the shorter height, we use non-binary splitting; this was considered under adaptive testing in [36, 17], and our algorithm can be viewed as a non-adaptive counterpart, in the same way that [11, 32] can be viewed as a non-adaptive counterpart of Hwang's binary splitting algorithm [21].
· In contrast to the unconstrained setting, we cannot readily use the idea of using N sequences of tests at each level while only increasing the number of tests by a factor of N = O(1). Here, such an approach turns out to be highly wasteful in terms of its use of the limited  budget, and we avoid it altogether.
· At the top level of the tree (excluding the root), we use individual testing (i.e., each node has its own test). This guarantees that no non-defective node from the second level can "continue" down the tree, which simplifies our analysis.
2.1 Description of the Algorithm
The levels of the tree, summarized in Figure 1, are indexed by l = 0, 1, . . . ,  . Since testing at the root is not informative (we will always get a positive outcome), we start our testing procedure at l = 1 (the second
 -1
level of nodes in Figure 1). We choose2 M = (n/k)  , Tlen = Ck(n/k)1/ and Tlen =  k(n/k)1/ , where C is a constant. Here the choice of M is taken to match the near-optimal adaptive splitting algorithm of [36], and the choices of Tlen and Tlen are motivated by the goal of having a number of tests matching the COMP algorithm (see Table 1). Under these preceding choices, the total number of tests (excluding the last
2Here and subsequently, we assume for notation convenience that (n/k)1/ and (n/k)1/ are integers. Since we focus on scaling laws, the resulting effect of rounding has no impact on our results.
7

=1 
1
-1  =  1

 
1
-1 1

/ len
len len

× ( -  + 1)

1
Figure 1: Tree structure of our algorithm. After the first level, the branching factor is M  . -1

Algorithm 1 Testing procedure for -divisible items

Require: Number of items n, number of defective items k, divisibility of each item , and parameters  ,

M , Tlen, Tlen, and Tlen 1: At l = 1, test each node separately in a single test (no randomization).

2: for each l = 2, 3, . . . ,  - 1 do

3: if l =  - 1 then form a sequence of tests of length Tlen.

4: else form a sequence of tests of length Tlen.

5:

for

j

=

1, 2, . . . ,

n M

(M

)(l-1)/(

-1)

do

6:

Place all items from Gj(l) into a single test within the sequence just formed, chosen uniformly at

random.

7: For l =  , form  -  + 1 sequences of tests, each of length Tlen. 8: for each singleton at the final level do

9: for each of the  -  + 1 sequences of tests do

10: Place the item in one of the tests in the sequence, chosen uniformly at random.

level) is given by

n

+ · Ck

n

1
 + k

n

1
 =O

k

n

1 

.

(3)

M

k

k

k

l=1

l=2,..., -2

l= -1

The overall testing procedure is described in Algorithm 1, and the decoding procedure is described in Algorithm 2. The j-th node at the l-th level is again written as Gj(l).
Here and subsequently, we assume that   3. We note that the case  = 1 is trivial, and while  = 2 could be handled by omitting the step at level l =  containing Tlen tests, this variant is omitted for the sake of brevity.

2.2 Algorithmic Guarantees
Theorem 1. (Algorithmic guarantees) Let S be a fixed (defective) subset of {1, . . . , n} of cardinality k, and let  = o(log n) (with   3) be the maximum number of times each item can be tested, and fix   {3, . . . , }

8

Algorithm 2 Decoding procedure for -divisible items

Require: Outcomes of T non-adaptive tests, number of items n, number of defective items k, divisibility of

each item , and parameters  , M , Tlen, Tlen, and Tlen

1: Initialize PD(lmin) =

G (lmin )
j

n/M j=1

,

where

lmin

=

1.

2: Place all nodes at l = 1 with a positive test outcome into PD(lmin).

3: for l = 2, 3, . . . ,  - 1 do 4: for each group G  PD(l) do

5: Check whether the single test corresponding to G is positive or negative.

6:

if the test is positive then add all M 1/( -1) children of G to PD(l+1)

7: Let the estimate S of the defective set be the elements in PD( ) that are not included in any of the

negative tests from the remaining ( -  + 1)Tlen tests. 8: Return S.

h 0.0 0.2 0.4 0.6 0.8 1.0

DD & Converse COMP Splitting (g =4) Splitting (g =10)

0.0

0.2

0.4

0.6

0.8

1.0

Sparsity parameter q

Figure 2: Plot of the asymptotic quantity  (see (6)) against the sparsity parameter  for the converse (i.e., the lower bound) [17], the DD algorithm [17], the COMP algorithm [16], and our splitting algorithm (with  = 4 and  = 10).
and any function n decaying as n increases. There exist choices3 of Tlen, Tlen, and Tlen such that with

T = O k max

n

1
,

k

n 1
- +1

1  (- +1)

,

(4)

k

n

k

the preceding algorithm satisfies the following with probability at least 1 - O(n) - e-(k): · The returned estimate S equals S;

· The decoding time is4 O k(n/k)1/ .

The proof of Theorem 1 is given in Appendix A.

In order to better understand this bound on T , we consider k =  n for some   [0, 1), and n =

1 poly(log

n)

.

These

choices

allow

us

to

hide

the

dependence

on

n

in

O(·)

notation

and

focus

on

the

remaining

1

1

3Specifically, we will set Tlen = O k(n/k)1/ , Tlen =  k(n/k)1/ , and Tlen = k(k/n) - +1 (n/k)  (- +1) .

4In certain scaling regimes, this decoding time may be lower than the number of tests. This is because the algorithm

sequentially decides which tests outcomes to observe, and does not necessarily end up observing every outcome.

9

terms. Substituting k =  n into (4), we obtain

T = O k max

n , n 1- 

 -

+1 + 

1- (- +1)

.

(5)

Momentarily ignoring the integer constraint on  , we obtain the optimal 

by solving

1- 

=

 - +1

+



1- (- +1)

,

which

simplifies

to



= (1 - ). Substituting 

= (1 - ) back into (4) gives T = O

kn1/

.

In addition, by the same substitution, we obtain O kn1/ decoding time. In this case, the bound on T is

the same as the bound for the COMP algorithm (see Table 1).

When  = (1), it is straightforward to establish that the integer constraint on  does not impact the

above findings. However, when  = O(1), we need to account for the integer constraint. One could naively

search over   {3, . . . , }, but in Appendix A.4, we use a convexity argument to show that considering

  {3, (1 - ) , (1 - ) } is sufficient.

To see how our algorithm compares to optimal behavior established in [17] (i.e., an upper bound for

the DD algorithm, and a matching algorithm-independent lower bound) and the COMP algorithm [16] for

different values of , we introduce the following quantity:

log  = lim
n  log

n k T k

.

(6)

Observe that for any fixed value of  > 0, re-arranging gives T  k

n k

1/

(1+o(1))/ .

With  defined, we

compare the performance in Figure 2. We observe that the splitting algorithm's curve quickly gets closer to

the COMP algorithm's curve even for fairly low values of . On the other hand, matching the DD algorithm's

curve with sublinear decoding time remains an interesting open challenge for future work.

3 Algorithm for Size-Constrained Tests
In the case of size-constrained tests, we again modify the tree structure (see Figure 3), and the main differences from the standard noiseless algorithm [11, 32] are as follows:
· The first level after the root is chosen to have groups of size , since larger groups are prohibited. In addition, at this level with nodes of size , we test each node individually, guaranteeing that we only "continue" down the tree for defective nodes at that level.
· We use non-binary splitting, geometrically decreasing the node size at each level until the final level with size one. We limit the number of levels to be O(1), whereas binary splitting would require O(log )

10

=0  = 1

 
1

/ × X  × X

Figure 3: Tree structure of our algorithm. After the first level, the branching factor is 1/C . levels, and (at least when using a similar level-by-level test design) would increase the number of tests by an O(log ) factor.

· We do not independently place nodes into tests, since doing so would cause a positive probability of

violating the -sized test constraint. Instead, at each level, we create a random testing sub-matrix with

a column weight

of

exactly one,

and

a

row

weight exactly

equal to to

 node

size

.

A

similar

doubly-constant

test design was also adopted in [16], but without the tree structure.

We now proceed with a more detailed description.

3.1 Description of the Algorithm

Our algorithm works with a tree structure (see Figure 3) similar to previous sections. The j-th node at the l-th level is again denoted by Gj(l). A distinction here as that the tree only has a constant depth, with the final index denoted by C = O(1); hence, the splits are 1/C -ary.5 More importantly, there are key differences

in the allocation of items to tests, which we describe as follows.

At each level l, we perform N independent iterations to boost the error probability, as mentioned above.

Within each iteration, we make use of a random matrix, which we write as Xl = x(til)  {0, 1}#tests×#nodes

(the dependence on the iteration number is left implicit), where #tests = n/ and #nodes =

. n
1-l/C

We

pick Xl

by sampling uniformly from all

× n

n



1-l/C

matrices with exactly l/C

nodes per test (i.e.,

a row

weight of l/C), and each node sampled exactly once (i.e., a column weight of one). These choices ensure

that each test contains at most  items, as required. The column weight of one is not strictly imposed by

the testing constraints, but helps in avoiding "bad" events where some nodes are not tested.

With this notation in place, the testing procedure is formally described in Algorithm 3, and the decoding

procedure is described in Algorithm 4.

5For notational convenience, we assume that 1/C is an integer. Since we already assumed that  is a power of two, if  = O(1), then it will suffice to let C be that power (see Lemma 7, in which we handle the case  = O(1) separately). Otherwise, if  = (1), then the rounding is insignificant since C = O(1).

11

Algorithm 3 Testing procedure for -sized tests

Require: Number of items n, number of defective items k, and maximal test size 

1: At level l = 0 (see Figure 3), perform an individual test for each node.

2: for each l = 1, . . . , C - 1 (for some constant C chosen later) do

3: for each iteration in {1, . . . , N } (for some constant N  1) do

4:

Pick

a

new

Xl

of

size

n 

×

, n
1-l/C

with

column

weight

1

and

row

weight

l/C .

5:

for each row t in Xl do

6:

Conduct a single test containing the nodes Gj(l) with x(tjl) = 1.

Final level:

7: for each iteration in {1, . . . , C } (for some constant C chosen later) do

8:

Pick

a

new

XC

of

size

n 

× n,

with

column

weight

1

and

row

weight

.

9: for each row t in XC do

10:

Conduct a single test containing the (singleton) nodes Gj(l) with x(tjl) = 1.

Algorithm 4 Decoding procedure for -sized tests

Require: Outcomes of T non-adaptive tests, number of items n, number of defective items k, and maximal

test size  1: Initialize PD(0) = {Gj(0)}nj=/1 2: for each group G  PD(0) do 3: if the single test of G is positive then add all children of G to PD(1)

4: for l = 1, . . . , C - 1 do

5: for each group G  PD(l) do

6:

if all N tests of G are positive then add all children of G to PD(l+1)

7: Let the estimate S be the set of elements in PD(C) that are not included in any negative test at the final

level.

8: Return S

3.2 Algorithmic Guarantees

We are now ready to state our main result for the case of size-constrained tests. In this case, we slightly

strengthen the assumption k = o(n) to k = n1-(1), and we slightly strengthen the assumption  = o

n k

(see the discussion following (2)) to  = (n/k)1-(1). These additional restrictions only rule out scaling

regimes

that

are

very

close

to

linear

(e.g.,

k

=

n log

n

),

and

were

similarly

imposed

in

[16].

Theorem 2. (Algorithmic guarantees) Let S be a (defective) subset of {1, . . . , n} of cardinality k = O n1- 1 for some 1  (0, 1] and the test size constraint be  = O (n/k)1- 2 for some 2  (0, 1]. For any  > 0, there exist choices of C, C , N = O(1) such that with O n/ tests, the preceding algorithm satisfies the following with probability 1 - O n- :

· The returned estimate S equals S;

· The decoding time is O n/ .

The proof of Theorem 2 is given in Appendix B. As summarized in Table 1, this is the first algorithm to attain O n/ scaling in both the number of tests and the decoding time.

12



 = log2  /

/

 = log2  1

1

 × len

 log2  ×

len

Figure 4: Tree structure of our algorithm for the noisy setting.

4 Algorithm for the Noisy Setting

For the unconstrained noisy setting, we revert to binary splitting (see Figure 4), as was used in [11, 32], though in Appendix D we also outline a non-binary approach that follows one used for the heavy hitters problem [12, 26]. The main difference between our noisy algorithm and [11, 32] is that when deciding whether a given node is defective or not, we look several levels further down the tree, instead of only considering the single test outcome of the given node. This complicates the analysis, and leads to a small increase in the decoding time. Additionally, in order to reduce the effective noise level, each node in the tree is placed in multiple tests, rather than just one.

4.1 Description of the Algorithm

Following [11, 32], our algorithm considers a tree representation (see Figure 4), in which each node corre-

sponds to a set of items. The levels of the tree are indexed by l = log2 k, . . . , log2 n and the j-th node at

the l-th level is denoted by Gj(l)  {1, . . . , n}.

At the top level we have |Gj(log2 k)| =

n k

,

and

the

sizes

are

subsequently halved until the final level with |Gj(log2 n)| = 1.

The algorithm works down the tree one level at a time, keeping a list of possibly defective (PD) nodes,

and performing tests to obtain such a list at the next level. When we perform tests at a given level, we

treat each node as a "super-item"; including a node in a test amounts to including all of the items in the corresponding node Gj(l). In addition, for the tree illustrated in Figure 4, we refer to nodes containing at least one defective item as defective nodes, to all other nodes as non-defective nodes, and to the sub-tree of

defective nodes as the defective tree.

The testing is performed as follows: At each level of the tree, N sequences of tests are formed, each

having length Tlen (i.e., a total of N Tlen tests per level). For each node and each of the N sequences, the

node is placed into a single test, chosen uniformly at random among the Tlen tests.

We define the intermediate label and final label of a given node as follows:

· The intermediate is formed via majority voting of the N tests the node is included in.

13

Algorithm 5 Testing procedure for the noisy setting

Require: Number of items n, number of defective items k, and parameters N , C, and C

1: Initialize Tlen = Ck

2: for each l = log2 k, . . . , log2 n - 1 do 3: for each iteration in {1, . . . , N } do

4:

Form a sequence of tests of length Tlen

5:

for j = 1, 2, . . . , 2l do

6:

Place all items from Gj(l) into a single test in the sequence just formed, chosen uniformly at random.

7: At level l = log2 n, form C N log2 n sequences of tests, each of length Tlen.

8: for each singleton at the final level do

9: for each of the C N log2 n test sequences do

10:

Place the singleton in one of the Tlen tests in the sequence, chosen uniformly at random.

· To obtain the final label of a given node, we look at the intermediate labels of all nodes up to r levels below the given node. If there exists any length-r path below the given node with more than r/2 positive intermediate labels, then we assign the node's final label to be positive. Otherwise, we assign it to be negative.
According to the tree structure in Figure 4, once we reach the later levels, there may be fewer than r levels remaining. To account for such cases, we simply ensure that sufficiently many tests are performed at the final level so that a length-r "path" can be formed (here, no further branching is done, and each "node" is the same singleton).
With the above notation and terminology in place, the overall test design is described in Algorithm 5, and the decoding procedure in Algorithm 6.

4.2 Algorithmic Guarantees

Theorem 3. (Algorithmic guarantees) Let S be a (defective) subset of {1, . . . , n} of cardinality k = o(n).

For any constants > 0 and t > 0 satisfying t > 1, there exist choices of C, C , N = O(1) and r = O(log k +

log log n) such that with O k log n tests, the preceding algorithm satisfies the following with probability at

least 1 - O

k

log

n k

1- t

:

· The returned estimate S equals S;

· The decoding time is O

k

log

n k

1+

.

The proof of Theorem 3 is given in Appendix C.

14

Algorithm 6 Decoding procedure for the noisy setting

Require: Outcomes of T non-adaptive tests, number of items n, number of defective items k, and parameters

N , C, C , and r

1: Initialize PD(lmin) =

G (lmin )
j

kj=1, where lmin = log2 k.

2: for l = log2 k, . . . , log2 n - 1 do

3: if l  log2 n - r (i.e., there are at least r levels below the node) then

4:

for each group G  PD(l) do

5:

Evaluate the intermediate labels of all nodes r levels below G.

6: else if l > log2 n - r (i.e., there are fewer than r levels below the node) then

7:

for each group G  PD(l) do

8:

Evaluate the intermediate labels of all nodes all levels below G except the final level.

9:

for each node reached at the final level do

10:

Iterate through the C N log2 n test outcomes in batches of size N : Conduct a majority vote for

each batch to obtain an intermediate label for the node.

11:

Use intermediate labels from each node in the final level to make up paths of length r (see Section

4.1). 12: for each group G  PD(l) do

13: If  a path with more than r/2 positive intermediate labels, then assign G's final label to be positive.

Otherwise, assign G's final label to be negative.

14:

If the final label of G is positive, then add both children of G to PD(l+1).

15: At the final level, for each node (singleton), repeat step 10 to obtain C log2 n intermediate labels for the node, and conduct a majority vote for the node's intermediate labels to obtain its final label.
16: Return S containing the elements of singletons in PD(log2 n) with a positive final label.

5 Conclusion

We have provided fast splitting algorithms for sparsity-constrained and noisy group testing, maintaining the

near-optimal number of tests provided by earlier works while also attaining a matching or near-matching

decoding time. Possible directions for future research include (i) in the finitely divisible setting, match the

number of tests used by the DD algorithm (see Table 1) with sublinear decoding time, and (ii) in the noisy

setting, further reduce the

k

log

n k

1+

runtime, ideally bringing it all the way down to O(k log n).

Appendix
A Proof of Theorem 1 (Finitely Divisible Items)
Throughout the analysis, the defective set S is fixed but otherwise arbitrary, and we condition on fixed placements of the defective items into tests (and hence, fixed test outcomes and a fixed defective tree). The test placements of the non-defective items are independent of those of the defective items, and our analysis will hold regardless of which particular tests the defectives were placed in. The defective test placements are written as TS , and we write P[· | TS ] to denote the conditioning.

15

We proceed with three lemmas that follow analogous steps to [32]. At level l = 1, the probability of a non-defective node being placed in a positive test is zero, because each node is placed in its own individual test. As for levels l  {2, . . . ,  - 2}, we proceed with the following simple lemma.
Lemma 1. (Probabilities of Non-Defectives Being in Positive Tests) Under the above test design, the following holds at any given level l = 2, . . . ,  - 2: Conditioned on any defective test placements TS , any given non-defective node at level l has probability at most (1/C)(n/k)-1/ of being placed in a positive test.
Proof. Since there are k defective items, at most k nodes at a given level can be defective. Hence, since each node is placed in a single test, at most k tests out of the Ck(n/k)1/ tests at the given level can be positive. Since the test placements are independent and uniform, it follows that for any non-defective node, the probability of being in a positive test is at most k/Tlen = k/ Ck(n/k)1/ = (1/C)(n/k)-1/ .
In view of this lemma, when starting at any non-defective child of any defective node, we can view any further branches down the non-defective sub-tree as "continuing" (i.e., the M 1/( -1) children are marked as possibility defective) with probability at most (1/C)(n/k)-1/ , in particular implying the following.
Lemma 2. (Probability of Reaching a Non-Defective Node) Under the setup of Lemma 1, any given nondefective node at distance  from the defective tree is reached (i.e., all of its ancestors are placed in positive tests, so the node is considered possibly defective) with probability at most (1/C)-1(n/k)(1-)/ .
We will use the preceding lemmas to control the quantity Ntotal, defined to be the total number of nondefective nodes that are reached --in the sense of Lemma 2--among levels l  {2, . . . ,  - 1}. It will be useful to upper bound Ntotal for the purpose of controlling the overall decoding time and the number of items considered at the final level.

A.1 Bounding Ntotal
We first present a lemma bounding the average of Ntotal.

Lemma 3. (Bounding Ntotal on Average) For any parameters C > 1 and  > 1, and any defective test
 -1
placements TS , under the choice M = (n/k)  , we have

E[Ntotal|TS ] = O

k

n k

1 

.

(7)

Proof. At level l = 1, we use n/M tests for individual nodes. This results in correct identification of the non-defective nodes, guaranteeing that they will not "continue" to branch. Hence, at level l = 1, we trivially upper bound the number of non-defective nodes by n/M .

16

For the remaining levels l = 2, . . . ,  - 1, all splits are M 1/( -1) -ary, and each defective node can have

at most M /( -1) descendants at distance . Since there are at most  k defective nodes in total among

levels l = 1, . . . ,  - 1, it follows that there are at most  kM  -1 non-defective nodes at distance  from

defective nodes starting at those levels. Furthermore, we established in Lemma 2 that a distance of  gives

a probability of at most

1 C

-1

n k

(1-)/

of being reached. This gives


E[Ntotal|TS ] 


 kM  -1

1 C

-1 n k

1-
+ n  M

=1


1
=  kM  -1

-1
M  -1

1

-1

n

1-
+ n 

C

k

M

=1

(a)

1

1

  kM  -1

1

1 - M  -1

1 C

n -1/ k

(=b)  k

n

1 

1

+k

n

1
,

k 1 - 1/C

k

n +
M

(8) (9) (10) (11)

where (a) applies the geometric series formula (increasing the upper limit of the sum from  to ), and (b)
 -1
follows by substituting M = (n/k)  .
We now wish to move from a characterization of the average to a high-probability characterization. At this point, we depart somewhat further from the analysis of [32], which is based on branching process theory, and appears to yield suboptimal results in the case that the tree's branching factor scales as (1).
We introduce the following definition, in which we refer to a full m-ary tree as a tree where every internal node has exactly m children.
Lemma 4. [3, Prop. 3.1] (Fuss-Catalan Numbers) For natural integers m, n  2, the order-m Fuss-Catalan number

Catnm

=

(m

-

1 1)n

+

1

mn n



mn n

 (em)n,

(12)

is the number of full m-ary trees with exactly n internal nodes.
We note that the Catalan numbers also played an important role in the analysis of the unconstrained setting it [11], but were used in a rather different manner that we were unable to extend to obtain a result comparable to Theorem 1. In the proof of the following lemma, these are used in a counting argument in order to establish the sub-exponential behavior of the random variable Ntotal.
Lemma 5. (High Probability Bound on Ntotal) For any parameters C  e2 and  > 1, and any defective
 -1
test placements TS , under the choice M = (n/k)  , we have Ntotal = O  k(n/k)1/ with probability

17

1 - e-( k).
Proof. Consider a single non-defective sub-tree following a defective node, and let Nb be the number of nodes in the sub-tree such that itself and all its ancestors only appear in positive tests (i.e., the number of nodes that lead to further branching). We have

P[Nb = nb]  P[ a full M 1/( -1)-tree reached with nb internal nodes]

(a)
 (#full M 1/( -1)-trees with nb internal nodes) ·

1 C

n

-

1 

k

nb

(b)


eM 1/( -1) nb

1

n

-

1 

nb

Ck

(c)
=

e

nb

(d)


e-nb ,

C

(13) (14) (15) (16)

where (a) applies Lemma 1 and the union bound, (b) applies Lemma 4, (c) is obtained by substituting
 -1
M = (n/k)  and simplifying, and (d) holds since C  e2. This implies that Nb is a sub-exponential random variable. Since we have at most ( - 1)k defective nodes in levels l = 1, . . . ,  - 1, we are
adding together O( k) independent copies of such random variables (each corresponding to a different nondefective sub-tree following a defective node).6 Letting Nb(i) denote the i-th copy, we can apply a standard concentration bound for sums of independent sub-exponential random variables [37, Prop. 5.16] to obtain

P Nb(1) + · · · + Nb(O( k))  E[Nb(1) + · · · + Nb(O( k))] + t|TS  exp

 min

t2 ,t
k

.

(17)

Setting t = ( k), we get

P[Nb(1) + · · · + Nb(O( k))  E[Nb(1) + · · · + Nb(O( k))] + ( k)|TS ]  e-( k).

(18)

Recall that each Nb(i) only counts "internal" nodes, whereas Ntotal also counts leaves, so passing from the former to the latter requires multiplying by the branching factor M 1/( -1) = (n/k)1/ . Multiplying on both sides inside the probability in (18) accordingly, we obtain

n 1/ P Ntotal  E[Ntotal] +   k k

TS  e-( k).

(19)

Substituting E[Ntotal] = O  k(n/k)1/ (see Lemma 3) into (19), we obtain the desired result.
We now briefly consider level l =  - 1, which uses Tlen =  k(n/k)1/ tests (see Figure 1). Since
6We do not consider the non-defective nodes at level l = 1, because they are guaranteed to be identified correctly as a result of individual testing of nodes.

18

|PD( -1)|  Ntotal + k holds trivially, Lemma 5 implies that |PD( -1)| = O  k(n/k)1/ with probability 1 - e-( k). Using the same argument as Lemma 1, the probability of a non-defective node being in a positive test at level l =  - 1 is at most k/Tlen = (1/ )(n/k)-1/ . Hence, conditioned on |PD( -1)| = O  k(n/k)1/ , the number of non-defective nodes placed in a positive test is stochastically dominated by

n 1/ 1 n -1/

Binomial O  k

,

.

(20)

k

k

By a multiplicative form of Chernoff bound, the number of such non-defective nodes in PD( -1) is O(k) with probability at least 1 - e-(k). Since the branching factor is (n/k)1/ , it follows that the number of non-defective nodes in PD( ) behaves as O(k(n/k)1/ ).

A.2 Analysis of the Final Level

Recall that at the final level, we perform  -  + 1 independent sequences of tests of length Tlen, with each item being randomly placed in one of these Tlen tests. Conditioned on the high probability event that |PD( )| = O(k(n/k)1/ ), we study the required Tlen for a vanishing error probability. Specifically, we upper bound the error probability by O(n) for some decaying function n  0 as n  .
For a given non-defective item and a given sequence of Tlen tests, the probability of colliding with any defective item is at most k/Tlen by the same argument as Lemma 1. Due to the  -  + 1 independent repetitions, the probability of a given non-defective item appearing only in positive tests is at most (k/Tlen)- +1. By a union bound over O(k(n/k)1/ ) non-defective items at the final level, we find that the estimate S differs from S with (conditional) probability O k(n/k)1/ (k/Tlen)- +1 . The error probability is thus upper bounded by O(n) provided that

k

n

1 

k

 Tlen  k

k - +1

Tlen

 n

k

1 - +1

n

1
.  (- +1)

n

k

1

1

Hence, we set Tlen = k(k/n) - +1 (n/k)  (- . +1)

(21) (22)

A.3 Number of Tests, Error Probability, and Decoding Time

· Number of tests: For l = 1, . . . ,  - 1, we used a total of n/M + C( - 3)k(n/k)1/ +  k(n/k)1/

tests, which scales as O  k(n/k)1/

 -1
by substituting M = (n/k)  and C = O(1).

For

1

1

the final level, we used ( -  + 1)Tlen = O k(k/n) - +1 (n/k)  (- +1) tests, since Tlen =

19

1

1

k(k/n) - +1 (n/k)  (- . +1) Combining these, we obtain

T = O k max

n

1
,

k

n 1
- +1

1  (- +1)

.

k

n

k

(23)

· Error probability: The concentration bound on Ntotal (see Lemma 5) holds with probability 1 - e-( k), and at level l =  - 1, we incur e-(k) error probability. Furthermore, the final stage incurs O(n) error (conditional) probability. In total, we incur n + e-( k) + e-(k) = O(n) + e-(k) error probability.

· Decoding time: We claim that conditioned on the high-probability events above (in particular,

Ntotal = O  k(n/k)1/ ), the decoding time is O k(n/k)1/ . Since we consider the word-RAM

model, it takes constant time to check whether each defective node or non-defective node is in a positive

or negative test. First considering the levels l = 2, . . . ,  - 1, we reached Ntotal = O  k(n/k)1/ non-defective nodes and O( k) defective nodes, which leads to a total of O  k(n/k)1/ decoding

time.

At level

l = 1,

we

iterate through

n M

=O

k(n/k)1/

nodes, and at the final level l =  , for

each of the O k(n/k)1/ relevant leaf nodes, we perform  -  + 1 = O() checks of tests for a total

time of O k(n/k)1/ . Combining these terms, we deduce the desired claim.

A.4 Note on Optimizing 

We note that the function f ( ) = max

1- 

,

 -

+1

+



1- (- +1)

is convex on [3, ]; this is easily proved

by computing the second derivative of each term in max{., .}. Since a convex function is monotone on either

side of its minimum (in this case (1 - )), it follows that the optimal choice of  is given by

 = argmin

k max n , n 1- 

 -

+1 + 

1- (- +1)

 {3,...,}



3  =

 

argmin



 { (1-) , (1-) }

max

1- 

,

 -

+1

+



1- (- +1)

if (1 - ) < 3 otherwise.

That is, we can simply evaluate the objective for three values of  , rather than all values.

(24) (25)

B Proof of Theorem 2 (Size-Constrained Tests)
We start at level l = 0 (see Figure 3), where we note that the probability of a non-defective node being placed in a positive test is zero because each node is placed in its own individual test. For subsequent levels,
20

we proceed with the following lemma.

Lemma 6. (Probabilities of Non-Defectives Being in Positive Tests) Under the above test design, for any given level l = 1, . . . , C and any given iteration indexed by {1, . . . , N }, each non-defective node has probability at most k/n of being placed in a positive test.

Proof. At any given iteration of level l, the probability that a non-defective node u collides (i.e., is in the same test) with a given defective node v is

#matrices with u & v in test 1 (a) =
#matrices with v in test 1 =

n 1-l/C

-2

l/C -2

n 1-l/C

-1

l/C -1

n 1-l/C

-2

l/C -2

n 1-l/C

-1

l/C -1

n/-1 i=1
n/-1 i=1

n 1-l/C

-il/C

l/C

n 1-l/C

-il/C

l/C

(b) l/C - 1

=

- 1 n
1-l/C

 l/C - 1 (c) 

= n

l/C - /n

, n

(26) (27) (28)

where:

·

(a)

follows

by

considering

the

rows

of

the

matrix

Xl

(of

size

n 

×

, n
1-l/C

column

weight

one,

and

row

weight l/C) sequentially to count the number of possible matrices. For the numerator, we start with

the first row, where u and v collide. The number of ways to fill this row (i.e., assigning items to this

test) is the first term in the numerator. For the remaining n/ - 1 rows, in any particular order, the

number of ways to fill those rows (while maintaining column weights of one) is represented by the

second product term. The same analysis is then repeated for the denominator.

· (b) follows by expanding the binomial coefficient in terms of factorials, and then simplifying.

· (c) follows from the fact that /n  1.

Since there are at most k defective nodes, by the union bound, we find that the probability that a nondefective node collides with any defective node is at most k/n.

The following technical lemma will also be used on several occasions.

Lemma 7. For any k and  satisfying k = O n1- 1 for some 1  (0, 1] and  = O (n/k)1- 2 for some 2  (0, 1], we have the following:

·

For

sufficiently

large

C,

we

have

k1/C n/

= n-(1);

·

For any 1 > 0 , we have for sufficiently large C and N that 1/C

k n

N = O(n-1 ).

21

In addition, if  = O(1), then the same holds true for any fixed C > 0, only requiring N to be sufficiently large in the second part.

Proof. For the first part, we write

n/ =

n/k

(=a) 

k1/C 1+1/C

n 2-

1- C

2

k

(=b) 

n 1(

2-

1- C

2

)

,

(29)

where (a) is by substituting  = O (n/k)1- 2 and simplifying, and (b) is by substituting k = O n1- 1 and simplifying. Note that the power is positive for sufficiently large C.
For the second part, we write

1/C

k

N (a)
=O

n

n 1- C

2

-

2N

k

(b)
=O

n 1(

1- C

2

-

2N)

,

(30)

where (a) is by substituting  = O (n/k)1- 2 and simplifying, and (b) is by substituting k = O n1- 1 and

simplifying. Note that the power can be made arbitrarily negative by choosing N and C sufficiently large.

For

the

final

part

regarding



=

O(1),

we

simply

note

that

the

two

claims

reduce

to

(i)

k n

=

n-(1),

and

(ii)

k n

N

= O(n-1 ) for sufficiently large N .

Both of these are true since k = O(n1- 1 ).

We will show that throughout the course of the algorithm, for levels l = 1, . . . , C, the size of the possibly defective set PD(l) remains at O k1/C with high probability. We show this using an induction argument.

B.1 Analysis of Levels l = 1, . . . , C - 1
For the base case l = 1, we start by looking at the preceding level l = 0. Each node at level l = 0 is allocated to an individual test, which implies that all nodes in l = 0 are identified correctly. Hence, only the children of the defective nodes in l = 0 are "explored" further in l = 1. Since the number of defective nodes in l = 0 is at most k and each node has 1/C children, we have |PD(1)|  k1/C .
Consider a non-defective node indexed by i at a given level l > 1 having k  k defective nodes, and let Ai be the indicator random variable of that non-defective node colliding with at least one defective node in all of its N repetitions. The dependence of these quantities on l is left implicit. We condition on all of the test placements performed at the earlier levels, writing El[·] for the conditional expectation. By the inductive hypothesis, we have |PD(l)| = O k1/C .
Lemma 8. Under the preceding setup and definitions, if |PD(l)| = O k1/C , then we have

El

Ai

=O

k1/C ·

k N n

.

(31)

i

22

Proof. From Lemma 6, we know that a given non-defective item i has a probability at most k/n of being placed in a positive test. Since we used N independent test design matrices Xl to assign i to N tests, we have Pl[Ai]  (k/n)N . Hence, we have

El Ai = El[Ai] = Pl[Ai = 1] 

i

i

i

i

k N =O

k1/C ·

k N

,

n

n

(32)

where we used the linearity of expectation and the fact that |PD(l)| = O k1/C .

Lemma 9. For any constant 1 > 0, there exist choices of C and N such that the following holds: Conditioned on the l-th level having |PD(l)| = O k1/C , the same is true at the (l + 1)-th level with probability 1 - O n-1 .

Proof. Among the possibly defective nodes at the l-th level, at most k are defective, amounting to at most k1/C children at the next level. Furthermore, by Lemma 8 and Markov's inequality, at most k non-defective nodes are marked as possibly defective, with probability at least

1 - O 1/C k N = 1 - O(n-1 ),

(33)

n

where the equality holds for any 1 > 0 by suitable choices of C and N (see Lemma 7). Thus, this also amounts to at most k1/C additional children at the next level. Summing these together, we have |PD(l+1)|  2k1/C , with probability at least 1 - O n-1 .
By induction, for any given level l, we have |PD(l)| = O k1/C with conditional probability at least 1 - O n-1 . Taking a union bound over all C levels (with C = O(1)), the same follows for all levels simultaneously with probability at least 1 - O n-1 .

B.2 Analysis of the Final Level
Recall that at the final level, we perform C n/ tests. We study the error probability conditioned on the high-probability event |PD(C)| = O k1/C .
For a given non-defective item in a single iteration of the C independent iterations of tests, by Lemma 6, the probability of appearing in a positive test is at most k/n. Since the non-defective item participates in C independent tests, the probability of it appearing only in positive tests is (k/n)C . By a union bound over the O k1/C non-defective singletons at the final level, the error probability is upper bounded by

O k1/C k C = O(n-2 ), n

(34)

23

where the equality holds for any 2 > 0 and suitably-chosen C and C due to Lemma 7 (with C replacing N ).
B.3 Number of Tests, Error Probability, and Decoding Time
· Number of tests: We used CN n/ tests in the first C levels and C n/ tests in the final level, which sums up to CN n/ + C n/ = O(n/).
· Error probability: For each level l, we have |PD(l)| = O k1/C with probability 1 - O n-1 . Furthermore, the final level incurs O n-2 error probability. This gives us a total error probability of O n-1 + n-2 = O n- , where  = min{1, 2}. Since we allowed 1 and 2 to be arbitrarily large, the same holds for .
· Decoding time: The decoding time is dominated by the test outcome checks in our decoding procedure. For the first level l = 0, we have |PD(0)| = n/, which coincides with the total number of test outcome checks. For the remaining C - 1 levels l  {1, . . . , C - 1}, we considered a total of O k1/C possibly defective nodes w.h.p.,7 and for each possibly defective item, we conducted N test outcome checks. This gives us total number of O k1/C test outcome checks. At the final level, for each of the O k1/C relevant leaf nodes, we perform C test outcome checks for a total time of O k1/C . Summing these gives O(n/), since O k1/C = o(n/) for a sufficiently large C (refer to (29)). Since it takes O(1) time to check whether each node is in a positive or negative test, we get a total decoding time of O(n/).
C Proof of Theorem 3 (Noisy Setting)
The outline of the analysis is as follows:
· We first consider levels l = log2 k, . . . , log2 n - 1, and bound the probability that any node among three kinds--non-defective nodes at level lmin = log2 k, defective nodes, and non-defective child nodes of defective nodes--are identified wrongly. Note that we do not have to consider other nodes, because if none of the nodes of these three kinds are identified wrongly, then the algorithm would not explore any of the other nodes when decoding.
· Conditioned on the correct identification of nodes of these three kinds, we consider the final level l = log2 n and provide a bound for its error probability.
7Here and subsequently, we write with high probability (w.h.p.) to mean holding under the high-probability events used in proving that the algorithm succeeds.
24

C.1 Analysis of Levels l = log2 k, . . . , log2 n - 1
We consider defective and non-defective nodes separately. Defective nodes: Recall the notions of intermediate labels and final labels from Section 4.1. Let
p(indt) (respectively, p(fidn)al) be the probability that the intermediate label (respectively, final label) of a given defective node is flipped from a one to a zero. Note that these may vary from node to node, but we will give upper bounds that hold uniformly.
For a given defective node, there are only two possible situations for each test it is in: A positive outcome due to no flip, or a negative test outcome due to a 1  0 flip. Hence, the number of negative tests that a given defective node participates in (i.e., the outcome is flipped) is distributed as Binomial(N, p). By the majority voting of N test outcomes at a given level, p(indt) is upper bounded by the probability that a given defective node participates in at least N/2 negative tests. Applying Hoeffding's inequality, we obtain

p(indt)  exp

1

2

- 2N - p .

2

(35)

At this point, we introduce the variable t appearing in the theorem statement. Since exp - 2N (1/2 - p)2 

2-2t 4

N



2t log 2+log 2(1/2-p)2

4

,

we

find

that

choosing

N



2t log 2+log 4 2(1/2-p)2

ensures

that

p(indt)



2-2t .
4

(36)

For the case that l  log2 n - r, we consider the length-r paths below the defective node. The defective node will be labeled as negative if all 2r paths below it have at least r/2 negative intermediate labels. The probability of this event is upper bounded by the probability that one particular defective path (i.e., every node along the path is defective) has at least r/2 negative intermediate labels, which is at most

r r/2

p(indt) r/2  4p(indt) r/2,

(37)

where the left hand side (LHS) is by the union bound, and the right hand side (RHS) is by

r r/2

 2r. This

gives p(fidn)al  4p(indt) r/2, and substituting (36) gives p(fidn)al  2-tr.

For the case that l > log2 n - r (i.e., there are less than r levels below the given node), the probability of

the (single) defective path having at least r/2 negative intermediate labels remains unchanged, and hence,

the preceding bound p(fidn)al  2-tr still holds. Note that this step requires C log2 n  r in order to have

enough intermediate labels per node in the final level to "pad" paths of length less than r (see Section 4.1),

and we will later set C and r to ensure this.

25

Non-defective nodes: Let p(inntd) (respectively, p(finnda)l) be the probability that the intermediate label (respectively, final label) of a given non-defective node is flipped from a zero to a one. Again, these may vary from node to node, but we will give upper bounds that hold uniformly. For a given non-defective node, there are four possible situations for each test: A negative outcome with no flip (i.e., no defectives), a negative outcome due to a 1  0 flip (i.e., at least one defective), a positive outcome with no flip (i.e., at least one defective), and a positive outcome due to a 0  1 flip (i.e., no defectives).
Focusing on one test sequence of length Tlen = Ck for now, let A be the event that a given non-defective node participates in a positive test, and let B be the event that the given node's test contains no defective item. We have

P[A] = P[B] · P[A|B] + P[¬B] · P[A|¬B]

(38)

(a)

1

 P[B] · p + C (1 - p)

(39)

1

p+ ,

(40)

C

where (a) holds since the probability of being in the same test as a given defective node is 1/Tlen = 1/(Ck), and thus the union bound over k defective nodes gives P[¬B]  1/C.
Equation (40) implies that for a given non-defective node, the number of positive tests that it participates in (out of N tests in total) is stochastically dominated by Binomial(N, p + 1/C). Recalling that p(inntd) is the probability that a given non-defective node participates in at least N/2 positive tests, Hoeffding's inequality gives

p(inntd)  exp

1

12

- 2N - p -

,

2

C

(41)

where we require 1/2 - p - 1/C > 0  C > 2/(1 - 2p). Hence, we set C = 2/(1 - 2p) + 1. Since

exp

- 2N (1/2 - p - 1/C)2



2-2t 16

N



2t log 2+log 16 2(1/2-p-1/C )2

,

we

find

that

choosing

N



2t log 2+log 16 2(1/2-p-1/C )2

ensures

that

p(inntd)



2-2t .
16

(42)

For the case that l  log2 n - r, we look at the length-r path below the non-defective node. The nondefective node will be labeled as positive if any of the 2r paths below it has at least r/2 positive intermediate

26

labels. By a union bound over all 2r paths, this probability is upper bounded as follows, similar to (37):

2r r r/2

p(inntd) r/2  2r 4p(inntd) r/2  16p(inntd) r/2.

(43)

This gives p(finnda)l  16p(inntd) r/2, and substituting (42) gives p(finnda)l  2-tr. Similarly to the defective nodes handled above, the case that l > log2 n - r follows essentially unchanged;
while the above analysis has an additional union bound over 2r paths, the number of paths when l > log2 n-r only gets smaller. Hence, the preceding bound on p(finnda)l  2-tr also holds in this case.
Combining the defective and non-defective cases: Taking the more stringent requirement on N
in the above two cases, we set

2t log 2 + log 16

N = 2(1/2 - p - 1/C)2 ,

(44)

and we observe that regardless of the defectivity of a given node, the probability of the node's final label being wrong is at most 2-tr.
Next, we upper bound the probability that any node among three groups--non-defective nodes at level lmin, defective nodes, and child nodes of defective nodes--is identified wrongly. Note that if all such nodes are identified correctly, then the branching is only ever continued for defective nodes, and it follows that at most 2k nodes remain at the final level (analyzed below).
Since there are log2(n/k) levels and k defectives, the number of non-defective children nodes of defective nodes is at most k log2(n/k), and the number of non-defective nodes at level lmin is at most k. Summing these up, we have at most 2k log2(n/k) + k nodes. By taking the union bound over all 2k log2(n/k) + k nodes, the probability of making an error in identifying any node in the mentioned three groups is at most 2-tr(2k log2(n/k) + k). This can be upper bounded by a given target value n (approaching zero as n  ), provided that

2-tr 2k log2

n k

+k

 n,

(45)

which rearranges to give

1

2k

nk

r  t log2

n log2

k

+ n

.

(46)

27

By choosing

1

3k

n

r = t log2 n log2 k

,

(47)

we deduce that the probability of any wrong decision is upper bounded by n.

C.2 Analysis of the Final Level

Recall from the analyses of (36) and (42) that given our choice of N in (44), regardless of the defectivity of a given node, the probability of a wrong intermediate label--let us call this pint--is at most 2-2t/4. To get the final label of each node (singleton), we conduct a majority voting of C log2 n intermediate labels. Hence, a given node is labeled wrongly when it has at least (C log2 n)/2 wrong intermediate labels. This gives the following upper bound on the probability of a wrong final label, denoted by pfinal:

pfinal 

C log2 n (C log2 n)/2

pint

(C

log2

n)/2

(a)


4pint

(C

log2 n)/2

(b)


2-tC

log2 n,

(48)

where (a) uses

x x/2

 2x, and (b) uses pint  2-2t/4. Taking the union bound over all n nodes at the final

level, we obtain

n 2-tC log2 n = n n-tC = O(n1-tC ),

(49)

which approaches zero as n   as long as tC > 1. Note that while we have shown that all n nodes (singletons) at the final level would be correctly identified if their final labels were to be computed, only at most 2k of these will actually be used by the algorithm, in accordance with the above analysis.

C.3 Number of Tests, Error Probability, and Decoding Time
For convenience, we restate all the values that we have assigned in our analysis above:

2

C=

+ 1 = O(1)

(50)

1 - 2p

2t log 2 + log 16

N = 2(1/2 - p - 1/C)2 = O(t)

(51)

1

3k

n

1 k log(n/k)

r = t log2 n log2 k

= O log t

n

,

(52)

28

where p  (0, 1/2) is the noise level. Now, we choose t = O(1) and n = k log2(n/k) 1- t, for some constant  (1/t, 1). Substituting n = k log2(n/k) 1- t into (52) gives

1 r = t log2

3k log2(n/k) k log2(n/k) 1- t

1

n

= t log2 3 k log2 k

t
.

(53)

Recall that we require C log2 n  r, or equivalently C  r/ log2 n. Substituting (53) into C  r/ log2 n, we find that we require

C

1 t

log2

3 k log2

n k

t
,

(54)

log2 n

Since is constant, we can choose C = O(1) that is large enough to satisfy (54). With our choices of C, C , N, t = O(1) and n =  (k log n)1- t , we obtain the following:

· Number of tests: We used CN k tests per level for l = log2 k, . . . , log2 n - 1. At the final level l = log2 n, we used CC N k log2 n tests. Summing these together gives

T  CN k log2

n k

+ CC N k log2 n (=a) O(k log n),

(55)

where (a) follows by substituting C, C , N = O(1) and simplifying.
· Error probability: Combining the error probabilities from all levels, we have a total error probability of at most

n + O n1-tC = O

n k log
k

1- t
,

(56)

by substituting n = k log2(n/k) 1- t and choosing C sufficiently large.

· Decoding time: To characterize the decoding time, we consider the number of test outcome checks

made throughout the course of the algorithm. For l = log2 k, . . . , log2 n - 1, w.h.p., we involved

O k log(n/k) nodes in total. For each node involved, we checked at most

r i=1

2i

=

O

2r

(52)
=

O

k log(n/k) 1/t n

intermediate labels of other nodes to decide the final label of the given node. For

each these nodes being checked, we checked N = O(t) test outcomes to determine the intermediate

label. Therefore, the decoding time for these levels is

n k log(n/k) 1/t

O k log ·

·t ,

(57)

k

n

29

At the final level l = log2 n, we have already shown that w.h.p., at most 2k nodes remain possibly defective. For each such node, we checked C log2 n intermediate labels to decide the final label of the given node. To decide each intermediate label, we checked N = O(t) test outcomes. Therefore, the decoding time at this level is O(2k · C log n · t). Summing this with (57) gives us the total decoding time of

n k log(n/k) 1/t

n 1+

O k log ·

· t + O(2k · C log n · t) = O k log

,

(58)

k

n

k

by substituting C , t = O(1) and n = k log2(n/k) 1- t, and noting that the O(k log n) term is

dominated by O

k

log

n k

1+

regardless of the scaling of k.

D Non-Binary Trees in the Noisy Setting

D.1 Unconstrained Noisy Setting

Our algorithm for the noisy setting in Section 4 is based on binary splitting, and combats noise by both (i)

performing independent repetitions at each level, and (ii) classifying a given node by exploring levels further

down the tree. Here we discuss an alternative approach based on non-binary splitting, which attains similar

results using only the former of these.8 Despite this, we believe that there is value in also showing that

binary splitting suffices, and that our technique of exploring further down the tree may be of independent

interest.

The non-binary approach we consider in this section is based on the analysis of the heavy hitters problem

in [28, Sec. B.2], which in turn builds on [12]. Instead of forming a binary tree as in Figure 4, consider

forming a b-ary tree for some value of b to be chosen later.

Hence, the depth of the tree is O

log n log b

.

At each level, instead of using O(1) independent repetitions (as was done in Algorithm 5), we use O(log b)

repetitions. Since there are O

log n log b

levels, and each repetition contains O(k) tests, the total number of tests

is O(k log n). In addition, by a similar analysis to that of p(indt) and p(inntd) in Appendix C, each majority vote

over

these

repetitions

succeeds

with

probability

at

least

1

-

1 poly(b)

,

where

the

polynomial

has

arbitrarily

high degree.

When all such majority votes are correct, the algorithm only visits O(kb) nodes, and thus, if b = (k log n) ,

the

probability

of

any

wrong

decision

can

be

made

to

decay

as

1 poly(k log

n)

.

While

the

list

size

at

the

final

level

increases from O(k) (in our binary splitting approach) to O(bk), the final level can still be analyzed in the

same way as Appendix C, and the total decoding time is O(kb log n) = O (k log n)1+ ). This is equivalent

8This approach was pointed out by an anonymous reviewer of an earlier version of this paper.

30

to the decoding time O

k

log

n k

1+

)

given

in

Theorem

3,

since

if

k

is

large

enough

for

log

n k

to significantly

differ from log n, then the logarithmic factor can be factored into the k term anyway.

D.2 Noisy Setting with Size-Constrained Tests

At first glance, it may appear to be difficult to combine our techniques for the size-constrained and noisy

settings, since the latter is based on searching (1) levels down the tree, whereas the former uses a tree

with depth O(1). However, even in [16] where the computation time is (n), moving to the noisy setting

increases the number of tests from O

n 

to O

n 

log

n

.

We can incur a similar increase by increasing our

tree depth from O(1) to O(log n), and this added depth permits us to combat noise in the same way as the

unconstrained setting. For the sake of brevity, we omit the details.

E Storage Reductions via Hashing
For all of our algorithms considered, the storage comprises of storing the assignments of nodes to tests, storing the possibly defective set PD, and storing the test outcomes. We observe that since every tree that we consider has a final level containing n nodes, storing the test assignments at that level alone requires (n) storage, meaning that the standard versions of our algorithms do not have sublinear storage.
In order to reduce the storage, we can make modifications to each algorithm in a similar manner to [32]: Instead of directly storing the test outcomes of every node, we interpret the node-to-test mappings at each level (except for one-to-one mappings) as hash functions. Since the high storage comes from explicitly storing the corresponding test outcomes of nodes, the key to reducing the overall storage is to use lower-storage hash families.
The reduced storage comes at the expense of reduced independence between different hash values. Fortunately, this drawback has a negligible effect on the guarantees of our algorithm under the noisy setting and size-constrained setting, as the proofs of Theorems 3 and 2 only require pairwise independence or weaker. However, the effect is more significant for our algorithm under the finitely divisible items constraint, as our proof of Theorem 1 uses full independence. In the following, we briefly describe suitable properties and choices for the hash families, and how they affect the algorithmic guarantees. We let Thash and Shash respectively denote the evaluation time for one hash value and the number of bits of storage required for one hash function.
Finitely divisible items: Consider using an O()-wise independent hash family to generate a hash function, with Thash = O() and Shash = O( log n) (e.g., see [32, Section 3.1]). Since the analysis in Appendix A requires full independence, a different analysis is required for the algorithmic guarantees.
31

To address this, we note that two distinct analyses were given in [32], with fully independent hashes attaining the stronger result, and limited-independence hashes reducing the storage but increasing the error probability. The latter of these in fact extends to the finitely divisible setting significantly more easily than the former does, so we simply state the corresponding result and omit the proof: For any function n decaying as n increases, using

T = O k max

n

1
,

k

n 1
- +1

1  (- +1)

k

n

k

(59)

tests, the algorithm has O Thashk(n/k)1/ = O 2k(n/k)1/ runtime, requires a storage of O k(n/k)1/ log n + Shash + T = O k(n/k)1/ + 2 log n + T bits, and incurs an error probability

of O(/k + n). Thus, we maintain a similar number of tests and decoding time as Theorem 1, but the error

probability increases, and in fact only behaves as o(1) in the case that  = o(k) (which occurs, for example,

under the mild condition k = (log n)).

Size-constrained tests: Some care is required here to ensure that the constraints of our design matrix

(i.e., fixed row and column weights) are satisfied. Specifically, at each level l  {1, . . . , C}, we desire a hash

function hl :

1,

.

.

.

,

n 1-l/C

 {1, . . . , n/} such at each "bucket" (test) has a "load" (number of nodes

in the test) of exactly l/C. An inspection of our analysis in Appendix B reveals that we only require the

probability of two nodes colliding to be O(/n), i.e., only an approximately pairwise independent family is

needed.

To

construct

the

hash

function

above,

we

first

consider

a

random

permutation



:

{1, . . . ,

} n
1-l/C



{1, .

.

.

,

n 1-l/C

}

such

that

for

any

i,

i



{1,

.

.

.

,

n 1-l/C

, we have P[|(i) - (i )|  t] = O t1-l/C /n . Such

permutations are well-understood (e.g., see Definition 4.1 and Lemma 4.1 in [8]), and we can use this to

design a hash function hl(·) in the following manner: First apply the permutation discussed above, and then

truncate the last (l/C) log2  bits of the permutation value.

Then, for any i, i



{1,

.

.

.

,

n 1-l/C

, we have

P[hl(i)

=

hl(i

)]

(a)


P[|(i)

-

(i

)|



l/C

]

(=b)

O

 n

,

(60)

where (a) holds since if i and i are in the same bucket, then all their bits except the last (l/C) log2  bits are the same, and (i) and (i ) can be at most l/C (bucket size) apart; then, (b) holds by applying the collision property of our permutation. This proves that the constructed hash function has the required properties. Moreover, we have Thash = O(1) and Shash = O(1).
Given the preceding hash construction, we again provide a brief analysis of the storage as follows: Recall that we use N = O(1) hashes at each level (except l = 0 and l = C), and C = O(1) hashes at the final level,

32

for a total of O(1) hashes, requiring O(Shash) = O(1) storage. In addition, under the high probability event that there are O(k1/C ) possibly defective nodes at each level, their storage requires O(k1/C ) integers, or O(k1/C log n) = o(n/) bits (see Lemma 7). Lastly, we need to store a total of O(n/) test outcomes, each requiring a bit of storage. Hence, the total storage is O(Shash + k1/C log n + n/) = O(n/) bits.
Noisy setting: Since we only need pairwise independence in our analysis in Appendix C, we can use any pairwise independent hash family to generate a hash function, which only requires Thash = O(1) and Shash = O(log n) (e.g., see [32, Section 3.1]). Here the analysis of the number of tests, error probability, and decoding time in Appendix C remain unchanged.
We provide a brief analysis of the storage as follows: Recalling our choices of C , t, N = O(1), we use N = O(1) hashes at each level except the last, and C N log2 n = O(log n) hashes at the final level, for a total of O(log n) hashes, requiring O(Shash log n) storage. In addition, for any level l, we know that |PD(l)| = O(k log n) w.h.p, which implies that the storage required for the possibly defective set is O(k log n) integers, or O(k log2 n) bits. Lastly, we need to store a total of O(k log n) test outcomes, each requiring a bit of storage. The total storage is O(Shash log n+k log2 n+k log n) = O(k log2 n) by substituting Shash = O(log n).
References
[1] M. Aldridge, L. Baldassini, and O. Johnson, "Group testing algorithms: Bounds and simulations," IEEE Trans. Inf. Theory, vol. 60, no. 6, pp. 3671­3687, June 2014.
[2] M. Aldridge, O. Johnson, and J. Scarlett, "Group testing: An information theory perspective," Found. Trend. Comms. Inf. Theory, vol. 15, no. 3­4, pp. 196­392, 2019.
[3] J.-C. Aval, "Multivariate fuss­catalan numbers," Discrete Mathematics, vol. 308, no. 20, pp. 4660 ­ 4669, 2008.
[4] W. H. Bay, E. Price, and J. Scarlett, "Optimal non-adaptive probabilistic group testing requires (min{k log n, n}) tests," 2020, https://arxiv.org/abs/2006.01325.
[5] R. Berinde, A. C. Gilbert, P. Indyk, H. Karloff, and M. J. Strauss, "Combining geometry and combinatorics: A unified approach to sparse signal recovery," in Allerton Conf. on Comm., Control and Comp., 2008.
[6] S. Bondorf, B. Chen, J. Scarlett, H. Yu, and Y. Zhao, "Sublinear-time non-adaptive group testing with O(k log n) tests via bit-mixing coding," IEEE Trans. Inf. Theory, vol. 67, no. 3, pp. 1559­1570, 2020.
33

[7] S. Cai, M. Jahangoshahi, M. Bakshi, and S. Jaggi, "Efficient algorithms for noisy group testing," IEEE Trans. Inf. Theory, vol. 63, no. 4, pp. 2113­2136, 2017.
[8] V. Cevher, M. Kapralov, J. Scarlett, and A. Zandieh, "An adaptive sublinear-time block sparse Fourier transform," in ACM Symp. Theory Comp. (STOC), 2017.
[9] C. L. Chan, S. Jaggi, V. Saligrama, and S. Agnihotri, "Non-adaptive group testing: Explicit bounds and novel algorithms," IEEE Trans. Inf. Theory, vol. 60, no. 5, pp. 3019­3035, May 2014.
[10] M. Cheraghchi, "Noise-resilient group testing: Limitations and constructions," in Int. Symp. Found. Comp. Theory, 2009, pp. 62­73.
[11] M. Cheraghchi and V. Nakos, "Combinatorial group testing and sparse recovery schemes with nearoptimal decoding time," 2020, https://arxiv.org/abs/2006.08420.
[12] G. Cormode and M. Hadjieleftheriou, "Finding frequent items in data streams," Proc. VLDB Endow., vol. 1, no. 2, p. 1530­1541, Aug. 2008.
[13] G. Cormode and S. Muthukrishnan, "An improved data stream summary: the count-min sketch and its applications," J. Algs., vol. 55, no. 1, pp. 58­75, 2005.
[14] G. Cormode and S. Muthukrishnan, "Combinatorial algorithms for compressed sensing," in Int. Colloq. Struct. Inf. Comm. Complex. Springer, 2006, pp. 280­294.
[15] R. Dorfman, "The detection of defective members of large populations," Ann. Math. Stats., vol. 14, no. 4, pp. 436­440, 1943.
[16] V. Gandikota, E. Grigorescu, S. Jaggi, and S. Zhou, "Nearly optimal sparse group testing," IEEE Trans. Inf. Theory, vol. 65, no. 5, pp. 2760 ­ 2773, 2019.
[17] O. Gebhard, M. Hahn-Klimroth, O. Parczyk, M. Penschuck, M. Rolvien, J. Scarlett, and N. Tan, "Near optimal sparsity-constrained group testing: improved bounds and algorithms," 2020, https://arxiv.org/ abs/2004.11860.
[18] O. Gebhard, O. Johnson, P. Loick, and M. Rolvien, "Improved bounds for noisy group testing with constant tests per item," 2020, https://arxiv.org/abs/2007.01376.
[19] A. C. Gilbert, M. J. Strauss, J. A. Tropp, and R. Vershynin, "One sketch for all: Fast algorithms for compressed sensing," in Proc. ACM-SIAM Symp. Disc. Alg. (SODA), New York, 2007, pp. 237­246.
34

[20] C. A. Hogan, M. K. Sahoo, and B. A. Pinsky, "Sample Pooling as a Strategy to Detect Community Transmission of SARS-CoV-2," J. Amer. Med. Assoc., vol. 323, no. 19, pp. 1967­1969, 05 2020.
[21] F. K. Hwang, "A method for detecting all defective members in a population by group testing," J. Amer. Stats. Assoc., vol. 67, no. 339, pp. 605­608, 1972.
[22] H. A. Inan, P. Kairouz, and A. Ozgur, "Sparse combinatorial group testing," IEEE Trans. Inf. Theory, vol. 66, no. 5, pp. 2729­2742, 2020.
[23] H. A. Inan, P. Kairouz, M. Wootters, and A. O¨ zgu¨r, "On the optimality of the Kautz-Singleton construction in probabilistic group testing," IEEE Trans. Inf. Theory, vol. 65, no. 9, pp. 5592­5603, Sept. 2019.
[24] H. A. Inan and A. Ozgur, "Strongly explicit and efficiently decodable probabilistic group testing," in IEEE Int. Symp. Inf. Theory, 2020.
[25] P. Indyk, H. Q. Ngo, and A. Rudra, "Efficiently decodable non-adaptive group testing," in ACM-SIAM Symp. Disc. Alg. (SODA), 2010.
[26] P. Indyk and E. Price, "K-median clustering, model-based compressive sensing, and sparse recovery for earth mover distance," in ACM Symp. Theory Comp. (STOC), 2011, pp. 627­636.
[27] O. Johnson, M. Aldridge, and J. Scarlett, "Performance of group testing algorithms with near-constant tests-per-item," IEEE Trans. Inf. Theory, vol. 65, no. 2, pp. 707­723, Feb. 2019.
[28] K. G. Larsen, J. Nelson, H. L. Nguyundefinedn, and M. Thorup, "Heavy hitters via cluster-preserving clustering," Comm. ACM, vol. 62, no. 8, p. 95­100, July 2019.
[29] K. Lee, R. Pedarsani, and K. Ramchandran, "SAFFRON: A fast, efficient, and robust framework for group testing based on sparse-graph codes," in IEEE Int. Symp. Inf. Theory, 2016.
[30] M. Malyutov, "The separating property of random matrices," Math. Notes Acad. Sci. USSR, vol. 23, no. 1, pp. 84­91, 1978.
[31] H. Q. Ngo, E. Porat, and A. Rudra, "Efficiently decodable error-correcting list disjunct matrices and applications," in Int. Colloq. Automata, Lang., and Prog. (ICALP), 2011.
[32] E. Price and J. Scarlett, "A fast binary splitting approach to non-adaptive group testing," in Int. Conf. Rand. Comp. (RANDOM), 2020.
35

[33] J. Scarlett and V. Cevher, "Phase transitions in group testing," in Proc. ACM-SIAM Symp. Disc. Alg. (SODA), 2016.
[34] J. Scarlett and V. Cevher, "Near-optimal noisy group testing via separate decoding of items," IEEE Trans. Sel. Topics Sig. Proc., vol. 2, no. 4, pp. 625­638, 2018.
[35] J. Scarlett and O. Johnson, "Noisy non-adaptive group testing: A (near-)definite defectives approach," IEEE Trans. Inf. Theory, vol. 66, no. 6, pp. 3775­3797, 2020.
[36] N. Tan and J. Scarlett, "Near-optimal sparse adaptive group testing," in IEEE Int. Symp. Inf. Theory, 2020.
[37] R. Vershynin, "Introduction to the non-asymptotic analysis of random matrices," Compressed Sensing: Theory and Applications, p. 210­268, 2010.
[38] I. Yelin, N. Aharony, E. Shaer-Tamar, A. Argoetti, E. Messer, D. Berenbaum, E. Shafran, A. Kuzli, N. Gandali, T. Hashimshony, Y. Mandel-Gutfreund, M. Halberthal, Y. Geffen, M. Szwarcwort-Cohen, and R. Kishony, "Evaluation of COVID-19 RT-qPCR test in multi-sample pools," medRxiv, 2020.
36

