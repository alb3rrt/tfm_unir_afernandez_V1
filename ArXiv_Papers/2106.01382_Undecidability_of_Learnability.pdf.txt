arXiv:2106.01382v1 [cs.CC] 2 Jun 2021

Undecidability of Learnability
Matthias C. Caro 
Technical University of Munich, Department of Mathematics, Garching, Germany Munich Center for Quantum Science and Technology (MCQST), Munich, Germany
caro@ma.tum.de
Abstract Machine learning researchers and practitioners steadily enlarge the multitude of successful learning models. They achieve this through in-depth theoretical analyses and experiential heuristics. However, there is no known general-purpose procedure for rigorously evaluating whether newly proposed models indeed successfully learn from data. We show that such a procedure cannot exist. For PAC binary classification, uniform and universal online learning, and exact learning through teacher-learner interactions, learnability is in general undecidable, both in the sense of independence of the axioms in a formal system and in the sense of uncomputability. Our proofs proceed via computable constructions of function classes that encode the consistency problem for formal systems and the halting problem for Turing machines into complexity measures that characterize learnability. Our work shows that undecidability appears in the theoretical foundations of machine learning: There is no one-sizefits-all algorithm for deciding whether a machine learning model can be successful. We cannot in general automatize the process of assessing new learning models.
ORCiD: 0000-0001-9009-2372
1

1 Introduction
One of the foundational questions in machine learning theory is "When is learning possible?" This is the question for necessary and sufficient conditions for learnability. Such conditions have been identified for different learning models. They can take the form of requiring a certain, often combinatorial, complexity measure to be finite. Well known examples of such complexity measures include the VC-dimension for binary classification in the PAC model, the Littlestone dimension for online learning, or different notions of teaching dimensions for teacher-learner interactions.
We consider a question that is slightly different from, but arguably just as important as the one above. Namely, we ask "Can we decide whether learning is possible?" At first glance, the ability to answer the first question might also seem to allow to resolve this second one. If, e.g., you know a complexity measure whose finiteness is equivalent to learnability, that gives you a criterion to decide learnability. However, whether this is indeed a satisfactory criterion strongly depends on the exact meaning of "decide" in the second question.
We consider two such meanings and thereby obtain two variants of the second question. The first is natural from a mathematician's perspective, namely "If a class is learnable, can we prove that this is the case?" The second is intimately familiar to computer scientists, namely "Does there exist an algorithm that decides learnability?" After specifying in either of these two ways what it means to "decide whether learning is possible," we see that the answer to the second question is not trivially positiveEven given the definition of a complexity parameter that is finite if and only if learning is possible, answering the second question still requires a proof of finiteness of that complexity measure or an algorithm that decides whether the complexity measure is finite or not.
In fact, we show that the answer to the question "Can we decide whether learning is possible?" is, in general, negative for both of the variants introduced above and for different learning scenarios. In particular, we demonstrate this for learning models in which criteria for learnability in terms of complexity measures are known. More concretely, we consider binary classification, uniform and universal online learning, and the task of exactly identifying a function through teacher-learner interactions. We show in all these scenarios: On the one hand, there is a function class that is learnable but whose learnability cannot be proved. On the other hand, there is no general-purpose algorithm that, upon input of a class, decides whether it is learnable.
1.1 Overview Over the Results
Our undecidability results come in two flavours, one about provability in a formal system, the other about computability via Turing machines. We summarize our line of reasoning in Figure 1 and explain it in more detail in the following paragraphs.
We first study binary classification in Probably Approximately Correct (PAC) learning. The relevant complexity measure for this learning scenario is the VC-dimension due to [VC71]. On the one hand, given a recursively enumerable formal system F , we define a class GF  {0, 1}N
2

Go¨del undecidability Formal system F

Turing undecidability Turing machine M

computable construction

computable construction

Function class GF  {0, 1}N

Function class HM  {0, 1}N

M halts

F consistent

M doesn't halt F inconsistent

GF has finite complexity

GF has infinite complexity

HM has finite complexity

HM has infinite complexity

GF is learnable

GF is not learnable

HM is learnable HM is not learnable

Cannot be proved in F

Cannot be computably distinguished

Figure 1: A depiction of our line of reasoning. "Complexity" is to be understood in terms of VC-dimension, teaching dimension, Littlestone dimension, or Littlestone trees, depending on the learning model. To conclude undecidability, we use Go¨del's second incompleteness theorem and the uncomputability of the halting problem, respectively.
(Definition 2.4) that is PAC learnable if and only if F is consistent (Corollary 2.9). If Go¨del's second incompleteness theorem applies to F , we conclude that the function class GF is PAC learnable, but its PAC learnability cannot be proved in F (Corollary 2.11). On the other hand, given a Turing machine M , we define a class HM  {0, 1}N (Definition 2.15) that is PAC learnable if and only if M halts on the empty input (Lemma 2.16). By reduction to the halting problem, there is no general-purpose algorithm that decides whether a computable binary-valued function class is PAC learnable (Corollary 2.20).
Our constructions start from the recursively enumerable set cc({0, 1}) of functions with compact support in N. Depending on the underlying object, i.e., the formal system or the Turing machine, we then further restrict the function class. We implement these restrictions based on consistency of finitely many provable theorems and halting after finitely many steps. Thereby, we ensure that they are computable from the underlying object (see Corollaries 2.14 and 2.19). For the Go¨del scenario,
3

this translates the assumption of the existence of a recursive enumeration of the provable theorems to a property of the function class. For the Turing scenario, this computability is necessary for a reduction to the halting problem.
We also consider GF and HM in the following scenario: A teacher can provide examples of a target function to help a learner identify that function. Here, the basic complexity measure is the teaching dimension [GK95], which is finite if and only if the teaching problem can be solved with finitely many examples. We show that GF has finite teaching dimension if and only if F is consistent (Proposition 3.5). In this case, the teaching problem can be solved but this cannot proved in F (Corollary 3.6), assuming again that Go¨del's second incompleteness theorem applies. Similarly, we show that HM has finite teaching dimension if and only if the underlying Turing machine M halts on the empty input (Proposition 3.8). So, there is no algorithm for deciding whether a function class can be taught/learned (Corollary 3.9).
We demonstrate the undecidability of one more decision problem motivated by teacher-learner interactions. Namely, in general, one cannot decide whether a given function in a known class can be taught/learned from finitely many examples. Again, this is true both in the sense of independence of the axioms of a formal system (Corollary 3.3) and in the sense of uncomputability (Remark 3.4).
Finally, our constructions also yield undecidability results for uniform and universal online learning. For online learning with uniform mistake bounds, the Littlestone dimension [Lit88] is the corresponding complexity parameter. For universal online learning, the relevant complexity condition is whether there exist infinite Littlestone trees [Bou+20]. After showing (in Propositions 4.5, 4.7, 4.9, and 4.11) that whether these complexity conditions are satisfied by GF and HM is again determined by whether F is consistent and whether M halts on the empty input, respectively, we conclude: Both uniform and universal online learnability are, in general, both Go¨del and Turing undecidable (Corollaries 4.6, 4.8, 4.10, and 4.12).
Compared to prior work on undecidability in learning theory, which we review in Subsection 1.2, our approach is at the same time more direct and is the first that simultaneously proves undecidability results for multiple established learning models both in the sense of formal independence and in the sense of uncomputability. Our main technical contribution consists in constructing and studying the function classes GF and HM , which we base on a careful elaboration of the computational model. Conceptually, we show that many of the established learnability criteria in terms of complexity measures are undecidable, thus demonstrating a limitation of the approach towards learnability and model selection via such complexity measures.
1.2 Related Work
[Lat96] made an early investigation into the relationship between computability and learnability. The main question in [Lat96] is whether and under which notions of "learnability" one can consider an uncomputable problem to be learnable. More precisely, [Lat96] considered the task of learning the halting problem relative to an oracle.
4

[Zha18] studied the computability of finiteness of the VC-dimension and discussed some philosophical implications. In particular, our Corollary 2.20, the Turing undecidability of finiteness of the VC-dimension, is contained in Theorem 1 of [Zha18]. The proof in [Zha18] used results from model theory and an equivalence due to [Las92]. The latter says that a function class uniformly definable via a first-order formula has finite VC-dimension if and only if the defining formula is a so-called NIP formula. While one of our results is already derived in [Zha18], we consider our work to be a significant extension in two directions: On the one hand, we consider both Turing and Go¨del undecidability. On the other hand, our proof strategies are at the same time more direct and more flexibly applicable to other complexity measures and learning scenarios.
[Ben+19] proposed the "estimating-the-maximum" (EMX) problem and proved that learnability in this model is independent of the ZFC axioms. While this already indicates that learning can be undecidable, our results add new insight in at least two ways. First, our results are for already established learning models. In particular, whereas [Ben+19] showed that, assuming consistency of ZFC, there is no dimension-like quantity of finite character that characterizes EMX learnability, our results include scenarios in which such dimensions for learning exist. Second, whereas [Ben+19] used the continuum, the continuum hypothesis, and the axiom of choice, we only use natural numbers and computable objects. This allows us to prove uncomputability results, which cannot be derived from the results of [Ben+19]. Some implications and limitations of the approach of [Ben+19] have been discussed, e.g., in [Har19; Tay19; Gan20].
[Aga+20] initiated a study of computable learners, which then truly deserve to be called "learning algorithms." In particular, [Aga+20] showed that not every PAC learnable class admits a computable learner and also identified conditions under which PAC learnability implies copmutable PAC learnability. Thereby, [Aga+20] extended considerations from [Sol08], which studied the task of non-uniform learning over all computable functions by a computable learner. As the underlying questions of [Aga+20] and our work differ, the results are not comparable. However, as we show the function classes GF and HM to be computable, the results of [Aga+20] imply that our undecidability results hold not only for PAC learning, but also for computable PAC learning.
1.3 Structure of the Paper
Section 2 contains our main constructions and results leading to undecidability of finiteness of the VC-dimension. In Section 3, we demonstrate that our constructions also yield undecidability results for teaching problems. In Section 4, we exhibit analogous results for both uniform and universal online learning. We conclude with an outlook and open questions in Section 5. Full proofs appear either directly in the text or in Appendix A. Appendices B and C contain standard definitions and results related to formal systems and computability that are used in the main text.
5

2 Undecidability of Finiteness of the VC-Dimension

2.1 Preliminaries: PAC Binary Classification and the VC-Dimension

We start by recalling one of the most influential learning models for binary classification:

Definition 2.1 (Probably approximately correct binary classification [Val84]). Let X be some

space, write Z = X × {0, 1}. Let G  {0, 1}X , and let D be a probability distribution on Z. A

map A :

 m=1

Z

m



{0, 1}X ,

S



hS ,

is

a

probably

approximately

correct

(PAC)

learner

for

G

if there exists a function m : (0, 1)2  N1 such that, given ,   (0, 1), if m  m(, ), then, with

probability  1 -  with respect to repeated sampling of S  Dm, it holds that

P(x,y)D[hS (x)

=

y]





+

inf
gG

P(x,y)D [g(x)

=

y].

The PAC learners of interest are polynomial PAC learners, for which the sample size m(, ) can be chosen to depend polynomially on 1/ and log (1/). Here, the "polynomial" refers to the sample size only, not to the runtime. If G admits a polynomial PAC learner, we call G PAC learnable.
For the scenario of binary classification, whether there exists a polynomial PAC learner can be understood in terms of a combinatorial quantity of the function class under consideration.

Definition 2.2 (VC-dimension [VC71]). Let G  {0, 1}X . The Vapnik-Chervonenkis dimension, abbreviated as VC-dimension, of G is defined to be

VCdim(G) := sup{n  N0 | S  X : |S| = n  |G|S | = 2n}.

If S  X is a set such that |G|S | = 2|S|, we say that S is shattered by G. Theorem 2.3 (Fundamental theorem of binary classification (see, e.g., [SB19])). Let G  {0, 1}X . G is PAC learnable if and only if VCdim(G) < .
Therefore, when studying PAC learnability of a binary-valued function class with repsect to the 0-1-loss, we focus on studying finiteness of the VC-dimension.

2.2 G¨odel Undecidability
For the purpose of this subsection, let F denote a recursively enumerable formal system in which infinitely many different theorems can be proved. (See Definition B.3 for a definition of "recursively enumerable.") Let  be a primitive recursive enumeration of the theorems provable in F . Here, we think of theorems being "different" in a symbolic way. I.e., two theorems are the same if and only if they are the exact same sequence of symbols from the alphabet available in F . This, in turn, is equivalent to the two theorems having the same Go¨del number in a fixed Go¨del numbering.
Also, we will denote by E2 : N  N × N a primitive recursive enumeration of N2. I.e., E2 is a total bijective function such that both component functions Ei2 : N  N, i = 1, 2, are primitive
6

recursive and such that the inverse (E2)-1 : N × N  N is primitive recursive. The existence of such an E2 can, e.g., be proved using so-called pairing functions.
We begin by defining our main object of study for this subsection.

Definition 2.4. Let F , , E2 be as above. For a compactly supported sequence a = (a(k))kN  cc({0, 1}), define the function ga : N  {0, 1} via


a(n) ga(n) =
0

if (E12(n)) = ¬(E22(n)) , else

and the function class GF := {ga}acc({0,1}). Here, the equality (E12(n)) = ¬(E22(n)) is to be understood as the symbolic equality between
the theorem with Go¨del number (E12(n)) and the negation of the theorem with Go¨del number (E22(n)). Equivalently, we require equality of the corresponding Go¨del numbers.
We first observe that the class GF "collapses" to a single function, the zero function, if and only if the underlying formal system F is consistent.
Proposition 2.5. F is consistent iff GF = {0}.
Proof. This follows from the construction of the function class because E12 and E22 are surjective and the range of  consists exactly of all Go¨del numbers of theorems provable in F .
For later reference, we note a direct consequence of this observation.
Corollary 2.6. If F is consistent, then VCdim(GF ) = 0.
We now make two more observations about the class G. The first concerns its VC-dimension for the case in which the underlying formal system is inconsistent. In that case, the restriction "(E12(n)) = ¬(E22(n))" is satisfied infinitely often and the VC-dimension of the function class G is infinite. This is the content of the following
Theorem 2.7. If F is inconsistent, then VCdim(G) = .
For the proof, we first recall that "anything can be deduced from a contradiction," sometimes also known as "ex falso quodlibet."
Proposition 2.8. Let F be an inconsistent formal system. Let q be a theorem in F . Then both q and ¬q can be proved in F .
Proof. See Appendix A.
With this we can now prove Theorem 2.7.

7

Proof of Theorem 2.7. As F is inconsistent and infinitely many different theorems can be proved in F , by "ex falso quodlibet" there are infinitely many n  N such that (E12(n)) = ¬(E22(n)), because E12 and E22 are surjective and the range of  consists exactly of all Go¨del numbers of theorems provable in F .
Let N  N. Then, by the above, there exist pairwise distinct n1, . . . , nN  N such that (E12(ni)) = ¬(E22(ni)) for all 1  i  N . Let b  {0, 1}N be arbitrary. Define ab  cc({0, 1}) as
ab(ni) = bi for 1  i  N, ab(n) = 0 for n  N \ {n1, . . . , nN }.

Then we clearly have gab (ni) = bi for all 1  i  N . So, {n1, . . . , nN } is shattered by GF . As N  N was arbitrary, we conclude VCdim(GF ) = .

If we now combine the statements of Corollary 2.6 and Theorem 2.7, we obtain the following

Corollary 2.9. F is consistent iff VCdim(GF ) < .

Remark 2.10. There is a na¨ive way of constructing a function class that satisfies the same property as the one just established for GF . Namely, given F , we could efine


G~F := {0} cc({0, 1})

if F is consistent .
else

Whereas we can understand F  GF as a computable mapping (see Corollary 2.14), the same is not the case for F  G~F . Hence, our procedure for constructing GF from F has a desirable property that the assignment F  G~F would not guarantee.
If the formal system F is capable of expressing both the class GF and the finiteness of its VC-dimension, we can combine Corollary 2.9 with Go¨del's second incompleteness theorem.
Corollary 2.11. Assume that F is a recursively enumerable and consistent formal system that contains elementary arithmetic such that infinitely many different theorems can be proved in F . Then VCdim(GF ) < , but the finiteness of VCdim(GF ) cannot be proved in F .
Proof. Assume for contradiction that the statement VCdim(GF ) <  can be proved in F . In Corollary 2.9, we have given a proof that this implies consistency of F . If this proof can be expressed in the formal system F , F proves its own consistency. This contradicts Go¨del's second incompleteness theorem.

Now we come to the second relevant observation about the class GF : Not only is it a computable function class, but even the mapping F  GF is computable. We first prove the slightly weaker result that GF is a computable function class in the sense of Definition C.3:
Theorem 2.12. Assume that F is a recursively enumerable formal system. Then the class GF is computable.

8

As a first step towards proving this result, we observe that the sequence space cc({0, 1}) used for indexing the class can be recursively enumerated.

Lemma 2.13. There exists a primitive recursive function C : N × N  {0, 1} that enumerates cc({0, 1}), i.e., such that cc({0, 1}) = {n  C(m, n) | m  N}.

Proof. See Appendix A.

With this ingredient at hand, we can prove Theorem 2.12.

Proof of Theorem 2.12. According to Definition C.3, we want to find a total computable function GF : N × N  {0, 1} such that GF = {n  GF (m, n) | m  N}. We define


C(m, n) GF (m, n) :=
0

if (E12(n)) = ¬(E22(n)) . else

Since C recursively enumerates cc({0, 1}), we indeed have GF = {n  GF (m, n) | m  N}. It remains to show that GF is a total computable function. As C is total computable, even primitive recursive by Lemma 2.13, it suffices to show that the predicate (E12(n)) = ¬(E22(n)) is total computable.
To this end, recall that E12, E22 and  are primitive recursive. Thus, we only have to show that, given the Go¨del numbers of two theorems, checking whether the theorem corresponding to the first number is the negation of the theorem corresponding to the second number can be done in a computable manner. This is even possible in a primitive recursive manner simply by how Go¨del numbers are constructed.
Note that our proof of Theorem 2.12 even shows that GF is primitive recursive if we define a primitive recursive class of functions analogously to Definition C.3. The proof tells us more about the construction of GF with respect to computability. Not only is the function class GF computable for every formal system F . (This is also true for G~F .) But we even see that the assignment F  GF is computable in the following sense:
Corollary 2.14. There exists a partial computable function G : N3  N such that GF = {N  n  G(, m, n) | m  N} for any recursively enumerable formal system F whose theorems are enumerated by the primitive recursive function  : N  N.
Proof sketch. As  is primitive recursive, it is in particular computable. Thus, we can represent it via its code with respect to our universal Turing machine. With this code, we can compute the predicate (E12(n)) = ¬(E22(n)) and the Corollary is proved just like Theorem 2.12.
Here, the "partial" is only with respect to the first argument, G is total with respect to the second and third input. Together, Theorem 2.12 and Corollary 2.14 provide an advantage of our

9

construction over the "trivial" G~F in Remark 2.10. Given a recursively enumerable system in terms of an explicit primitive recursive enumeration  of theorems, they provide us with an explicit algorithmic procedure for evaluating elements of the function class GF and thereby with an explicit description of GF obtained by fixing certain inputs of the concrete function G.

2.3 Turing Undecidability

We now change the perspective and ask whether there is a general-purpose algorithmic procedure for deciding whether a binary-valued function class has finite VC-dimension. We begin by describing what such a hypothetical algorithm should do: It would take as input the code of an arbitrary computable binary-valued function class G. It should output 0 if VCdim(G) is infinite and 1 if VCdim(G) is finite. Note that such an algorithm would decide finiteness of the VC-dimension "only" for computable function classes since it is exactly the computability which allows us to provide their code as input.
We show that such an algorithm does not exist by reduction to the halting problem. The "encoding" of the halting problem into the finiteness of the VC-dimension of a function class is achieved by the following construction.

Definition 2.15. Let M be a finite-state Turing machine. For a compactly supported sequence a = (a(k))kN  cc({0, 1}), define the function ha : N  {0, 1} via

 a(n) ha(n) = 0

if M does not halt after  n steps on the empty input ,
else

and the function class HM := {ha}acc({0,1}).
From this definition, we immediately see that whether VCdim(HM ) is finite or infinite is determined by whether the underlying Turing machine M halts on the empty input or not.

Lemma 2.16. Let M be a Turing machine. The binary-valued function class HM satisfies


K VCdim(HM ) =


if M halts after exactly K steps on the empty input .
else

Proof. First suppose that M halts after exactly K  N>0 steps on the empty input. Then the set {0, . . . , K - 1}  N is shattered by HM . Namely, if b : {0, . . . , K - 1}  {0, 1}, then we can append zeros to b to define ab  cc({0, 1}) via

 b(n) ab(n) = 0

if n  K - 1 , for n  N.
else

10

Clearly hab (k) = b(k) for all k  {0, . . . , K - 1}. So VCdim(HM )  K. As ha(n) = 0 for all n  K and for all a  cc({0, 1}), no set of cardinality  K + 1 is shattered by HM . Thus, also VCdim(HM )  K.
Now suppose that M does not halt on the empty input. Then, using the same reasoning that gave us the VC-dimension lower bound above, we see that the set {0, . . . , N } is shattered by HM for every N  N. Hence, VCdim(HM ) = .

Remark 2.17. As in Subsection 2.2, there is a na¨ive way of constructing a function class with property just established for HM . Namely, for a Turing machine M , we could define


H~M := {0, 1}{0,...,K-1} cc({0, 1})

if M halts after exactly K steps on the empty input ,
else

where we think of {0, 1}{0,...,K-1} as being embedded into {0, 1}N as the first K sequence elements, to which we append zeros. Actually, we have HM = H~M , the two function classes are equal. But whereas it might not be obvious from the definition of H~M that the mapping M  H~M is computable, based on Lemma 2.13 it is relatively easy to prove computability of M  HM (see Corollary 2.19). This is why we start from the possibly less intuitive definition of HM = H~M .
To use Lemma 2.16 for a reduction to the halting problem, we need to establish two claims. First, we need to show that HM is computable according to Definition C.3, so that is makes sense to talk about HM as input to a hypothetical algorithm that decides finiteness of the VC-dimension. Only then will HM , or more precisely the corresponding function HM , have a code that we can use as input for our hypothetical decision algorithm. Second, we need to show that constructing the class HM from the Turing machine M can be done in a computable way. I.e., we need to prove that the mapping M  HM is computable. We begin by establishing computability of HM .
Theorem 2.18. Let M be a Turing machine. The function class HM is computable.
Proof. We have already seen in Lemma 2.13 that there exists a primitive recursive function C : N × N  {0, 1}such that cc({0, 1}) = {m  C(m, n) | m  N}. Therefore, if we define


C(m, n) HM (m, n) =
0

if M does not halt after  n steps on the empty input ,
else

then HM = {n  HM (m, n) | m  N}. Moreover, HM is a computable function because is defined from computable functions and a case distinction with a computable predicate. Hence, HM is a computable function class according to Definition C.3.
Computability of HM can be seen more easily: HM is either finite and thus trivially computable or it is equal to cc({0, 1}) and thus computable by Lemma 2.13. We present the proof above because, similarly to our reasoning in Subsection 2.2, it already gives us the computability of M  HM :

11

Corollary 2.19. There exists a partial computable function H : N3  N such that HM = {N  n  H(M, m, n) | m  N} for any Turing machine M .

Again, H is total with respect to the second and third input. The computability of M  HM is crucial for the final step in our proof of Turing undecidability. And it provides an explicit description of the class HM obtained by fixing "input parameters" of the function H.
Now we have everything we need to finish the reduction to the halting problem and thereby our proof of Turing undecidability.

Corollary 2.20. There is no Turing machine that, upon input of the code of an arbitrary computable binary-valued function class, decides whether that class has finite VC-dimension. In other words, finiteness of the VC-dimension is Turing undecidable.

Proof. Assume for contradiction that there is such a Turing machine MVC. Then we could construct a Turing machine for solving the halting problem on the empty input as follows:
Given as input the code of a Turing machine M , compute the code of the corresponding class HM , or, more precisely, the function HM . This step is possible because the code of a concatenation of Turing machines is a primitive recursive function of their respective codes and because the mapping M  HM is computable by Corollary 2.19. Now feed that code to the Turing machine MVC. If it outputs 1 output, "yes, halts," otherwise output "no, doesn't halt."
As the halting problem is Turing undecidable, we have reached a contradiction. Therefore, the assumed Turing machine does not exist.

Remark 2.21. We can imitate the construction of HM for formal systems. Namely, with F and  as in Subsection 2.2, we can define, for a  cc({0, 1}),


a(n) g~a(n) =
0

if (1), . . . , (n) are consistent ,
else

and the function class G~F := {g~a}acc({0,1}). Then, VCdim(G~F ) <  if and only if F is inconsistent. And the mapping F  G~F is computable. We see that, for a suitable F , the infiniteness of VCdim(G~F ) is Go¨del undecidable.
Remark 2.22. Both GF and HM have a finite VC-dimension if and only if they consist only of finitely many distinct functions. Hence, our reasoning implies that, unsurprisingly, (in-)finiteness of a function class is in general Go¨del/Turing undecidable. However, as there are infinite function classes with finite VC-dimension, undecidability of the (in-)finiteness of function classes does not yet imply undecidability of the (in-)finiteness of the VC-dimension.
Remark 2.23. One could attempt to derive our Turing undecidability result from Rice's theorem. Informally, Rice's theorem states that any non-trivial semantic property of Turing machines is Turing undecidable [Ric53]. The property "M is a (2-input) Turing machine implementing a class

12

of {0, 1}-valued functions on N that has finite VC-dimension" is non-trivial and semantic, thus it is Turing undecidable. However, whether a Turing machine implements a class of {0, 1}-valued functions on N according to Definition C.3 is basically equivalent to whether it halts on every input. Thus, we have found a well known Turing undecidable property hiding in the one above, which makes its undecidability less surprising.
Note the contrast to our result: We only require our hypothetical decision algorithm to work on inputs that describe valid computable function classes. In particular, the algorithm can operate under the premise that it will only receive codes as input that describe total computable functions. Thus, our Turing undecidability result cannot be obtained directly from Rice's theorem.
Remark 2.24. There is a standard way, explained, e.g, in Section 2 of [Poo14], of deriving a Go¨del undecidability result from a Turing undecidability result. This allows us to derive from Corollary 2.20: For any recursively enumerable formal system F , there exists a class of {0, 1}-valued functions on N such that neither finiteness nor infiniteness of its VC-dimension can be proved in F .
The advantage of our reasoning in Subsection 2.2 over this result: Corollary 2.11 provides us with a concrete example of a function class for which finiteness of the VC-dimension is Go¨del undecidable. In that sense, the relationship between Corollary 2.11 and the Go¨del undecidability result just derived from Corollary 2.20 is analogous to the relationship between Go¨del's second and Rosser's [Ros36] strengthening of the first incompleteness theorem.
In fact, starting from a Turing undecidability result, one can derive a Go¨del undecidabiltiy result akin to the second incompleteness theorem, compare the essays [Obe19; Cub21]. In our case, starting from Corollary 2.20, given a recursively enumerable formal system F , one can explicitly describe a Turing machine M , depending on F , such that neither VCdim(HM ) <  nor VCdim(HM ) =  can be proved in F . This HM is then a concrete function class for which finiteness of VCdim(HM ) is Go¨del undecidable in F and thus gives a result comparable to Corollary 2.11. We have presented our results on independence of the axioms of a formal system and on uncomputability separately, so that these parts of the paper can be read independently from one another.
3 Undecidability in Teaching Problems
In this section, we demonstrate that the function classes constructed in Section 2 are useful beyond the scenario of PAC binary classification, namely also for teaching problems.
3.1 Preliminaries: Teaching Problems and the Teaching Dimension
We now turn our attention to a different learning problem. The differences to the PAC model are two-fold. The source of the training data is now a benevolent teacher who knows the function to be learned. And, instead of requiring the learner to approximate the unknown function with high probability, the unknown function must be exactly identified. To help the learner identify the target function, the teacher has to provide a training data set that uniquely characterizes it. The
13

difficulty of the learning/teaching problem is then captured by the worst case size of a smallest such training data set. This is made formal in the following
Definition 3.1 (Teaching sets and the teaching dimension [GK95]). Let G  {0, 1}X , g  G. A set S = {(xi, yi)}Ni=1  X × {0, 1}, N  N  {}, is a teaching set for g in G if g(xi) = yi for all (xi, yi)  S and for every g~  G \ {g} there exists (xj, yj)  S such that g~(xj) = yj. I.e., g is the unique concept in G that is consistent with the labelled data S.
The teaching dimension of G is the worst case size of a minimal teaching set, i.e.,

Tdim(G) := sup inf{|S| | S is a teaching set for g}.
gG
We consider a learning/teaching problem for a class G to be solvable if Tdim(G) < . Note that we will use this notion specifically for X = N. This is non-standard. Usually, X is assumed to be finite so that the teaching dimension is automatically finite.
If the teacher and the learner are allowed to make additional assumptions about the respectively other party's strategy, more refined notions of teaching dimensions should be used (see [Zil+11] for an overview). We, however, restrict our attention to the simplest complexity measure for teaching tasks, namely the one in Definition 3.1.

3.2 G¨odel Undecidability of the Existence of Finite Teaching Sets
Before coming to the teaching dimension itself, we discuss a different problem in teaching. Namely, we ask whether, given a function that can be taught to a learner by a teacher using finitely many examples, we can always prove that this is the case. The answer will turn out to be no, in general.
For this and the next subsection, we take F and  as in Subsection 2.2. We consider the class of threshold functions on N and allow for the possibility of a "threshold at infinity." I.e., we consider

Fstep := {N  n  sgn(n - k) | k  N}  {0},


1 where we use the convention sgn(x) =
0

if x  0 . Note that Fstep consists of computable
if x < 0

functions and is a computable class in the sense introduced in Definition C.3.

We consider the function


0 fF : N  {0, 1}, fF (n) =
1

if (1), . . . , (n) are consistent .
else

Here, (1), . . . , (n) are said to be inconsistent if and only if for some 1  i, j  n we have (i) = ¬(j), and consistent otherwise.

14

Note that the mapping F  fF , where we think of F as given via the code of the corresponding , is computable. Clearly, fF  Fstep for any formal system F . Therefore we can study whether fF admits a finite teaching set in the class Fstep.
Proposition 3.2. fF admits a finite teaching set in Fstep iff F is inconsistent.
Proof. If F is inconsistent, then there exists k  N such that fF (n) = sgn(n - k) for all n  N. So fF is the only element of Fstep that is consistent with the training data set {(k - 1, 0), (k, 1)}. Thus, we have found a teaching set of size 2 for fF .
If F is consistent, then fF  0 is the zero-function. So any finite training data set consistent with fF is of the form {(ni, 0)}Ni=1 for ni  N, 1  i  N , N  N. But also the function N  n  sgn(n - k) with k = max1iN ni + 1 is an element of Fstep that is consistent with such a training data set. So fF cannot be uniquely identified in Fstep by a finite training data set. I.e., fF does not have a teaching set of finite size.

We see that the teaching dimension of Fstep is infinite. The formal system determines which element of Fstep we consider and Proposition 3.2 states that, if F is consistent, this "filters out" precisely the one concept in Fstep that does not have a finite teaching set.
If F is capable of expressing the function fF , the class Fstep, and the (non-)existence of finite teaching sets, we are again in the position to apply Go¨del's second incompleteness theorem.

Corollary 3.3. Assume that F is a recursively enumerable and consistent formal system that contains elementary arithmetic. The function fF defined above does not have a finite teaching set in Fstep, but this statement is not provable in F .
Remark 3.4. We can use a similar construction to establish an analogous Turing undecidability result. Namely, given a Turing machine M , we can define


0 fM : N  {0, 1}, fM (n) =
1

if M does not halt after  n steps on the empty input .
else

fM admits a finite teaching set in Fstep if and only if M halts on the empty input. Hence, as the mapping M  fM is computable, we conclude that there cannot be a general-purpose algorithm that, upon input of a computable function class and a function in that class, decides whether the function admits a finite teaching set in the class.

3.3 G¨odel Undecidability of Finiteness of the Teaching Dimension
Next, we study GF from the perspective of the teaching dimension. For the purpose of this discussion, F ,  and E2 are again as in Subsection 2.2. Our first observation is that also finiteness of the teaching dimension of GF can be related to consistency of underlying formal system. Proposition 3.5. F is consistent iff Tdim(GF ) < .
15

Proof. The proof is similar to that of Corollary 2.9. See Appendix A for details.
The proof of Proposition 3.5 shows that, if F is inconsistent, then in fact no element of GF has a finite teaching set. This is different from our result of the previous subsection, where a single function in Fstep required teaching sets of infinite size and whether this function was the one characterized by F depended on (in-)consistency.
Again, if F can reason about GF and the finiteness of its teaching dimension, we can combine Proposition 3.5 with Go¨del's second incompleteness theorem:
Corollary 3.6. Assume that F is a recursively enumerable and consistent formal system that contains elementary arithmetic. Then GF has finite teaching dimension, but this statement cannot be proved in F .
Thus, we have shown that also the teaching dimension captures the contrast between the "collapse" of GF in the consistent case and the "richness" of GF in the inconsistent case. Therefore, finiteness of the teaching dimension is also Go¨del undecidable.
Remark 3.7. If we leave aside questions of computability, we could also consider the following construction: Take F~step  {0, 1}N to be the class of proper step functions and consider the class {fF }  F~step. This class has finite teaching dimension if and only if F is inconsistent.

3.4 Turing Undecidability of Finiteness of the Teaching Dimension

We can also view HM through the lens of teacher-learner interactions. As before, the first step in our approach consists in relating whether the underlying Turing machine M halts to whether the teaching dimension of HM is finite.

Proposition 3.8. Let M be a Turing machine. The binary-valued function class HM satisfies


K Tdim(HM) =


if M halts after exactly K  N steps on the empty input .
else

Proof. This follows from the equality HM = H~M (Remark 2.17). See Appendix A for details.
As we already know from Corollary 2.19 that HM can be computed from the underlying Turing machine M , we can again reduce to the halting problem and obtain

Corollary 3.9. There is no Turing machine that, upon input of the code of an arbitrary computable binary-valued function class, decides whether that class has finite teaching dimension. In other words, finiteness of the teaching dimension is Turing undecidable.

16

4 Undecidability in Online Learning Problems
As a final demonstration of the applicability of our constructions to different learning models, we show that universal and uniform online learning are undecidable in the by now familiar two senses.
4.1 Preliminaries: Online Learning and Littelstone Trees and Dimension
In online learning, we consider a game between two players, a learner L and an adversary A, both of which know the function class G  {0, 1}X . The game consists of infinitely many rounds. Round t  N1 consists of three steps: First, A chooses a "question" xt  X . Second, L guesses a label y^t  {0, 1}. Third, A reveals the true label yt  {0, 1} to L. Crucially, A must ensure that the sequence of true labels can actually be realized within G. I.e., the produced sequence ((xt, yt)) t=1 must be such that, for every t  N1, there exists a function g  G with g(xs) = ys for all 1  s  t. Note: A does not have to pick a fixed g  G in advance. Instead A can choose the true labels adaptively, based on the actions of L and A in previous rounds.
The goal of L is to make as few mistakes as possible, where we say that L makes a mistake in round t  N1 if y^t = yt. Conversely, A wants to make the number of mistakes made by L as large as possible. Note that, while we can also interpret teaching problems as two-player games, the role of the second player is quite different. A teacher is seen as benevolent and has the same goal as the learner. In contrast, an adversary's goal is exactly opposite to that of the learner.
We consider two variants of the online learning problem. On the one hand, we work in the scenario of universal online learning, recently introduced in [Bou+20]. We say that G  {0, 1}X is universally online learnable if there exists an adaptive strategy y^t = y^t(x1, y1, . . . , xt-1, yt-1, xt) for L such that, for any adversary A, L makes only finitely many mistakes in the above game. On the other hand, we also formulate results in the uniform mistake bound model of online learning, which we refer to as uniform online learning. We say that G  {0, 1}X is uniformly online learnable if there exist a d  N and an (adaptive) strategy y^t = y^t(x1, y1, . . . , xt-1, yt-1, xt) for L such that, for any adversary A, L makes at most d mistakes in the above game.
Both whether a class is universally or uniformly online learnable can be understood in terms of so-called Littlestone trees.
Definition 4.1 (Littlestone trees [Lit88; Bou+20]). A set of points {xv}v{0,1}k,1k<d  X is a Littlestone tree of depth d   of a binary-valued function class G  {0, 1}X if, for every y1, . . . , yd and for every 0  n < d, there exists g  G such that g(xy1...yk ) = yk+1 holds for all 0  k  n. We say that G has an infinite Littlestone tree if there exists a Littlestone tree of depth  of G.
A Littlestone tree of G is a complete binary tree in which the nodes are labelled by points in X and the edges are labelled by 0 or 1 in such a way that for every path of finite length, starting from the root of the tree, there is a function in G that labels all nodes along the path according to the respectively outgoing edges. Note that the definition is only concerned with finite paths, even for an infinite Littlestone tree.
17

The relation between universal online learnability and the (non-)existence of infinite Littlestone trees is summarized in the following Theorem 4.2 (Theorem 3.1 in [Bou+20]). G  {0, 1}X is universally online learnable iff G does not have an infinite Littlestone tree.
Going from "universal" to "uniform" on the level of Littlestone trees corresponds to requiring a uniform bound on the depth of all Littlestone trees of a class. This gives rise to Definition 4.3 (Littlestone dimension [Lit88]). Let G  {0, 1}X . The Littlestone dimension of G is defined to be
Ldim(G) := sup {d  N0 | G has a Littlestone tree of depth d} .
Note that, if G has an infinite Littlestone tree, then Ldim(G) = . The converse, however, is not true, as Ldim(G) =  also holds if G has Littlestone trees of arbitrarily large depth but no infinite Littlestone tree.
The Littlestone dimension characterizes uniform online learnability according to the following Theorem 4.4 (Theorem 3 in [Lit88]). G  {0, 1}X is uniformly online learnable with at most d  N mistakes iff Ldim(G)  d. In particular, G  {0, 1}X is uniformly online learnable iff Ldim(G) < .
4.2 G¨odel Undecidability of Finiteness of the Littlestone Dimension
We have seen in Theorem 4.4 that uniform online learnability is equivalent to the Littlestone dimension being finite. Therefore, we again start by relating consistency of the formal system F underlying GF to finiteness of the Littlestone dimension of GF . For both this subsection and for Subsection 4.4, we use the notation and classes introduced in Subsection 2.2. Proposition 4.5. F is consistent iff Ldim(GF ) < . Proof. This follows from our results on the VC-dimension and the inequality VCdim  Ldim. See Appendix A for details.
The Go¨del undecidability of uniform online learnability now follows as in Subsection 2.2: Corollary 4.6. Assume that F is a recursively enumerable and consistent formal system that contains elementary arithmetic. Then GF has finite Littlestone dimension, but this statement cannot be proved in F .
18

4.3 Turing Undecidability of Finiteness of the Littlestone Dimension

This subsection as well as Subsection 4.5 use the notation and constructions from Subsection 2.3. With Theorem 4.4 and the results from Subsection 2.3 in place, the only step left is to observe that HM has finite Littlestone dimension iff M halts.

Proposition 4.7. Let M be a Turing machine. The binary-valued function class HM satisfies

 K Ldim(HM ) = 

if M halts after exactly K steps on the empty input .
else

Proof. This follows from our results on the VC-dimension and the inequalities VCdim(HM )  Ldim(HM )  log2|HM |. See Appendix A for details.
Now, the line of reasoning presented in Subsection 2.3 implies the Turing undecidability of uniform online learnability:
Corollary 4.8. There is no Turing machine that, upon input of the code of an arbitrary computable binary-valued function class, decides whether that class has finite Littlestone dimension. In other words, finiteness of the Littlestone dimension is Turing undecidable.

4.4 G¨odel Undecidability of the Existence of Infinite Littlestone Trees
Because of Theorem 4.2, we first establish an equivalence between the formal system F underlying the class GF being consistent and GF having no infinite Littlestone tree.
Proposition 4.9. F is consistent iff GF does not have an infinite Littlestone tree.
Proof. The proof is similar to that of Corollary 2.9. See Appendix A.
With this observation, the same reasoning, using Go¨del's second incompleteness theorem, as in Subsection 2.2 yields:
Corollary 4.10. Assume that F is a recursively enumerable and consistent formal system that contains elementary arithmetic. Then GF does not have an infinite Littlestone tree, but this statement cannot be proved in F .

4.5 Turing Undecidability of the Existence of Infinite Littlestone Trees
Analogously to the other scenarios, we want to connect the (non-)existence of infinite Littlestone trees for HM to whether or not M halts.
Proposition 4.11. Let M be a Turing machine. The binary-valued function class HM has an infinite Littlestone tree iff M does not halt on the empty input.

19

Proof. The proof is similar to that of Lemma 2.16. See Appendix A for details.
As before, because of the computability of M  HM , through Theorem 4.2, this implies that universal online learnability is Turing undecidable:
Corollary 4.12. There is no Turing machine that, upon input of the code of an arbitrary computable binary-valued function class, decides whether that class has an infinite Littlestone tree. In other words, the existence of infinite Littlestone trees is Turing undecidable.
5 Conclusion
In this work, we have shown that in the standard model of binary classification, in two models of online learning, and in a basic model describing teacher-learner interactions, it is in general undecidable whether the learning task can be completed. We have established this for two different meanings of "undecidable," the first being "true, but not provable in a formal system" and the second being "not computable." In both cases, our results follow by providing computable constructions that allow for a reduction of the problem of deciding finiteness of the complexity measure for the respective learning task to the prototypic undecidable problem, i.e., to proving consistency of a formal system or to deciding whether a Turing machine halts.
It was already known, due to [Ben+19], that learnability can be independent of the axioms of ZFC. We have proved a similar-in-spirit result for the arguably most influential learning model, the PAC model of binary classification. By discussing our proof strategy also for a teacher-learner model and for online learning, we have demonstrated that it is not specific to the PAC setting. Moreover, learnability can be undecidable also in other formal systems and in the terminology of computer science. A crucial feature of our constructions, especially for establishing undecidability in the latter sense, is that we are only dealing with computable objects. This is to be contrasted with [Ben+19], where the continuum is used. In particular, the arguments of [Ben+19] do not give uncomputability results. Note that, because our constructions are computable, instead of PAC learnability, equivalently we could have considered computable PAC learnability, because of Theorem 10 of [Aga+20], when restricting our attention to the realizable scenario.
We hope that our work adds to the ongoing research aiming towards a better understanding of the theoretical prospects and limits of machine learning. Our results indicate that potential problems for applications of machine learning do not only arise on the level of algorithmic design, which in itself is an extremely challenging task. Rather, already when faced with a task, we encounter a fundamental difficulty: It is in general not possible to decide whether that task is, from the information-theoretic perspective of sample complexity, leaving questions of computational complexity aside, learnable, i.e., in principle amenable to a solution via machine learning.
From a more practical perspective, our results can be interpreted as follows: When faced with a learning task, one can usually choose which hypothesis class to use. This choice will be guided
20

by different considerations, such as prior knowledge about the problem, potential issues for optimization, and questions of learnability. In particular, one usually chooses a class that is known to be learnable. Thereby, the "library" of candidate function classes is restricted to those whose learnability has already been established. Our results say that there is no generic way of enlarging this library: Every time one faces a learning problem for which all classes from the current library perform poorly, identifying a new suitable candidate class, even leaving questions of optimization aside, presents a new challenge because of learnability alone.
Finally, we mention some questions raised by our work: · We have approached learnability through criteria based on complexity measures of the func-
tion class under consideration. Can (un-)decidability be established for learnability via algorithmic properties, e.g., stability or compression-based schemes? · For our PAC learning scenario, we require a sample complexity bound that is uniform over the function class. Can our results be extended to non-uniform learning models in which the sample size is allowed to depend on the function to be learned, e.g., via some "weight parameter"? · As discussed in Remark 2.22, it would be interesting to see whether the finiteness of the VC-, teaching or Littlestone dimension remains undecidable also when restricting the potential inputs to codes of infinite function classes. · As observed in [Bou+20], universal online learning is closely connected to Gale-Stewart games. Do undecidability results in one of these two scenarios translate to the respectively other one? For example, can we recover the undecidability results for Gale-Stewart games due to [Rab58] and [Jon82] from our results in Section 4? Or can we these works to gain further insight into undecidability in online learning?
21

Acknowledgements
I want to thank Michael M. Wolf for stimulating discussions on questions of (un-)decidability and for suggesting the reasoning used in Subsection 3.2. I also thank the anonymous reviewers and the meta-reviewer from COLT 2021 for their feedback.
Support from the TopMath Graduate Center of TUM the Graduate School at the Technische Universita¨t Mu¨nchen, Germany, from the TopMath Program at the Elite Network of Bavaria, and from the German Academic Scholarship Foundation (Studienstiftung des deutschen Volkes) is gratefully acknowledged.

References

[Aga+20]

S. Agarwal et al. "On Learnability wih Computable Learners". In: Algorithmic Learning Theory (2020), pp. 48­60. issn: 1938-7228. url: http://proceedings.mlr.press/v117/agarwal20b.h (cited on pp. 5, 20).

[Bar93]

J. Barwise, ed. Handbook of mathematical logic. 8. impr. Vol. 90. Studies in logic and the foundations of mathematics. Amsterdam [u.a.]: North-Holland Publ, 1993. isbn: 978-0444863881 (cited on p. 27).

[Ben+19]

S. Ben-David et al. "Learnability can be undecidable". In: Nature Machine Intelligence 1.1 (2019), pp. 44­48. issn: 2522-5839. doi: 10.1038/s42256-018-0002-3 (cited on pp. 5, 20).

[Bou+20] O. Bousquet et al. A Theory of Universal Learning. Version 1. Nov. 9, 2020. arXiv: 2011.04483 [cs.LG] (cited on pp. 4, 17, 18, 21).

[Cub21] T. S. Cubitt. A Note on the Second Spectral Gap Incompleteness Theorem. Version 1. May 20, 2021. arXiv: 2105.09854 [quant-ph] (cited on p. 13).

[Dav82]

M. Davis. Computability and unsolvability. Dover books on advanced mathematics. New York: Dover, 1982. isbn: 978-0486614717 (cited on p. 27).

[End13] H. B. Enderton. A mathematical introduction to logic. 3rd ed. Oxford: Academic, 2013. isbn: 978-0123869777 (cited on p. 27).

[Gan20] A. Gandolfi. Decidability of Sample Complexity of PAC Learning in finite setting. Version 1. Feb. 26, 2020. arXiv: 2002.11519 [cs.LG] (cited on p. 5).

[GK95]

S. A. Goldman and M. J. Kearns. "On the Complexity of Teaching". In: Journal of Computer and System Sciences 50.1 (1995), pp. 20­31. issn: 00220000. doi: 10.1006/jcss.1995.1003 (cited on pp. 4, 14).

22

[Go¨d31]
[Har19] [Jon82] [Kle02] [Las92] [Lat96] [Lit88] [Obe19] [Poo14] [Rab58]
[Ric53] [Ros36] [SB19]

K. Go¨del. "U¨ ber formal unentscheidbare Sa¨tze der Principia Mathematica und verwandter Systeme I". In: Monatshefte fu¨r Mathematik und Physik 38.1 (1931), pp. 173­ 198. issn: 1436-5081. doi: 10.1007/BF01700692. url: https://link.springer.com/article/10.100 (cited on p. 27).
K. P. Hart. Machine learning and the Continuum Hypothesis. Version 3. Mar. 14, 2019. arXiv: 1901.04773 [math.LO] (cited on p. 5).
J. P. Jones. "Some undecidable determined games". In: International Journal of Game Theory 11.2 (1982), pp. 63­70. issn: 0020-7276. doi: 10.1007/BF01769063 (cited on p. 21).
S. C. Kleene. Mathematical logic. Dover ed. Mineola, N.Y.: Dover Publications, 2002. isbn: 0486425339 (cited on p. 27).
M. C. Laskowski. "Vapnik-Chervonenkis classes of definable sets". In: Journal of The London Mathematical Society-second Series 45 (1992), pp. 377­384 (cited on p. 5).
R. H. Lathrop. "On the learnability of the uncomputable". In: Proc. 13th International Conference on Machine Learning. Morgan Kaufmann, 1996, pp. 302­309 (cited on p. 4).
N. Littlestone. "Learning Quickly When Irrelevant Attributes Abound: A New LinearThreshold Algorithm". In: Machine Learning 2.4 (1988), pp. 285­318. issn: 1573-0565. doi: 10.1023/A:1022869011914 (cited on pp. 4, 17, 18).
S. Oberhoff. Incompleteness Ex Machina. Version 1. Sept. 6, 2019. arXiv: 1909.04569 [cs.LO] (cited on p. 13).
B. Poonen. "Undecidable problems: A sampler". In: Interpreting Go¨del: Critical Essays. Cambridge University Press, 2014, pp. 211­241. doi: 10.1017/CBO9780511756306.015 (cited on p. 13).
M. O. Rabin. "Effective computability of winning strategies". In: Contributions to the Theory of Games (AM-39), Volume III. Ed. by Melvin Dresher, Albert William Tucker, and Philip Wolfe. Princeton University Press, 1958, pp. 147­158. doi: doi:10.1515/9781400882151-00 url: https://doi.org/10.1515/9781400882151-008 (cited on p. 21).
H. G. Rice. "Classes of recursively enumerable sets and their decision problems". In: Transactions of the American Mathematical Society 74.2 (1953), pp. 358­366 (cited on p. 12).
B. Rosser. "Extensions of some theorems of Go¨del and Church". In: Journal of Symbolic Logic 1.3 (1936), pp. 87­91. issn: 0022-4812. doi: 10.2307/2269028 (cited on p. 13).
S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. 12th printing. Cambridge: Cambridge University Press, 2019. isbn: 9781107057135 (cited on p. 6).

23

[Soa16] [Sol08] [Tay19] [Tur37]
[Val84] [VC71]
[Zha18] [Zil+11]

R. I. Soare. Turing Computability. Berlin, Heidelberg: Springer Berlin Heidelberg, 2016. isbn: 978-3-642-31932-7. doi: 10.1007/978-3-642-31933-4 (cited on p. 27).
D. Soloveichik. Statistical learning of arbitrary computable classifiers. Version 2. July 10, 2008. arXiv: 0806.3537 [cs.LG] (cited on p. 5).
W. Taylor. Learnability can be independent of zfc axioms: Explanations and implications. Version 1. Sept. 16, 2019. arXiv: 1909.08410 [cs.LG] (cited on p. 5).
A. M. Turing. "On Computable Numbers, with an Application to the Entscheidungsproblem". In: Proceedings of the London Mathematical Society s2-42.1 (1937), pp. 230­265. issn: 0024-6115. doi: 10.1112/plms/s2-42.1.230 (cited on pp. 27, 28).
L. G. Valiant. "A theory of the learnable". In: Communications of the ACM 27.11 (1984), pp. 1134­1142. issn: 00010782. doi: 10.1145/1968.1972 (cited on p. 6).
V. N. Vapnik and A. Ya. Chervonenkis. "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities". In: Theory of Probability & Its Applications 16.2 (1971), pp. 264­280. doi: 10.1137/1116025 (cited on pp. 2, 6).
K. Zhao. A statistical learning approach to a problem of induction. Dec. 8, 2018. url: http://philsci-archive.pitt.edu/15422/ (cited on p. 5).
S. Zilles et al. "Models of Cooperative Teaching and Learning". In: J. Mach. Learn. Res. 12 (2011), pp. 349­384. url: http://portal.acm.org/citation.cfm?id=1953059 (cited on p. 14).

24

Appendix

A Proofs

Proof of Proposition 2.8. As F is inconsistent, there exists a theorem p such that both p and ¬p can be proved in F . As p can be proved in F , we have q  p (which is our notation for "q implies p"). By negation we then have ¬p  ¬q. As ¬p can be proved in F , also ¬q can be proved in F . If we now exchange q by ¬q in the above reasoning, we see that also q can be proved in F .
Proof of Lemma 2.13. We define C : N × N  {0, 1} via


nth bit in the binary representation of m C(m, n) =
0

if m > 0 and 2n  m .
else

As exponentiation and finding the binary representation of a natural number can be done in a primitive recursive manner, the function C is defined in terms of primitive recursive functions and a case distinction via a primitive recursive predicate Thus, C is itself primitive recursive.
Clearly, the function n  C(m, n) has finite support and is thus an element of cc({0, 1}). Conversely, if a  cc({0, 1}), then there exists K  N such that a(n) = 0 for all n > K. Hence, if we take ma to be the natural number with binary representation a(0) . . . a(K), then a(n) = C(ma, n) for all n  N.
Proof of Proposition 3.5. If F is consistent, GF = {0} and the claim is trivial. If F is inconsistent, then uniquely identifying a function ga  GF requires one to uniquely identify the subsequence (akl)lN of a  cc({0, 1}) chosen such that kl+1 > kl and such that (E12(k)) = ¬(E22(k)) iff k = kl for some l  N. As (E12(k)) = ¬(E22(k)) is satisfied for infinitely many k  N (see Proposition 2.8) and the size of the support of an element of cc({0, 1}) can be arbitrarily large, any training data set that uniquely identifies ga has to consist of infinitely many labelled examples.
Proof of Proposition 4.5. If F is consistent, GF = {0} and clearly Ldim(GF ) = 0. If F is inconsistent, we can use the well known inequality VCdim  Ldim together with Theorem 2.7 to obtain Ldim(GF ) = .
Proof of Proposition 4.7. This follows quite directly the well known fact that, for any function class G  {0, 1}X , VCdim(G)  Ldim(G)  log2|G|.
Namely, if M halts on the empty input, these two inequalities, due to Lemma 2.16 and Remark 2.17, become K  Ldim(G)  log2|H| = K. And if M does not halt on the empty input, the lower bound via the VC-dimension, together with Remark 2.17, implies Ldim(G) = .
Proof of Proposition 4.9. If F is consistent, then Ldim(GF ) <  by Proposition 4.5. In particular, GF does not have an infinite Littlestone tree.

25

If F is inconsistent, then, as we have seen in the proof of Theorem 2.7, there exists a sequence (nk) k=1  N such that {n1, . . . , nN } is shattered by GF for every N  N1. Therefore, we obtain an infinite Littlestone tree of HM by labelling every node in the kth layer by nk+1, for k  N0.
Proof of Proposition 4.11. If M halts on the empty input, then Ldim(HM ) <  by Proposition 4.7. In particular, HM does not have an infinite Littlestone tree.
If M does not halt on the empty input, then {0, . . . , N } is shattered by HM for every N  N, as we have seen in the proof of Lemma 2.16. Therefore, we obtain an infinite Littlestone tree of HM by labelling every node in the kth layer by k, for k  N0.
B Go¨del and Incompleteness of Formal Systems
Here, we compile standard notions connected to formal systems which appear in the main body of the paper. However, some notions will only be introduced informally and the interested reader is referred to other sources for the formal definitions.
We denote by N the natural numbers including 0. We call a function f : Nk  N primitive recursive if it can be built from the zero function, the successor function, and the coordinate projection functions via composition and primitive recursion. From a modern perspective, the primitive recursive functions are those that can be implemented using basic arithmetic as well as IF THEN ELSE, AND, OR, NOT, =, >, and FOR loops. WHILE loops are not allowed here.
Next, we recall, albeit only informally, the notion of a formal system.
Definition B.1 (Formal systems - Informal). A formal system F consists of a finite alphabet of symbols, a language of statements that can be well-formed from the alphabet, a distinguished set of statements called axioms, and rules for how to derive/prove new theorems from these axioms. A formal system F is called consistent if there is no well-formed statement such that both it and its negation can be proved in F . Otherwise, we call F inconsistent.
We will be interested in a particular kind of formal systems in which the provable theorems, i.e., the statements that can be deduced from the axioms according to the derivation rules, can be recursively enumerated. To make this assumption more rigorous, we first recall
Definition B.2 (Go¨del numbering - Informal). A Go¨del numbering for a formal system F is an injective function that maps each symbol in the alphabet and each well-formed statement to an element of N.
For our purposes, it does not matter which Go¨del numbering is used. We only use that Go¨del numberings exist for which "translating" between a string of Go¨del numbers of symbols describing a statement and the actual Go¨del number of that statement can be done primitive recursively in both directions. Go¨del's original construction has this property. From now on, we fix such a Go¨del numbering. This allows us to identify statements in a formal system with elements of N and
26

"manipulations" of statements with primitive recursive maps between natural numbers. Both of these identifications will sometimes be implicit throughout the paper.
From this perspective, we can describe the type of formal systems used in this work.
Definition B.3 (Recursively enumerable formal systems). A formal system F is called recursively enumerable (or effectively axiomatized) if there exists a primitive recursive function  : N  N such that {(n) | n  N} is exactly the set of all G¨odel numbers in a fixed Go¨del numbering of statements that can be proved in F .
Given such a primitive recursive enumeration  of provable theorems, we will sometimes abuse notation and take (n) to denote both a theorem and its Go¨del number. The exact meaning, if not made explicit, will be clear from the context.
Go¨del's second incompleteness theorem provides, for any recursively enumerable and consistent formal system that contains elementary arithmetic, an explicit statement that is true but cannot be proved in that formal system.
Theorem B.4 (Go¨del's second incompleteness theorem [Go¨d31]). Assume that F is a recursively enumerable and consistent formal system that contains elementary arithmetic. Then the consistency of F is not provable in F .
We call a statement that is true but not provable in a formal system F Go¨del undecidable in F . This is not standard terminology, we merely use it to shorten some formulations.
For a more formal presentation of these and other notions from mathematical logic, the reader is referred to textbooks such as [Bar93; Kle02; End13].
C Turing and Uncomputability
This section recalls standard definitions and results related to Turing machines and computability. Again, sometimes we give only an informal presentation and refer to textbooks for details.
In [Tur37], Turing introduced what are now known as a Turing machines. We do not give a formal definition, but instead describe the workings of a Turing machine informally. For a more rigorous presentation, see, e.g., [Dav82; Soa16].
Definition C.1 (Turing machines - Informal). A Turing machine M consists of
· a 1-dimensional tape with infinitely many cells extending in both directions, each of which contains a symbol from a finite alphabet ,
· a head that can read and write symbols in a single cell and move to the left or to the right by one cell,
· a finite set of states Q containing an initial state and a halting state,
27

· and an instruction function I :  × Q   × {L, R} × Q describing the write-, move- and state-update-behaviour of M upon reading a given symbol while in a given state.
The two distinguished states are the initial state, in which the Turing machine begins any of its computations, and the halting state, that causes the Turing machine to halt when it is reached.
According to the Church-Turing thesis, which could be considered a "law of nature" for the world of computing, everything that can be reasonably considered computable is computable by a Turing machine. Hence, we take Turing machines as our model for defining computability.
Definition C.2 ((Turing) Computable functions). A partial function f : Nk  N, for k  N1, is (Turing) computable if there exists a finite-state Turing machine M such that, whenever we run M on a tape with an encoding of x  dom(f ) written on it, M eventually halts with the tape containing an encoding of f (x), and whenever we run M on a tape with an encoding of x  dom(f ) written on it, M does not halt.
One possible choice of encoding is the unary encoding. I.e., x  N is represented by x + 1 consecutive ones on the tape. The remaining tape is left blank. An element of Nk can be represented by k blocks of unary encodings of the components, separated by single zeros.
It is useful to note at this point that any primitive recursive function is computable. However, there are computable functions that are not primitive recursive.
We call a decision problem whose corresponding function, mapping instances of the problem to a binary "yes-or-no" output, is not computable Turing undecidable. The prototypic example of a Turing undecidable decision problem is the halting problem, i.e., the problem of deciding whether a given Turing machine halts on the empty input. Already [Tur37] observed that this cannot be achieved in a computable way.
We will also use a notion of computability of function classes.
Definition C.3. We say that a class G  NN is computable if there exists a total computable function G : N × N  N such that G = {n  G(m, n) | m  N}.
We recall one last fact related to Turing machines. Namely, there exist universal Turing machines capable of simulating any Turing machine [Tur37]. From now on, for each k  N1, we fix such a universal Turing machine understood as a partial computable function M : Nk+1  N. Then, for any Turing machine M and corresponding partial computable function fM : Nk  N, there exists a natural number, also denoted by M , such that fM (x) = M(M, x) for every x  N. The natural number M is called the code of the Turing machine M with respect to M. This allows us to think of Turing machines, or, equivalently, computable functions, as input when representing them by their code with respect to our fixed universal Turing machine.
28

