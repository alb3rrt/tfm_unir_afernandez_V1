SemiFL: Communication Efficient Semi-Supervised Federated Learning with Unlabeled Clients

arXiv:2106.01432v1 [cs.LG] 2 Jun 2021

Enmao Diao Department of Electrical and Computer Engineering
Duke University enmao.diao@duke.edu

Jie Ding School of Statistics University of Minnesota-Twin Cities dingj@umn.edu

Vahid Tarokh Department of Electrical and Computer Engineering
Duke University vahid.tarokh@duke.edu

Abstract
Federated Learning allows training machine learning models by using the computation and private data resources of a large number of distributed clients such as smartphones and IoT devices. Most existing works on Federated Learning (FL) assume the clients have ground-truth labels. However, in many practical scenarios, clients may be unable to label task-specific data, e.g., due to lack of expertise. In this work, we consider a server that hosts a labeled dataset, and wishes to leverage clients with unlabeled data for supervised learning. We propose a new Federated Learning framework referred to as SemiFL in order to address the problem of Semi-Supervised Federated Learning (SSFL). In SemiFL, clients have completely unlabeled data, while the server has a small amount of labeled data. SemiFL is communication efficient since it separates the training of server-side supervised data and client-side unsupervised data. We demonstrate various efficient strategies of SemiFL that enhance learning performance. Extensive empirical evaluations demonstrate that our communication efficient method can significantly improve the performance of a labeled server with unlabeled clients. Moreover, we demonstrate that SemiFL can outperform many existing FL results trained with fully supervised data, and perform competitively with the state-of-the-art centralized Semi-Supervised Learning (SSL) methods. For instance, in standard communication efficient scenarios, our method can perform 93% accuracy on the CIFAR10 dataset with only 4000 labeled samples at the server. Such accuracy is only 2% away from the result trained from 50000 fully labeled data, and it improves about 30% upon existing SSFL methods in the communication efficient setting.

1 Introduction
For billions of users around the world, mobile devices and Internet of Things (IoT) devices are becoming common computing platforms [1]. These devices produce a large amount of data that can be used to improve a variety of existing applications [2]. Consequently, it has become increasingly appealing to process data and train models locally from privacy and economic standpoints. To address this, distributed machine learning framework of Federated Learning (FL) has been proposed [3, 4]. This method aggregates locally trained model parameters in order to produce a global inference model without sharing private local data.
Preprint. Under review.

Most existing works of FL focus on supervised learning tasks assuming that clients have groundtruth labels. However, in many practical scenarios, most clients may not be experts in the task of interest to label their data. In particular, the private data of each client may be completely unlabeled. For instance, a healthcare system may involve a central hub ("server") with domain experts and a limited number of labeled data (such as medical records), together with many rural branches with non-experts and a massive number of unlabeled data. As another example, an autonomous driving startup ("server") may only be able to afford beta-users assistance in labeling a road condition, but desires to improve its modeling quality with the information provided by many decentralized vehicles that are not beta-users. The above scenarios, summarized in Figure 1, naturally lead to the following important question. How a server that hosts a labeled dataset can leverage clients with unlabeled data for a supervised learning task in the Federated Learning setting?
In this work, we propose a new Federated Learning framework referred to as SemiFL to address the problem of Semi-Supervised Federated Learning (SSFL). Our solution is inspired by a series of recent Federated Learning and Semi-Supervised Learning (SSL) solutions. The key ingredients that enable SemiFL to utilize decentralized unsupervised data are: (i) We alternate the training of labeled server and unlabeled clients to ensure that the quality of pseudo-labeling is highly maintained during the training, (ii) We use a suitable blend of two loss components for each client, including the "fix" loss that operates on high-confidence and strongly augmented data and the "mix" loss that operates on the weakly augmented data. We perform extensive empirical experiments to evaluate SemiFL and compare it with various baselines and state-of-the-art techniques. The results verify that SemiFL can achieve state-of-the-art performance results.
Our main contributions are summarized below.
· We introduce a new distributed machine learning solution named SemiFL to address Federated Learning scenarios in which the private data at the local clients can be completely unlabeled. We identify several key components that enable SemiFL to outperform the state-of-the-art methods, and highlight how they are connected to several recently developed FL and SSL techniques.
· We perform extensive experimental studies to evaluate SemiFL and verify its performance for Semi-Supervised Federated learning. In particular, we demonstrate the following. (i) SemiFL can significantly improve the performance of a labeled server using unlabeled clients, e.g., from 42% to 88% with 250 labeled data, or to 93% accuracy with 4000 labeled data on the CIFAR10 dataset. The latter accuracy is only 2% away from the state-of-the-art result trained from 50000 fully labeled data, and provides 30% improvement over the existing FL methods in the same setting. (ii) SemiFL can outperform many existing FL methods with fully supervised data such as FedAvg [4], LG-FedAvg [5], FedDyn [6], and HeteroFL [7] in both IID and Non-IID client-side data scenarios. (iii) SemiFL performs competitively with the state-of-the-art centralized semi-supervised learning methods, including MixMatch [8], UDA [9], FixMatch [10], and ReMixMatch [11], and significantly improves upon earlier centralized semi-supervised learning methods such as -Model [12], PseudoLabeling [13], and Mean Teacher [13].
Overall, our extensive experimental studies indicate that SemiFL is well-suited to Federated Learning scenarios where the local data are completely unlabeled. The outline of the rest of this paper is given below. In Section 2, we review the related work. In Section 3, we present the proposed SemiFL solution and some intuitive explanations. In Section 4, we evaluate the empirical performance of the SemiFL. We make some concluding remarks in Section 5.
2 Related Work
Federated Learning The goal of Federated Learning is to scale and speed up the training of distributed models [14,15]. Communication efficiency, system heterogeneity, statistical heterogeneity, and privacy are all major issues in FL [16]. To reduce the communication costs in FL, some studies propose using data compression techniques [3, 17, 18], adding regularization terms for local optimization [6,19], and developing FL counterparts of Batch Normalization [7,20,21]. Moreover, the use of local momentum and global momentum [22] have been shown to facilitate faster convergence. In order to address potential system heterogeneity, asynchronous communication and active client sampling techniques have been developed [14, 23]. Statistical heterogeneity potentially poses yet another major challenge. Several methods have been proposed in order to address Non-IID data in FL, such as personalized local models [5], assisted learning [24], meta-learning [25, 26], multi-task
2

Server

Client 1 Client 2

Client 1 Client 2

Server

Client M

Client M

Figure 1: A resourceful server with labeled data can significantly improve its predictive performance by leveraging distributed clients with unlabeled data under the FL framework.

Figure 2: A glimpse of the SemiFL procedure. The subscripts s and u denote the supervised data of server and unsupervised data of clients, respectively. The x and y denote their private data and pseudo-labels, respectively. The W represents model parameters.

learning [27], transfer learning [28, 29], knowledge distillation [30], lottery ticket hypothesis [31], and masked cross-entropy [7] methods.
Semi-Supervised Learning Semi-Supervised Learning (SSL) refers to the general problem of learning with partially labeled data, especially when the amount of unlabeled data is much larger than that of the labeled data [12, 32]. The idea of self-training (namely to obtain artificial labels for unlabeled data from a pre-trained model) can be traced back to decades ago [33, 34], and has been applied to various domains such as language processing [35], object detection [36, 37], image classification [38,39], and domain adaptation [40]. Pseudo-labeling [38], a component of many recent SSL techniques [41], is a form of entropy minimization [42] by converting model predictions into hard labels. Consistency regularization [43] refers to training models via minimizing the distance among stochastic outputs [12, 43]. Various stochastic approaches have been proposed, such as exponential moving average of model parameters [13], previous model checkpoints [44], stochastic regularization [44­46], and adversarial perturbations [41]. More recently, It has been demonstrated that the technique of strong data augmentation can lead to better outcomes [8, 9, 11, 47]. Strongly augmented examples are frequently found outside of the training data distribution, which has been shown to benefit SSL [48]. Noisy Student [39] has combined these strategies into a self-training framework, demonstrating outstanding performance on ImageNet with a large quantity of unlabeled data. A unified theoretical analysis of self-training with deep networks for SSL was recently proposed in [49]. Our work is based on the aforementioned SSL works, particularly the FixMatch [10] and ReMixMatch [11]. The FixMatch combines the two existing techniques, namely pseudo-labeling and consistency regularization [10]. Our proposed method leverages the strong data augmentation used in FixMatch, and the technique of Mixup data augmentation [50] used in ReMixMatch for training unlabeled data at the client-side.
Semi-Supervised Federated Learning (SSFL) The majority of existing FL works focus on supervised learning tasks, with clients having ground-truth labels. However, in many real-world scenarios, most clients are unlikely to be experts in the task of interest, an issue raised in a recent survey paper [51]. In the research line of SSFL, a consistency loss based on the agreement among clients was developed in [52]. The paper [53] assumes that part of clients have unsupervised data, and trains a convergent model at the server to label them. The paper [54] considers using shared unlabeled data for Federated Distillation [55, 56]. Another related work [57] trains and aggregates the model parameters of the labeled server, and unlabeled clients in parallel. Applications of SSFL to specific applications can be found in, e.g., [58, 59]. In the standard communication efficient scenario [4] with unlabeled clients, existing methods fail to perform closely to the state-of-the-art centralized SSL methods [52, 57]. This is somewhat surprising given that their underlying methods of training unlabeled data are similar. We will show that the current SSFL methods cannot outperform training with only the labeled data. Our proposed method (SemiFL) should be the first work that performs competitively with the state-of-the-art centralized SSL methods to the best of our knowledge. Moreover, we demonstrate that SemiFL can outperform some existing FL results trained from fully supervised data. Moreover, our proposal provides a large extension of FL to many practical applications.

3

3 Method

3.1 The SemiFL Algorithm
In a supervised learning classification task, we are given a dataset D = {xi, yi}Ni=1, where xi is a feature vector, yi is an one-hot vector representing the class label in a K-class classification problem, and N is the number of training examples. In a semi-supervised learning classification task, we have two datasets, namely a supervised dataset S and an unsupervised dataset U . Let S = {xis, ysi }Ni=S1 be a set of NS labeled data observations, and U = {xiu}Ni=U1 be a set of NU unlabeled observations (without the corresponding true label yui ). It is often interesting to study the case where NS NU .
In this work, we focus on Semi-Supervised Federated Learning (SSFL) with unlabeled clients as illustrated in Figure 2. Assume that there are M clients and let xu,m denote the set of unsupervised data available at client m = 1, 2, · · · , M . Similarly, let (xs, ys) denote the set of labeled data available at the server. The server model is parameterized by model parameters Ws. The client models are parameterized respectively by model parameters {Wu,1, . . . , Wu,M }. We assume that all models share the same model architecture, denoted by f : (x, w)  f (x, w), which maps an input x and parameters W to a vector on the K-dimensional simplex, for instance using softmax function applied to model outputs.

We summarize the pseudo-code of the proposed solution in Algorithm 1. At each iteration t, the server will first update the model with the standard supervised loss Ls with data batch (xb, yb) of size Bs randomly split from the supervised dataset Ds, using

Ls = (f ((xb), Ws), yb), Ws = Ws - W Ls,

(1)

where (·) represents a weak data augmentation, such as random horizontal flipping and random
cropping, that maps one image to another one. Subsequently, the server will update the static Batch
Normalization (sBN) statistics [7] (as discussed in Subsection 3.4). Next, the server will distribute server model parameters Ws to a subset of clients. We denote the proportion of active clients at each communication round t as activity rate Ct  (0, 1]. Without loss of generality, we assume that Ct = C is a constant over time. After each active local client, say client m, receives the transmitted Ws, it will generate pseudo-labels yu,m as follows:

Wu,m  Ws, yum = f ((xu,m), Wu,m).

(2)

Each local client will construct a high-confidence dataset Du+,m at each iteration t, defined as:

Du+,m = {(xu,m, yu,m) with max (yu,m)   }.

(3)

for a global confidence threshold 0 <  < 1 pre-selected by all clients. If for some client m, we have Du+,m =  then it will stop and refrain from transmission to the server. Otherwise, it will construct a
"Mixup" dataset by sampling (with replacement) from its low-confidence data (unlabeled data not in the set Du+,m). In other words,

Du-,m = Sample |Du+,m| with replacement{(xu,m, yu,m) with max (yu,m) <  }, (4)

where |Du+,m| denotes the number of elements of Du+,m. Thus |Du-,m| = |Du+,m|. Subsequently, client m trains its local model for E epoch to speed up convergence [4]. For each local training epoch of the client m, it randomly splits local data Du+,m, Du+,m into batches Bu+,m, Bu-,m of size Bm. For each batch iteration, as in [50], client m constructs Mixup data from one particular data batch (x+b , yb+), (x-b , yb-) in the following way.

mix  Beta(a, a), xmix  mixx+b + (1 - mix)x-b ,

(5)

where a is the Mixup hyperparameter. Next, client m defines the "fix" loss Lfix [10] and "mix" loss Lmix [11] by

Lfix = (f (A(x+b ), Wu,m), yb+),

(6)

Lmix = mix · (f ((xmix), Wu,m), yb+) + (1 - mix) · (f ((xmix), Wu,m), yb-) .

(7)

Here, A represents a strong data augmentation mapping, e.g., the RandAugment [60] used in our experiments, and is often the cross entropy loss for classification tasks. Finally, client m performs a gradient descent step with

Wu,m = Wu,m - W (Lfix +  · Lmix),

(8)

4

Algorithm 1 SemiFL: Semi-Supervised Federated Learning with Unlabeled Clients

Input: Unlabeled data xu,1:M distributed on M local clients, activity rate C, the number of communication rounds T , the number of local training epochs E, server and client respective

batch sizes Bs and Bm, local learning rate , server model parameterized by Ws client models parameterized by {Wu,1, . . . , Wu,M }, weak data augmentation function (·), strong data augmentation function A(·), confidence threshold  , Mixup hyper-parameter a, loss

hyperparameter , common model architecture function f (·)

System executes:

for each communication round t = 1, 2, . . . T do

Wst  ServerUpdate(xs, ys, Wst) Update the sBN statistics

St  max( C · M , 1) active clients uniformly sampled without replacement
for each client m  St in parallel do Distribute server model parameters to local client m, namely Wut,m  Wst Wut,m  ClientUpdate(xu,m, Wut,m)

end

Receive model parameters from Mt clients, and calculate Wst = Mt-1

Mt m=1

Wut,m

end

WsT  ServerUpdate(xs, ys, WsT ) Update the sBN statistics

ServerUpdate (xs, ys, Ws): Construct supervised dataset Ds = (xs, ys) for each local epoch e from 1 to E do

Bs  Randomly split local data Ds into batches of size Bs for batch (xb, yb)  Bs do
Ls  (f ((xb), Ws), yb) Ws  Ws - W Ls end

end
Return Ws ClientUpdate (xu,m, Wu,m):
Generate pseudo-label with weakly augmented data (xu,m), namely yu,m = f ((xu,m), Wu,m) Construct a high-confidence dataset, namely Du+,m = {(xu,m, yu,m) with max (yu,m)   } If Du+,m =  then Stop. Return. Construct an equal-size Mixup dataset, namely Du-,m = Sample |Du+,m| with replacement{(xu,m, yu,m) with max (yu,m) <  }
for each local epoch e from 1 to E do Bu+,m, Bu-,m  Randomly split local data Du+,m, Du+,m into batches of size Bm for batch (x+b , yb+), (x-b , yb-)  Bu+,m, Bu-,m do mix  Beta(a, a) xmix  mixx+b + (1 - mix)x-b Lfix  (f (A(x+b ), Wu,m), yb+) Lmix  mix · (f ((xmix), Wu,m), yb+) + (1 - mix) · (f ((xmix), Wu,m), yb-) Wu,m  Wu,m - W (Lfix +  · Lmix)
end

end Return Wu,m and send it to the server

where  > 0 is a hyperparameter set to be one in our experiments. After training for E local epochs, client m transmits Wu,m to the server.

Without loss of generality assume that clients 1, 2, · · · , Mt have sent their models to the server at time t. The server then aggregates client model parameters {Wu,1, . . . , Wu,Mt } by averaging as [4]:

1 Mt

Ws

=

Mt

Wu,m.
m=1

(9)

5

This process is then repeated for multiple communication rounds T . After the training is finished, the server will further fine-tune the aggregated model by additional training with the server's supervised data using its supervised loss Ls. Finally, it will update the sBN statistics one final time.
3.2 Alternate training
Compared with the existing SSFL methods such as [52, 57], a key ingredient of our proposed method is the alternate training of labeled server and unlabeled clients. In the state-of-the-art works on SSL, such as FixMatch, the training of supervised loss and unsupervised data is synchronized for every data batch [10]. Therefore, a vanilla implementation of SSL methods with FedAvg trains and aggregates model parameters of labeled server and unlabeled clients in parallel [52, 57]. However, existing papers [52, 57] indicate that this vanilla extension fails to perform closely to the state-of-the-art centralized SSL methods, even if the unlabeled clients are trained with the aforementioned SSL methods. To understand the bottleneck of this vanilla extension, we need to intuitively clarify the reason that the centralized SSL methods work. We can always use a model to generate pseudo-labels for unlabeled data [38]. However, the quality (Accuracy) of those pseudo-labels can be low, especially at the beginning of the training. In this light, several papers [9, 10] propose to hard-threshold or sharpen the pseudo-labels in order to improve the quantity of accurately labeled pseudo-labels. The problem with hard thresholding is that the data samples satisfying the confidence threshold have a small training loss. Therefore, the model cannot be significantly improved as it already performs well on the data above the threshold. To address this issue, we can use strong data augmentation [10, 48] to generate data samples that have larger training loss. In summary, a successful SSL method must be able to generate more and more high-quality pseudo-labels during training, while the corresponding data used for training the model must have a larger loss than that of the original data.
We can combine hard thresholding and strong data augmentation with the vanilla extension. However, in the FL setting, we cannot guarantee an increase in the quantity of accurately labeled pseudo-labels during training. Especially for the communication efficient scenario where the activity rate C is small, the aggregation of a server model trained with ground-truth labels and a subset of client models trained with pseudo-labels does not constantly improve the performance of the global model over the previous communication round. A poorly aggregated model of the previous communication round will result in worse quality pseudo-labels. Subsequently, the performance of the aggregated model will degrade at the next communication round. Therefore, the performance of the vanilla extension may suffer heavily from low-quality pseudo labels generated from parallelly aggregated models.
In order to avoid the above issue, we propose to train the labeled server and unlabeled clients alternatively. Moreover, we only generate pseudo-labels upon receiving the server model. In contrast, the original SSL methods often simultaneously generate pseudo-labels and train the model. Our proposed approach ensures that the clients can continually generate better quality pseudo-labels because the model used for pseudo-labeling is constantly fine-tuned with the ground-truth labels at the server-side. In fact, our experimental studies show that the proposed method performs competitively even with the state-of-the-art centralized semi-supervised learning methods.
3.3 Loss function
We use the standard supervised loss to train the labeled server. For training the unlabeled clients, the "fix" loss Lfix (originally proposed in FixMatch [10]) leverages the techniques of consistency regularization and pseudo-labeling simultaneously. Specifically, the pseudo-labels are generated from weakly augmented data, and the model is trained with strongly augmented data. The "mix" loss is adapted from the Mixup data augmentation, which reduces the memorization of corrupted labels and increases the robustness to adversarial examples [50]. It was also shown to benefit the SSL [11] and FL [61]. We have conducted an ablation study and demonstrated that the mix loss moderately improves performance.
3.4 Static Batch Normalization
We utilize a recently proposed adaptation of Batch Normalization (BN) named Static Batch Normalization (sBN) [7]. It was shown that this method greatly facilitates the convergence speed of FL compared with other forms of normalization, including InstanceNorm [62], GroupNorm (GN) [63], and LayerNorm [64]. During the training phase, sBN does not track the running statistics µ and 
6

with momentum as in BN. Instead, it simply standardizes the data batch xb in the following way.

y = xb - µ 2 +

·  + , µ = E[xb], 2 = Var[xb]

In FL training, the affine parameters  and  can be aggregated as usual. After the training process
is completed, the server will sequentially query local clients and cumulatively update the global statistics µ = E[x] and 2 = Var[x], where x represents the whole dataset.

In the context of SemiFL, we need to produce pseudo-labels at every communication round. Thus, the latency of sequentially querying local clients may not be negligible. Fortunately, we can utilize the server data xs to update the global statistics instead of querying each local client, where µ = E[xs] and 2 = Var[xs]. We will provide experimental results of querying the sBN statistics from all the clients here, and include an ablation study using only the server data in the supplementary document. Our ablation study shows that the alternative way of using the server data to update the global sBN statistics does not degrade the training performance.

3.5 Performance goal
Before the experiments, we outline the general performance goal of Semi-Supervised Federated Learning. The performance ceiling is obviously that of the Fully Supervised Learning (FSL) (namely, assuming that all the server's and clients' data are centralized and fully labeled). For our context where clients' data are unlabeled, a vanilla approach trains the labeled data only at the server-side, referred to as Partially Supervised Learning (PSL). Clearly, the PSL performance can serve as a lower bound benchmark for other approaches that employ additional unlabeled data. When the server contains a small amount of labeled data and a substantial amount of unlabeled data (centralized), the Semi-Supervised Learning (SSL) seeks the use of unlabeled data to improve over the PSL. It was shown that state-of-the-art SSL methods such as FixMatch [10] could produce similar results as FSL.

Our work focuses on Semi-Supervised Federated Learning (SSFL), where the unlabeled data are distributed among many clients. The general goal of SSFL is to perform similarly to the state-ofthe-art SSL, and significantly outperform PSL and the existing SSFL methods. In other words, our performance goal is to achieve

FSL SSL SSFL PSL.

(10)

4 Experiments
Experimental setup To evaluate our proposed method, we conduct experiments with CIFAR10 [65] and SVHN [66] datasets. To compare our method with existing FL and SSFL methods, we follow the standard communication efficient FL setting, which is originally used in FedAvg [4] and widely adopted by following works, such as [5­7]. We have 100 clients throughout our experiments, and the activity rate per communication round is C = 0.1. For IID data partition, we uniformly assign the same number of data examples to each client. For a balanced Non-IID data partition, we make sure each client has data at most from K classes, and the sample size of each class is the same. We set K = 2 because it is the most label-skewed case for classification, and it has been evaluated in [5­7]. For unbalanced Non-IID data partition, we sample data for each client from a Dirichlet distribution Dir() [6, 67]. As   , this reduces to IID data partition. We perform experiments with  = {0.1, 0.3}. To compare our method with the state-of-the-art SSL methods, we follow the experimental setup in [10]. We use Wide ResNet28x2 [68] as our backbone model for CIFAR10 and SVHN datasets throughout our experiments. The number of labeled data at the server for CIFAR10 and SVHN datasets NS are {250, 4000} and {100, 2500} respectively. Similar to [10], we use SGD as our optimizer and a cosine learning rate decay as our scheduler [69]. We also use the same hyperparameters as [10], where the local learning rate  = 0.03, the local momentum l = 0.9, and the confidence threshold  = 0.95. The Mixup hyperparameter a is set to be 0.75 as suggested by [50]. More details regarding the experimental setup can be found in the supplementary material. We conduct four random experiments for all the datasets with different seeds, and the standard deviations are shown inside the parentheses for tables and by error bars in figures.
Comparison with SSL methods We demonstrate our experimental results in Table 1 and the learning curves of CIFAR10 dataset in Figure 3. We also demonstrate the results of Fully Supervised and Partially Supervised cases, and existing SSL methods for comparison in Table 1. Our results

7

(a)  = 250

(b)  = 4000

Figure 3: Experimental results for CIFAR10 dataset with (a) NS = 250 and (b) NS = 4000.

Table 1: Test Accuracy of CIFAR10 and SVHN datasets. Our method performs competitively with SSL methods and significantly outperforms PSL (training with only the labeled data). All results are obtained with the same model architecture.

Dataset

Number of Supervised

Fully Supervised Partially Supervised

-Model [12] Pseudo-Labeling [13]
Mean Teacher [13] MixMatch [8] UDA [9]
ReMixMatch [11] FixMatch [10]

SemiFL

Non-IID, K = 2 Non-IID, Dir(0.1) Non-IID, Dir(0.3)
IID

CIFAR10

250

4000

95.33(0.12) 42.37(1.76) 76.92(0.17)

45.74(3.97) 50.22(0.43) 67.68(2.30) 88.95(0.86) 91.18(1.08) 94.56(0.05) 94.93(0.65)

85.99(0.38) 83.91(0.28) 90.81(0.19) 93.58(0.10) 95.12(0.18) 95.28(0.13) 95.74(0.05)

60.03(0.87) 63.05(0.61) 71.85(1.23) 88.23(0.28)

85.34(0.28) 84.53(0.35) 88.89(0.30) 93.10(0.14)

SVHN

250

1000

97.27(0.04) 77.14(2.86) 90.38(0.51)

81.04(1.92) 79.79(1.09) 96.43(0.11) 96.02(0.23) 94.31(2.76) 97.08(0.48) 97.52(0.38)

92.46(0.36) 90.06(0.61) 96.58(0.07) 96.50(0.28) 97.54(0.24) 97.35(0.08) 97.72(0.11)

87.54(1.10) 91.22(0.33) 93.97(0.54) 96.76(0.30)

92.20(0.78) 93.01(0.50) 95.16(0.21) 96.87(0.09)

significantly outperform the Partially Supervised case. In other words, SemiFL can significantly improve the performance of a labeled server with unlabeled clients in the communication efficient scenario. For IID data partition of CIFAR10 and SVHN datasets, our method performs competitively with the state-of-the-art SSL methods. Because we cannot train labeled and unlabeled data simultaneously, as the supervised data size decreases, the performance of SemiFL degrades more than the SSL methods. Moreover, it is foreseeable that as the clients become more label-skewed for Non-IID data partition, the performance of our method degrades. However, even the most label-skewed unlabeled clients can improve the performance of the labeled server using our proposed approach.
Comparison with FL and SSFL methods We compare our results with the state-of-the-art FL and SSFL methods in Table 3. We demonstrate that SemiFL can outperform many existing FL results trained with fully supervised data. There are two major reasons. First, we note that SemiFL uses WResNet28x2, which is known to perform better than other model architectures when data are fully supervised. We note that WResNet28x2 also has the smallest number of model parameters and thus transmits less information per communication round. Additionally, SemiFL adopts various training techniques such as sBN, global momentum [22], strong and Mixup data augmentation. We also demonstrate that our method significantly outperforms existing SSFL methods in communication efficient scenarios. We note that existing methods fail to perform closely to the state-of-the-art centralized SSL methods, even if their underlying methods of training unlabeled data are similar. Moreover, their results cannot outperform the case of training with only labeled data. Therefore, our proposed SemiFL may establish a strong benchmark candidate for SSFL. We compare the technical
8

Table 2: Comparison of training techniques among SSFL methods.

Method
FedMatch [52] FedRGD [57]
SemiFL

Pseudo-labeling
  

Consistency regularization
  

Data augmentation
Strong Strong Strong + Mixup

Training
Parallel Parallel Alternate

Normalization
None GN [63] sBN [7]

Table 3: Comparison of SemiFL with existing FL and SSFL methods on the CIFAR10 dataset. SemiFL significantly outperforms existing SSFL methods.

Method

Number of Supervised

M

C

T

Model

Parameters

FLOPs

Space (MB)

Accuracy

Non-IID K =2

IID

Fully Supervised

All

1 1 400 WResNet28x2 1.5 M 433 M

5.6

Partially Supervised 4000

1 1 400 WResNet28x2 1.5 M 433 M

5.6

95.33 76.92

FedAvg [4] LG-FedAvg [5]
FedDyn [6] HeteroFL [7]

All

100 0.1 2000

CNN

2.2 M 71 M

8.2

All

100 0.1 1800

CNN

2.2 M 71 M

8.2

All

100 0.1 600

CNN

2.2 M 71 M

8.2

All

100 0.1 800 ResNet18

11.2 M 1.1 G

42.6

58.99 60.79 N/A 56.88

85.00 69.76 84.50 91.19

FedMatch [52] FedRGD [57]
SemiFL

5000 100 0.1 200

AlexNet

6.5 M 91 M

24.9

5000 100 0.1 200

ResNet9

4.9 M 509 M

18.7

4000 100 0.1 800 WResNet28x2 1.5 M 433 M

5.6

N/A 63.24 85.34

41.97 63.32 93.10

novelties of SSFL methods in Table 2. It is evident that alternate training and sBN are the crucial ingredients of the success of our method.
Ablation study We perform an ablation study of the training techniques adopted in our experiments. We study the efficacy of the number of local training epoch E, the global SGD momentum g [22], and the Mixup data augmentation as shown in Table 4. Less local training epoch significantly hurts the performance due to slow convergence. The Mixup data augmentation has around 2% Accuracy improvement for CIFAR10 dataset. It follows that it is beneficial to combine strong data augmentation with Mixup data augmentation for training unlabeled data. The global momentum marginally improves the result. We provide additional ablation studies in the supplementary document.
Figure 4: Ablation study on the CIFAR10 dataset with 4000 labeled data at the server, for the cases of (a) IID and (b) Non-IID, K = 2 data partition.

(a) IID

(b) Non-IID,  = 2

5 Conclusion
In this work, we propose a new Federated Learning framework referred to as SemiFL to address the problem of Semi-Supervised Federated Learning (SSFL). We propose to alternatively train the labeled server and unlabeled clients. We utilize several training techniques and establish a strong benchmark for SSFL. Extensive experimental studies demonstrate that our communication efficient method can significantly improve the performance of a labeled server with unlabeled clients. Moreover, we demonstrate that SemiFL can outperform many existing FL results trained with fully supervised data, and perform competitively with the state-of-the-art centralized Semi-Supervised Learning (SSL) methods. Our proposal provides a practical FL framework and extends possible applications of FL.

9

E g mixup

SemiFL

Non-IID, K = 2

IID

1 0.5  5 0.5  50  5 0.5 

83.39(0.49) 84.17(0.44) 85.41(0.58) 85.34(0.28)

88.86(0.31) 91.27(0.24) 92.43(0.11) 93.10(0.14)

Table 4: Ablation study on the CIFAR10 datasets with 4000 labeled data at the server.

Acknowledgments and Disclosure of Funding
This work was supported by the Office of Naval Research (ONR) under grant number N00014-18-12244 and the Army Research Office (ARO) under grant number W911NF-20-1-0222.
References
[1] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang, D. Niyato, and C. Miao, "Federated learning in mobile edge networks: A comprehensive survey," IEEE Communications Surveys & Tutorials, 2020.
[2] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon, and D. Ramage, "Federated learning for mobile keyboard prediction," arXiv preprint arXiv:1811.03604, 2018.
[3] J. Konecny`, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon, "Federated learning: Strategies for improving communication efficiency," arXiv preprint arXiv:1610.05492, 2016.
[4] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, "Communication-efficient learning of deep networks from decentralized data," in Artificial Intelligence and Statistics. PMLR, 2017, pp. 1273­1282.
[5] P. P. Liang, T. Liu, L. Ziyin, R. Salakhutdinov, and L.-P. Morency, "Think locally, act globally: Federated learning with local and global representations," arXiv preprint arXiv:2001.01523, 2020.
[6] D. A. E. Acar, Y. Zhao, R. M. Navarro, M. Mattina, P. N. Whatmough, and V. Saligrama, "Federated learning based on dynamic regularization," in International Conference on Learning Representations, 2021.
[7] E. Diao, J. Ding, and V. Tarokh, "HeteroFL: Computation and communication efficient federated learning for heterogeneous clients," in International Conference on Learning Representations, 2021.
[8] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. Raffel, "Mixmatch: A holistic approach to semi-supervised learning," arXiv preprint arXiv:1905.02249, 2019.
[9] Q. Xie, Z. Dai, E. Hovy, M.-T. Luong, and Q. V. Le, "Unsupervised data augmentation for consistency training," arXiv preprint arXiv:1904.12848, 2019.
[10] K. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D. Cubuk, A. Kurakin, H. Zhang, and C. Raffel, "Fixmatch: Simplifying semi-supervised learning with consistency and confidence," arXiv preprint arXiv:2001.07685, 2020.
[11] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel, "Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring," arXiv preprint arXiv:1911.09785, 2019.
[12] A. Rasmus, H. Valpola, M. Honkala, M. Berglund, and T. Raiko, "Semi-supervised learning with ladder networks," arXiv preprint arXiv:1507.02672, 2015.
[13] A. Tarvainen and H. Valpola, "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results," arXiv preprint arXiv:1703.01780, 2017.
10

[14] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny`, S. Mazzocchi, H. B. McMahan et al., "Towards federated learning at scale: System design," arXiv preprint arXiv:1902.01046, 2019.
[15] C. He, S. Li, J. So, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh, H. Qiu, L. Shen, P. Zhao, Y. Kang, Y. Liu, R. Raskar, Q. Yang, M. Annavaram, and S. Avestimehr, "Fedml: A research library and benchmark for federated machine learning," arXiv preprint arXiv:2007.13518, 2020.
[16] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, "Federated learning: Challenges, methods, and future directions," IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50­60, 2020.
[17] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, "Qsgd: Communication-efficient sgd via gradient quantization and encoding," in Advances in Neural Information Processing Systems, 2017, pp. 1709­1720.
[18] N. Ivkin, D. Rothchild, E. Ullah, I. Stoica, R. Arora et al., "Communication-efficient distributed sgd with sketching," in Advances in Neural Information Processing Systems, 2019, pp. 13 144­ 13 154.
[19] A. K. Sahu, T. Li, M. Sanjabi, M. Zaheer, A. Talwalkar, and V. Smith, "Federated optimization for heterogeneous networks," arXiv preprint arXiv:1812.06127, vol. 1, no. 2, p. 3, 2018.
[20] K. Hsieh, A. Phanishayee, O. Mutlu, and P. Gibbons, "The non-iid data quagmire of decentralized machine learning," in International Conference on Machine Learning. PMLR, 2020, pp. 4387­4398.
[21] X. Li, M. Jiang, X. Zhang, M. Kamp, and Q. Dou, "Fedbn: Federated learning on non-iid features via local batch normalization," arXiv preprint arXiv:2102.07623, 2021.
[22] J. Wang, V. Tantia, N. Ballas, and M. Rabbat, "Slowmo: Improving communication-efficient distributed sgd with slow momentum," arXiv preprint arXiv:1910.00643, 2019.
[23] T. Nishio and R. Yonetani, "Client selection for federated learning with heterogeneous resources in mobile edge," in ICC 2019-2019 IEEE International Conference on Communications (ICC). IEEE, 2019, pp. 1­7.
[24] X. Xian, X. Wang, J. Ding, and R. Ghanadan, "Assisted learning: A framework for multiorganization learning," Advances in Neural Information Processing Systems, vol. 33, 2020.
[25] Y. Jiang, J. Konecny`, K. Rush, and S. Kannan, "Improving federated learning personalization via model agnostic meta learning," arXiv preprint arXiv:1909.12488, 2019.
[26] M. Khodak, M.-F. F. Balcan, and A. S. Talwalkar, "Adaptive gradient-based meta-learning methods," in Advances in Neural Information Processing Systems, 2019, pp. 5917­5928.
[27] V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, "Federated multi-task learning," in Advances in Neural Information Processing Systems, 2017, pp. 4424­4434.
[28] K. Wang, R. Mathews, C. Kiddon, H. Eichner, F. Beaufays, and D. Ramage, "Federated evaluation of on-device personalization," arXiv preprint arXiv:1910.10252, 2019.
[29] Y. Mansour, M. Mohri, J. Ro, and A. T. Suresh, "Three approaches for personalization with applications to federated learning," arXiv preprint arXiv:2002.10619, 2020.
[30] D. Li and J. Wang, "Fedmd: Heterogenous federated learning via model distillation," arXiv preprint arXiv:1910.03581, 2019.
[31] A. Li, J. Sun, B. Wang, L. Duan, S. Li, Y. Chen, and H. Li, "Lotteryfl: Personalized and communication-efficient federated learning with lottery ticket hypothesis on non-iid datasets," arXiv preprint arXiv:2008.03371, 2020.
[32] Z.-H. Zhou and M. Li, "Tri-training: Exploiting unlabeled data using three classifiers," IEEE Transactions on knowledge and Data Engineering, vol. 17, no. 11, pp. 1529­1541, 2005.
[33] H. Scudder, "Probability of error of some adaptive pattern-recognition machines," IEEE Transactions on Information Theory, vol. 11, no. 3, pp. 363­371, 1965.
[34] G. J. McLachlan, "Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis," Journal of the American Statistical Association, vol. 70, no. 350, pp. 365­369, 1975.
11

[35] D. McClosky, E. Charniak, and M. Johnson, "Effective self-training for parsing," in Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, 2006, pp. 152­159.
[36] C. Rosenberg, M. Hebert, and H. Schneiderman, "Semi-supervised self-training of object detection models," 2005.
[37] K. Sohn, Z. Zhang, C.-L. Li, H. Zhang, C.-Y. Lee, and T. Pfister, "A simple semi-supervised learning framework for object detection," arXiv preprint arXiv:2005.04757, 2020.
[38] D.-H. Lee et al., "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks," in Workshop on challenges in representation learning, ICML, vol. 3, no. 2, 2013.
[39] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, "Self-training with noisy student improves imagenet classification," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 687­10 698.
[40] Y. Zou, Z. Yu, B. Kumar, and J. Wang, "Unsupervised domain adaptation for semantic segmentation via class-balanced self-training," in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 289­305.
[41] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, "Virtual adversarial training: a regularization method for supervised and semi-supervised learning," IEEE transactions on pattern analysis and machine intelligence, vol. 41, no. 8, pp. 1979­1993, 2018.
[42] Y. Grandvalet, Y. Bengio et al., "Semi-supervised learning by entropy minimization." in CAP, 2005, pp. 281­296.
[43] P. Bachman, O. Alsharif, and D. Precup, "Learning with pseudo-ensembles," arXiv preprint arXiv:1412.4864, 2014.
[44] S. Laine and T. Aila, "Temporal ensembling for semi-supervised learning," arXiv preprint arXiv:1610.02242, 2016.
[45] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, "Dropout: a simple way to prevent neural networks from overfitting," The journal of machine learning research, vol. 15, no. 1, pp. 1929­1958, 2014.
[46] M. Sajjadi, M. Javanmardi, and T. Tasdizen, "Regularization with stochastic transformations and perturbations for deep semi-supervised learning," arXiv preprint arXiv:1606.04586, 2016.
[47] G. French, M. Mackiewicz, and M. Fisher, "Self-ensembling for visual domain adaptation," arXiv preprint arXiv:1706.05208, 2017.
[48] Z. Dai, Z. Yang, F. Yang, W. W. Cohen, and R. Salakhutdinov, "Good semi-supervised learning that requires a bad gan," arXiv preprint arXiv:1705.09783, 2017.
[49] C. Wei, K. Shen, Y. Chen, and T. Ma, "Theoretical analysis of self-training with deep networks on unlabeled data," in International Conference on Learning Representations, 2021.
[50] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, "mixup: Beyond empirical risk minimization," arXiv preprint arXiv:1710.09412, 2017.
[51] Y. Jin, X. Wei, Y. Liu, and Q. Yang, "Towards utilizing unlabeled data in federated learning: A survey and prospective," arXiv e-prints, pp. arXiv­2002, 2020.
[52] W. Jeong, J. Yoon, E. Yang, and S. J. Hwang, "Federated semi-supervised learning with inter-client consistency," arXiv preprint arXiv:2006.12097, 2020.
[53] A. Albaseer, B. S. Ciftler, M. Abdallah, and A. Al-Fuqaha, "Exploiting unlabeled data in smart cities using federated learning," arXiv preprint arXiv:2001.04030, 2020.
[54] S. Itahara, T. Nishio, Y. Koda, M. Morikura, and K. Yamamoto, "Distillation-based semisupervised federated learning for communication-efficient collaborative training with non-iid private data," arXiv preprint arXiv:2008.06180, 2020.
[55] J.-H. Ahn, O. Simeone, and J. Kang, "Wireless federated distillation for distributed edge learning with heterogeneous data," in 2019 IEEE 30th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC). IEEE, 2019, pp. 1­6.
[56] F. Sattler, A. Marban, R. Rischke, and W. Samek, "Communication-efficient federated distillation," arXiv preprint arXiv:2012.00632, 2020.
12

[57] Z. Zhang, Y. Yang, Z. Yao, Y. Yan, J. E. Gonzalez, and M. W. Mahoney, "Improving semisupervised federated learning by reducing the gradient diversity of models," arXiv preprint arXiv:2008.11364, 2020.
[58] Y. Zhao, H. Liu, H. Li, P. Barnaghi, and H. Haddadi, "Semi-supervised federated learning for activity recognition," arXiv preprint arXiv:2011.00851, 2020.
[59] D. Yang, Z. Xu, W. Li, A. Myronenko, H. R. Roth, S. Harmon, S. Xu, B. Turkbey, E. Turkbey, X. Wang et al., "Federated semi-supervised learning for covid region segmentation in chest ct using multi-national data from china, italy, japan," Medical image analysis, vol. 70, p. 101992, 2021.
[60] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, "Randaugment: Practical automated data augmentation with a reduced search space," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 702­703.
[61] T. Yoon, S. Shin, S. J. Hwang, and E. Yang, "Fedmix: Approximation of mixup under mean augmented federated learning," in International Conference on Learning Representations, 2021.
[62] D. Ulyanov, A. Vedaldi, and V. Lempitsky, "Instance normalization: The missing ingredient for fast stylization," arXiv preprint arXiv:1607.08022, 2016.
[63] Y. Wu and K. He, "Group normalization," in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 3­19.
[64] J. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization," arXiv preprint arXiv:1607.06450, 2016.
[65] A. Krizhevsky et al., "Learning multiple layers of features from tiny images," 2009. [66] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, "Reading digits in natural
images with unsupervised feature learning," 2011. [67] T.-M. H. Hsu, H. Qi, and M. Brown, "Measuring the effects of non-identical data distribution
for federated visual classification," arXiv preprint arXiv:1909.06335, 2019. [68] S. Zagoruyko and N. Komodakis, "Wide residual networks," arXiv preprint arXiv:1605.07146,
2016. [69] I. Loshchilov and F. Hutter, "Sgdr: Stochastic gradient descent with warm restarts," arXiv
preprint arXiv:1608.03983, 2016.
13

Supplementary material

We provide supplementary experimental results below. In Table 5, we give the hyperparameters used in the experiments. In Figure 5, we show experimental results for the SVHN dataset with NS = {250, 4000}. In Table 6, we demonstrate the ablation study of the sBN statistics on the CIFAR10 dataset. Compared with updating the sBN statistics with only the server data, updating the sBN statistics with both server and clients does not provide significant improvements.
Table 5: Hyperparameters used in our experiments.

Dataset

Number of Supervised

Architecture

Server

Batch size Epoch
Optimizer Learning rate Weight decay Momentum
Nesterov

Client

Batch size Epoch
Optimizer Learning rate Weight decay Momentum
Nesterov

Communiction round

Global

Momentum

Scheduler

CIFAR10 SVHN
250 4000 250 1000
WResNet28x2
10 250 10 250 5
SGD 3.0E-02 5.0E-04
0.9 
10 5 SGD 3.0E-02 5.0E-04 0.9 
800 0.5 Cosine Annealing

(a)  = 250

(b)  = 1000

Figure 5: Experimental results for SVHN dataset with (a) NS = 250 and (b) NS = 1000.

Table 6: Ablation study of sBN statistis on the CIFAR10 dataset. The alternative way of using the server data to update the global sBN statistics does not degrade the training performance.

sBN statistics
server server and clients

250

Non-IID, K = 2

IID

59.99(0.77) 60.03(0.87)

86.25(0.22) 85.34(0.28)

4000

Non-IID, K = 2

IID

85.47(0.09) 88.23(0.28)

93.14(0.16) 93.10(0.14)

14

