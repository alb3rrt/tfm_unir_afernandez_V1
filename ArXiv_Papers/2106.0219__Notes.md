
# Question-aware Transformer Models for Consumer Health Question Summarization

[arXiv](https://arxiv.org/abs/2106.0219), [PDF](https://arxiv.org/pdf/2106.0219.pdf)

## Authors

- Shweta Yadav
- Deepak Gupta
- Asma Ben Abacha
- Dina Demner-Fushman

## Abstract

Searching for health information online is becoming customary for more and more consumers every day, which makes the need for efficient and reliable question answering systems more pressing. An important contributor to the success rates of these systems is their ability to fully understand the consumers' questions. However, these questions are frequently longer than needed and mention peripheral information that is not useful in finding relevant answers. Question summarization is one of the potential solutions to simplifying long and complex consumer questions before attempting to find an answer. In this paper, we study the task of abstractive summarization for real-world consumer health questions. We develop an abstractive question summarization model that leverages the semantic interpretation of a question via recognition of medical entities, which enables the generation of informative summaries. Towards this, we propose multiple Cloze tasks (i.e. the task of filing missing words in a given context) to identify the key medical entities that enforce the model to have better coverage in question-focus recognition. Additionally, we infuse the decoder inputs with question-type information to generate question-type driven summaries. When evaluated on the MeQSum benchmark corpus, our framework outperformed the state-of-the-art method by 10.2 ROUGE-L points. We also conducted a manual evaluation to assess the correctness of the generated summaries.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{yadav2021questionaware,
      title={Question-aware Transformer Models for Consumer Health Question Summarization}, 
      author={Shweta Yadav and Deepak Gupta and Asma Ben Abacha and Dina Demner-Fushman},
      year={2021},
      eprint={2106.00219},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

