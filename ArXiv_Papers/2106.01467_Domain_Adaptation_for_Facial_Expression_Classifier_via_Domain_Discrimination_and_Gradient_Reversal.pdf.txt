Domain Adaptation for Facial Expression Classifier via Domain Discrimination and Gradient Reversal
Kamil Akhmetov Institute of Data Science & AI
Innopolis University Innopolis, Russia k.ahmetov@innopolis.ru

arXiv:2106.01467v1 [cs.CV] 2 Jun 2021

Abstract--Bringing empathy to a computerized system could significantly improve the quality of human-computer communications, as soon as machines would be able to understand customer intentions and better serve their needs. According to different studies (Literature Review), visual information is one of the most important channels of human interaction and contains significant behavioral signals, that may be captured from facial expressions. Therefore, it is consistent and natural that the research in the field of Facial Expression Recognition (FER) has acquired increased interest over the past decade due to having diverse application area including health-care, sociology, psychology, driver-safety, virtual reality, cognitive sciences, security, entertainment, marketing, etc. We propose a new architecture for the task of FER and examine the impact of domain discrimination loss regularization on the learning process. With regard to observations, including both classical training conditions and unsupervised domain adaptation scenarios, important aspects of the considered domain adaptation approach integration are traced. The results may serve as a foundation for the further research in the field.
Index Terms--Machine Learning, Deep Learning, Facial Expression Recognition, Domain Adaptation
I. INTRODUCTION
Countless amount of factors (pose, brightness, background, occlusions, subject ethnicity, face shape, etc.) make Facial Expression Recognition (FER) a complex Computer Vision (CV) problem [1]. Conventionally, manual Face Detection (FD) and handcrafted Feature Extraction (FE) designed by domain experts involved traditional Machine Learning (ML) techniques for emotion classification [1]­[3]. Later, automatic FER has moved to Deep Learning (DL), which has emerged as a general approach for ML tasks. Convolutional Neural Networks (CNN) followed by Fully Connected (FC) classification layers reduce the need for pre-processing and enable `end-toend' learning.
Modern research focuses on domain agnostic models developed for real-world scenarios. Domain Adaptation approaches mainly focus on the segregation between domain-specific and task-specific (domain-independent) features from the original data. However, according to Siddiqi et al. modern approaches are still weak to face real-world scenarios [4]. Thus, further studies focus on discovering new ways of learning facial expressions improving performance and required computation cost.

The paper summarizes the key findings of the past research, introduces a new architecture for FER using transfer learning, and examines the applicability of domain discrimination loss. The rest of the work is organized per the following sections: Literature Review, Methodology, Evaluation & Discussion and Conclusion.
II. LITERATURE REVIEW
The section reviews the most relevant literature in the appropriate subfields and finishes with a summary of the key findings, identifies the research gap, and introduces the research questions.
A. Facial Expression Recognition
Historically, works in the field of FER are grouped as follows: Hand Crafted Features (HCFs) based models (also known as Classical Machine Learning) and Deep Learning (DL) approaches. With the emergence of deep learning, traditional methods became comparatively less used due to limitations covered by CNN based models, that generally provide competitive (with state-of-the-art) results and allow learning shared representations, which is especially important for Domain Adaptation.
· Classical FER Hand-Crafted Features and traditional classifiers are less expensive in terms of resources, while having less expressiveness power. Learned patterns are easier to analyze and understand causal relationships. [5]­[24]
· Attentional Deep Learning based FER Focus attention during learning on particular regions since only specific details contribute to each of the facial expressions. Hence, give more weight to the features containing more information (statistically significant). [11], [25]­[31]
· Disentangled representation learning Disentangle data into task significant features while reducing the impact of non-important sources of variation. The caused separation facilitates domain generalization via moving feature space to higher levels of abstraction. [31]­[35]
· Identity-Aware CNN Utilize the knowledge on identities to remove the bias

introduced by personal characteristics of a subject. Separating identity impact yields clearer expression related features, thus improving the learning generalization. [34], [36]­[39] · Generative Adversarial Networks Adversarial learning nature applied for the task of FER yields models having higher domain adaptation, being though more complex. Features learned in adversarial process may be simultaneously exploited for expression classification. [26], [30], [40]­[45] · Other single frame based methods Approaches include suggest sophisticated data sources (stylized character samples and utilizing non-peak expressions), model architectures (inception layers) and learning process design (teacher and student instances). [11], [30], [38], [46]­[62] · Frame sequence based Deep Learning Extract expression or geometric features for each frame in the sequence of a data sample in context of time. Use feature sequences for expression final learning and inference. [63]­[65]
B. Domain Adaptation & Generalization
According to cross domain scenarios, Domain Adaptation (DA) and Domain Generalization (DG) are addressed sequentially in order of ascending complexity as follows:
· Domain Adaptation The approach of the described methods is mainly segregation of domain specific features to the deepest as possible layers of the network, so that the shallow layers learn transferable task specific knowledge, whereas the ending model structures may be tailored to the target domain by using smaller amount of labeled or pseudolabeled dataset. The practice of generating samples in the context of lacking data is moderately used, however mostly avoided in newer approaches due to being resource intense and expensive in terms of time. [30], [51], [66]­[72]
· Domain Generalization Better model generalization entails incorporating more datasets in the learning process. Extending DA approaches towards more domains also involves application of various sophisticated regularization techniques via, for example, meta-learning or discarding dominant features. Modern design of models is still far from perfect and cannot meet the needs of real use in the industry. Thus, improvements and new approaches are highly demanded. [4], [45], [68], [73]­[78]
C. Summary
To summarize, the choice of a solution strongly depends on the specific problem and the context of its solution. There is always a trade off between many factors to consider when developing and applying systems for Facial Expression Recognition. The field of the research still remains broad and thus encourages for the search of improvements and new approaches. As observed, the application of existing DA

approaches, for example, domain discrimination, to the task of FER is insufficiently studied. To cover the mentioned research gap we pose the research questions:
1) What is the improvement of utilizing domain discrimination loss in the learning process for a FER system?
2) What performance is introduced by separation of domaininvariant and domain specific feature learning neural layers? III. METHODOLOGY
The section introduces methods for solving the research problem. Section III-A studies the operational data and the information preparation techniques, section III-B explains the structure of the proposed FER classifier, section Unsupervised Domain Adaptation presents the experiment design, whereas other important components of the system are demonstrated in subsequent parts: Factor function, Scheduled clamping, Data sampling mechanism. A. Data preprocessing
Domain Adaptation (DA) task assumes source and target domains to be related to the same task, however, follow different distributions. Our work operates with one source and three target domains modelled by Karolinska Directed Emotional Faces (KDEF) [47], Extended Cohn-Kanade (CK) [30], [79], Japanese Female Facial Expression Dataset (JAFFE) [11], Taiwanese Facial Expression Image Database (TFEID) [80] correspondingly.
Since the data vary significantly (in chromaticity, image size, face positioning, photography method and the degree of post-processing), pre-processing is an important step of the pipeline. As only KDEF is colored, all images are converted to monochrome. In the next steps face detection is applied and photos are rescaled to 224 × 224, which is the standard input shape to VGG-19 network used further (Fig. 1)
Figure 1: Data preprocessing: grayscaling, face detection, rescaling and normalization.
B. Architecture The architecture of the presented approach represents the
expression classifier based on the transfer learning component (feature extracting CNN). Earlier mentioned VGG19 [36] described in Simonyan2015deep showed state-of-the-art results

in ImageNet [81] competition in 2014 on object localization and classification.
Several linear layers follow VGG19 component to adapt and compact features to a lower-dimensional task space. CNNs tend to extract different levels of visual information depending on the deepness of the layer. Hence, features coming from each of the max pooling and the first linear layers of the original VGG are densed to 100 component vectors each and concatenated to form a single dimensional 600 component vector. Fully connected linear layers are followed by leaky ReLU activation function. Features most significant for facial emotions are expected to be extracted to 600 dimensional latent space by the feature extractor F .
Label predictor compacts the extracted features to 7 neurons representing all considered facial emotions. The last layer utilizes Log Softmax instead of activation to achieve the predicted distribution logits. Negative Log Likelihood (N LL) function is used for calculating classification loss and updating the network parameters by the back-propagation of the computed gradients. Classifier conditional distribution of logits EX^ P |x, label prediction ex^p and training minimization objective LCLF may be represented as follows:

Figure 2: Facial Expression Recognition model: detailed architecture. Colored arrows represent the loss back-propagation direction, whereas black ones correspond to the forward pass.

EXP |x  CLF (F (x))

exp = argmax[EXP |x]

(1)

LCLF = N LL(EXP |x, EXP )

Domain Classifier (DC) is designed to enforce feature extractor to generate domain-invariant features. It is similar to previously mentioned label predictor, however, to achieve the expected result, the domain discrimination loss backpropagated from DC is passed through the Gradient Reversal (GradRev) layer, where it is reversed in direction (multiplied by -1). GradRev applies absolutely no action during the forward pass:

GradRev(f ) = f (2)
f GradRev(f ) = -1
Consequently, optimization of DC makes features domaininvariant. Respective domain label conditional distribution DM^ N |x, domain prediction dm^ n and training minimization objective LDMN may be represented as follows:

Regardless of the use of the proposed domain adaptation technique, the model is granted complete access to the labels of only the source domain and the data origin identifier (domain label). Hence, the experiments provide fair comparison of both settings.
For the unsupervised domain adaptation setup, additional stimulus in the form of domain discrimination loss is present, hence, the domain classifier is activated at each training step. Classification loss is calculated only for the samples from the source domain. In such case, both losses are combined using  hyper-parameter as follows:

L = LCLF +   LDMN

(4)

Therefore, expected outcomes include:
1) feature extractor generates domain-indistinguishable features
2) trained classifier performs better on target domains

D. Factor function

DM N |x  DM N (F (x))

dmn = argmax[DM N |x]

(3)

LDMN = N LL(DM N |x, DM N )

Figure 2 gives a more detailed representation of the whole system altogether with domain discriminator plugged in.

C. Unsupervised Domain Adaptation
The section describes the design of experiments. The presented domain adaptation model is naturally suitable for the unsupervised training setup.

In order to reduce the noise coming from domain discrimination loss in the beginning of the training and allow increasing the impact of the domain discriminator gradually, the previously mentioned parameter  is controlled by factor function of the following form:

2

(n)

=

1

+

exp

(-

n N

)

-

1

(5)

where

n N

represents the

fraction of

the

steps

passed to the

total number of steps. Such functional dependency gradually

increase  from 0 to 1.

E. Scheduled clamping
Big emissions of the domain discrimination loss may cause high instability during the training. To decrease the undesired impact and control the growth of the domain loss, we introduce an auxiliary technique called clamping. values exceeding the pre-determined limit convert to the maximal borderline by clamping operation for each term before averaging the loss across mini-batches. The maximum value is controlled by previously mentioned f actor function and clamp parameter as follows:

LDMN (n) = max(LDMN , clamp  (n))

(6)

F. Data sampling mechanism
Balanced data sampling mechanism is very important in case of domain adaptation scenario. Our approach suggests updating the model parameters after the calculation of the loss for all four datasets at the same time step. For this purpose, mini-batches from all datasets are combined into an aggregated batch before passing in the training step. Such design, notably, increases the training volume because some datasets, being shorter than others, are exploited several times to maintain the inter-domain balance during training.

IV. EVALUATION AND DISCUSSION
The section describes the conducted experiments, presents and discusses the corresponding results. The observations include baseline classifier selection and performance verification on several datasets, fine-tuning of the pre-trained model to target domains and the application of the unsupervised domain adaptation approach. Each of the delivered series of experiments is discussed in detail to explain the obtained results and build the conclusions of the performed work. The paper finishes with a concluding section that summarizes the key findings Conclusion.

A. Baseline emotion classifier selection
Starting experiments build a baseline facial expression classifier. Several datasets, as well as their combinations, are used to validate the performance. Moreover, the model, pre-trained on one of the domains chosen as the source domain (KDEF), is fine-tuned to several source domains (CK, JAFFE, TFEID) in order to investigate its initial ability for domain adaptation.
The experiments train model for 100 epochs, with a batch size of 40 images and a learning rate of 1  10-4. The joint training data source is formed with the use of datasets concatenation and randomized shuffling. Four combinations of domains are chosen: sequentially expanding single KDEF by one dataset at a time up to a set containing all data origins. The data split into 80% training and 20% validation sets.
Reasonable to assume that the model improves the results only for the new dataset added to the joint training set at a time. However, according to the Table I, one can perceive that the accuracy for KDEF slightly increases (+1.53%) with the extension by CK. Another pair of datasets attend the same idea: the accuracy for JAFFE also increases by 2.32% with the

Domain
KDEF CK
JAFFE TFEID
Domain
KDEF CK
JAFFE TFEID

KDEF
87.75% 48.38% 48.83% 86.95%
KDEF
0.55 5.62 3.16 0.36

Accuracy

KDEF CK

KDEF CK
JAFFE

89.28% 86.22% 98.92% 98.92% 37.20% 81.39% 76.08% 73.91%
Loss

KDEF CK

KDEF CK
JAFFE

0.59

0.59

0.01

0.02

3.86

0.79

1.08

1.10

KDEF CK
JAFFE TFEID 89.28% 98.92% 83.72% 97.82%
KDEF CK
JAFFE TFEID
0.68 0.06 0.74 0.05

Table I: Validation accuracy and loss of the baseline classifier trained on KDEF and three joint domains (columns).

addition of TFEID. Approximately similar results are reflected by the loss indicators represented in Table I as well. It is known that KDEF and CK represent European, while the other two contain Asian faces. Thus, the improvement observed on earlier seen data due to the new data indicates the degree of their similarity.
Based on the results of fine-tuning the model previously trained on source domain KDEF to the target domains, one notes that the retrained classifier loses its original experiences. Consequently, even though the model was previously pretrained on KDEF, retraining, for example, to CK, causes the accuracy drop from 87.75% to 40.81% on the KDEF, but rises from 48.38% to 98.92% on the target CK dataset (Table II).

Domain KDEF
CK JAFFE TFEID
Domain KDEF
CK JAFFE TFEID

KDEF 87.75% 48.38% 48.83% 86.95%
KDEF 0.55 5.62 3.16 0.36

Accuracy

CK

JAFFE

40.81% 75.00%

98.92% 56.20%

27.90% 86.04%

52.17% 58.69%

Loss

CK

JAFFE

7.43

2.74

0.02

4.98

6.90

0.95

4.24

3.50

TFEID 59.69% 32.87% 18.60% 97.82%
TFEID 4.82 6.77 13.57 0.05

Table II: Validation accuracy and loss of the baseline classifier fine-tuned to the target domains.

T-SNE visualization of the latent space representation of the features displayed in Figure 3 confirm the results: the model quite expectedly may lose its original skills in favor of a new domain in case they are not continued to be used.
However, the separation of some clusters related to same emotion into groups is also noticeable. For example, Sad emotion samples from CK form a separate group, which is albeit close to the corresponding expression instances from other origins. This observation shows that despite learning to recognize emotions, the model retains the information about

Thus, the evidence demonstrates that the model converges to a point of stability.

(a) training on KDEF

(b) training on all domains

Figure 5: Emotion classification loss (clamp: 3000, 5000, 7000, 10000).
Figure 6 visualizes the latent space of the features obtained in the experiments.

(c) fine-tuned to CK
Figure 3: Latent space visualization of the baseline classifier features.

the data origin, which makes feature generation less domaininvariant.
B. Discrimination Loss integration
The section describes experiments with the use of discrimination loss.
The hyper-parameters are the following: 100 epochs; clamp varies in values of 3000, 5000, 7000 and 10000;  = 10.
Figure 4 demonstrates the behavior of the domain discrimination loss, which grows according to the law of the factor function in all cases. It may be noted that fluctuations decrease as the training proceeds and the losses eventually stabilize.

(a) clamp = 5000

(b) clamp = 10000

Figure 6: Latent space visualization of features comparison: clamp varies in values: 3000, 5000, 7000, 10000.

According to the results presented in the Table III, the best indicators for the accuracy of emotion classification are achieved with clamp = 5000 for all datasets. As for the latent space representation (6), the best indistinguishability of data sources is naturally observed at higher values of the parameter clamp = 10000.

Domain KDEF
CK JAFFE TFEID
Domain KDEF
CK JAFFE TFEID

clamp = 3000 66.32% 51.61% 25.58% 28.26%
clamp = 3000 1.45 6.61 10.29 24.33

Accuracy clamp = 5000
79.08% 52.68% 25.58% 32.60%
Loss clamp = 5000
2.17 24.72 24.57 14.96

clamp = 10000 72.44% 44.62% 25.58% 19.56%
clamp = 10000 4.75 33.22 42.88 30.75

Figure 4: Domain discrimination loss: clamp varies in values: Table III: Validation classification results (clamp: 3000, 5000,

3000, 5000, 7000, 10000.

10000).

The same situation is reflected in Figure 5: classification Table IV compares the validation accuracy provided by loss maintains the downtrend and reaches its minimum values. the baseline model trained on labeled KDEF and two other

candidates utilizing the unlabeled data from other domains through the application of the unsupervised domain adaptation.

Domain KDEF
CK JAFFE TFEID
Domain KDEF
CK JAFFE TFEID

baseline 87.75% 48.38% 48.83% 86.95%
baseline 0.55 5.62 3.16 0.36

Accuracy clamp = 5000
79.08% 52.68% 25.58% 32.60%
Loss clamp = 5000
2.17 24.72 24.57 14.96

clamp = 10000 72.44% 44.62% 25.58% 19.56%
clamp = 10000 4.75 33.22 42.88 30.75

Table IV: Comparison of validation results for the baseline model (option A) and domain adapted models using clamp values 5000 or 10000.

The comparison results show that although the model converged towards the point of stability, the approach itself provides insufficient improvements over the baseline model. Despite that fact, the key contribution of the research lies in the observed patterns identified during the whole work. Key findings of the experiments are summarized in the next section.
V. CONCLUSION
The section summarizes the key findings identified by the conducted work.
A. New Architecture for FER
One of the most important results is the discovery of a new architecture for the task of Facial Expression Recognition. The model referred to in the work as the baseline is an implementation of the popular DL pattern named Transfer Learning. The design is fully end-to-end and demonstrates high validation results in terms of accuracy and loss. The model shows positive dynamics in the source-to-source learning setup and achieves high validation results on several of the popular datasets (KDEF, CK, JAFFE, TFEID). The architecture has shown flexibility in joint dataset training and finetuning environments. Latent space representations confirm the formation of distant clusters and the high availability of the framework to learn facial expressions by distancing the corresponding features. In general, the conducted experiments demonstrate an affirmative experience of using the transfer learning developments for the FER task. The outcome of this part serves as the basis for testing the operation of various optimizations and regularization mechanisms for the improvement of the learning properties. The Domain Adaptation approach attended in the research may be viewed as one of those expansions of the baseline model.
B. Sampling mechanism
Balancing between domains and aggregated scoring is critical to learning in a domain adaptation context. Our approach, for instance, implies the growth of domain discrimination loss, which is necessary to be balanced. The sampling mechanism

chosen in this work assumes the loss averaging over all minibatches before back-propagation. Mini-batches, in turn, are added to one data packet supplied at one training step to the learning process. Thus, ensuring domain-balanced data is crucial for the learning consistency in a DA setup.
C. Features
One of the useful observations is that emotion classification features are strongly inter-connected with domain characteristics. According to the conducted studies, any sharp fluctuations in domain discrimination loss negatively affect the classification loss. Therefore, it is highly important to control the smoothness of the domain loss ascending. Moreover, the strong coherence between domain related and emotion related characteristics entails the prediction quality decrease due to forcing the model generating domain-invariant features. Such information loss involves over-fitting to the source domain as the training loss approaches zero but the validation results worsen. One of the useful observations is that while the target domains become less distinguishable and move their datapoints towards the original source in latent space representations, the overall features quality decreases.
D. Scheduling function
The conducted work persistently uses scheduling of several quantities. It is important to ensure the noise coming from the domain discriminator impacts the system less in the beginning of the training process, so the back-propagated gradients are multiplied to the proportion growing from zero to one as the learning progresses. Later, clamping of the domain discrimination loss utilized similar proportioning for providing stability of the training. For the both cases the f actor, discussed in sub-section III-D is successfully used as a nonlinear increasing function. Smooth scheduling of important quantities allow for building more stable and consistent systems.
E. Future plans
As discussed before, despite under-performing results of application of the domain adaptation approach, useful observations allow to build several ideas to explain the behavior and suggest ways for further improvement of the model.
Decreasing complexity of emotion classifier may reduce over-fitting to the source domain that was examined in last experiments. One of the possible reasons why the feature extractor part remains ineffective at disentangling domaininvariant features could be insufficient capacity of the model. Hence, stacking more layers is a common way to solve problems, however is not preferred considering the initial interest in efficiency. Other methods to solve imperfections consider application of the domain discrimination loss to deeper structures of the feature extractor or the introduction of additional stimuli and regularization techniques.
To conclude, although the conducted experiments cannot provide the world with a new competitive solution, they discovered useful observations. The further analysis of the findings allows to determine the directions for further research and improvements.

REFERENCES
[1] S. Li and W. Deng, "Deep facial expression recognition: A survey," ArXiv, vol. abs/1804.08348, 2018.
[2] C. Corneanu, M. Oliu, J. Cohn, and S. Escalera, "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affectrelated applications," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, pp. 1548­ 1568, 2016.
[3] Y. Wu and Q. Ji, "Facial landmark detection: A literature survey," International Journal of Computer Vision, vol. 127, pp. 115­142, 2018.
[4] M. Siddiqi, M. Ali, M. Eldib, A. Khan, O. Ban~os, S. Lee, and H. Choo, "Evaluating real-life performance of the state-of-the-art in facial expression recognition using a novel youtube-based datasets," Multimedia Tools and Applications, vol. 77, pp. 917­937, 2016.
[5] R. E. Schapire, "Explaining adaboost," in Empirical inference, Springer, 2013, pp. 37­52.
[6] A. Liaw and M. Wiener, "Classification and regression by randomforest," R News, vol. 2, no. 3, pp. 18­22, 2002. [Online]. Available: https://CRAN.R-project.org/ doc/Rnews/.
[7] C. Cortes and V. Vapnik, "Support-vector networks," Machine learning, vol. 20, no. 3, pp. 273­297, 1995.
[8] J. Chen, Z. Chen, Z. Chi, and H. Fu, "Facial expression recognition based on facial components detection and hog features," 2014.
[9] C. Shan, S. Gong, and P. McOwan, "Facial expression recognition based on local binary patterns: A comprehensive study," Image Vis. Comput., vol. 27, pp. 803­816, 2009.
[10] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. R. Fasel, and J. Movellan, "Recognizing facial expression: Machine learning and application to spontaneous behavior," 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), vol. 2, 568­573 vol. 2, 2005.
[11] M. Lyons, M. Kamachi, and J. Gyoba, "Coding facial expressions with gabor wavelets (ivc special issue)," ArXiv, vol. abs/2009.05938, 2020.
[12] J. Whitehill and C. Omlin, "Haar features for facs au recognition," 7th International Conference on Automatic Face and Gesture Recognition (FGR06), 5 pp.­101, 2006.
[13] R. A. Khan, A. Meyer, H. Konik, and S. Bouakaz, "Framework for reliable, real-time facial expression recognition for low resolution images," Pattern Recognit. Lett., vol. 34, pp. 1159­1168, 2013.
[14] D. Ghimire, S. Jeong, J. Lee, and S. Park, "Facial expression recognition based on local region specific features and support vector machines," Multimedia Tools and Applications, vol. 76, pp. 7803­7821, 2016.
[15] M. Suk and B. Prabhakaran, "Real-time mobile facial expression recognition system ­ a case study," 2014

IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 132­137, 2014. [16] D. Ghimire and J. Lee, "Geometric feature-based facial expression recognition in image sequences using multiclass adaboost and support vector machines," Sensors (Basel, Switzerland), vol. 13, pp. 7714­7734, 2013. [17] C. F. Benitez-Quiroz, R. Srinivasan, and A. Mart´inez, "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5562­5570, 2016. [18] F. De la Torre, W. Chu, X. Xiong, F. Vicente, X. Ding, and J. Cohn, "Intraface," in 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), vol. 1, 2015, pp. 1­8. DOI: 10.1109/FG.2015.7163082. [19] M. Siddiqi, S. Lee, Y. Lee, A. Khan, and P. T. H. Truc, "Hierarchical recognition scheme for human facial expression recognition systems," Sensors (Basel, Switzerland), vol. 13, pp. 16 682­16 713, 2013. [20] M. Siddiqi, R. Ali, M. Idris, A. Khan, E. Kim, M. C. Whang, and S. Lee, "Human facial expression recognition using curvelet feature extraction and normalized mutual information feature selection," Multimedia Tools and Applications, vol. 75, pp. 935­959, 2014. [21] M. Siddiqi, R. Ali, A. Sattar, A. Khan, and S. Lee, "Depth camera-based facial expression recognition system using multilayer scheme," IETE Technical Review, vol. 31, pp. 277­286, 2014. [22] M. Siddiqi, R. Ali, A. Khan, E. Kim, G. Kim, and S. Lee, "Facial expression recognition using active contour-based face detection, facial movement-based feature extraction, and non-linear feature selection," Multimedia Systems, vol. 21, pp. 541­555, 2014. [23] M. Siddiqi, R. Ali, A. Khan, Y.-T. Park, and S. Lee, "Human facial expression recognition using stepwise linear discriminant analysis and hidden conditional random fields," IEEE Transactions on Image Processing, vol. 24, pp. 1386­1398, 2015. [24] M. Siddiqi, M. G. R. Alam, C. Hong, A. Khan, and H. Choo, "A novel maximum entropy markov model for human facial expression recognition," PLoS ONE, vol. 11, 2016. [25] P. Khorrami, T. Paine, and T. Huang, "Do deep neural networks learn facial action units when doing expression recognition?" 2015 IEEE International Conference on Computer Vision Workshop (ICCVW), pp. 19­27, 2015. [26] J. M. Susskind, A. K. Anderson, and G. E. Hinton, "The toronto face database," Department of Computer Science, University of Toronto, Toronto, ON, Canada, Tech. Rep, vol. 3, 2010. [27] R. Memisevic, K. R. Konda, and D. Krueger, "Zero-bias autoencoders and the benefits of co-adapting features," CoRR, vol. abs/1402.3337, 2015.

[28] T. Paine, P. Khorrami, W. Han, and T. Huang, "An analysis of unsupervised pre-training in light of recent advances," CoRR, vol. abs/1412.6597, 2015.
[29] S. Minaee and A. Abdolrashidi, "Deep-emotion: Facial expression recognition using attentional convolutional network," ArXiv, vol. abs/1902.01019, 2019.
[30] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews, "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression," in 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, 2010, pp. 94­101. DOI: 10 . 1109 / CVPRW.2010.5543262.
[31] Y. Li, J. Zeng, S. Shan, and X. Chen, "Occlusion aware facial expression recognition using cnn with attention mechanism," IEEE Transactions on Image Processing, vol. 28, pp. 2439­2450, 2019.
[32] Y. Bengio, A. Courville, and P. Vincent, Representation learning: A review and new perspectives, 2014. arXiv: 1206.5538 [cs.LG].
[33] G. E. Hinton and R. Zemel, "Autoencoders, minimum description length and helmholtz free energy," in NIPS, 1993.
[34] Y. Liu, F. Wei, J. Shao, L. Sheng, J. Yan, and X. Wang, "Exploring disentangled feature representation beyond face identification," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2080­ 2089, 2018.
[35] M. Halawa, M. Wo¨llhaf, E. Vellasques, U. S. Sanz, and O. Hellwich, Learning disentangled expression representations from facial images, 2020. arXiv: 2008.07001 [cs.CV].
[36] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, 2015. arXiv: 1409.1556 [cs.CV].
[37] C.-H. Tu, C.-Y. Yang, and J. Hsu, "Idennet: Identityaware facial action unit detection," 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019), pp. 1­8, 2019.
[38] S. Mavadati, M. Mahoor, K. Bartlett, P. Trinh, and J. Cohn, "Disfa: A spontaneous facial action intensity database," Affective Computing, IEEE Transactions on, vol. 4, pp. 151­160, Apr. 2013. DOI: 10.1109/T-AFFC. 2013.4.
[39] Z. Zhang, S. Zhai, and L. Yin, "Identity-based adversarial training of deep cnns for facial action unit recognition," in BMVC, 2018.
[40] J. Cai, Z. Meng, A. S. Khan, Z. Li, J. O'Reilly, and Y. Tong, Identity-free facial expression recognition using conditional generative adversarial network, 2019. arXiv: 1903.08051 [cs.CV].
[41] K. Ali and C. E. Hughes, Facial expression recognition using disentangled adversarial learning, 2019. arXiv: 1909.13135 [cs.CV].
[42] M. Arjovsky, S. Chintala, and L. Bottou, "Wasserstein generative adversarial networks," in ICML, 2017.

[43] C. Xu, Y. Cui, Y. Zhang, P. Gao, and J. Xu, "Personindependent facial expression recognition method based on improved wasserstein generative adversarial networks in combination with identity aware," Multimedia Systems, vol. 26, pp. 53­61, 2019.
[44] Y. Kim, B. Yoo, Y. Kwak, C. Choi, and J. Kim, "Deep generative-contrastive networks for facial expression recognition," ArXiv, vol. abs/1703.07140, 2017.
[45] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, Adversarial autoencoders, 2016. arXiv: 1511. 05644 [cs.LG].
[46] D. Aneja, A. Colburn, G. Faigin, L. Shapiro, and B. Mones, "Modeling stylized character expressions via deep learning," in ACCV, 2016.
[47] D. Lundqvist, A. Flykt, and A. O¨ hman, "Karolinska directed emotional faces," 2015.
[48] M. Pantic, M. F. Valstar, R. Rademaker, and L. Maat, "Web-based database for facial expression analysis," in Proceedings of IEEE Int'l Conf. Multimedia and Expo (ICME'05), Amsterdam, The Netherlands, Jul. 2005, pp. 317­321.
[49] A. Mollahosseini, D. Chan, and M. Mahoor, "Going deeper in facial expression recognition using deep neural networks," 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1­10, 2016.
[50] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, "Multi-pie," in 2008 8th IEEE International Conference on Automatic Face Gesture Recognition, 2008, pp. 1­8. DOI: 10.1109/AFGR.2008.4813399.
[51] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. C. Courville, M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. Athanasakis, J. Shawe-Taylor, M. Milakov, J. Park, R. T. Ionescu, M. Popescu, C. Grozea, J. Bergstra, J. Xie, L. Romaszko, B. Xu, C. Zhang, and Y. Bengio, "Challenges in representation learning: A report on three machine learning contests," Neural networks : the official journal of the International Neural Network Society, vol. 64, pp. 59­63, 2015.
[52] P. Liu, S. Han, Z. Meng, and Y. Tong, "Facial expression recognition via a boosted deep belief network," 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1805­1812, 2014.
[53] X. Zhao, X. Liang, L. Liu, T. Li, Y. Han, N. Vasconcelos, and S. Yan, "Peak-piloted deep network for facial expression recognition," ArXiv, vol. abs/1607.06997, 2016.
[54] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, "Going deeper with convolutions," 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1­9, 2015.
[55] H. Ding, S. Zhou, and R. Chellappa, "Facenet2expnet: Regularizing a deep face recognition net for expression recognition," 2017 12th IEEE International Conference

on Automatic Face & Gesture Recognition (FG 2017), pp. 118­126, 2017. [56] J. Ba and R. Caruana, "Do deep nets really need to be deep?" ArXiv, vol. abs/1312.6184, 2014. [57] O. M. Parkhi, A. Vedaldi, and A. Zisserman, "Deep face recognition," in BMVC, 2015. [58] I. C¸ ugu, E. Sener, and E. Akbas, "Microexpnet: An extremely small and fast model for expression recognition from face images," 2019 Ninth International Conference on Image Processing Theory, Tools and Applications (IPTA), pp. 1­6, 2019. [59] S. Li, D. Yi, Z. Lei, and S. Liao, "The casia nir-vis 2.0 face database," Jun. 2013, pp. 348­353. DOI: 10.1109/ CVPRW.2013.59. [60] H. Siqueira, S. Magg, and S. Wermter, "Efficient facial feature learning with wide ensemble-based convolutional neural networks," in AAAI, 2020. [61] A. Mollahosseini, B. Hasani, and M. Mahoor, "Affectnet: A database for facial expression, valence, and arousal computing in the wild," IEEE Transactions on Affective Computing, vol. 10, pp. 18­31, 2019. [62] E. Barsoum, C. Zhang, C. Canton-Ferrer, and Z. Zhang, "Training deep networks for facial expression recognition with crowd-sourced label distribution," Proceedings of the 18th ACM International Conference on Multimodal Interaction, 2016. [63] M. Liu, S. Shan, R. Wang, and X. Chen, "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition," 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1749­ 1756, 2014. [64] S. Kahou, V. Michalski, K. R. Konda, R. Memisevic, and C. Pal, "Recurrent neural networks for emotion recognition in video," in ICMI '15, 2015. [65] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, "Joint fine-tuning in deep neural networks for facial expression recognition," 2015 IEEE International Conference on Computer Vision (ICCV), pp. 2983­2991, 2015. [66] M. Long, Y. Cao, J. Wang, and M. Jordan, "Learning transferable features with deep adaptation networks," in International conference on machine learning, PMLR, 2015, pp. 97­105. [67] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, "Simultaneous deep transfer across domains and tasks," in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 4068­4076. [68] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, "Adapting visual category models to new domains," in ECCV, 2010. [69] X. Wang, X. Wang, and Y. Ni, "Unsupervised domain adaptation for facial expression recognition using generative adversarial networks," Computational intelligence and neuroscience, vol. 2018, 2018. [70] Y. Ganin and V. Lempitsky, "Unsupervised domain adaptation by backpropagation," in International con-

ference on machine learning, PMLR, 2015, pp. 1180­ 1189. [71] I. E. I. Bekkouch, Y. Youssry, R. Gafarov, A. Khan, and A. Khattak, "Triplet loss network for unsupervised domain adaptation," Algorithms, vol. 12, p. 96, 2019. [72] E. Batanina, I. E. I. Bekkouch, A. Khan, A. Khattak, and M. Bortnikov, "Domain adaptation for car accident detection in videos," 2019 Ninth International Conference on Image Processing Theory, Tools and Applications (IPTA), pp. 1­6, 2019. [73] M. Ghifary, W. Kleijn, M. Zhang, and D. Balduzzi, "Domain generalization for object recognition with multitask autoencoders," 2015 IEEE International Conference on Computer Vision (ICCV), pp. 2551­2559, 2015. [74] H. Li, S. J. Pan, S. Wang, and A. Kot, "Domain generalization with adversarial feature learning," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5400­5409, 2018. [75] D. Li, J. Zhang, Y. Yang, C. Liu, Y.-Z. Song, and T. M. Hospedales, "Episodic training for domain generalization," 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1446­1455, 2019. [76] Y. Balaji, S. Sankaranarayanan, and R. Chellappa, "Metareg: Towards domain generalization using metaregularization," in NeurIPS, 2018. [77] Z. Huang, H. Wang, E. P. Xing, and D. Huang, "Self-challenging improves cross-domain generalization," arXiv preprint arXiv:2007.02454, 2020. [78] D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales, "Learning to generalize: Meta-learning for domain generalization," in AAAI, 2018. [79] T. Kanade, J. Cohn, and Y. Tian, "Comprehensive database for facial expression analysis," in Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), 2000, pp. 46­53. DOI: 10.1109/AFGR.2000.840611. [80] L. Chen and Y. Yen, "Taiwanese facial expression image database," Brain Mapping Laboratory, Institute of Brain Science, National Yang-Ming University, 2007. [81] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," in 2009 IEEE conference on computer vision and pattern recognition, Ieee, 2009, pp. 248­255.

