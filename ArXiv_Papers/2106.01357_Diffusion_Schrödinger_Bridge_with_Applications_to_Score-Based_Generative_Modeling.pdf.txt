Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling

arXiv:2106.01357v1 [stat.ML] 1 Jun 2021

Valentin De Bortoli Department of Statistics, University of Oxford, UK
Jeremy Heng ESSEC Business School,
Singapore

James Thornton Department of Statistics, University of Oxford, UK
Arnaud Doucet Department of Statistics, University of Oxford, UK

Abstract
Progressively applying Gaussian noise transforms complex data distributions to approximately Gaussian. Reversing this dynamic defines a generative model. When the forward noising process is given by a Stochastic Differential Equation (SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the associated reverse-time SDE may be estimated using score-matching. A limitation of this approach is that the forward-time SDE must be run for a sufficiently long time for the final distribution to be approximately Gaussian. In contrast, solving the Schrödinger Bridge (SB) problem, i.e. an entropy-regularized optimal transport problem on path spaces, yields diffusions which generate samples from the data distribution in finite time. We present Diffusion SB (DSB), an original approximation of the Iterative Proportional Fitting (IPF) procedure to solve the SB problem, and provide theoretical analysis along with generative modeling experiments. The first DSB iteration recovers the methodology proposed by Song et al. (2021), with the flexibility of using shorter time intervals, as subsequent DSB iterations reduce the discrepancy between the final-time marginal of the forward (resp. backward) SDE with respect to the prior (resp. data) distribution. Beyond generative modeling, DSB offers a widely applicable computational optimal transport tool as the continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi, 2013).
1 Introduction
Score-Based Generative Modeling (SGM) is a recently developed approach to probabilistic generative modeling that exhibits state-of-the-art performance on several audio and image synthesis tasks; see e.g. Song and Ermon (2019); Cai et al. (2020); Chen et al. (2021a); Kong et al. (2021); Gao et al. (2020); Jolicoeur-Martineau et al. (2021); Ho et al. (2020); Song and Ermon (2020); Song et al. (2020, 2021); Niu et al. (2020); Durkan and Song (2021); Hoogeboom et al. (2021); Saharia et al. (2021); Luhman and Luhman (2021, 2020); Nichol and Dhariwal (2021); Popov et al. (2021); Dhariwal and Nichol (2021). Existing SGMs generally consist of two parts. Firstly, noise is incrementally added to the data in order to obtain a perturbed data distribution approximating an easy-to-sample prior density e.g. Gaussian. Secondly, a neural network is used to learn the reverse-time denoising dynamics, which when initialized at this prior distribution, defines a generative model (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song and Ermon, 2019; Song et al., 2021). Song et al. (2021) have shown that one could fruitfully think of the noising process as a Stochastic Differential Equation (SDE) progressively perturbing the initial data distribution into an approximately Gaussian one. The corresponding reverse-time SDE is an inhomogeneous diffusion whose drift depends on the
Preprint. Under review.

logarithmic gradients of the perturbed data distributions, i.e. the scores. In practice, these scores are approximated using neural networks and score-matching techniques (Hyvärinen and Dayan, 2005; Vincent, 2011) while numerical SDE integrators are used for the sampling procedure.

DSB 1

Forward

DSB Steps

Backward Forward

DSB 5

t=0

Backward t

Generative Model t=T

Figure 1: The reference forward diffusion initialized from the 2-dimensional data distribution fails to converge to the Gaussian prior in T = 0.2 diffusion-time (N = 20 discrete time steps), and the reverse diffusion initialized from the Gaussian prior does not converge to the data distribution. However, convergence does occur after 5 DSB iterations.

Although SGM provides state-of-the-art results (Dhariwal and Nichol, 2021), sample generation is computationally expensive. In order to learn the reverse-time SDE from the prior, i.e. the generative model, the forward noising SDE must be run for sufficiently long to converge to the prior and the step size must be sufficiently small for a good SDE approximation. By reformulating generative modeling as a Schrödinger bridge (SB) problem we alleviate this issue and propose a novel algorithm to solve SB problems. Our detailed contributions are as follows.
Generative modeling as a Schrödinger bridge problem. The SB problem is a famous entropyregularized Optimal Transport (OT) problem introduced by Schrödinger (1932); see e.g. (Léonard, 2014b; Chen et al., 2021b) for reviews. Given a reference diffusion with finite time horizon T , a data distribution and a prior distribution, solving the SB amounts to finding the closest diffusion to the reference (in terms of Kullback­Leibler divergence on path spaces) which admits the data distribution as marginal at time t = 0 and the prior at time t = T . The reverse-time diffusion solving this Schrödinger bridge problem provides a new SGM algorithm which enables approximate sample generation from the data distribution using shorter time intervals compared to the original SGM methods. This approach differs from the entropy-regularized OT formulation proposed in (Genevay et al., 2018) which deals with discrete distributions and relies on a static formulation of SB, as opposed to our dynamical approach for continuous distributions which operates on path spaces.
Solving the Schrödinger bridge problem using score-based diffusions. The Schrödinger bridge problem can be solved using Iterative Proportional Fitting (IPF) (Fortet, 1940; Kullback, 1968; Chen et al., 2021b). We propose Diffusion SB (DSB), a novel implementation of IPF using score-based diffusion techniques and an original mean-matching loss. DSB does not require discretizing the statespace (Chen et al., 2016; Reich, 2019), approximating potential functions using regression (Bernton et al., 2019; Dessein et al., 2017; Pavon et al., 2021), nor performing kernel density estimation (Pavon et al., 2021). The first DSB iteration recovers the method proposed by Song et al. (2021), with the flexibility of using shorter time intervals, as additional DSB iterations reduce the discrepancy between the final-time marginal of the forward (resp. backward) SDE w.r.t. the prior (resp. data) distribution; see Figure 1 for an illustration.
Theoretical analysis. We provide quantitative convergence results for the methodology of Song et al. (2021), which elucidates the need for long diffusion times in existing SGM methods. Additionally, we derive novel quantitative convergence results for IPF in continuous state-space which do not rely on classical compactness assumptions (Chen et al., 2016; Ruschendorf et al., 1995) and improve on

2

the recent results of Léger (2020). Finally, we show that our methodology may be viewed as the time discretization of a dynamic version of IPF on path spaces.
Experiments. We validate our methodology by generating image datasets such as MNIST and CelebA. In particular, we show that using multiple steps of DSB always improve the generative model. We also show how DSM can be used to interpolate between two data distributions.
Notation.
In the continuous-time setting, we set C = C([0, T ] , Rd) the space of continuous functions from [0, T ] to Rd and B(C) the Borel sets on C. For any measurable space (E, E), we denote by P(E) the space of probability measures on (E, E). For any  N, let P = P((Rd) ). When it is defined, we denote H(p) = - Rd p(x) log p(x)dx as the entropy of p and KL(p|q) as the Kullback­Leibler divergence between p and q. When there is no ambiguity we use the same notation for the distributions and their densities. All proofs are postponed to the supplementary.

2 Denoising Diffusion, Score-Matching and Reverse-Time SDEs

2.1 Discrete-Time: Markov Chains and Time Reversal

Consider a data distribution with positive density pdata1, a positive prior density pprior w.r.t. Lebesgue measure both with support on Rd and a Markov chain with initial density p0 = pdata on Rd evolving according to positive transition densities pk+1|k for k  {0, . . . , N - 1}. Hence for any x0:N = {xk}Nk=0  X = (Rd)N+1, the joint density may be expressed

p(x0:N ) = p0(x0)

N -1 k=0

pk+1|k

(xk+1|xk

).

(1)

This joint density also admits the backward decomposition

p(x0:N ) = pN (xN )

N -1 k=0

pk|k+1 (xk |xk+1 ),

with

pk|k+1 (xk |xk+1 )

=

, pk (xk )pk+1|k (xk+1 |xk )
pk+1 (xk+1 )

(2)

where pk(xk) = pk|k-1(xk|xk-1)pk-1(xk-1)dxk-1 is the marginal density at step k  1. For the
purpose of generative modeling, we will choose transition densities such that pN (xN )  pprior(xN ) for large N , where pprior is an easy-to-sample prior density. One may sample approximately from pdata using ancestral sampling with the reverse-time decomposition (2), i.e. first sample XN  pprior followed by Xk  pk|k+1(·|Xk+1) for k  {N - 1, . . . , 0}. This idea is at the core of all recent SGM. The reverse-time transitions in (2) cannot be simulated exactly but may be approximated if we
consider a forward transition density of the form

pk+1|k(xk+1|xk) = N (xk+1; xk + k+1f (xk), 2k+1I),

(3)

with drift f : Rd  Rd and stepsize k+1 > 0. We first make the following approximation from (2)

pk|k+1(xk|xk+1) = pk+1|k(xk+1|xk) exp[log pk(xk) - log pk+1(xk+1)]

(4)

 N (xk; xk+1 - k+1f (xk+1) + 2k+1 log pk+1(xk+1), 2k+1I), (5)

using that pk  pk+1, a Taylor expansion of log pk+1 at xk+1 and f (xk)  f (xk+1). In practice, the approximation holds if xk+1 - xk is small which is ensured by choosing k+1 small enough. Although  log pk+1 is not available, one may obtain an approximation using denoising scorematching methods (Hyvärinen and Dayan, 2005; Vincent, 2011; Song et al., 2021).

Assume that the conditional density pk+1|0(xk+1|x0) is available analytically as in (Ho et al., 2020;
Song et al., 2021). We have pk+1(xk+1) = p0(x0)pk+1|0(xk+1|x0)dx0 and elementary calculations show that  log pk+1(xk+1) = Ep0|k+1 [xk+1 log pk+1|0(xk+1|X0)]. We can therefore formulate score estimation as a regression problem and use a flexible class of functions, e.g. neural networks, to parametrize an approximation s (k, xk)   log pk(xk) such that

 = arg min

N k=1

Ep0,k [||s(k,

Xk )

-

xk

log

pk|0 (Xk |X0 )||2 ],

1In this presentation, we assume that all distributions admit a density w.r.t. the Lebesgue measure for simplicity. However, the algorithms presented here only require having access to samples from pdata and pprior.

3

where p0,k(x0, xk) = p0(x0)pk|0(xk|x0) is the joint density at steps 0 and k. If pk|0 is not available,

we use  = arg min

N k=1

Epk-1,k

[||s

(k,

Xk )

-

xk

log

pk|k-1 (Xk |Xk-1 )||2 ].

In summary,

SGM involves first estimating the score function s from noisy data, and then sampling X0 using

XN  pprior and the approximation (5), i.e.

Xk = Xk+1 - k+1f (Xk+1) + 2k+1s (k + 1, Xk+1) + 2k+1Zk+1, Zk i.i.d. N (0, I). (6)

The random variable X0 is approximately p0 = pdata distributed if pN (xN )  pprior(xN ). In what follows, we let {Yk}Nk=0 = {XN-k}Nk=0 and remark that {Yk}Nk=0 satisfies a forward recursion.

2.2 Continuous-Time: SDEs, Reverse-Time SDEs and Theoretical results

For appropriate transition densities, Song et al. (2021) showed that the forward and reverse-time

Markov chains may be viewed as discretized diffusions. We derive the continuous-time limit of the

procedure presented in Section 2.1 and establish convergence results. The Markov chain with kernel

(3) corresponds to an Euler­Maruyama discretization of (Xt)t[0,T ], solving the following SDE



dXt = f (Xt)dt + 2dBt , X0  p0 = pdata,

(7)

where (Bt)t[0,T ] is a Brownian motion and f : Rd  Rd is regular enough so that (strong) solutions

exist. Under conditions on f , it is well-known (see Haussmann and Pardoux (1986); Föllmer (1985);

Cattiaux et al. (2021) for instance) that the reverse-time process (Yt)t[0,T ] = (XT -t)t[0,T ] satisfies



dYt = {-f (Yt) + 2 log pT -t(Yt)} dt + 2dBt,

(8)

with initialization Y0  pT , where pt denotes the marginal density of Xt. The reverse-time Markov chain {Yk}Nk=0 associated with (6) corresponds to an Euler­Maruyama discretization of (8), where the score functions  log pt(x) are approximated by s (t, x).

In what follows, we consider f (x) = -x for   0. This framework includes the one of Song and Ermon (2019) ( = 0, pprior(x) = N (x; 0, 2T )) for which (Xt)t[0,T ] is simply a Brownian motion and Ho et al. (2020) ( > 0, pprior(x) = N (x; 0, 1/)) for which it is an Ornstein­Uhlenbeck process, see Appendix C.3 for more details. Contrary to Song et al. (2021) we consider time homogeneous diffusions. Both approaches approximate (6) using distinct discretizations but our setting leverages the ergodic properties of the Ornstein-Uhlenbeck process to establish Theorem 1.
Theorem 1. Assume that there exists M  0 such that for any t  [0, T ] and x  Rd

s (t, x) -  log pt(x)  M,

(9)

with s  C([0, T ] × Rd, Rd). Assume that pdata  C3(Rd, (0, +)) is bounded and that there exist d1, A1, A2, A3  0, 1, 2, 3  N and m1 > 0 such that for any x  Rd and i  {1, 2, 3}
i log pdata(x)  Ai(1 + x i ),  log pdata(x), x  -m1 x 2 + d1 x ,
with 1 = 1. Then for any   0, there exist B, C, D  0 such that for any N  N and {k}Nk=1 with k > 0 for any k  {1, . . . , N }, the following hold:

(a) if  > 0, we have L(X0) - pdata TV  B exp[-1/2T ] + C(M + ¯1/2) exp[DT ]; (b) if  = 0, we have L(X0) - pdata TV  B0(T -1 + T -1/2) + C0(M + ¯1/2) exp[D0T ];

where T =

N k=1

k ,

¯

=

supk{1,...,N }

k

and

L(X0)

is

the

distribution

of

X0

given

in

(6).

Condition (9) ensures that the neural network approximates the score with a given precision M  0. Under this assumption and conditions on pdata, Theorem 1 states how the Markov chain defined by (6) approximates pdata in the total variation norm · TV. In both cases,  = 0 and  > 0, the error consists of two terms. The first term decreases with T  0 and corresponds to the error between
pT and pprior. The second term stems from the error between the continuous-time process (8) with initialization Y0  pprior and its discrete-time approximation (6). Those bounds show that there is a trade-off between the mixing properties of the Markov chain which increases with , and the
quality of the discrete-time approximation which deteriorates as  and T increase. To the best of our
knowledge Theorem 1 is the first result assessing the convergence of SGM methods.

4

3 Diffusion Schrödinger Bridge and Generative Modeling

3.1 Schrödinger Bridges

The Schrödinger Bridge (SB) problem is a classical problem appearing in applied mathematics, optimal control and probability; see e.g. Föllmer (1988); Léonard (2014b); Chen et al. (2021b). In the discrete-time setting, it takes the following (dynamic) form. Consider as reference density p(x0:N ) given by (1), describing the process adding noise to the data. We aim to find   PN+1 such that

 = arg min {KL(|p) :   PN+1, 0 = pdata, N = pprior} .

(10)

Assuming  is available, a generative model can be obtained by sampling XN  pprior, followed by the reverse-time dynamics Xk  k|k+1(·|Xk+1) for k  {N - 1, . . . , 0}. Before deriving a method to approximate  in Section 3.2, we highlight some desirable features of Schrödinger bridges.

Static Schrödinger bridge problem. First, we recall that the dynamic formulation (10) admits a

static analogue. Using e.g. Léonard (2014a, Theorem 2.4), the following decomposition holds for

any   PN+1, KL(|p) = KL(0,N |p0,N ) + E0,N [KL(|0,N |p|0,N )], where for any µ  PN+1 we have µ = µ0,N µ|0,N with µ|0,N the conditional distribution of X1:N-1 given X0, XN 2. Hence we have  (x0:N ) = s, (x0, xN )p|0,N (x1:N-1|x0, xN ) where s,  P2 with marginals 0s, and Ns, is the solution of the static SB problem

s, = arg min {KL(s|p0,N ) : s  P2, 0s = pdata, Ns = pprior} .

(11)

Link with optimal transport. Under mild assumptions, the static SB problem can be seen as an

entropy-regularized optimal transport problem since (11) is equivalent to

s, = arg min -Es [log pN|0(XN |X0)] - H(s) : s  P2, 0s = pdata, Ns = pprior .

If pk+1|k(xk+1|xk) = N (xk+1; xk, k2+1) as in Song and Ermon (2019), then pN|0(xN |x0) =

N (xN ; x0, 2) with 2 =

N k=1

k2

which

induces

a

quadratic

cost

and

s, = arg min Es [||X0 - XN ||2] - 22H(s) : s  P2, 0s = pdata, Ns = pprior .

Mikami (2004) showed that s,  W weakly and 22 KL(s, |p0,N )  W22(pdata, pprior) as   0, where W is the optimal transport plan between pdata and pprior and W2 is the 2-Wasserstein distance. Note that the transport cost c(x, x ) = - log pN|0(x |x) is not necessarily symmetric.

3.2 Iterative Proportional Fitting and Time Reversal

In all but trivial cases, the SB problem does not admit a closed-form solution. However, it can be
solved using Iterative Proportional Fitting (IPF) (Fortet, 1940; Kullback, 1968; Ruschendorf et al., 1995) which is defined by the following recursion for n  N with initialization 0 = p given in (1):

2n+1 = arg min KL(|2n) :   PN+1, N = pprior ,

(12)

2n+2 = arg min KL(|2n+1) :   PN+1, 0 = pdata .

(13)

This sequence is well-defined if there exists ~  PN+1 such that ~0 = pdata, ~N = pprior and KL(~|p) < +. A standard representation of n is obtained by updating the joint density p using potential functions, see Appendix D.2 for details. However, this representation of the IPF iterates is difficult to approximate as it requires approximating the potentials. Our methodology builds upon an alternative representation that is better suited to numerical approximations for generative modeling where one has access to samples of pdata and pprior.
Proposition 2. Assume that KL(pdata  pprior|p0,N ) < +. Then for any n  N, 2n and 2n+1 admit positive densities w.r.t. the Lebesgue

measure denoted as pn resp. qn and for any x0:N  X , we have p0(x0:N ) = p(x0:N ) and

qn(x0:N ) = pprior(xN )

N -1 k=0

pnk|k+1 (xk |xk+1 ),

pn+1(x0:N

)

=

pdata(x0)

N -1 k=0

qkn+1|k

(xk+1|xk

).

(14)

2See Appendix D.1 for a rigorous presentation using the disintegration theorem for probability measures.

5

In practice we have access to pnk+1|k and qkn|k+1. Hence, to compute pnk|k+1 and qkn+1|k we use

pnk|k+1 (xk |xk+1 )

=

pnk+1|k(xk+1|xk)pnk (xk pnk+1(xk+1)

)

,

qkn+1|k (xk+1 |xk )

=

qkn|k+1 (xk |xk+1 )qkn+1 qkn (xk )

(xk+1)

.

To the best of our knowledge, this representation of the IPF iterates has surprisingly neither been

presented nor explored in the literature. One may interpret these formulas as follows. At iteration 2n, we have 2n = pn with p0 = p given by the noising process (1). This forward process initalized with pn0 = pdata defines reverse-time transitions pnk|k+1, which, when combined with an initialization pprior
at step N defines the reverse-time process 2n+1 = qn. The forward transitions qkn+1|k associated to
qn are then used to obtain 2n+2 = pn+1. IPF then iterates this procedure.

3.3 Diffusion Schrödinger Bridge as Iterative Mean-Matching Proportional Fitting

To approximate the IPF recursion defined in Proposition 2, we use similar approximations to Section 2.1. If at step n  N we have pnk+1|k(xk+1|xk) = N (xk+1; xk + k+1fkn(xk), 2k+1I) where p0 = p and fk0 = f , then we can approximate the reverse-time transitions in Proposition 2 by
qkn|k+1(xk|xk+1) = pnk+1|k(xk+1|xk) exp[log pnk (xk) - log pnk+1(xk+1)]
 N (xk; xk+1 + k+1bnk+1(xk+1), 2k+1I),
with bnk+1(xk+1) = -fkn(xk+1) + 2 log pnk+1(xk+1). We can also approximate the forward transitions in Proposition 2 by pnk++11|k(xk+1|xk)  N (xk+1; xk + k+1fkn+1(xk), 2k+1I) with fkn+1(xk) = -bnk+1(xk)+2 log qkn(xk). Hence we have fkn+1(xk) = fkn(xk)-2 log pnk+1(xk)+ 2 log qkn(xk). It follows that, one could estimate fkn+1, bnk+1 using score-matching to approximate { log pik+1(x)}ni=0 , { log qki (x)}ni=0. This approach is prohibitively costly in terms of memory and compute, see Appendix E. We follow an alternate approach which avoids these difficulties.

Proposition 3. Assume that for any n  N and k  {0, . . . , N - 1}, qkn|k+1(xk|xk+1) = N (xk; Bkn+1(xk+1), 2k+1I), pnk+1|k(xk+1|xk) = N (xk+1; Fkn(xk), 2k+1I),

with Bkn+1(x) = x + k+1bnk+1(x), Fkn(x) = x + k+1fkn(x) for any x  Rd. Then we have for any n  N and k  {0, . . . , N - 1}

Bkn+1 = arg minBL2(Rd,Rd) E [ pnk,k+1 B(Xk+1) - (Xk+1 + Fkn(Xk) - Fkn(Xk+1)) 2], Fkn+1 = arg minFL2(Rd,Rd) E [ qkn,k+1 F(Xk) - (Xk + Bkn+1(Xk+1) - Bkn+1(Xk)) 2].

(15) (16)

Proposition 3 shows how one can recursively approximate Bkn+1 and Fkn+1. In practice, we use

neural networks Bn (k, x)  Bkn(x) and Fn (k, x)  Fkn(x).

Network parameters n, n are learnt

Algorithm 1 Diffusion Schrödinger Bridge

through gradient descent to minimize

1: for n  {0, . . . , L} do

empirical versions of the sum over k of

2: while not converged do

the loss functions given by (291) and

3:

Sample {Xkj }Nk,,jM=0, Xkj+1 = Fn (k, Xkj

)w+here2Xk0j+1Zpkjd+at1a

,

and

4:

Compute ^bn(n) approximating (291)

(292) computed using M samples and denoted ^bn() and ^fn+1(). The resulting algorithm approximating L IPF it-
erations is called Diffusion Schrödinger

5:

n  Gradient Step(^bn(n))

Bridge (DSB) and is summarized in Al-

6: end while

7: while not converged do

8:

Sample {Xkj }Nk,,jM=0, Xkj-1 = Bn (k, Xkj

)w+here2XkNjZ~kj

pprior,

and

gorithm 1 with Zkj, Z~kj i.i.d. N (0, I), see Figure 1 for an illustration. This algo-
rithm is initialized using the reference dynamics f0 (k, x) = f (x). Once L is

9:

Compute ^fn+1(n+1) approximating (292)

10:

n+1



Gradient

Step(

^f
n+1

(n+1

))

learnt we can easily approximately sam-
ple from pdata by sampling XN  pprior and then using Xk-1 = BL (k, Xk) +

11: end while 12: end for

 2k Zk

with Zk

i.i.d.

N (0, I).

The

13: Output: (L+1, L)

resulting samples X0 will be approxi-

mately distributed from pdata.

Although the DSB requires learning a sequence of network parameters, n, n, fewer diffusion steps

6

are needed compared to standard SGM and hence the networks are quicker to train. In addition, as detailed in Appendix I, the initial 0 may be trained efficiently similar to previous SGM methods and then n+1, n+1 are refinements of n, n hence may be fine-tuned from previous iterations.

3.4 Convergence of Iterative Proportional Fitting
In this section, we investigate the theoretical properties of IPF. When the state-space is discrete and finite (Franklin and Lorenz, 1989; Peyré and Cuturi, 2019) or in the case where pdata and pprior are compactly supported (Chen et al., 2016), IPF converges at a geometric rate w.r.t. the HilbertBirkhoff metric, see Lemmens and Nussbaum (2014) for a definition. Other than recent work by Léger (2020), only qualitative results exist in the general case where pdata or pprior is not compactly supported (Ruschendorf et al., 1995; Rüschendorf and Thomsen, 1993). We establish here quantitative convergence of IPF in this non-compact setting as well as novel monotonicity results. We require only the following mild assumption.
A1. pN , pprior > 0, H(pprior) < +, Rd | log(pN|0(xN |x0))|pdata(x0)pprior(xN )dx0dxN < +.
Assumption A1 is satisfied in all of our experimental settings. We recall that for µ,   P(E) with (E, E) a measurable space, the Jeffreys divergence is given by J(µ, ) = KL(µ|) + KL(|µ). Proposition 4. Assume A1. Then (n)nN is well-defined and for any n  1 we have
KL(n+1|n)  KL(n-1|n), KL(n|n+1)  KL(n|n-1).
In addition, ( n+1 - n TV)nN and (J(n+1, n))nN are non-increasing. Finally, we have limn+ n {KL(0n|pdata) + KL(Nn |pprior)} = 0.
A more general result with additional monotonicity properties is given in Appendix F. Under similar assumptions, Léger (2020, Corollary 1) derives that KL(0n|p0)  C/n with C  0 using a Bregman divergence gradient descent perspective. In contrast, our proof relies only on tools from information geometry. In addition, we improve the convergence rate and show that (n)nN converges in total variation towards , i.e. we not only obtain the convergence of the marginals but the convergence of the joint distribution. Under restrictive conditions on pdata and pprior, Ruschendorf et al. (1995) show that  is the Schrödinger bridge. In the following proposition we avoid this assumption using results on automorphisms of measures (Beurling, 1960).
Proposition 5. Assume A1. Then there exists a solution   PN+1 to the SB problem and we have limn+ n -  TV = 0 with   PN+1. Let h = p0,N /(p0  pN ) and assume that h  C((Rd)2, (0, +)) and that there exist 0, N  C(Rd, (0, +)) such that
Rd×Rd (|log h(x0, xN )| + |log 0(x0)| + |log N (xN )|)pdata(x0)pprior(xN )dx0dxN < +,
with h(x0, xN )  0(x0)N (xN ). If p is absolutely continuous w.r.t.  then  =  .
Proposition 5 extends previous IPF convergence results without the assumption that the mapping h is lower bounded, see Ruschendorf et al. (1995); Chen et al. (2016). Our assumption on h can be relaxed and replaced by a tighter condition on , see Appendix F.2. Proposition 4 suggests a convergence of order o(n) for the IPF in the non compact setting. However, in some situations, we recover geometric convergence rates with explicit dependency w.r.t. the problem constants, see Appendix G.

3.5 Continuous-time IPF

We describe an IPF algorithm for solving SB problems in continuous-time. We show that DSB proposed in Algorithm 1 can be seen as a discretization of this IPF. Given a reference measure P  P(C), the continuous formulation of the SB involves solving the following problem

 = arg min {KL(|P) :   P(C), 0 = pdata, T = pprior} ,

T=

N -1 k=0

k+1

.

(17)

Similarly to (12), we define the IPF (n)nN with 0 = P associated with (7) and for any n  N

2n+1 = arg min KL(|2n) :   P(C), T = pprior , 2n+2 = arg min KL(|2n+1) :   P(C), 0 = pdata .

7

One can show that for any n  N, n = s,nP|0,T , with (s,n)nN the IPF for the static SB problem.
In particular, Proposition 4 and Proposition 5 extend to the continuous IPF framework. In what follows, for any P  P(C), we define PR as the reverse-time measure, i.e. for any A  B(C) we have PR(A) = P(AR) where AR = {t  (T - t) :   A}. The following result is the continuous
counterpart of Proposition 2 and states that each IPF iteration is associated with a diffusion, showing

that DSB can be seen as a discretization of the continuous IPF.

Proposition 6. Assume A1 and that there exist M  P(C), U  C1(Rd, R), C  0 such that for

any n  N, x  Rd, KL(n|M) < +, x, U (x)  -C(1 + x 2) and M is associated with 

dXt = -U (Xt)dt + 2dBt,

(18)

with X0 distributed according to the invariant distribution of (18). Then, for any n  N we have:

(a)

(2n+1)R is associated with dYt2n+1

=

bnT -t(Yt2n+1)dt

+

 2dBt

with

Y02n+1

 pprior;

(b)

2n+2 is associated with dX2t n+2

=

ftn+1(X2t n+2)dt

+

 2dBt

with

X20n+2

 pdata;

where for any n  N, t  [0, T ] and x  Rd, bnt (x) = -ftn(x) + 2 log pnt (x), ftn+1(x) =

-bnt (x) + 2 log qtn(x), with ft0(x) = f (x), see (7), and pnt , qtn the densities of 2t n and 2t n+1.

4 Experiments

Two dimensional toy experiments. We eval-

Data distribution

uate the validity of our approach on toy two di-

mensional examples. Contrary to existing SGM

approaches we do not require that the number

of steps is large enough for pN  pprior to hold.

DSB Iteration 1

We use a fully connected network with posi-

tional encoding (Vaswani et al., 2017) to approx-

imate Bkn and Fkn, see Appendix J.1 for details about our implementation. Animated plots of
the DSB iterations may be found online on our

DSB Iteration 20

project webpage3. In Figure 2, we illustrate

the benefits of DSB over classical SGM. We fix

f (x) = x and choose pprior = N (0, d2ataI), hence  = 1/d2ata where d2ata is the variance of the dataset. We let N = 20 and k = 0.01, i.e. T = 0.2. Since T is low, the noising process

Figure 2: Data distributions pdata vs distribution at final time T = 0.2 after 1 and 20 DSB iterations.

does not satisfy pN  pprior and the reverse-time process obtained after the first DSB iteration

(corresponding to original SGM methods) does not yield a satisfactory generative model. However,

multiple iterations of DSB improve the quality of the synthesis. We emphasize that even though in

theory Schrödinger bridges allow for T to be arbitrary small, we observe that decreasing values of T

require an increasing number of DSB iterations to obtain valid generative models. We discuss this

trade-off and present additional experiments in Appendix J.1.

Generative modeling. DSB is the first practical algorithm for approximating the solution to the SB problem in high dimension. Whilst our implementation does not yet compete with state-of-the-art methods, we show promising results with fewer diffusion steps compared to initial SGMs (Song and Ermon, 2019) and demonstrate its performance on MNIST (LeCun and Cortes, 2010) and CelebA (Liu et al., 2015). A reduced U-net architecture based on Nichol and Dhariwal (2021) is used to approximate Bkn and Fkn. Further details are given in Appendix J.2. Our method is validated on downscaled CelebA in Figure 3. Figure 4 illustrates qualitative improvement over 8 DSB iterations with as few as N = 12 diffusion steps. Note, as shown in the Appendix J.2, we obtain better results with higher N yet still significantly fewer steps than in the original SGM procedures (Song and Ermon, 2020, 2019) which use N = 100. Figure 4 also shows good diversity of generated samples (red) and coverage of the original dataset (blue) as shown by the two dimensional representation in the latent space of a pre-trained Variational Auto-Encoder (VAE). Figure 5 illustrates how the sample quality, measured quantitatively in terms of Fréchet Inception Distance (FID) (Heusel et al., 2017), improves with the number of DSB iterations for various numbers of steps N .
3https://vdeborto.github.io/publication/schrodinger_bridge/

8

t=0

t = 0.31

t = 0.60

t = 0.63

Figure 3: Generative model for CelebA 32 × 32 after 10 DSB iterations with N = 50 (T = 0.63)

DSB 1

DSB 8
Figure 4: Generated samples (N = 12) and two-dimensional visualization of samples (red) compared to original MNIST data (blue) using pre-trained VAE

Figure 5: FID vs DSB Iterations. Green dashed line: baseline FID obtained with 1 DSB iteration and N = 40.

Datasets interpolation. Schrödinger bridges not only allow us to reduce the number of iterations
in SGM methods but also enable flexibility in the choice of the prior density pprior which is not necessarily Gaussian contrary to previous works on SGM. In particular, our approach is still valid if
pprior is any other data distribution pdata. In this case DSB converges towards a bridge between pdata and pdata, see Figure 6. These experiments pave the way towards high-dimensional optimal transport between arbitrary data distributions.

Figure 6: First row: Swiss-roll to S-curve (2D). Iteration 9 of DSB with T = 1 (N = 50). From left to right: t = 0, 0.4, 0.6, 1. Second row: EMNIST (Cohen et al., 2017) to MNIST. Iteration 10 of DSB with T = 1.5 (N = 30). From left to right: t = 0, 0.4, 1.25, 1.5.
5 Discussion
Score-based generative modeling (SGM) may be viewed as the first stage in approximating a solution to the Schrödinger bridge problem. Through this interpretation we have developed a novel methodology, the Diffusion Schrödinger Bridge (DSB), that extends initial SGM approaches and allows one to perform generative modeling with fewer diffusion steps. DSB complements recent techniques used to speed up existing SGM algorithms which rely on either different noise schedules (Nichol and Dhariwal, 2021; San-Roman et al., 2021) or knowledge distillation (Luhman and Luhman, 2021). Additionally, as the solution of the Schrödinger problem is a diffusion, it is possible as in Song
9

et al. (2021, Section 4.3) to obtain an equivalent neural ordinary differential equation that admits the same marginals as the diffusion but enables exact likelihood computation. From a theoretical point of view, we have provided the first convergence result for SGM methods and derived new state-of-the-art convergence bounds for IPF as well as novel monotonicity results. We have demonstrated DSB on generative modeling and interpolation tasks. Finally, while motivated by generative modeling, DSB is much more widely applicable as it can be thought of as the continuous state-space counterpart of the celebrated Sinkhorn algorithm (Cuturi, 2013; Peyré and Cuturi, 2019). For example, DSB could be used to solve multi-marginal Schrödinger bridges problems (Di Marino and Gerolin, 2020), compute Wasserstein barycenters, find the minimizers of entropy-regularized Gromov-Wasserstein problems (Mémoli, 2011) or perform domain adaptation in continuous state-spaces.
6 Acknowledgements
Valentin De Bortoli and Arnaud Doucet are supported by the EPSRC CoSInES (COmputational Statistical INference for Engineering and Security) grant EP/R034710/1 and James Thornton by the OxWaSP CDT through grant EP/L016710/1.
References
Ambrosio, L., Gigli, N., and Savaré, G. (2008). Gradient Flows in Metric Spaces and in the Space of Probability Measures. Lectures in Mathematics ETH Zürich. Birkhäuser Verlag, Basel, second edition.
Bakry, D., Gentil, I., and Ledoux, M. (2014). Analysis and Geometry of Markov Diffusion Operators, volume 348. Springer.
Bernton, E., Heng, J., Doucet, A., and Jacob, P. E. (2019). Schrödinger bridge samplers. arXiv preprint arXiv:1912.13170.
Beurling, A. (1960). An automorphism of product measures. Annals of Mathematics, 72(1):189­200.
Cai, R., Yang, G., Averbuch-Elor, H., Hao, Z., Belongie, S., Snavely, N., and Hariharan, B. (2020). Learning gradient fields for shape generation. European Conference on Computer Vision.
Cattiaux, P., Conforti, G., Gentil, I., and Léonard, C. (2021). Time reversal of diffusion processes under a finite entropy condition. arXiv preprint arXiv:2104.07708.
Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. (2021a). Wavegrad: Estimating gradients for waveform generation. International Conference on Learning Representations.
Chen, Y., Georgiou, T., and Pavon, M. (2016). Entropic and displacement interpolation: a computational approach using the Hilbert metric. SIAM Journal on Applied Mathematics, 76(6):2375­2396.
Chen, Y., Georgiou, T. T., and Pavon, M. (2021b). Optimal transport in systems and control. Annual Review of Control, Robotics, and Autonomous Systems, 4.
Cohen, G., Afshar, S., Tapson, J., and van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters. arXiv preprint arXiv:1702.05373.
Constantine, G. M. and Savits, T. H. (1996). A multivariate Faà di Bruno formula with applications. Trans. Amer. Math. Soc., 348(2):503­520.
Csiszár, I. (1975). I-divergence geometry of probability distributions and minimization problems. The Annals of Probability, 3(1):146­158.
Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems.
Dellacherie, C. and Meyer, P.-A. (1988). Probabilities and Potential. C, volume 151 of North-Holland Mathematics Studies. North-Holland Publishing Co., Amsterdam. Potential theory for discrete and continuous semigroups, Translated from the French by J. Norris.
10

Deming, W. E. and Stephan, F. F. (1940). On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. The Annals of Mathematical Statistics, 11(4):427­444.
Dessein, A., Papadakis, N., and Deledalle, C.-A. (2017). Parameter estimation in finite mixture models by regularized optimal transport: A unified framework for hard and soft clustering. arXiv preprint arXiv:1711.04366.
Dhariwal, P. and Nichol, A. (2021). Diffusion models beat GAN on image synthesis. arXiv preprint arXiv:2105.05233.
Di Marino, S. and Gerolin, A. (2020). An optimal transport approach for the Schrödinger bridge problem and convergence of Sinkhorn algorithm. Journal of Scientific Computing, 85(2):1­28.
Douc, R., Moulines, E., Priouret, P., and Soulier, P. (2019). Markov Chains. Springer.
Durkan, C. and Song, Y. (2021). On maximum likelihood training of score-based generative models. arXiv preprint arXiv:2101.09258.
Durmus, A. and Moulines, E. (2017). Nonasymptotic convergence analysis for the unadjusted Langevin algorithm. Ann. Appl. Probab., 27(3):1551­1587.
Enderton, H. B. (1977). Elements of Set Theory. Academic Pres, New York-London.
Ethier, S. N. and Kurtz, T. G. (1986). Markov Processes. Wiley Series in Probability and Mathematical Statistics: Probability and Mathematical Statistics. John Wiley & Sons, Inc., New York. Characterization and convergence.
Föllmer, H. (1985). An entropy approach to the time reversal of diffusion processes. In Stochastic Differential Systems: Filtering and Control, pages 156­163. Springer.
Föllmer, H. (1988). Random fields and diffusion processes. In École d'Été de Probabilités de Saint-Flour XV­XVII, 1985­87, pages 101­203. Springer.
Fortet, R. (1940). Résolution d'un système d'équations de M. Schrödinger. Journal de Mathématiques Pures et Appliqués, 1:83­105.
Franklin, J. and Lorenz, J. (1989). On the scaling of multidimensional matrices. Linear Algebra and Its Applications, 114:717­735.
Gao, R., Song, Y., Poole, B., Wu, Y. N., and Kingma, D. P. (2020). Learning energy-based models by diffusion recovery likelihood. arXiv preprint arXiv:2012.08125.
Genevay, A., Peyré, G., and Cuturi, M. (2018). Learning generative models with Sinkhorn divergences. In International Conference on Artificial Intelligence and Statistics.
Haussmann, U. G. and Pardoux, E. (1986). Time reversal of diffusions. The Annals of Probability, 14(4):1188­1205.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). GANs trained by a two time-scale update rule converge to a local Nash equilibrium. arXiv preprint arXiv:1706.08500.
Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems.
Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P., and Welling, M. (2021). Argmax flows and multinomial diffusion: Towards non-autoregressive language models. arXiv preprint arXiv:2102.05379.
Hyvärinen, A. and Dayan, P. (2005). Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4).
Ikeda, N. and Watanabe, S. (1989). Stochastic Differential Equations and Diffusion Processes, volume 24 of North-Holland Mathematical Library. North-Holland Publishing Co., Amsterdam; Kodansha, Ltd., Tokyo, second edition.
11

Jolicoeur-Martineau, A., Piché-Taillefer, R., Tachet des Combes, R., and Mitliagkas, I. (2021). Adversarial score matching and improved sampling for image generation. International Conference on Learning Representations.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Kober, H. (1939). A theorem on Banach spaces. Compositio Math., 7:135­140.
Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2021). Diffwave: A versatile diffusion model for audio synthesis. International Conference on Learning Representations.
Kruithof, J. (1937). Telefoonverkeersrekening. De Ingenieur, 52:15­25.
Kullback, S. (1968). Probability densities with given marginals. The Annals of Mathematical Statistics, 39(4):1236­1243.
Kullback, S. (1997). Information Theory and Statistics. Dover Publications, Inc., Mineola, NY. Reprint of the second (1968) edition.
Laumont, R., De Bortoli, V., Almansa, A., Delon, J., Durmus, A., and Pereyra, M. (2021). Bayesian imaging using plug & play priors: when Langevin meets Tweedie. arXiv preprint arXiv:2103.04715.
LeCun, Y. and Cortes, C. (2010). MNIST handwritten digit database.
Léger, F. (2020). A gradient descent perspective on Sinkhorn. Applied Mathematics & Optimization, pages 1­13.
Leha, G. and Ritter, G. (1984). On diffusion processes and their semigroups in Hilbert spaces with an application to interacting stochastic systems. Ann. Probab., 12(4):1077­1112.
Lemmens, B. and Nussbaum, R. D. (2014). Birkhoff's version of Hilbert's metric and its applications in analysis. Handbook of Hilbert Geometry, pages 275­303.
Léonard, C. (2014a). Some properties of path measures. In Séminaire de Probabilités XLVI, pages 207­230. Springer.
Léonard, C. (2014b). A survey of the Schrödinger problem and some of its connections with optimal transport. Discrete & Continuous Dynamical Systems-A, 34(4):1533­1574.
Léonard, C. (2019). Revisiting Fortet's proof of existence of a solution to the Schrödinger system. arXiv preprint arXiv:1904.13211.
Liptser, R. S. and Shiryaev, A. N. (2001). Statistics of random processes. I, volume 5 of Applications of Mathematics (New York). Springer-Verlag, Berlin, expanded edition. General theory, Translated from the 1974 Russian original by A. B. Aries, Stochastic Modelling and Applied Probability.
Liu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep learning face attributes in the wild. In International Conference on Computer Vision.
Luhman, E. and Luhman, T. (2021). Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388.
Luhman, T. and Luhman, E. (2020). Diffusion models for handwriting generation. arXiv preprint arXiv:2011.06704.
Mémoli, F. (2011). Gromov-Wasserstein distances and the metric approach to object matching. Foundations of Computational Mathematics, 11(4):417­487.
Meyn, S. P. and Tweedie, R. L. (1993). Stability of Markovian processes. III. Foster-Lyapunov criteria for continuous-time processes. Adv. in Appl. Probab., 25(3):518­548.
Mikami, T. (2004). Monge's problem with a quadratic cost by the zero-noise limit of h-path processes. Probability Theory and Related Fields, 129(2):245­260.
12

Nichol, A. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672.
Niu, C., Song, Y., Song, J., Zhao, S., Grover, A., and Ermon, S. (2020). Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics.
Pavon, M., Trigila, G., and Tabak, E. G. (2021). The data-driven Schrödinger bridge. Communications on Pure and Applied Mathematics, 74:1545­1573.
Peyré, G. and Cuturi, M. (2019). Computational optimal transport. Foundations and Trends® in Machine Learning, 11(5-6):355­607.
Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. (2021). Grad-tts: A diffusion probabilistic model for text-to-speech. arXiv preprint arXiV:2105.06337.
Reich, S. (2019). Data assimilation: the Schrödinger perspective. Acta Numerica, 28:635­711.
Revuz, D. and Yor, M. (1999). Continuous Martingales and Brownian Motion, volume 293 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. Springer-Verlag, Berlin, third edition.
Ruschendorf, L. et al. (1995). Convergence of the iterative proportional fitting procedure. The Annals of Statistics, 23(4):1160­1174.
Rüschendorf, L. and Thomsen, W. (1993). Note on the Schrödinger equation and i-projections. Statistics & Probability letters, 17(5):369­375.
Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J., and Norouzi, M. (2021). Image superresolution via iterative refinement. arXiv preprint arXiv:2104.07636.
San-Roman, R., Nachmani, E., and Wolf, L. (2021). Noise estimation for generative diffusion models. arXiv preprint arXiv:2104.02600.
Schrödinger, E. (1932). Sur la théorie relativiste de l'électron et l'interprétation de la mécanique quantique. Annales de l'Institut Henri Poincaré, 2(4):269­310.
Sinkhorn, R. and Knopp, P. (1967). Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343­348.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning.
Song, J., Meng, C., and Ermon, S. (2020). Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502.
Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems.
Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models. Advances in Neural Information Processing Systems.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021). Scorebased generative modeling through stochastic differential equations. International Conference on Learning Representations.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.
Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661­1674.
13

A Organization of the supplementary
The supplementary is organized as follows. We define our notation in Appendix B. In Appendix C, we prove Theorem 1 and draw links between our approach of SGM and existing works. We recall the classical formulation of IPF, prove Proposition 2 and draw links with autoencoders in Appendix D. In Appendix E we present alternative variational formulas for Algorithm 1 and prove Proposition 3. We gather the proofs of our theoretical study of Schrödinger bridges (Proposition 4 and Proposition 5) in Appendix F. A quantitative study of IPF with Gaussian targets and reference measure is presented in Appendix G. In particular, we show that the convergence rate of IPF is geometric. In Appendix H we study the links between continuous-time and discrete-time IPF and prove Proposition 6. We detail training techniques to improve training times in Appendix I then present architecture details and additional experiments in Appendix J.

B Notation

For ease of reading in this section we recall and detail some of the notation introduced in Section 1. For any measurable space (E, E), we denote by P(E) the space of probability measures over E. For any
 N, we also denote P = P((Rd) ). For any   P(E) and Markov kernel K : E × F  [0, 1] where (F, F) is a measurable space, we define K  P(F) such that for any A  F we have K(A) = E K(x, A)d(x). If E = C then for any P  P(E) and s, t  [0, T ], we denote by Ps,t the marginals of P at time s and t. In addition, we denote by P|s,t the disintegration Markov kernel
given by the mapping   ((s), (t)), see Appendix D.1 for a definition. In particular, we have P = Ps,tP|s,t. All defined mappings are considered to be measurable unless stated otherwise.

For any P  P(C) we define PR the reverse-time measure, i.e. for any A  B(C) we have PR(A) = P(AR) where AR = {t  (T - t) :   A}. We say that P  P(C) is associated with a
diffusion if it solves the corresponding martingale problem. More precisely, P  P(C) is associated with dXt = b(t, Xt)dt + 2dBt for b : [0, T ] × Rd  Rd measurable if for any v  C2c(Rd, R), (Mtv)t[0,T ] is a P-local martingale, where for any t  [0, T ]

Mtv = v(Xt) -

t 0

As(v)(Xs

)ds

(19)

with for any v  C2(Rd, R), t  [0, t] and x  Rd

A(v)(x) = b(t, x), v(x) + v(x) .

(20)

We refer to Revuz and Yor (1999) for a rigorous treatment of local martingales. Note that (19) uniquely defines Pt|s for any s, t  [0, T ] with t  s. Hence P is uniquely defined up to P0.

In some cases, we say that P  P(C) is associated with a diffusion if it solves the corresponding martingale problem with initial condition. More precisely, P  P(C) is associated with dXt = b(t, Xt)dt + 2dBt and X0  µ0  P(Rd) if it solves the martingale problem and P0 = µ0. Note that in this case P is uniquely defined.

Finally, for any measurable space (E, E) and µ,   P(E) we recall that the Jeffreys divergence is given by J(µ, ) = KL(µ|) + KL(|µ).

C Time-reversal and existing work
Before giving the proof of Theorem 1 we start by deriving estimates on the logarithmic derivatives of the density of the Ornstein-Ulhenbeck process given growth condition on the initial density in Appendix C.1. Note that our estimates are uniform w.r.t. the time variable. We give the proof of Theorem 1 in Appendix C.2. Finally, we draw links with existing works in Appendix C.3.
C.1 Estimates for logarithmic derivatives
We start by recalling the following multivariate Faa di Bruno's formula and a useful technical lemma. Then in Appendix C.1.1 we derive bounds for the logarithmic derivatives which are non-vacuous for small times. In Appendix C.1.2 we derive bounds for the logarithmic derivatives which are non-vacuous for large times. We combine them in Appendix C.1.3.

14

For any   Nd we denote || =

d i=1

i

and !

=

d i=1

i!.

If f

:

Rd  R is m-

differentiable with m  N, then for any   Nd with ||  m we denote for any x  Rd,

f (x) = 11 . . . dd f (x). Similarly to Constantine and Savits (1996), we define  the order on Nd

such that for any 1, 2  Nd, 1  2 if |1| < |2| or |1| = |2| and there exists j  {1, . . . , d}

such that 1j < 2j and for any i  {1, . . . , j}, 1i = 2i .

Proposition 7. Let U  R open, N  N, f  CN (U, R), g  CN (Rd, U) and h = f  g. Then for any   Nd with ||  N and x  Rd we have

h(x) =

|| k,s=1

ps(,k) f (k)(g(x))!

s j=1



j g(x)mj /(mj !

j !mj )

,

(21)

with

ps(, k) = {{ }di=1  (Nd)s, {mi}di=1  Ns : 1  · · ·  s,

s i=1

mi

=

k

,

s i=1

mi

i

= } .

(22)

Proof. The proposition is a direct application of Constantine and Savits (1996).

From this multivariate Faa di Bruno formula we derive the following lemma drawing links between exponential and logarithmic derivatives.

Lemma 8. Let N  N, g1  CN (Rd, R), g2  CN (Rd, (0, +)), h1 = exp[g1] and h2 = log(g2).

Then for any   Nd with ||  N let cd, =

|| k=1

dk

and

the

following

hold:

(a) There exists P,exp a real polynomial with cd, variables such that for any x  Rd

h1(x) = P,exp(( g1(x))| |||)h1(x) .

(23)

(b) There exists P,log a real polynomial with cd, variables such that for any x  Rd

h2(x) = P,log(( g2(x)/g2(x))| |||) .

(24)

Proof. The proof of (a) is a direct application of Proposition 7 upon noting that for any k  N,

f (k) = exp if f = exp. Similarly, the proof of (b) is a direct application of Proposition 7 upon noting

that, in the case where f = log, for any k  N and x > 0, f (k)(x) = (-1)k-1(k - 1)!x-k and that

for any s  {1, . . . , ||} and ( 1, . . . , s, m1, . . . , ms)  ps(, k) we have

s i=1

mi

=

k.

We will also make use of the following technical lemma. Lemma 9. Let p  N. Then for any a  0, b > 0 and x  Rd we have

- b x 2p + a x 2p-1  -(b/2) x 2p + a(2a/b)2p-1 ,

(25)

- b x 2p + a x 2p-2  -(b/2) x 2p + a(2a/b)p-1 .

(26)

In addition for any a  0, b > 0 and x  Rd we have

- b x 2p + a x 2p-1  (2p - 1)2p-1(2p)-2pa2pb1-2p .

(27)

Proof. For the first part of the proof, we only prove (25). The proof of (26) is similar. Let a  0, b > 0. For any x  Rd with x  (b/2a)-1 we have a x 2p-1  a(b/2a)-2p+1. For any x  Rd
with x  (b/2a)-1 we have a x 2p-1  (b/2) x 2p. Hence, we get that for any x  Rd we have

a x 2p-1 - b x 2p  a(b/2a)-2p+1 - (b/2) x 2p ,

(28)

which concludes the first part of the proof. For the second part of the proof, remark that the maximum of h : t  -bt2p + at2p-1 is attained for t = (2p - 1)/(2p)(a/b). We conclude upon noting that h(t ) = (2p - 1)2p-1(2p)-2pa2pb1-2p.

15

C.1.1 Small times estimates

Lemma 8 is key in the following proposition which establishes upper bounds on the logarithmic

derivatives of the density of the Ornstein-Ulhenbeck process. In what follows, we define (pt)t[0,T ]

the density w.r.t. the Lebesgue measure of Xt satisfying



dXt = -Xtdt + 2dBt , X0  pprior ,

(29)

with   0. In the rest of this section,  is fixed.
Proposition 10. Let N  N. Assume that pprior  CN (Rd, (0, +)) is bounded and that for any  {1, . . . , N } there exist A  0 and   N such that for any x  Rd

 log(pprior)(x)  A (1 + x  ) .

(30)

Then for any t  0, pt  CN (Rd, (0, +)) and for any  {1, . . . , N }, there exist B  0 and   N such that for any t  0

 log(pt)(x)  c-t 2 B (1 + Rd x0  p0|t(x0|xt)dx0)

(31)

with c2t = exp[-2t].

Proof. First note that for any t  0 and xt  Rd we have

pt(xt) = Rd pprior(x0)g(xt - ctx0)dx0 ,

(32)

with for any x~  Rd

ct = exp[-t] , g(x~) = (2t2)-d/2 exp[- x~ 2 /(2t2)] , t2 = (1 - exp[-2t])/ . (33)
Let t  0. We have that pt  CN (Rd, (0, +)) upon combining the fact that pprior is bounded, (32)
and the dominated convergence theorem. Let  {1, . . . , N } and   Nd such that ||  . Using
Lemma 8-(b) we have for any xt  Rd

 log(pt)(xt) = P,log((mpt(xt)/pt(xt))|m|||) .

(34)

Using (32) and the change of variable z = xt - ctx0, we have for any xt  Rd

pt(xt) = c-t 1 Rd pprior((xt - z)/ct)g(z)dz .

(35)

Hence, combining this result, the dominated convergence theorem and Lemma 8-(a) we get that for any xt  Rd and m  Nd with |m| 

mpt(xt) = c-t |m| Rd mpprior(x0)g(xt - ctx0)dx0

(36)

= c-t |m| Rd Pm,exp((j log(pprior)(x0))|j||m|)pprior(x0)g(xt - ctx0)dx0 .

(37)

We conclude the proof upon combining this result, (30), (34) and the fact that ct  1.

For any t  0 and xt  Rd we introduce the infinitesimal generator At,xt : C2(Rd, R)  C2(Rd, R) given for any   C2(Rd, R) and x0  Rd by

At,xt ()(x0) =  log(p0|t(x0|xt)), (x0) + (x0)

(38)

=  log(pprior)(x0), (x0) + (ct/t2) xt - ctx0, (x0) + (x0) . (39)

Establishing Foster-Lyapunov drift condition for this infinitesimal generator will allow us to derive
moment bounds for x0  p0|t(x0|xt). We now introduce the Lyapunov functional which will allow us to control these moments. For any p  N, t > 0 and xt  Rd, let Vp,t,xt : Rd  [1, +) given for any x0  Rd by

Vp,t,xt (x0) = 1 + x0 - xt/ct 2p ,

ct = exp[-t] .

(40)

16

Proposition 11. Assume pprior  C1(Rd, R) and that there exist m0 > 0, d0, C0  0 such that for any x0  Rd we have

x0,  log(pprior)(x0)  -m0 x0 2 + d0 x0 ,  log(pprior)(x0)  C0(1 + x0 ) . (41)

Then for any t > 0, xt  Rd and p  N there exist p  N, ap > 0 and bp  0 (independent of t and xt) such that for any x0  Rd we have

At,xt (Vp,t,xt )(x0)  -apVp,t,xt (x0) + bp(1 + xt/ct p ) ,

(42)

with p = 2p.

Proof. Let t  0, x0, xt  Rd and p  N. First, we have for any x0  Rd

Vp,t,xt (x0) = x0 - xt/ct 2p , Vp,t,xt (x0) = 2p(x0 - xt/ct) x0 - xt/ct 2(p-1) , (43)

Vp,t,xt (x0) = 2p(2p - 1) x0 - xt/ct 2(p-1) .

(44)

Second, using Lemma 9, the Cauchy-Schwarz inequality and (41), we have for any x0  Rd

 log(pprior)(x0), x0 - xt/ct  -m0 x0 2 + d0 x0 +  log(pprior)(x0) xt/ct

(45)

 -m0 x0 - xt/ct 2 + 2m0 x0 xt /ct + C0(1 + x0 ) xt /ct

(46)

+ d0 x0 - xt/ct + d0 xt /ct + m0 xt 2/c2t

(47)

 -m0 x0 - xt/ct 2 + {(2m0 + C0) xt /ct + d0} x0 - xt/ct

(48)

+ (3m0 + C0) xt 2/c2t + (C0 + d0) xt /ct .

(49)

Combining this result and (43), we have for any x0  Rd

 log(pprior)(x0), Vp,t,xt (x0)

(50)

 -2pm0 x0 - xt/ct 2p + 2p{(2m0 + C0) xt /ct + d0} x0 - xt/ct 2p-1

(51)

+ 2p((3m0 + C0) xt 2/c2t + (C0 + d0) xt /ct) x0 - xt/ct 2p-2 .

(52)

Combining this result with (38) and the fact that for any x0  Rd, (ct/t2) xt-ctx0, Vp,t,xt (x0)  0, we get that for any x0  Rd
At,xt (Vp,t,xt )(x0)  -2pm0 x0 - xt/ct 2p + 2p{(2m0 + C0) xt /ct + d0} x0 - xt/ct 2p-1 (53)
+ 2p((3m0 + C0) xt 2/c2t + (C0 + d0) xt /ct) x0 - xt/ct 2p-2 . (54)

Using Lemma 9 there exist p  N, ap > 0 and bp  0 (independent of xt and t) such that for any x0  Rd we have

At,xt (Vp,t,xt )(x0)  -apVp,t,xt (x0) + bp(1 + ( xt /ct)p ) ,

(55)

which concludes the proof.

Using this Foster-Lyapunov drift we are now ready to bound the moments of x0  p0|t(x0|xt).
Proposition 12. Assume that pprior  C2(Rd, R) and that there exist m0 > 0, d0, C0  0 such that for any x0  Rd we have
x0,  log(pprior)(x0)  -m0 x0 2 + d0 x0 ,  log(pprior)(x0)  C0(1 + x0 ) . (56)

Then, for any p  N there exist Cp  0 and p  N such that for any t  0 and xt  Rd

Rd x0 p p(x0|xt)dx0  Cpc-t 2p (1 + xt p ) ,

(57)

with c2t = exp[-2t] and p = p.

17

Proof. Let t  0 and xt  Rd. Using (Ikeda and Watanabe, 1989, Theorem 2.3, Theorem 3.1), Proposition 11 and (Meyn and Tweedie, 1993, Theorem 2.1) for any x  Rd, there exists a unique

strong solution (Xxu)u0 such that Xx0  x and



dXxu =  log(p0|t(Xxu|xt))du + 2dBu .

(58)

Using (Leha and Ritter, 1984, Theorem 5.19) we get that {(Xxu)u0 : x  Rd} is associated with a Feller semi-group. In addition, we have that for any f  C2c(Rd), Rd At,xt (f )(x0)p(x0|xt)dx0 = 0. Therefore, using (Revuz and Yor, 1999, Proposition 1.5) and (Ethier and Kurtz, 1986, Theorem 9.17)
we get that the probability distribution with density x0  p(x0|xt) is an invariant distribution for the semi-group associated with {(Xxu)u0 : x  Rd}. Therefore, using Proposition 11 and (Meyn and Tweedie, 1993, Theorem 4.6) we get that for any p  N

Rd (1 + x0 - c-t 1xt 2p)p(x0|xt)dx0  bp(1 + xt/ct p )/ap

(59)

which concludes the proof upon using that ct  1 and Jensen's inequality.

C.1.2 Large times estimates

In Proposition 12, the bound in (57) goes to + as t  + since limt+ c-t 1 = + (if  > 0). This does not yield any degeneracy in our setting since we consider a fixed time horizon T > 0.
However, we can improve the result by deriving another bound which is bounded at t  + but
explodes as t  0. In this section we assume that h : u  (exp[u] - 1)/u is extended to 0 by
continuity with h(0) = 1.

The following proposition is the equivalent of Proposition 10 with a bound which explodes for t  0 instead of t  +. Note that contrary to Proposition 10 we do not require any differentiability assumption for the initial condition.
Proposition 13. Let N  N. Assume that pprior  CN (Rd, (0, +)) is bounded. Then for any t  0, pt  CN (Rd, (0, +)) and for any  {1, . . . , N }, there exist B  0 and   N such that for any t  0

 log(pt)(x)  t- B (1 + Rd xt - ctx0  p0|t(x0|xt)dx0)

(60)

 t- B (1 + Rd xt - x0  q0|t(x0|xt)dx0) .

(61)

with t2 = (1 - exp[-2t])/ and for any x~  Rd

q(x0|xt) = p0(x0/ct)g(xt - x0)/ Rd p0(x0/ct)g(xt - x0)dx0 ,

(62)

g(x~) = (2t2) exp[- x~ 2 /(2t2)] .

(63)

Proof. First note that for any t  0 and xt  Rd we have

pt(xt) = Rd p0(x0)g(xt - ctx0)dx0 ,

(64)

with

ct = exp[-t] , g(x~) = (2t2)-d/2 exp[- x~ 2 /(2t2)] , t2 = (1 - exp[-2t])/ . (65)
Let t  0. We have pt  CN (Rd, (0, +) upon combining the fact that p0 is bounded, (64) and the dominated convergence theorem. Let  {0, . . . , N } and   Nd such that ||  . Using
Lemma 8-(b) we have for any xt  Rd

 log(pt)(xt) = P,log((mpt(xt)/pt(xt))|m|||) .

(66)

For any m  Nd with |m|  ||, using the dominated convergence theorem, there exist Cm  0 and m  N such that for any xt  Rd we have

|mpt(xt)|  Cmt-2m Rd (1 + xt - ctx0 m )p0(x0)g(xt - ctx0)dx0 ,

(67)

which concludes the proof.

18

For any t  0 and xt  Rd we introduce the infinitesimal generator A~t,xt : C2(Rd, R)  C2(Rd, R) given for any   C2(Rd, R) and x0  Rd by

A~t,xt (f )(x0) =  log(q0|t(x0|xt)), (x0) + (x0)

(68)

= c-t 1  log(pprior)(x0/ct), (x0) + t-2 xt - x0, (x0) + (x0) . (69)

For any p  N, let Vp : Rd  [1, +) given for any x0  Rd by

Vp(x0) = 1 + x0 2p .

(70)

The following proposition is the counterpart to Proposition 11.

Proposition 14. Assume that pprior  C1(Rd, R) and that there exist m0 > 0, d0  0 such that for any x0  Rd we have

x0,  log(pprior)(x0)  -m0 x0 2 + d0 x0 .

(71)

Then for any t > 0, xt  Rd and p  N there exist p  N, ap > 0 and bp  0 (independent of t and xt) such that for any x0  Rd we have

A~t,xt (Vp)(x0)  -apt-2Vp(x0) + bp(1 + xt/t2 p ) ,

(72)

with p = 2p.

Proof. Let t  0, x0, xt  Rd and p  N. First, we have for any x0  Rd

Vp(x0) = 1+ x0 2p ,

Vp(x0) = 2p x0 2(p-1) x0 ,

Vp(x0) = 2p(2p-1) x0 2(p-1) .

(73)

Using this result, (71) and Lemma 9, we get that for any x0  Rd

2p  log(p0)(x0/ct), x0/ct x0 2(p-1)  2pc-t 1(-m0 x0 2p /ct + d0 x0 2p-1)

(74)

 c-t 1(2p - 1)2p-1(2p)1-2p(m0/ct)1-2pd20p . (75)

Combining this result and the fact that ct  1, there exists dp  0 (independent from t and xt) such that for any x0  Rd

2p  log(p0)(x0/ct), x0/ct x0 2(p-1)  dp .

(76)

In addition, we have for any x0  Rd

(2p/t2) x0, xt - x0 x0 2(p-1) + 2p(2p - 1) x0 2(p-1)

(77)

 -(2p/t2) x0 2p + (2p/t2) x0 2p-1 xt + 2p(2p - 1) x0 2p-1 + 2p(2p - 1) . (78)

Combining this result and (76) we have for any x0  Rd

A~t,xt (Vp)(x0)

(79)

 -(2p/t2) x0 2p + (2p/t2) x0 2p-1 xt + 2p(2p - 1) x0 2p-1 + 2p(2p - 1) + dp . (80)

We conclude upon using Lemma 9.

The next proposition is the counterpart of Proposition 12.

Proposition 15. Assume that pprior  C2(Rd, R) and that there exist m0 > 0, d0  0 such that for any x0  Rd we have

x0,  log(pprior)(x0)  -m0 x0 2 + d0 x0 .

(81)

Then, for any p  N there exist Cp  0 and p  N such that for any t  0 and xt  Rd

Rd xt - x0 p q0|t(x0|xt)dx0  Cpt-2p (1 + xt p ) ,

(82)

with t2 = (1 - exp[-2t])/ and p = p.

Proof. The proof is similar to the one of Proposition 12.

19

C.1.3 Uniform in time logarithmic derivatives estimates

In this section we combine the results of Appendix C.1.2 and Appendix C.1.1 to establish uniform in time estimates for the logarithmic derivatives of the density of the Ornstein-Ulhenbeck diffusion.
Theorem 16. Let N  N with N  2. Assume that pprior  CN (Rd, R) and that there exist m0 > 0, d0, C0  0 such that for any x0  Rd we have
x0,  log(pprior)(x0)  -m0 x0 2 + d0 x0 ,  log(pprior)(x0)  C0(1 + x0 ) . (83)

In addition, assume that pprior is bounded and that for any  {1, . . . , N } there exist A  0 and   N such that for any x  Rd

 log(pprior)(x)  A (1 + x  ) .

(84)

Then for any t  0, pt  CN (Rd, (0, +)) and for any  {1, . . . , N }, there exist D  0 and   N such that for any t  0

 log(pt)(x)  D (1 + xt  ) .

(85)

In particular if 1 = 1 then 1 = 1.

Proof. Let t  0 and  {1, . . . , N }. Using Proposition 10 and Proposition 12 there exist D1  0 and 1  N such that for any xt  Rd we have

 log(pt)(x)  D1c-t 21 (1 + xt 1 ) .

(86)

Similarly, using Proposition 13 and Proposition 15 there exist D2  0 and 2  N such that for any

xt  Rd we have

 log(pt)(x)  D2(1/2t)-22 (1 + xt 2 ) .

(87)

Therefore, there exist D~  0 and   N such that for any xt  Rd we have

 log(pt)(x)  D~ min(-1t-2, c-t 2) (1 + xt  ) .

(88)

Since for any c-t 2 = exp[2t] and -1t-2 = (1 - exp[-2t])-1. Hence we have

min(-1t-2, c-t 2)  max{min(1/u, 1/(1 - u)) : u  [0, 1]}  2 ,

(89)

which concludes the first part proof. We now show that if 1 = 1 then 1 = 1. Recall that for any t  0 and xt  Rd we have

pt(xt) = Rd p0(x0)g(xt - ctx0)dx0 ,

(90)

with for any x~  Rd

ct = exp[-t] , g(x~) = (2t2)-d/2 exp[- x~ 2 /(2t2)] , t2 = (1 - exp[-2t])/ . (91)
Therefore, using the dominated convergence theorem we get that for any xt  Rd

 log(pt)(xt) = t-2 Rd (xt - ctx0)p0|t(x0|xt)dx0 = t-2 Rd (xt - ctx0)q0|t(x0|xt)dx0 . (92)

Similarly, using the dominate convergence theorem and change of variable z = xt - ctx0, we have for any xt  Rd

 log(pt)(xt) = c-t 1 Rd  log(p0)(x0)p0|t(x0|xt)dx0 .

(93)

We conclude the proof upon combining this result, (92), (84) with 1 = 1, Proposition 15 and Proposition 12.

20

C.2 Proof of Theorem 1

We start by recalling the following basic lemma.

Lemma 17. Let (E, E) and (F, F) be two measurable spaces and K : E × F  [0, 1] be a Markov kernel. Then for any µ0, µ1  P(E) we have

µ0K - µ1K TV  µ0 - µ1 TV .

(94)

In addition, for any  : E  F measurable we get that

#µ0 - #µ1 TV  µ0 - µ1 TV ,

(95)

with equality if  is injective.

Proof. We divide the proof into two parts.

(a) Note that for any f : F  R such that f   1 we have Kf   1. Using this result we get

µ0K - µ1K TV = sup{ F f (y)d(µ0K)(y) - F f (y)d(µ1K)(y) : f   1}

(96)

= sup{ E Kf (x)dµ0(x) - E Kf (x)dµ0(x) : f   1}  µ0 - µ1 TV . (97)

(b) We have

#µ0 - #µ1 TV = sup{ E f ((x))dµ0(x) - E f ((x))dµ1(x) : f   1}

(98)

 sup{ E f (x)dµ0(x) - E f (x)dµ1(x) : f   1}  µ0 - µ1 TV . (99)

If  is injective then there exists -1 : F  F (measurable) such that -1   = Id. Therefore, for any f : E  R with f   1 we have f = (f  -1)   and f  -1   1. Therefore we
have

µ0 - µ1 TV = sup{ E f (x)dµ0(x) - E f (x)dµ1(x) : f   1}

(100)

 sup{ E f ((x))dµ0(x) - E f ((x))dµ1(x) : f   1}  #µ0 - #µ1 TV , (101)

which concludes the proof.

We will also make use of the following inequality.
Lemma 18. Let  > 0, x, y  Rd, t > 2/ and  : [0, 1]  R such that for any s  [0, 1], (x) = exp[- x - sy 2 /(4t)]. Then   C1([0, 1] , R) and we have for any s  [0, 1]

| (s)|  2(1 + -1)(1 + x ) exp[- x 2 /(8t)] exp[ y 2]/t .

(102)

Proof. Let s  [0, 1], we have

 (s) = ( x, y - s y 2) exp[- x - sy 2 /(4t)]/(2t) .

(103)

Using the Cauchy-Schwarz inequality and that for any a, b  Rd, - a + b 2  - a 2 /2 + b 2

we get

| (s)|  ( x y + y 2) exp[- x 2 /(8t) + y 2 /(4t)]/(2t) .

(104)

In addition, we have

y exp[ y 2/(4t)]  y exp[ y 2/2]  (1 + y 2) exp[ y 2 /2]  2(1 + -1) exp[ y 2] . (105)

Finally we also have y 2 exp[ y 2/(4t)]  (1 + -1) exp[ y 2]. Combining this result, (104) and (105) concludes the proof.

21

Finally we show the following lemma which is a straightforward consequence of Girsanov's theorem
(Liptser and Shiryaev, 2001, Theorem 7.7). A similar version of this lemma can be found in the proof
of (Durmus and Moulines, 2017, Proposition 2) and in (Laumont et al., 2021, Lemma 26) (version where the dependence of the drift in w  C([0, T ] , Rd) is replaced by a (simpler) dependence in x  Rd). We refer to (Liptser and Shiryaev, 2001, Section 4) for the definitions of semi-group, non-anticipative processes and diffusion type processes.

Lemma any i 

19. {1,

Let T 2} and

> x

0, b1,  Rd,

b2 : [0, +) dX(ti) = bi(t,

× C([0, T ] , Rd) (X(si))s[0,T ])dt

Rd measurable + 2dBt admits a

such that for unique strong

solution with X(0i) = x and (bi(t, (X(si)))t[0,T ] is non-anticipative, with Markov semi-group

(P(ti))t0.

In addition, assume that for any x  Rd

and i  {1, 2}, P(

T 0

{

bi(t, (X(si))s[0,T ])

2+

bi(t, (Bs)s[0,T ]) 2}dt < +) = 1. Then for any x  Rd we have

xP(T1) - xP(T2)

2 TV



(1/2)

T 0

E[

b1(t, (X(s1))s[0,T ]) - b2(t, (X(s1))s[0,T ])

2]dt .

(106)

Proof. Let T > 0 and x  Rd. For any i  {1, 2}, denote µx(i) the distribution of (X(ti))t[0,T ] on
the Wiener space (C, B(C)) with X(0i) = x. Similarly denote µxB the distribution of (Bt)t[0,T ] with B0 = x, where we recall that (Bt)t[0,T ] is a Brownian motion. Using Pinsker's inequality (Bakry et al., 2014, Equation 5.2.2) and the transfer theorem (Kullback, 1997, Theorem 4.1) we get that

xP(T1) - xP(T2)

2 TV



2 KL(µ(1)|µ(2))

.

(107)

Since for any i  {1, 2}, P(

T 0

{

bi(t, (X(si))s[0,T ])

2+

bi(t, (Bs)s[0,T ]) 2}dt < +) = 1 and

the processes (X(ti))t[0,T ] are of diffusion type for i  {1, 2} we can apply Girsanov's theorem

(Liptser and Shiryaev, 2001, Theorem 7.7) and µB-almost surely for any w  C([0, T ] , R) we get

(dµx(1)/dµxB )((wt)t[0,T ])

= exp[(1/2)

T 0

b1(t, (ws)s[0,T ]), dwt

- (1/4)

T 0

b1(t, (ws)s[0,T ]) 2dt]

(dµxB /dµx(2))((wt)t[0,T ])

= exp[-(1/2)

T 0

b2(t, (ws)s[0,T ])), dwt

+ (1/4)

T 0

b2(t, (ws)s[0,T ])) 2dt] .

(108) (109) (110) (111)

Hence, we obtain that

KL(µx(1)|µx(2)) = E[log((dµx(1)/dµx(2))((X(t1))t[0,T ]))]

= (1/4)

T 0

E[

b1(t, (X(s1))s[0,T ]) - b2(t, (X(s1))s[0,T ])

2]dt

which concludes the proof.

(112) (113)

We study distributions satisfying some curvature assumption and show that they are sub-Gaussian. More precisely, we show the following proposition.
Lemma 20. Let q  C1(Rd, (0, +)) and m > 0 and c  0 such that for any x  Rd we have  log(q)(x), x  -m x 2 + c x . Then for any   [0, m/2) we have

Rd exp[ x 2]q(x)dx < + .

(114)

Proof. For any x  Rd we have

log(q)(x) = log(q)(0) +

1 0

 log(q)(tx), x

dt

 log(q)(0) - m

1 0

t

x

2 dt + c  log(q)(0) + c - (m/2)

x

2,

which concludes the proof.

(115) (116)

Finally, we will use the following basic lemma.

22

Lemma 21. Let µ  P(Rd), 1  R, 1 > 0 and (Xt)t0 such that X0 has distribution µ and

dXt = 1Xtdt + 11/2dBt ,

(117)

where (Bt)t0 is a Brownian motion. Then for any 2  R and 2 > 0 we have that (Yt)t0 given for any t  0 by Yt = 2X2t satisfies

dYt = 21Ytdt + 2(21)1/2dB~ t ,

(118)

where (B~ t)t0 is a Brownian motion, and Y0 has distribution (2 )#µ, where for any x  Rd, 2 (x) = 2x.

Proof. Let t  0. Using the change of variable u  2u we have

Yt = 21

2 t 0

Xsds

+

211/2B2t

= 221

t 0

X2sds

+

2(12)1/2Bt

=

21

t 0

Ysds

+

2(12)1/2Bt

,

which concludes the proof.

(119) (120)

We now turn to the proof of Theorem 1

Proof. Let   0. For any k  {1, . . . , N }, denote Rk the Markov kernel such that for any x  Rd, A  B(Rd) and k  {0, . . . , N - 1} we have

Rk+1(x, A) = (4k+1)-1/2 A exp[- x~ - Tk+1(x) 2 /(4k+1)]dx~ ,

(121)

where for any x  Rd, Tk+1(x) = x + k+1 {x + 2s(tk, x)}, where tk =

k-1 =0



.

Define

for

any k0, k1  {1, . . . , N } with k1  k0 Qk0,k1 =

k1 =k0

R

.

Finally,

for

ease

of

notation,

we

also

define for any k  {1, . . . , N }, Qk = Q1,k. Note that for any k  {1, . . . , N }, Yk has distribution

Qk, where   P(Rd) with density w.r.t. the Lebesgue measure pprior. Let P  P(C) be the

probability measure associated with the diffusion



dXt = -Xtdt + 2dBt , X0  0 ,

(122)

where 0  P(Rd) admits a density w.r.t. the Lebesgue measure given by pdata. First note that using that P0 = 0 we have for any A  B(Rd)

0PT |0(PR)T |0(A) = PT (PR)T |0(A) = (PR)0(PR)T |0(A) = (PR)T (A) = 0(A) . (123)

Hence 0 = 0PT |0(PR)T |0. Using this result and Lemma 17, we have

0 - QN TV =  

0PT |0(PR)T |0 - QN TV 0PT |0(PR)T |0 - (PR)T |0 TV + (PR)T |0 - QN TV 0PT |0 -  TV + (PR)T |0 - QN TV .

(124) (125) (126)

Note that L(X0) = L(YN ) = QN and therefore

L(X0) - 0 TV  0PT |0 -  TV + (PR)T |0 - QN TV .

(127)

We now bound each one of these terms.

(a) First, assume that  > 0. Let T = T and P~  P(C([0, T] , Rd)) be associated with (Zt)t[0,T] the classical Ornstein-Ulhenbeck process with Z0  ()#0, where for any x  Rd we have (x) = 1/2x, satisfying the following SDE: dZt = -Ztdt + 2dBt. We denote 0 = ()#0, µ = ()#. Note that since pprior is the Gaussian density with zero mean and covariance matrix (1/) Id, µ is the Gaussian distribution with zero mean and identity covariance matrix.
First, using (Bakry et al., 2014, Proposition 4.1.1, Proposition 4.3.1, Theorem 4.2.5), we get that for any t  [0, T], f  L1(µ) and x  Rd
Rd (P~t|0g(x))2dµ(x)  exp[-2t] Rd g2(x)dµ(x) , with g(x) = f (x) - Rd f (x~)dµ(x~) . (128)

23

Recall that (Xt)t0 satisfies dXt = -Xt + dBt. Using Lemma 21 we have that for any t  [0, T ], Zt and 1/2X-1t have the same distribution. Hence for any t  [0, T ] we have Pt = (-1)#P~t. Therefore, using that ()# = µ, that P~ is Markov and Lemma 17, we get that

0Pt|0 -  TV = Pt -  TV = (-1)#Pt - (-1)# TV = P~t - µ TV = P~t0 P~(t-t0)|0 - µ TV .

(129) (130)

Finally, note that we have for any t  t0  [0, T ] and x  Rd

(d(P~t0 P~(t-t0)|0)/dµ)(x) = P~(t-t0)|0f (x) , with f (x) = (dP~t0 /dµ)(x) .

(131)

Let g = f - 1. Using (131), (128) and that ()# = µ, we get that for any t  t0 with t  [0, T ]

0Pt|0 -  TV  P~t0 P~(t-t0)|0 - µ TV
 Rd |P~(t-t0)|0f (x) - 1|dµ(x)  ( Rd (P~(t-t0)|0g(x))2dµ(x))1/2  exp[-(t - t0)]( Rd g2(x)dµ(x))1/2  exp[-(t - t0)]( Rd g2(1/2x)d(x))1/2 .

(132) (133) (134) (135) (136)

In addition, we have for any   Cc(Rd, R)

Rd (x)f (1/2x)d(x) = Rd (-1/2x)f (x)dµ(x) = Rd (-1/2x)dP~t0 (x) = Rd (x)dPt0 (x) .

(137) (138)

Hence, for any x  Rd, g(1/2x) = (dPt0 /d)(x) - 1. Combining this result and (132) we get that for any t  t0 with t  [0, T ]

0Pt|0 - 

 TV  2 exp[-(t - t0)]

1+

Rd (dPt0 /d)(x)2d(x) 1/2 .

(139)

Let t0  [0, T ]. We now derive an upper bound for Rd (dPt0 /d)(x)2d(x). We recall that Pt0 and  admit density w.r.t. the Lebesgue measure denoted pt0 and p such that for any x  Rd

pt0 (x) = Rd Gt0 (x, x~)d0(x~) , p(x) = (2/)-d/2 exp[- x 2 /2] ,

(140)

where for any x, x~  Rd

Gt0 (x, x~) = (2t20 )-d/2 exp[- x - mt0 (x~) 2 /(2t20 )] , t20 = (1 - exp[-2t0])/ , mt0 (x~) = exp[-t0]x~ .

(141) (142)

Combining this result and Jensen's inequality we get

Rd p2t0 (x)p-1(x)dx  -d/2(2)-d/2t-02d Rd exp[- x - mt0 (x~) 2 /t20 +  x 2 /2]dxd0(x~) . (143)
For any x, x~  Rd we have
x - mt0 (x~) 2 /t20 -  x 2 /2 = x - mt0 (x~)(2~t20 /t20 ) 2 /(2~t20 ) - mt0 (x~) 2 (, t0)/t20 , (144)

with ~t20 = (t20 /2)(1 - t20 /2)-1 and (, t0) = (1 - t20 )/(2 - t20 ). Using this result, we get that

Rd exp[- x - mt0 (x~) 2/t20 +  x 2/2]dxd0(x~)  (2~t20 )d/2 Rd exp[(, t0) x~ 2]d0(x~) , (145)
Let  = m/4 and t0  0 such that (, t0)  . Using Lemma 20, we get that

Rd exp[- x - mt0 (x~) 2 /t20 +  x 2 /2]dxd0(x~)  (2~t20 )d/2 Rd exp[ x~ 2]d0(x~) . (146)
Combining this result, the fact that t20  -1, (143) and that for any t  0, (1 - e-t)-1  1 + 1/t, we obtain

Rd p2t0 (x)p-1(x)dx (-1~t20 t-04)d/2 Rd exp[ x~ 2]d0(x~)

(147)

24

 (1 - exp[-2t0])-d/2 Rd exp[ x~ 2]d0(x~)  (1 + 1/(2t0))d/2 Rd exp[ x~ 2]d0(x~) .
Combining this result and (139), we get that for any t > t0
0Pt|0 -  TV  C1a exp[-t] ,

with



C1a = 2(1 + 1/(2t0))d/2(1 + ( Rd exp[ x~ 2]d0(x~))1/2) exp[t0] .

For t  t0, using that 0Pt|0 -  TV  1 we have

0Pt|0 -  TV  C1b exp[-t] ,

with C1b = exp[t0] .

Let C1 = C1a + C1b and we have that for any t  [0, T ]

0Pt|0 -  TV  C1 exp[-t] .

(148) (149) (150) (151) (152)
(153)

(b) Second assume that  = 0.

0PT |0 -  TV  Rd Rd (4T )-d/2| exp[- x - x~ 2 /(4T )] - exp[- x 2 /(4T )]|dxd0(x~) . (154)
For any x, x~  Rd, let   C1([0, 1], R) with for any s  [0, 1], (s) = exp[- x - sx~ 2 /(4T )].
First, assume that T  2/. Using Lemma 18, we get that for any s  [0, 1]

| (s)|  (1 + -1)(1 + x ) exp[- x 2 /(8T )] exp[ y 2]/T .

(155)

Using this result we get that

0PT |0 -  TV  Rd Rd (4T )-d/2| exp[- x - x~ 2 /(4T )] - exp[- x 2 /(4T )]|dxd0(x~) (156)

 Rd Rd (4T )-d/2(1 + -1)(1 + x ) exp[- x 2 /(8T )] exp[ x~ 2]/T dxd0(x~) (157)

 2d/2(1 + -1) Rd (8T )-d/2(1 + x ) exp[- x 2 /(8T )]dx Rd exp[ x~ 2]/T d0(x~)

(158)

  2d/2(1 + -1)(1 + 2 2d1/2T 1/2) Rd exp[

x~

2]/T d0(x~) .

(159)

If T  2/ then

0PT |0 -  TV  (/2 + (/2)1/2)-1(T -1 + T -1/2) . Hence, we get that there exists C2  0 such that
0PT |0 -  TV  C2(T -1 + T -1/2) ,

(160) (161)

with



C2 = (/2 + (/2)1/2)-1 + 2d/2(1 + -1)(1 + 2 2d1/2) .

(162)

(c) Recall that PR is associated with the diffusion (Yt)t0 such that for any t  [0, T ] and x  Rd



dYt = b1(t, Yt)dt + 2Bt , b1(t, x) = x + 2 log(pT -t(x)) .

(163)

Similarly, for any k  {1, . . . , N } we have Qk = Qtk where Q is associated with the diffusion (Y¯ t)t[0,T ] such that for any (wt)t[0,T ]  C([0, T ], Rd) we have

 dY¯ t = b2(t, (Y¯ s)s[0,T ])dt + 2Bt ,

(164)

b2(t, (wt)t[0,T ]) =

1 N -1
k=0

[tk,tk+1)(t) {2wtk + s(tk, wtk )}

(165)

where for any k  {0, . . . , N }, tk =

k-1 =0



+1.

Recall

that

for

any

i



{1,

2,

3}

there

exist

Ai



0

and i  N such that for any x  Rd

i log(p0)(x)  Ai(1 + x i ) ,

(166)

25

with 1 = 1. Using this result and Theorem 16 we get that for any i  {1, 2, 3} there exist Bi  0 and i  N with 1 = 1 such that for any x  Rd and t  [0, T ]

i log(pt)(x)  Bi(1 + x i ) .

(167)

In addition, for any t  [0, T ] and x  Rd we have

tpt(x) = -div(bpt)(x) + pt(x) ,

(168)

with b(x) = -x. Therefore, since log(p)  C((0, T ] × Rd, R) we obtain that for any t  (0, T ] and x  Rd

t log(pt)(x) = -div(b log(pt))(x) +  log(pt)(x) +  log(pt)(x) 2 .

(169)

Finally, we get that for any t  (0, T ] and x  Rd

t log(pt)(x) = -div(b log(pt))(x) +  log(pt)(x) +   log(pt) 2 (x) . (170)

Therefore combining this result and (167) there exist A~  0 and   N such that for any x  Rd and t  (0, T ], t log(pt)(x)  A~(1 + x ). Hence, for any t1, t2  [0, T ] and x  Rd

 log(pt2 )(x) -  log(pt1 )(x)  A~ |t2 - t1| (1 + x ) . In addition, using (167), we have for any t  [0, T ] and x1, x2  Rd

(171)

 log(pt)(x1) -  log(pt)(x2)



1 0

2 log(pt)((1 - s)x1 + sx2) ds x1 - x2

 B2(1 +

1 0

(1 - s)x1 + sx2

2 ds) x1 - x2

 B2(1 + x1 2 + x2 2 ) x1 - x2 .

(172) (173) (174)

Since s  C([0, T ] × Rd, Rd) and  log(p)  C([0, T ] × Rd, Rd) we have using Lemma 19, (171), (172) and the Cauchy-Schwarz inequality

(PR)T |0 - QN

2 TV



(1/2)

T 0

E[

b1(t, Yt) - b2(t, (Yt)t[0,T ])

2]dt

2

N -1 k=0

tk+1 tk

E[

 log(pT -t)(Yt) - s(Ytk )

2]dt

+

N -1 k=0

tk+1 tk

2E[

Yt - Ytk

2]dt

6

N -1 k=0

tk+1 tk

E[

 log(pT -t)(Yt) -  log(pT -t)(Ytk )

2]dt

+6

N -1 k=0

tk+1 tk

E[

 log(pT -t)(Ytk ) -  log(pT -tk )(Ytk )

2]dt

+6

N -1 k=0

tk+1 tk

E[

 log(pT -tk )(Ytk ) - s(tk, Ytk )

2]dt



+ 

N -1 k=0

tk+1 tk

2E[

Yt -

18 2B22(1 + 2NT (42))1/2

Ytk 2]dt
N -1 tk+1 k=0 tk

E[

Yt - Ytk

4]1/2dt

+ 12A~2(1 + NT (2))

N -1 k=0

tk+1 tk

(t

-

tk

)2dt

+

6T

M2



+

N -1 k=0



{18 2B22(1

tk+1 tk

2E[

Yt -

+ 2NT (42))1/2

Ytk 2]dt

+ 2}

N -1 k=0

tk+1 tk

E[

Yt - Ytk

4]1/2dt



+ 4A~2(1 + NT (2)) 

kN=-01(tk+1 -

{18 2B22(1 + 2NT (42))1/2 + 2}

tk)3 + 6T M2

N -1 k=0

tk+1 tk

E[

Yt - Ytk

4]1/2dt

+ 4A~2(1 + NT (2))T ¯2 + 6T M2 ,

(175) (176) (177) (178) (179) (180) (181) (182) (183) (184) (185) (186) (187) (188)

where for any  N, NT ( ) = supt[0,T ] E[ Yt ]. For any t  [0, T ], let At : C2(Rd)  C2(Rd, R) the generator given for any t  0,   C2(Rd, R) and x  Rd by

At()(x) = x + 2 log(pT -t)(x), (x) + (x) .

(189)

For any  N, let V (x) = x 2 . Hence, for any  N, x  Rd and t  [0, T ] we have using (172)

At(V )(x) = 2  x 2 + 2 B1 x 2 -1 + 2 B1 x 2 + 2 (2 - 1) x 2( -1) .

(190)

26

Hence, for any  N there exist B~ such that x  Rd and t  [0, T ]

|At(V )(x)|  B~ (1 + V (x)) .

(191)

For any

 N, (M ,t)t[0,T ] = (V (Yt) - V (Y0) -

t 0

At(V

)(Ys)ds)t[0,T ]

is

a

local

martingale.

For any  N, there exists ( ,k)kN a sequence of stopping times such that limk+  ,k = T and

(M ,t ,k )t[0,T ] is a martingale. Using (191), we have for any t  [0, T ],  N and k  N

E[V (Yt ,k )]  E[V (Y0)] + B~ 0t(1 + E[V (Ys ,k )])ds .

(192)

Hence, using Grönwall's lemma we get that for any



N,

supkN

E[V

(Yt

)]
,k

<

+.

Therefore

for any  N, ((M ,tk )t[0,T ])kN is uniformly integrable and we have that for any  N,

(M ,t)t[0,T ] is a martingale. Therefore we get that for any t  [0, T ],  N

E[V (Yt)]  E[V (Y0)] + B~ 0t(1 + E[V (Ys)]ds) .

(193)

Using Grönwall's lemma we get that for any  N there exist C~  0 such that

NT ( ) = sup E[ Yt 2 ]  C~ exp[B~ T ] .
t[0,T ]

(194)

We have that for any s, t  [0, T ]

Yt = Ys +

t s

{Yu

+

2

log(pT

-t)(Yu)}du

+

 2

t s

dBu

.

(195)

Using (171) and Cauchy-Schwarz inequality we have for any s, t  [0, T ]

E[

Yt - Ys

4]  64(t - s)3

t s

{4E[

Yu

4] + 16E[

 log(pT -t)(Yu)

 4]}du + 48 2(t - s)2

(196)

 64(t - s)3

t s

{4E[

Yu

4] + 128B14(1 + E[

Yu

 4])}du + 48 2(t - s)2

(197)

  64(4 + 128B14)(1 + NT (4))(t - s)4 + 48 2(t - s)2 .

(198)

Combining (194) and (198) in (175) we get that there exist C3  0 such that

(PR)T |0 - QN

2 TV



C3

exp[C3T ](¯

+

M2)

,

(199)

We conclude the proof upon combining (153) and (199) if  > 0 and (161) and (199) if  = 0.

C.3 General SGM and links with existing works
In this section we describe a general algorithm for SGM in Appendix C.3.1 and show that the formulation (7) encompasses the ones of (Song et al., 2021; Ho et al., 2020) in Appendix C.3.2.

C.3.1 General SGM algorithm

We first present a general algorithm to compute approximate reverse dynamics, i.e. to compute the

reverse-time Markov chain associated with the forward process

 dXt = f (t, Xt)dt + 2dBt ,

X0  pdata .

(200)

We use the Euler-Maruyama discretization of (200), i.e. let X0  pdata and for any k  {0, . . . , N -1}

Xk+1 = Xk + k+1fk(Xk) + 2k+1Zk+1 .

(201)

In general, we do not have that p(xk|x0) is a Gaussian density contrary to Song and Ermon (2019); Ho et al. (2020). However, in this case, we obtain that for any x  Rd,

pk+1(x) = Rd pk(x~) exp[- Tk+1(x~) - x 2 /(4k+1)]dx~ ,

(202)

with Tk+1(x) = x~ + k+1fk(x~). Therefore, we get that for any x  Rd

(2k+1pk+1(x)) log(pk+1)(x) = Rd (Tk+1(x~) - x)pk(x~) exp[- Tk+1(x~) - x 2 /(4k+1)]dx~ . (203)

27

Hence, we get that for any x  Rd
 log(pk+1)(x) = E[Tk+1(Xk) - Xk+1|Xk+1 = x]/(2k+1) = (2k+1)1/2E[Zk+1|Xk+1 = x] . (204)
From this formula we derive a regression problem similar to the one of Section 2.1. We obtain Algorithm 2. We highlight a few differences between our approach and the ones of Song and Ermon (2019); Ho et al. (2020):
(a) As emphasized in (204), the regression problem in Algorithm 2 is different from the one usually considered in SGM which restrict themselves to the setting fk(x) = x with  = 0 (Song and Ermon, 2019) or  > 0 (Ho et al., 2020).
(b) In the present algorithm we do not use any corrector step (Song et al., 2021) at sampling time. Note that the use of a corrector step is only justified in the context of classical SGM algorithms and not the SB method introduced in Section 3.3. This is because, we do not have access to the marginal of the time-reverse density during the IPF iterations contrary to classical SGMs.
(c) Finally, we do not present the Exponential Moving Average (EMA) procedure Song and Ermon (2020) which is key to prevent the network from oscillating. Contrary to the corrector step, this technique can easily be incorporated in Algorithm 2.

Algorithm 2 Generalized score-matching

1: Inputs: (bk)k{0,...,N-1} , N  N (nb. of iterations), M  N (batch size), Nepochs (nb. of

epochs), (k)k{0,...,N-1} (stepsizes), {s :   } (neural network), opt (optimizer), pprior

(prior distribution), (k) (weights)

2: for nepoch = 0, . . . , Nepoch - 1 do

3: for j  {1, . . . , M } do

4:

X0j  pdata

5: 6:

for k  {0, Xkj+1 =

... Xkj

, N - 1} do + k+1fk(Xkj

)

+

2k+1Zkj+1

7: end for

8: 9:

end for () = M -1

N -1 k=0

(k)/(2k+1)

M j=1

 2k s (k,

Xkj+1)

+

Zkj+1

2

10: nepoch+1 = opt( , nepoch ) 11: end for

12: XN  pprior

13: for k  {0, . . . , N - 1} do



14: Xk = Xk+1 + k+1{Xk+1 + 2sNepoch (k + 1, Xk+1)} + 2k+1Zk+1

15: end for

16: Output: X0

C.3.2 Links with existing work
In this section, we show that we can recover the training and sampling algorithm of Song and Ermon (2019) and Ho et al. (2020) by reversing homogeneous diffusions. Note that Song et al. (2021) identified links with non-homogeneous SDEs. We explicitly characterize the fundamental difference between the approaches of Song and Ermon (2019); Ho et al. (2020) by identifying the two corresponding forward homogeneous processes (Brownian motion or Ornstein-Ulhenbeck).

Brownian motion First, we show that we can recover the sampling procedure and the loss function

of Song and Ermon (2019) by reversing a Brownian motion. Assume that we have

 dXt = 2dBt ,

X0  pdata .

(205)

In what follows we define {Yk}Nk=-01 such that {Yk}Nk=-01 approximates {XT -tk }kN=-01 for a specific

sequence of times {tk}Nk=-01  [0, T ]N . We recall that the time-reversal of (205) is associated with

the following SDE



dYt = 2 log pT -t(Yt) + 2dBt .

(206)

28

The Euler-Maruyama discretization of (206) yields for any k  {0, . . . , N - 1}

Y~k+1 = Y~k + 2k+1 log pT -tk (Y~k)dt + 2k+1Zk+1 .
where {k}Nk=-01 is a sequence of stepsizes and for any k  {0, . . . , N - 1}, tk = close form for { log ptk }Nk=-01 is not available and in practice we consider

(207)

k-1 j=0

j

+1.

A

Yk+1 = Yk + 2k+1s (T - tk, Yk)dt + 2k+1Zk+1 ,

(208)

where for any k  {0, . . . , N - 1}, s (tk, ·) is an approximation of  log(ptk ). The sampling procedure (208) is similar to the one of Song and Ermon (2019) upon setting T  1 in (Song and
Ermon, 2019, Algorithm 1) (no corrector step), k/2  k and s (·, k+1)  2s (T - tk, ·). It remains to show that 2s is the solution to the same regression problem as s in (Song and Ermon, 2019, Equation 6). First, note that for any t > 0 and xt  Rd we have

pt(xt) = (4t)-d/2 Rd pdata(x0) exp[- xt - x0 2 /(4t)]dx0 .

(209)

Therefore, we get that for any t > 0 and xt  Rd

 log pt(xt)= Rd (x0 - xt)/(2t)p0|t(x0|xt)dx0 = E[X0 - Xt|Xt = xt]/(2t) . Hence, we have that  satisfies the following regression problem

(210)

 = arg min

N -1 k=0

(k)E[

(X0 - XT -tk )/(T

- tk) - 2s(tk, XT -tk )

].

(211)

Note that this loss function is similar to the one of (Song and Ermon, 2019, Equation 6) upon letting

k2+1  2(T - tk) and L  N . Hence, the two recursions approximately define the same scheme if

for any k  {0, . . . , N - 1}, 12 - k2+1  (1/2)

k-1 j=0

j+1

since

t0

=

0

implies

T

=

(1/2)12.

In

Song and Ermon (2019) we have for any k  {0, . . . , N - 1}, k2 = N-kN2 (recall that N = L) with  > 1. In addition, we have for any k  {0, . . . , N - 1}, k = k2/N2 for some  > 0. We

get that

(1/2)

k-1 j=0

j+1

=

(/2)N -1

k-1 j=0

-j

= (/2)(N-1 - N-k-1)/(1 - -1)

(212) (213)

= /(2(1 - -1)N2 )(12 - k2+1) .

(214)

Hence, the two schemes are identical if  = 2(1 - -1)N2 . In practice in Song and Ermon (2019) the authors choose N = 10, N = 10-2, 1 = 1 (hence  = 104/9) and  = 2 × 10-5. We have 2(1 - -1)N2  1.3 × 10-4 which has one order of difference with .

Ornstein-Ulhenbeck Second, we show that we can recover the sampling procedure and the loss function of Ho et al. (2020) by reversing an Ornstein-Ulhenbeck process. Contrary to the previous analysis we do not show a strict equivalence between the two recursions but instead that our algorithm can be seen as a first order approximation of the one of Ho et al. (2020).

In this section, we consider the following diffusion 
dXt = -Xtdt + 2dBt ,

X0  pdata .

(215)

In what follows we define {Yk}Nk=-01 such that {Yk}Nk=-01 approximates {XT -tk }kN=-01 for a specific

sequence of times {tk}Nk=-01  [0, T ]N . We recall that the time-reversal of (215) is associated with

the following SDE



dYt = {Yt + 2 log pT -t(Yt)}dt + 2dBt .

(216)

In what follows, we fix  = 1. The Euler-Maruyama discretization of (216) yields for any k  {0, . . . , N - 1}

Y~k+1 = (1 + k+1)Y~k + 2k+1 log pT -tk (Y~k) + 2k+1Zk+1 .
where {k}Nk=-01 is a sequence of stepsizes and for any k  {0, . . . , N - 1}, tk = close form for { log ptk }Nk=-01 is not available and in practice we consider

(217)

k-1 j=0

j

+1.

A

Yk+1 = (1 + k+1)Yk + 2k+1s (T - tk, Yk)dt + 2k+1Zk+1 .

(218)

29

In (Ho et al., 2020, Equation 11) and the following remark the backward recursion is given for any k  {0, . . . , N - 1}

Yk+1 = N-1-/k2(Yk - N-k/(1 - ¯N-k)1/2 (Yk, T - tk)) + N-kZk+1 .

(219)

In (219) we set k = k as suggested in Ho et al. (2020) with for any k  {0, . . . , N - 1}

k2+1 = k+1 ,

k+1 = 1 - k+1 ,

¯k+1 =

k+1 i=1

i

.

(220)

We consider a first-order expansion of (219) with respect to {k+1}Nk=-01. We obtain the following recursion for any k  {0, . . . , N - 1}

Yk+1 = (1 - N-k/2)Yk - N-k/(1 - ¯N-k)1/2 (Yk, T - tk) + N-kZk+1 .

(221)

This last recursion is equivalent to (218) upon setting N-k  2k+1 and (·, T - tk)/(1 - ¯N-k)1/2  -s (T - tk, ·). It remains to show that s is the solution to the same regression problem as /(1 - ¯N-·) in (Ho et al., 2020, Equation 12). First, note that for any t > 0 and xt  Rd we have

pt(xt) = (2t2)-d/2 Rd pdata(x0) exp[- xt - ctx0 2 /(2t2)]dx0 ,

(222)

with c2t = exp[-2t] , t2 = 1 - exp[-2t] .
Therefore we get that for any t  [0, T ] and xt  Rd

(223)

 log(pt)(xt) = Rd (ctx0 - xt)pdata(x0) exp[- xt - ctx0 2 /(2t2)]dx0 = E [ctX0 - Xt|Xt = xt] /t2 = -E [Z|Xt = xt] /t ,

(224) (225)

where we recall that Xt has the same distribution as ctX0 + tZ, with Z a d-dimensional Gaussian random variable with zero mean and identity covariance matrix. Hence, we have that  satisfies the
following regression problem

 = arg min

N -1 k=0

(k)E[

Z/T -tk + s(T

- tk, XT -tk )

].

Note that we have

N -k i=1

i

=

N -1 i=k

N -i

=

2

N -1 i=k

i+1

=

2(T

- tk)

.

Using this result we have for any k  {0, . . . , N - 1}

(226) (227)

1 - ¯N-k = 1 - exp[-

N -k i=1

log(1

-

i)]



1

-

exp[-

Let ~ the minimizer of (Ho et al., 2020, Equation 12) we have

N -k i=1

i]



T2 -tk

.

(228)

~  arg min  arg min
 arg min

Nk=-01(2N-k(1 - N-k))-1E[ Z - (XT -tk , T - tk) 2]

(229)

Nk=-01(2N-k)-1E[ Z/(1 - N-k)1/2 - (XT -tk , T - tk)/(1 - N-k)1/2 2] (230)

Nk=-01(2N-k)-1E[ Z/T -tk + s(T - tk, XT -tk ) 2] .

(231)

Hence the two regression problems are approximately the same (for small values of {k+1}Nk=-01) if we set (k) = (2N-k)-1.

D Schrödinger bridges with potentials and DSB recursion
In this section, we start by proving an additive formula for the Kullback­Leibler divergence in Appendix D.1 following Léonard (2014a). We recall the classical IPF formulation using potentials in Appendix D.2. Then, Proposition 2 is proved in Appendix D.3. Finally, we highlight a link between our formulation and autoencoders in Appendix D.4.

30

D.1 Additive formula for the Kullback­Leibler divergence

In this section, we prove an formula for the Kullback­Leibler divergence following the proof of Léonard (2014a) which extends the result to unbounded measures defined on the space of rightcontinuous left-limited functions from [0, T ]. We recall that a Polish space is a complete metric separable space.
We start with the following disintegration theorem for probability measures. Theorem 22. Let (X, X ) and (Y, Y) be two Polish spaces. Let   P(X) and  : X  Y measurable. Then there exists a Markov kernel K : Y × X  [0, 1] such that the following hold:
(a) For any y  Y, K(y, -1({y})) = 1.
(b) For any f : X  [0, +) measurable we have X f (x)d(x) = Y K(y, f )d(y),
where  = #.

Proof. See (Dellacherie and Meyer, 1988, III-70) for instance.

K is called the disintegration of  w.r.t.  and is unique, see (Dellacherie and Meyer, 1988, III-70). In particular, for any random variable X with distribution  we have E[f (X)|(X)] = K((X), f ). Next we prove the following proposition, see (Léonard, 2014a, Proposition A.13) for an extension to unbounded measures.
Proposition 23. Let (X, X ) and (Y, Y) be two Polish spaces. Let , µ  P(X) and  : X  Y measurable. Assume that  µ. Then the following holds:

(a)  µ (b) There exists A  Y with (A) = 1 such that for any y  A, K(y, ·)

Kµ(y, ·).

In addition, we have for any y  Y, y  A and x  X

(d/dµ)(y) = Kµ(y, (d/dµ)) , (dK(y , ·)/dKµ(y , ·))(x) = (d/dµ)(x)/(d/dµ)(y ) . (232)
Finally, there exists C  X with (C) = 1 such that for any x  C we have

(d/dµ)(x) = (d/dµ)((x))(dK((x), ·)/dKµ((x), ·))(x) .

(233)

Proof. Let f : X  [0, +) measurable. Using Theorem 22 we have
[f ] = X f ((x))d(x) = X f ((x))(d/dµ)(x)dµ(x) = X f (y)Kµ(y, (d/dµ))dµ(y) , (234)

which concludes the first part of the proof. For the second part of the proof, let B = {y  Y :

(d/dµ)(y) = 0}. We have

0 = Y 1B(y)(d/dµ)(y)dµ(y) = (B) .

(235)

Therefore, there exists A1  Y such that (A1) = 1 and for any y  A1, (d/dµ)(y) > 0. Let g : Y  [0, +). Using Theorem 22 we have

X g((x))f (x)d(x) = X g((x))f (x)(d/dµ)(x)dµ(x) = Y g(y)Kµ(y, f × (d/dµ))dµ(y) . (236)
Similarly, using Theorem 22 we have

X g((x))f (x)d(x) = Y g(y)K(y, f )d(y) = Y g(y)K(y, f )(d/dµ)(y)d(y) . (237)

Hence, we get that there exists A2  Y with µ(A2) = 1 (hence (A2) = 1) such that for any

y  A2 we have

K(y, f )(d/dµ)(y) = Kµ(y, f × (d/dµ)) .

(238)

We conclude upon letting A = A1  A2 and using the fact that for any y  A, (d/dµ)(y) > 0. Finally, since (A) = 1 if and only if (-1(A)) = 1, we have for any x  -1(A)

(d/dµ)(x) = (d/dµ)((x))(dK((x), ·)/dKµ((x), ·))(x) ,

(239)

which concludes the proof.

31

We are now ready to state the additive formula. Proposition 24. Let (X, X ) and (Y, Y) be two Polish spaces and , µ  P(X) with  for any  : X  Y we have
KL (|µ) = KL (|µ) + Y KL K(y, ·)|Kµ(y, ·) d(y) .

µ. Then (240)

Proof. First assume that X |log((d/dµ)(x))| d(x) = +. Then, using Proposition 23 we have X |log((d/dµ)((x)))| d(x) = + or X log((dK((x), ·)/dKµ((x), ·))(x)) d(x) = +, i.e. either KL (|µ) = + or Y KL K(y, ·)|Kµ(y, ·) = + using Theorem 22. Second,
assume that X |log((d/dµ)(x))| d(x) < +. Using Pinsker's inequality (Bakry et al., 2014,
Equation 5.2.2) we get that KL (|µ) < +, i.e. X |log((d/dµ)((x)))| d(x) < +. Hence, we get that X log((dK((x), ·)/dKµ((x), ·))(x)) d(x) < +. Therefore we have

KL (|µ) = KL (|µ) + Y KL K(y, ·)|Kµ(y, ·) d(y)

(241)

which concludes the proof

We emphasize that in the case where X = Rd × Rd,  = proj0 the projection on the first variable and , µ admit densities w.r.t. the Lebesgue measure denoted p and q such that for any x, y  Rd, p(x, y) = p0(x)p1|0(y|x) and q(x, y) = q0(x)q1|0(y|x) then one can avoid using disintegration theory and Proposition 24 can be proved directly.

D.2 Iterative Proportional Fitting via potentials

In this section, before recalling the usual definition of the IPF via potentials we provide a condition under which the IPF sequence is well-defined which is used throughout Section 3.2.
Proposition 25. Assume that there exists ~  PN+1 such that ~0 = pdata, ~N = pprior and KL(~|0) < +. Then the IPF sequence is well-defined.

Proof. We prove the existence of the IPF sequence by recursion. First, note that 1 is well-defined since ~  PN+1 with ~N = pprior and KL(~|0) < +. Second, assume that the sequence is
well-defined up to n with n  N. Using (Csiszár, 1975, Theorem 2.2) we have

KL(~|0) = KL(~|n) +

n-1 j=0

KL(j+1|j )

.

(242)

Hence KL(~|n) < +. Using that ~0 = pdata if n is odd and that ~N = pprior if n is even, we get that n+1 is well-defined, which concludes the proof.

We now introduce the IPF using potentials. This construction is not new and can be found in Bernton et al. (2019); Chen et al. (2016, 2021b); Pavon et al. (2021); Peyré and Cuturi (2019) for instance (in continuous state spaces). In discrete settings the recursion can be found in the following earlier works Kruithof (1937); Deming and Stephan (1940); Fortet (1940); Sinkhorn and Knopp (1967); Kullback (1968); Ruschendorf et al. (1995). The IPF is defined by the following recursion 0 = p given in (1) and for n  0

2n+1 = arg min KL(|2n) :   PN+1, N = pprior , 2n+2 = arg min KL(|2n+1) :   PN+1, 0 = pdata .

(243) (244)

In the classical IPF presentation we obtain under mild assumptions that 2n+1 admits a density qn w.r.t the Lebesgue measure and that 2n admits a density pn w.r.t the Lebesgue measure, given by
the following expressions

qn(x0:N ) = pndata(x0)

N -1 k=0

pnk++11|k (xk+1 |xk )

,

pn+1(x0:N ) = pdata(x0)

N -1 k=0

pnk++11|k (xk+1 |xk )

,

(245) (246)

where (pndata(x0))nN and (pnk+1|k(xk+1|xk))nN are densities which are iteratively computed.

In the context of generative modelling the derivation (245) is not useful because it does not provide a generative model, i.e. a transition from pprior to pdata but instead defines a transition from pdata to pprior.

32

Therefore, in this section only, we reverse the roles of pprior and pdata and consider a reference density p¯ such that for any x0:N  X we have

p¯(x0:N ) = pprior(x0)

N -1 k=0

p¯k+1|k (xk+1 |xk )

.

Then, we consider the following recursion 0 = p¯ given in (247) and for n  N

2n+1 = arg min KL(|2n) :   PN+1, N = pdata , 2n+2 = arg min KL(|2n+1) :   PN+1, 0 = pprior .

(247)
(248) (249)

Again, we emphasize that the roles of pprior and pdata are exchanged in this formulation. Using the classical IPF presentation we obtain the following expressions under mild assumptions

q¯n(x0:N ) = pnprior(x0)

N -1 k=0

p¯n+1 (xk+1 |xk )

,

p¯n+1(x0:N ) = pprior(x0)

N -1 k=0

p¯n+1 (xk+1 |xk )

.

(250) (251)

In this case, we get that 2n (approximately) defines a generative model for large values of n  N
since it provides a transition from to pprior to (approximately) pdata. In the following proposition we give the precise statement corresponding to (252). We assume that p¯0 = p¯.

Proposition 26. Assume that KL(pprior  pdata|q0,N ) < +. Then (n)nN given by (248) is
well-defined and for any n  N we have that 2n+1 and 2n+2 admit a density w.r.t. the Lebesgue measures denoted q¯n and p¯n+1. In addition, we have for any n  N and x0:N  X

q¯n(x0:N ) = pnprior(x0)

N -1 k=0

p¯n+1(xk+1

|xk

)

p¯n+1(x0:N ) = pprior(x0)

N -1 k=0

p¯n+1

(xk+1|xk

)

where for any n  N we have for any x0:N  X and k  {0, . . . , N - 1}

(252) (253)

pnprior(x0) = 0n(x0)pprior(x0) , p¯n+1(xk+1|xk) = p¯n(xk+1|xk)kn+1(xk+1)/kn(xk) , (254)

with

Nn (xN ) = pdata(xN )/p¯nN (xN ) , kn(xk) = Rd kn+1(xk+1)p¯n(xk+1|xk)dxk+1 .

(255)

Proof. Let ~ = (pprior  pdata)q|0,N . Using Proposition 24 we get that KL(~|q) = KL(pprior 
pdata|p0,N ) < +. Using Proposition 25 the IPF sequence is well-defined. In addition, using (Csiszár, 1975, Theorem 3.1) for any n  N there exists Nn : Rd  [0, +) such that for any x0:N  A with ~(A) = 1 we have

q¯n(x0:N ) = p¯n(x0:N )Nn (xN ) .

(256)

Since ~ is equivalent to the Lebesgue measure we get that for any x0:N  Rd

q¯n(x0:N ) = p¯n(x0:N )Nn (xN ) .

(257)

Let n  N. We have for any xN  Rd, pdata(xN ) = q¯n(xN ) = p¯nN (xN )Nn (xN ). Hence, we get that for any N  N, Nn (xN ) = pdata(xN )/p¯nN (xN ). For any x0:N  X and k  {0, . . . , N - 1} let

kn(xk) = Rd kn+1(xk+1)p¯n(xk+1|xk)dxk+1 .

(258)

We obtain that for any x0:N  X

q¯n(x0:N ) = pprior(x0)0(x0) Nk=-01(p¯n(xk+1|xk)k+1(xk+1)/k(xk)) .

(259)

Hence, we get that for any x0:N  X , q¯n(x0) = pnprior(x0)

N -1 k=0

p¯n (xk+1 |xk ).

Using

Proposition

24

we get that for any x0:N  X , p¯n+1(x0) = pprior(x0)

N -1 k=0

p¯n (xk+1 |xk ),

which

concludes

the

proof.

The previous expression is not symmetric and the IPF iterations appear as a policy refinement of the original forward dynamic q. In the next proposition we present another potential formulation of the IPF iterations which is symmetric.

33

Proposition 27. Assume that KL(pprior  pdata|q0,N ) < +. Then (n)nN given by (248) is
well-defined and for any n  N we have that 2n+1 and 2n+2 admit a density w.r.t. the Lebesgue measures denoted q¯n and p¯n+1. In addition, we have for any n  N and x0:N  X

q¯n(x0:N ) = n0 (x0)

N -1 k=0

p¯(xk+1

|xk

)Nn

(xN

)

p¯n+1(x0:N ) = n0 +1(x0)

N -1 k=0

p¯(xk+1|xk

)Nn

(xN

)

(260) (261)

where for any n  N we have for any x0:N  X and k  {0, . . . , N - 1}

Nn (xN ) = pdata(xN )/nN (xN ) , n0 +1(x0) = pprior(x0)/0n(x0) ,

kn(xk) = Rd kn+1(xk+1)p¯(xk+1|xk)dxk+1 , nk++11(xk+1) = Rd nk+1(xk)p¯(xk+1|xk)dxk ,

(262) (263)

and 00 = pprior and N-1 = 1.

Proof. Let ~ = (pprior  pdata)q|0,N . Using Proposition 24 we get that KL(~|q) = KL(pprior 
pdata|p0,N ) < +. Using Proposition 25 the IPF sequence is well-defined. In addition, using (Csiszár, 1975, Theorem 3.1) for any n  N there exists Nn : Rd  [0, +) such that for any x0:N  A with ~(A) = 1 we have

q¯n(x0:N ) = p¯n(x0:N )~Nn (xN ) ,

p¯n+1(x0:N ) = p¯n(x0:N )~n0 (x0) .

(264)

Since ~ is equivalent to the Lebesgue measure we get that for any x0:N  Rd

q¯n(x0:N ) = p¯n(x0:N )~Nn (xN ) ,

p¯n+1(x0:N ) = q¯n(x0:N )~n0 (x0) .

(265)

For any n  N, let Nn = Nn-1~Nn and n0 +1 = n0 ~n0 . By recursion, we get that for any n  N and x0:N  X

q¯n(x0:N ) = n0 (x0)

N -1 k=0

p¯(xk+1

|xk

)Nn

(xN

)

p¯n+1(x0:N ) = n0 +1(x0)

N -1 k=0

p¯(xk+1 |xk )Nn

(xN

)

.

(266) (267)

Let n  N. For any xN  Rd we have

q¯Nn (xN ) = pdata(xN ) = p¯nN (xN )~Nn (xn) .

(268)

In addition, for any k  {0, . . . , N - 1} and x0:N  X we define nk++11(xk+1) = Rd nk+1(xk)p¯(xk+1|xk)dxk. We have for any xN  Rd, p2Nn(xN ) = nN (xN )Nn-1(xn). Combining this result with (268) we get that for any xN  Rd

Nn (xN ) = pdata(xN )/nN (xN ) .

(269)

Similarly, we get that for any x0  Rd, n0 +1(x0) = pprior(x0)/0n(x0), which concludes the proof.

D.3 Proof of Proposition 2

Let ~ = (pprior  pdata)q|0,N . Using Proposition 24 we get that KL(~|q) = KL(pprior  pdata|p0,N ) <
+. Using Proposition 25 the IPF sequence is well-defined. Note that 0 admits a density w.r.t. the Lebesgue measure given by p > 0. Let n  N and assume that pn > 0 is given for any x0:N  X by

pn(x0:N ) = pdata(x0)

N -1 k=0

q n-1 (xk+1 |xk )

.

(270)

Using Proposition 24 we get that for any   PN+1 such that N = pprior we have

KL(|2n) = KL(pprior|02n) + Rd KL(|N ||2Nn)pprior(xN )dxN .

(271)

Hence, we have that 2n+1 = pprior|2Nn. Since pn > 0 we get that for any |2Nn satisfies for any A  B(X ) and xN  Rd

|2Nn(A|xN ) = A pn(x0:N )/pn(xN )dx0:N xN (AN ) .

(272)

34

Therefore, 2n+1 admits a density w.r.t. the Lebesgue measure denoted qn and given for any x0:N  X by

qn(x0:N ) = pn(x0:N )pprior(xN )/pn(xN )

(273)

= pprior(xN )

N -1 k=0

pn(xk+1|xk)pn(xK )/pn(xk+1)

=

pprior(xN

)

N -1 k=0

pn(xk

|xk+1)

(274)

where we have used (270). Note that qn > 0. Similarly, we get that for any x0:N  X

pn+1(x0:N ) = pdata(x0)

N -1 k=0

q n (xk+1 |xk )

.

Note that again that pn+1 > 0. We conclude by recursion.

(275)

D.4 Link with autoencoders

Consider the maximum likelihood problem

q = arg max{Epdata [log(q0(X0))] : q  Pd(X ), qN = pprior} ,

(276)

where Pd(X ) is the subset of the probability distribution over X which admit a density w.r.t. the Lebesgue measure. Using Jensen's inequality we have for any q  Pd(X )

Epdata [log(q0(X0))] = Rd log( (Rd)N q(x0:N )p(x1:N |x0)/p(x1:N |x0)dx1:N )p0(x0)dx0 (277)  X log(q(x0:N )/p(x1:N |x0))p(x0:N )dx0:N  - KL (p|q) - H(p0) . (278)
This Evidence Lower Bound (ELBO) is similar to the one identified in Ho et al. (2020). Maximizing this ELBO is equivalent to solving the following problem

q0 = arg min{KL (q|p) : q  Pd(X ), qN = pprior} ,

(279)

which is the first step of IPF. Hence subsequent steps can be obtained by maximizing ELBOs associated with the following maximum likelihood problems for any n  N

q = arg max{Epdata [log(q0(X0))] : q  Pd(X ), qN = pprior} , p = arg max{Epprior [log(pN (XN ))] : p  Pd(X ), p0 = pdata} .

(280) (281)

E Alternative variational formulations

In this section, we draw links between IPF and score-matching techniques. We start by proving Proposition 3 in Appendix E.1. We then present alternative variational formulations in Appendix E.2.

E.1 Proof of Proposition 3

We only prove (291) since the proof (292) is similar. Let n  N and k  {0, . . . , N - 1}. For any xk+1  Rd we have

pnk+1(xk+1) = (4k+1)-d/2 Rd pn(xk) exp[- Fkn(xk) - xk+1 2/(4k+1)]dxk ,

(282)

with Fkn(xk) = xk + k+1fkn(xk). Since pnk > 0 is bounded using the dominated convergence theorem we have for any xk+1  Rd

 log(pnk+1)(xk+1) = Rd (Fkn(xk) - xk+1)/(2k+1)p(xk|xk+1)dxk . Therefore we get that for any xk+1  Rd

(283)

bnk+1(xk+1) = Rd (Fkn(xk) - Fkn(xk+1))/k+1p(xk|xk+1)dxk . This is equivalent to

(284)

Bkn+1(xk+1) = E[Xk+1 + Fkn(Xk) - Fkn(Xk+1)|Xk+1 = xk+1] ,

(285)

with (Xk, Xk+1)  p(xk, xk+1). Hence, we get that

Bkn+1 = arg minBL2(Rd,Rd) E [ pnk,k+1 B(Xk+1) - (Xk+1 + Fkn(Xk) - Fkn(Xk+1)) 2] , (286)

which concludes the proof.

35

E.2 Variational formulas

In Proposition 3 and Section 3.3 we present a variational formula for Bkn+1 and Fkn+1 for any n  N and k  {0, . . . , N - 1}, where we recall that for any x  Rd we have

Bkn+1(x) = x + k+1bnk+1(x) , where we have

Fkn+1 = x + k+1fkn+1(x) ,

(287)

bnk+1(x) = -fkn(x) + 2 log(pnk+1)(x) ,

fkn+1(x) = -bnk+1(x) + 2 log(qkn)(x) . (288)

In the rest of this section we assume that for any n  N, k  {0, . . . , N - 1} and x  Rd we have

qkn|k+1(xk|xk+1) = (4k+1)-d/2 exp[- xk - Bkn+1(xk+1) 2/(4k+1)] , pnk++11|k(xk+1|xk) = (4k+1)-d/2 exp[- xk+1 - Fkn+1(xk) 2/(4k+1)] .

(289) (290)

We recall that in this case Proposition 3 ensures that for any n  N and k  {0, . . . , N - 1}

Bkn+1 = arg minBL2(Rd,Rd) E [ pnk,k+1 B(Xk+1) - (Xk+1 + Fkn(Xk) - Fkn(Xk+1)) 2], (291) Fkn+1 = arg minF L2(Rd,Rd) E [ qkn,k+1 F (Xk) - (Xk + Bkn+1(Xk+1) - Bkn+1(Xk)) 2]. (292)

In the rest of this section we derive other variational formulas and discuss their practical limitations/advantages.

E.2.1 Score-matching formula and sum of networks

First, using (288) we have for any n  N, k  {0, . . . , N - 1} and x  Rd

bnk+1(x) = x + 2 fkn(x) = -x + 2

n j=0



log

pjk+1

(x)

-

2

n-1 j=0



log

qkj (x)

,

n-1 j=0



log

qkj

(x)

-

2

n-1 j=0



log

pjk+1(x)

.

(293) (294)

In the following proposition we derive a variational formula for  log pnk+1 and  log qkn(x) for any n  N and k  {0, . . . , N - 1}.
Proposition 28. For any n  N and k  {0, . . . , N - 1} we have

 log pnk+1 = arg minuL2(Rd,Rd) E [ pnk,k+1 u(Xk+1) - (Fkn(Xk) - Xk+1)/(2k+1) 2], (295)

 log qkn = arg minvL2(Rd,Rd) E [ qkn,k+1 v(Xk) - (Bkn+1(Xk+1) - Xk)/(2k+1) 2].

(296)

Proof. The proof is similar to the one of Proposition 3 but is provided for completeness. We only prove (302) since the proof (303) is similar. Let n  N and k  {0, . . . , N - 1}. For any xk+1  Rd
we have

pnk+1(xk+1) = (4k+1)-d/2 Rd pn(xk) exp[- Fkn(xk) - xk+1 2/(4k+1)]dxk ,

(297)

with Fkn(xk) = xk + k+1fkn(xk). Since pnk > 0 is bounded using the dominated convergence theorem we have for any xk+1  Rd

 log(pnk+1)(xk+1) = Rd (Fkn(xk) - xk+1)/(2k+1)p(xk|xk+1)dxk . This is equivalent to

(298)

 log(pnk+1)(xk+1) = E[(Fkn(Xk) - Xk+1)/(2k+1)|Xk+1 = xk+1] ,

(299)

with (Xk, Xk+1)  p(xk, xk+1). Hence, we get that

 log(pnk+1) = arg minuL2(Rd,Rd) E [ pnk,k+1 u(Xk+1) - (Fkn(Xk) - Xk+1)/(2k+1) 2] , (300)

which concludes the proof.

36

Note that (302) and (303) can be simplified upon remarking that for any n  N and k  {0, . . . , N -

1}

Xkn+1 = Fkn(Xkn) + 2k+1Zkn+1 ,

X~kn

=

Fkn(X~kn+1)

+

 2k+1

Z~kn+1

,

(301)

with {Xkn}Nk=0  pn, {X~kn}Nk=0  qn and {(Zkn+1, Z~kn+1) : n  N, k  {0, . . . , N - 1}} a family of independent Gaussian random variables with zero mean an identity covariance matrix. Using this

result we get that for any n  N and k  {0, . . . , N - 1}  log pnk+1 = arg minuL2(Rd,Rd) E [ pnk,k+1 u(Xk+1) - Zkn+1/2k+1 2],  log qkn = arg minvL2(Rd,Rd) E [ qkn,k+1 v(Xk) - Z~kn+1/2k+1 2].

(302) (303)

In practice, neural networks un (k, x)   log pnk (x), and vn (k, x)   log qkn(x) are used. Hence, we sample approximately from qn and pn for any n  N using the following recursion:

X~kn = ~k+1X~kn+1 + 2k+1{

n j=0

uj

(k

+

1,

X~kn+1)

-

n-1 j=0

vj (k, X~kn+1)}

+

 2k+1

Z~kn+1

,

Xkn+1 = k+1Xkn + 2k+1{

n j=0

uj

(k

+

1,

Xkn)

-

(304)

n j=0

vj (k,

Xkn)}

+

 2k+1

Zkn+1

,

(305)

where ~k+1 = 1 + k+1, k+1 = 1 - k+1 and X0n  pdata, X~Nn  pprior.

E.2.2 Drift-matching formula

In Proposition 3 we have given a variational formula for Bkn+1 and Fkn+1 for any n  N and k  {0, . . . , N - 1}. In Proposition 28 we have given a variational formula for  log pnk+1 and  log qkn for any n  N and k  {0, . . . , N - 1}. In the following proposition we give a variational formula for the drifts bnk+1 and fkn+1.

Proposition 29. For any n  N and k  {0, . . . , N - 1} we have

bnk+1 = arg minbL2(Rd,Rd) E [ pnk,k+1 b(Xk+1) - (Fkn(Xk) - Fkn(Xk+1))/k+1 2] fkn+1 = arg minfL2(Rd,Rd) E [ qkn,k+1 f (Xk) - (Bkn+1(Xk+1) - Bkn+1(Xk))/k+1 2]

(306) (307)

Proof. The proof is similar to the one of Proposition 3 but is provided for completeness. We only prove (306) since the proof (307) is similar. Let n  N and k  {0, . . . , N - 1}. For any xk+1  Rd
we have

pnk+1(xk+1) = (4k+1)-d/2 Rd pn(xk) exp[- Fkn(xk) - xk+1 2/(4k+1)]dxk ,

(308)

with Fkn(xk) = xk + k+1fkn(xk). Since pnk > 0 is bounded using the dominated convergence theorem we have for any xk+1  Rd

 log(pnk+1)(xk+1) = Rd (Fkn(xk) - xk+1)/(2k+1)p(xk|xk+1)dxk . Therefore we get that for any xk+1  Rd

(309)

bnk+1(xk+1) = Rd (Fkn(xk) - Fkn(xk+1))/k+1p(xk|xk+1)dxk . This is equivalent to

(310)

bnk+1(xk+1) = E[(Fkn(Xk) - Fkn(Xk+1))/k+1|Xk+1 = xk+1] ,

(311)

with (Xk, Xk+1)  p(xk, xk+1). Hence, we get that

bnk+1 = arg minbL2(Rd,Rd) E [ pnk,k+1 b(Xk+1) - (Fkn(Xk) - Fkn(Xk+1))/k+1 2] , (312)

which concludes the proof.

In practice, neural networks bn (k, x)  bnk (x), and fn (k, x)  fkn(x) are used. Hence, we sample approximately from qn and pn for any n  N using the following recursion:
X~kn = X~kn+1 + k+1bn (k + 1, X~kn+1)2k+1Z~kn+1 , Xkn+1 = Xkn + k+1fn (k, Xkn)2k+1Zkn+1 , (313)
X0n  pdata, X~Nn  pprior.

37

E.2.3 Discussion
We identify three variational formulas associated with Proposition 3, Proposition 28 and Proposition 29. In practice we discard the approach of Appendix E.2.1 because it requires storing 2n neural networks to sample from pn, see (305). Hence the algorithm requires more memory as n increases and the sampling procedure requires O(nN ) passes through a neural network. The approaches described in Proposition 3 and Proposition 29 yield sampling procedures which only require O(N ) passes through a neural network and have fixed memory cost for any n  N. However, in practice we observed that the approach of Proposition 3 yields better results. We conjecture that this improved behavior is mainly due to the architecture of the neural networks used to approximate Bkn+1 and Fkn+1 which have residual connections and therefore are better suited at representing functions of the x  x + (x) where  is a perturbation.

F Theoretical study of Schrödinger bridges and the IPF

In this section, we explore some of the theoretical properties of Schrödinger bridges and the IPF procedure. Proposition 4 and Proposition 5 are proved in Appendix F.1 and Appendix F.2 respectively.

F.1 Proof of Proposition 4
In this section, we prove Proposition 4. First we gather novel monotonicity results for the IPF in Proposition 31, see Appendix F.1.1. Then we prove our quantitative convergence bounds in Theorem 36, see Appendix F.1.2.

F.1.1 Monotonicity results

We consider the static IPF recursion: 0 = µ  P2 and 2n+1 = arg min KL(|2n) :   P2, 1 = 1 , 2n+2 = arg min KL(|2n+1) :   P2, 0 = 0 ,

(314) (315)

where 0, 1  P(Rd). We also consider the following assumption.
B1. µ is absolutely continuous w.r.t. µ0  µ1 and KL(0  1|µ) < +. In addition, i and µi are equivalent for i  {0, 1}.

First we draw links between A1 and B1. Proposition 30. A1 implies B1 with µ = p0,N .

Proof. Since pN > 0 we get that pN and pprior are equivalent. Hence µ1 and 1 are equivalent. µ0 = 0. Let us show that µ is absolutely continuous w.r.t. µ0  µ1, i.e. that p0,N is absolutely continuous w.r.t. pdata  pN . Since pN > 0 we get that p0,N is absolutely continuous w.r.t. pdata  pN with density pN|0/pN . Finally we have

(Rd)2 | log(pdata(x0)pprior(xN )/(pdata(x0)pN|0(xN |x0)))|pdata(x0)pprior(xN )dx0dxN = (Rd)2 | log(pprior(xN )/pN|0(xN |x0))|pdata(x0)pprior(xN )dx0dxN  H(pprior) + Rd | log(pN|0(xN |x0))|pdata(x0)pprior(xN )dx0dxN < +
which concludes the proof.

(316) (317) (318)

In this section we prove the following proposition.

Proposition 31. Assume B1. Then, the IPF sequence is well-defined and for any n  N with n  1

we have

KL(n+1|n)  KL(n-1|n) , KL(n|n+1)  KL(n|n-1) .

(319)

In addition, the following results hold:

(a) ( n+1 - n TV)nN and (J(n+1, n))nN are non-increasing. (b) (KL(2n+1|2n))nN and (KL(2n+2|2n+1))nN are non-increasing.

38

(c) (KL(12n+1|1))nN and (KL(02n|0))nN are non-increasing. (d) ( 12n+1 - 1 TV)nN and ( 02n - 0 TV)nN are non-increasing.

First, we show that under B1, the IPF sequence is well-defined and is associated with a sequence of potentials.

Proposition 32. Assume B1. Then, the IPF sequence is well-defined and there exist (an)nN and (bn)nN such that for any n  N, an, bn : Rd  (0, +) and for any x, y  Rd

(d2n+1/d(µ0  µ1))(x, y) = an(x)h(x, y)bn(y) (d2n+2/d(µ0  µ1))(x, y) = an+1(x)h(x, y)bn(y) ,

(320) (321)

and

v0(x) = an+1(x) Rd h(x, y)bn(y)dµ1(y) , v1(y) = bn(y) Rd h(x, y)an(x)dµ0(x) , (322) where vi = di/dµi for i  {0, 1}.

Proof. First, we show that the IPF sequence is well-defined. Note that 1 is well-defined since

KL(0  1|µ) < +. Assume that { }n=1 is well-defined. Using (Csiszár, 1975, Theorem 2.2)

we have

KL(0  1|µ) = KL(0  1|n) +

n-1 =0

KL(

+1|

)

.

(323)

In particular, KL(0  1|n) < + and n+1 is well-defined. We conclude by recursion.

Using (Csiszár, 1975, Theorem 3.1) and B1, there exists (~bn)nN such that for any n  N, ~bn : Rd  [0, +) and for any x, y  An, (d2n+1/d2n)(x, y) = ~bn(y) with An  B(Rd), ~(An) = 0 for any ~ such that ~1 = 1 and KL(~|2n) < +. In particular we have (01)(An) = 0. Since i is
equivalent to µi for any i  {0, 1} we have (µ0  µ1)(An) = 0. Similarly, there exists (a~n)nN such that for any n  N a~n : Rd  [0, +) and for any x, y  Bn, (d2n+2/d2n+1)(x, y) = a~n+1(x)
with Bn  B(Rd) and (µ0  µ1)(Bn) = 0. As a result, there exist (an)nN and (bn)nN with an : Rd  [0, +) and bn : Rd  [0, +) such that for any n  N and x, y  Rd

(d2n+1/d(µ0  µ1))(x, y) = an(x)h(x, y)bn(y)

(324)

(d2n+2/d(µ0  µ1))(x, y) = an+1(x)h(x, y)bn(y) ,

(325)

where h = dµ/d(µ0  µ1) and a0 = 1. In addition, setting b-1 = 1, we have for any x, y  Rd,

(d0/d(µ0  µ1))(x, y) = a0(x)h(x, y)b-1(y) .

(326)

Using that i is absolutely continuous w.r.t. µi for i  {0, 1} with density vi : Rd  (0, +) we get that for any x, y  Rd and n  N

v0(x) = an+1(x) Rd h(x, y)bn(y)dµ1(y) , v1(y) = bn(y) Rd h(x, y)an(x)dµ0(x) . (327) Since v0, v1 > 0 for any n  N, an, bn > 0.

Note that the system of equations (322) corresponds to iteratively solving the Schrödinger system, see Léonard (2014b) for a survey. In addition, (322) has connections with Fortet's mapping (Léonard, 2019; Fortet, 1940).

In the rest of the section we detail the proof of Proposition 4. We start by deriving identities between
the marginals of the IPF and its joint distribution both w.r.t. the Kullback-Leibler divergence and the total variation norm in Lemma 33. Second, we establish that ( n+1 - n TV)nN is non-increasing in Lemma 34. Then, we prove (319) in Lemma 35. We conclude with the proof of Proposition 31.

Lemma 33. Assume B1. Then, for any n  N we have

2n+1 - 2n TV = 12n - 1 TV ,

2n+2 - 2n+1 TV = 02n+1 - 0 TV . (328)

In addition, we have

KL(2n|2n+1) = KL(12n|1) ,

KL(2n+1|2n+2) = KL(02n+1|0) .

(329)

Proof. We divide the proof into two parts. First, we prove (328). Second, we show that (329) holds.

39

(a) We only show that for any n  N we have 2n+1 - 2n TV = 12n - 1 TV. The proof that for any n  N, 2n+2 - 2n+1 TV = 02n+1 - 0 TV is similar. Let n  N. Using (322) we have

2n+1 - 2n TV = (Rd)2 |bn(y) - bn-1(y)| an(x)h(x, y)dµ0(x)dµ1(y) = Rd |1 - bn-1(x)/bn(x)| d1(y) .

(330) (331)

In addition, we have that for any A  B(()Rd)

12n(A) = Rd×A an(x)bn-1(y)h(x, y)dµ0(x)dµ1(y) = A(bn-1/bn)(y)d1(y) .

(332)

We get that for any y  Rd, (d12n/d1)(y) = (bn-1/bn)(y). Hence, using (330) we get that

12n - 1 TV = Rd |1 - an(x)/an+1(x)| d0(x) = 2n+1 - 2n TV .

(333)

(b) We only show that for any n  N we have KL(2n|2n+1) = KL(12n|1). The proof that for any n  N, KL(2n+1|2n+2) = KL(02n+1|0) is similar. Let n  N. Using that for any x, y  Rd, (d12n/d1)(y) = bn-1(y)/bn(y) and that (d2n+1/d2n)(x, y) = bn(y)/bn-1(y) we have

KL(2n|2n+1) = - Rd log(bn(y)/bn-1(y))d12n(y) = KL(12n|1) .

(334)

This concludes the proof.

Lemma 34. Assume B1. Then ( n+1 - n TV)nN is non-increasing.

Proof. We only prove that for any n  N with n  1, 2n+1 - 2n TV  2n - 2n-1 TV. The proof that for any n  N, 2n+2 - 2n+1 TV  2n+1 - 2n TV is similar. Let n  N with n  1. Similarly to the proof of Lemma 33 we have that

2n+1 - 2n TV = Rd |1 - bn-1(y)/bn(y)| d1(y) = Rd b-n 1(y) - b-n-1 1(y) bn-1(y)d1(y) . (335)
In addition, we have that for any y  Rd

b-n-1 1(y) - b-n 1(y)  v1-1(y) Rd h(x, y) |an-1(x) - an(x)| dµ0(x) . Combining this result and (335) we get that

(336)

2n+1 - 2n TV   

Rd b-n-1 1(y) - b-n 1(y) bn-1(y)d1(y) (Rd)2 |an(x) - an-1(x)| h(x, y)bn-1(y)dµ0(x)dµ1(y) Rd |1 - an-1(x)/an(x)| d0(x)  2n - 2n-1 TV ,

which concludes the proof.

(337) (338) (339)

Lemma 35. Assume B1. Then for any n  N with n  1 we have KL(n+1|n)  KL(n-1|n) , KL(n|n+1)  KL(n|n-1) .

(340)

Proof. Using Lemma 33 and the data processing theorem (Ambrosio et al., 2008, Lemma 9.4.5) we get that for any n  N

KL(2n|2n+1) = KL(12n|1)  KL(2n|2n+1) .

(341)

Similarly, we get that for any n  N, KL(2n+1|2n+2)  KL(2n+1|2n). Hence, we get that for any n  N, KL(n|n+1)  KL(n|n-1).

In addition, using that for any n  N with n  1 and y  Rd, (d2n+1/d2n)(y) = bn(y)/bn-1(y) we have that for any n  N with n  1

KL(2n+1|2n) = - Rd log(bn-1(y)/bn(y))d0(x) . Using Jensen's inequality we have for any n  N

(342)

- log(bn-1(y)/bn(y))  - log Rd h(x, y)an(x)dµ0(x) Rd h(x, y)an-1(x)dµ0(x) (343)

40

 - log Rd (an(x)/an-1(x))h(x, y)an-1(x)dµ0(y) Rd h(x, y)an-1(x)dµ0(x) (344)

 - Rd log(an(x)/an-1(x))bn-1(y)h(x, y)an-1(x)/v1(y)dµ0(x) .

(345)

Combining this result, (342), Fubini's theorem and that for any n  N with n  1 and x  Rd, (d02n-1/d0)(x) = an-1(x)/an(x) we get that for any n  N with n  1

KL(2n+1|2n) (Rd)2 log(an-1(x)/an(x))an-1(x)h(x, y)bn-1(y)dµ1(y)dµ0(x)

(346)

 (Rd)2 log(an-1(x)/an(x))(an-1(x)/an(x))d0(x)  KL(02n-1|0) . (347)

Using, Lemma 33 (or the data processing theorem) we get that for any n  N with n  1, KL(2n+1|2n)  KL(2n-1|2n). Similarly, we get that for any n  N, KL(2n+2|2n+1)  KL(2n|2n+1), which concludes the proof.

We now turn to the proof of Proposition 31

Proof. First, (319) is a direct consequence of Lemma 35. Using Lemma 34 we get that ( n+1 - n TV)nN is non-increasing. Since for any 0, 1  P(Rd) we have J(0, 1) = (1/2){KL(0|1) + KL(0|1)} and using (319), we get that (J(n+1, n))nN is non-increasing which proves Proposition 31-(a). Proposition 31-(b) is a straightforward consequence of (319).
Proposition 31-(c) is a consequence of Lemma 33 and Proposition 31-(a). Finally, Proposition 31-(c)
is a consequence of Lemma 33 and (319).

Note that we also have that for any n  N, (KL(2n|2n+1))nN and (KL(2n+1|2n+2))nN are non-increasing.

F.1.2 Quantitative convergence bounds

Similarly to Appendix F.1, we consider the static IPF recursion: 0 = µ  P2 and 2n+1 = arg min KL(|2n) :   P2, 1 = 1 , 2n+2 = arg min KL(|2n+1) :   P2, 0 = 0 ,

(348) (349)

where 0, 1  P(Rd). In this section we prove the following theorem. Theorem 36. Assume B1. Then, the IPF sequence (n)nN is well-defined and there exists a probability measure  such that limn+ n -  TV = 0 and the following hold:

(a) limn+ n1/2 { 0n - 0 TV + 1n - 1 TV} = 0 . (b) limn+ n {KL(0n|0) + KL(1n|1)} = 0 .

We begin with Lemma 37 which is an adaption of (Ruschendorf et al., 1995, Proposition 2.1). Then we state and prove Lemma 38 which is a classical lemma from real analysis. Combining these two lemmas and the monotonicity results from Proposition 31 conclude the proof.
Lemma 37. Assume B1. Then, (n)nN is well-defined and we have nN KL(n+1|n) < +.

Proof. The sequence is well-defined using Proposition 32. In addition, using (Csiszár, 1975, Theorem 2.2) we have for any n  N

KL(µ |0) = KL( |n) +

n-1 k=0

KL( k+1 | k )

(350)

which concludes the proof.

Lemma 38. Let (cn)nN  [0, +)N a non-increasing sequence such that nN cn < +. Then limn+ cnn = 0.

Proof. Let  > 0 and n0  N such that for any n  n0,

+ k=n

ck



.

Let

n



N

with

n



2n0.

Note that n - n0  n/2  n0. Therefore we have   (n - n0)cn  (n/2)cn. Hence, for any

n  N with n  2n0, cnn  2, which concludes the proof.

41

We now conclude with the proof of Theorem 36.

Proof. Using Lemma 37 and Pinsker's inequality (Bakry et al., 2014, Equation 5.2.2) we have

nN n+1 - n TV < +. For any N  N, let SN =

N n=0

n+1

-

n

=

N +1

-

µ.

Since

the space of finite signed measures endowed with · TV is a Banach space (Douc et al., 2019, Theorem D.2.7) we have that (SN )NN converges. Hence there exists a finite signed measure  such that limn+ n -  TV = 0.  is a probability measure since for any n  N, n is a

probability measure.

In addition, since (KL(2n+1|2n))nN and (KL(2n+2|2n+1))nN are non-increasing by Proposition 31, using Lemma 38, we get that

lim
n+

n

{KL(0n|0)

+

KL(1n|1)}

=

0

(351)

We conclude upon using Pinsker's inequality (Bakry et al., 2014, Equation 5.2.2).

F.2 Proof of Proposition 5

Similarly to Appendix F.1, we consider the static IPF recursion: 0 = µ  P2 and 2n+1 = arg min KL(|2n) :   P2, 1 = 1 , 2n+2 = arg min KL(|2n+1) :   P2, 0 = 0 ,

(352) (353)

where 0, 1  P(Rd). We recall that in this context that if the Schrödinger bridge  exists it is given by

 = arg min{KL(|µ) :   P2, 0 = 0, 1 = 1} .

(354)

In this section, we prove the following proposition which directly implies Proposition 5.

Proposition 39. Assume B1 and denote h = dµ/(dµ0  µ1). Assume that h  C(Rd × Rd, (0, +]) and that there exist 0, 1  C(Rd, (0, +)) such that for any x, y  Rd

h(x, y)  0(x)1(y) , and

(355)

Rd×Rd (|log h(x0, x1)| + |log(0(x0))| + |log(1(x1))|)dµ0(x0)dµ1(x1) < + . (356)
Then there exists a solution  to the Schrödinger bridge and the IPF sequence satisfies limn+ n -  TV = 0 with   P2. If µ is absolutely continuous w.r.t.  then  =  .

We begin with an adaptation of (Rüschendorf and Thomsen, 1993, Proposition 2).

Proposition 40. Let µ  P((Rd)2) and assume that µ is absolutely continuous w.r.t. µ0  µ1. Let (an)nN and (bn)nN such that for any n  N, an : Rd  (0, +) and bn : Rd  (0, +). Assume that there exists  : (Rd)2  [0, +) and A  B(Rd)  B(Rd) with µ(A) = 1 such that
for any (x, y)  A

lim
n+

an(x)bn(y)

=

(x,

y)

.

(357)

Then, there exist a : Rd  [0, +), b : Rd  [0, +) and B  B(Rd)  B(Rd) with µ(B) = 1 such that for any x, y  B

(x, y) = a(x)b(x) , or (x, y) = 0 .

(358)

Proof. Let A~ = {(x, y)  (Rd)2 : (x, y) = 0} and Aa = A~  A and Ab = A~c  A. If Ab = , we conclude the proof. Otherwise, let (x0, y0)  Ab. Let C0, C1  B(Rd)  B(Rd) be given by

C00 = {x  Rd

:

lim
n+

a0n(x)

=

a0(x)

exists

and

a0(x)

>

0}

,

(359)

C01 = {y  Rd

:

lim
n+

b0n(y)

=

b0(y)

exists

and

b0(y)

>

0}

,

(360)

where for any n  N and x, y  Rd, a0n(x) = an(x)/an(x0) and b0n(y) = bn(y)an(x0), which is well-defined since for any n  N, an(x0) > 0. Note that x0  C00 and that y0  C01. If Ab  C00 × C01, we conclude the proof. Otherwise, let (x1, y1)  Ab  (C00 × C01)c and define

C10 = {x  Rd

:

lim
n+

a1n(x)

=

a1(x)

exists

and

a1(x)

>

0}

,

(361)

42

C11 = {y  Rd

:

lim
n+

b1n(y)

=

b1(y)

exists

and

b1(y)

>

0}

,

(362)

where for any n  N and x, y  Rd, a1n(x) = an(x)/an(x1) and b1n(y) = bn(y)an(x1), which is well-defined since for any n  N, an(x1) > 0. Note that C00  C10 =  and C01  C11 = . Indeed, if there exists x  C00  C10, then a0(x) = limn+ an(x)/an(x0) > 0 and a1(x) = limn+ an(x)/an(x1) > 0 exists. Therefore limn+ an(x1)/an(x0) > 0 exists and limn+ bn(y1)an(x0) > 0 exists. Hence (x1, y1)  C00 × C01 which is absurd. Similarly, if there exists y  C01  C11 then (x1, y1)  C00 × C01 which is absurd. Hence, we consider T : Ab  2(Rd)2 such that for any (x, y)  Ab, T (x, y) = C(0x,y) × C(1x,y), where C(0x,y) × C(1x,y) is constructed as in (359) replacing (x0, y0) by (x, y).

Consider a well order on (Ab, ), which is possible by the well-ordering principle (Enderton, 1977, p. 196). For any (x, y)  Rd, let A(bx,y) = {(x , y )  (Rd)2 : (x , y ) < (x, y)}. Using the transfinite recursion theorem (Enderton, 1977, p. 175) there exists f : Ab  {0, 1} such that for any (x, y)  Ab if there exists (x , y )  (Rd)2 such that (x , y ) < (x, y), f (x , y ) = 1 and (x, y)  T (x , y ) then f (x, y) = 0 and f (x, y) = 1 otherwise. Let I = f -1({1}). Let
(x, y), (x , y)  I with (x, y) = (x , y ) then for (x, y) < (x , y ) for instance. Since f (x, y) =
f (x , y ) = 1 we have that (C(0x,y) × C(1x,y))  (C(0x ,y ) × C(1x ,y )) = . Let (x, y)  Ab. If f (x, y) = 1 then (x, y)  C(0x,y) × C(1x,y). If f (x, y) = 0 then there exists (x , y ) < (x, y) such that (x, y)  C(0x ,y ) × C(1x ,y ). Therefore, we get that {C(x,y) = (C(0x,y) × C(1x,y))  Ab : (x, y)  I} is a partition of Ab.
Since µ(Ab)  1, and {C(x,y) = (C(0x,y) × C(1x,y))  Ab : (x, y)  I}, we get that J = {C(x,y) : (x, y)  I, µ0(C0(x,y))µ1(C(1x,y)) > 0} is countable. Denote Ac = (x,y)J C(x,y). Let us show that µ(AccAb) = µ((x,y)IJc C(x,y)) = 0. Let x  Rd and define Dx = {y  Rd : (x, y)  AbAcc}. If Dx is not empty, then there exists (x , y )  I such that x  C(0x ,y ). Then, for any y  Dx, y  C(1x ,y ). Hence, (x , y )  I  J c by definition of Dx and µ1(Dx) = 0. We get that

µ(Ab  Acc) = Rd Dx h(x, y)dµ1(y) dµ0(x) = 0 ,

(363)

where h is the density of µ w.r.t. µ0  µ1. Note that this is the only instance in the proof, where we use that µ is absolutely continuous w.r.t. µ0  µ1. For any (x, y)  Ac define for any n  N

a^n(x) = 1 (x ,y )J C0(x ,y ) (x)a(nx ,y )(x) , ^bn(y) = 1 (x ,y )J C(1x ,y ) (x)bn(x ,y )(y) . (364)
There exist a^, ^b : Rd  (0, +) such that for any (x, y)  Ac, limn+ a^n(x) = a^(x) and limn+ ^bn(y) = ^b(y). In addition, for any (x, y)  Ac, an(x)bn(y) = a^n(x)^bn(y). Hence, for any (x, y)  Ac, (x, y) = a^(x)^b(y). Since Aa  Ac =  and µ(Ac) = µ(Ab), we have

µ(Aa) + µ(Ac) = µ(Aa) + µ(Ab) = µ(A) = 1 .

(365)

We conclude the proof upon remarking that for any (x, y)  Aa, (x, y) = 0 and for any (x, y)  Ac, (x) = a^(x)^b(y).

In what follows we prove Proposition 39.

Proof. Since limn+ n -  TV = 0 by Theorem 36 and KL(|µ) < +, there exist A with µ(A) = 1 and  : (Rd)2  [0, +) such that, up to extraction, for any x, y  A

lim an(x)bn(y) = (x, y) ,
n+

(366)

and (d/dµ) = . Using Proposition 40, there exist a, b : Rd  [0, +) and B with (B) = 1 such that for any x, y  B, (d/dµ)(x, y) = a(x)b(y). Since µ is absolutely continuous w.r.t.
, we get that for any x, y  Rd, (d/d(µ0  µ1))(x, y) = a(x)b(y)h(x, y). In addition, the Schrödinger bridge   P((Rd)2) exists, see (Rüschendorf and Thomsen, 1993, Theorem 3), and
there exist a , b : Rd  [0, +) and B with µ(B ) = 1 such that for any x, y  B

(d /d(µ0  µ1))(x, y) = a (x)b (y)h(x, y) .

(367)

43

Let M+,× be the space of non-negative product measures over Rd × Rd. Let h¯ : M+,×  M+,× be given for any  = (0, 1)  M+,× by h¯ () = h where for any A, B  B(Rd)

h(A × B) = ( A×Rd h¯(x, y)d0(x)d1(x))( Rd×B h¯(x, y)d0(x)d1(y))

(368)

where for any x, y  Rd, h¯(x, y) = h(x, y)-0 1(x)-1 1(y). Note that h¯  C(Rd × Rd, (0, +]) and is bounded. Hence, using (Beurling, 1960, Theorem 2) we get that h¯ is a bijection. Let  = (a-0 1µ0, b-1 1µ1) and  = (a -0 1µ0, b -1 1µ1). Then, since i = i = i for i  {0, 1} we get that h() = h( ). Hence  =  and  =  which concludes the proof.

Lemma 41. Let   P((Rd)2) with i = i for i  {1, 2}. Assume that KL( |µ) < + and that L1(0)  L1(1) is closed in L1( ). In addition, assume that there exist a, b : Rd  [0, +) and A with  (A) = 1 such that for any (x, y)  A,

(d /dµ)(x, y) = a(x)b(y) .

(369)

Then  is the Schrödinger bridge.

Proof. Since KL( |µ) < + we have that

(Rd)2 |log(a(x)b(y))| d (x, y) < + .

(370)

Using (Kober, 1939, Theorem 1) and that i = i for i  {1, 2}, we get that

Rd |log(a(x))| d0(x) + Rd |log(b(y))| d1(y) < + .

(371)

Let   P((Rd)2) such that i = i for i  {1, 2} and KL(|µ) < +. Using (371), we have that (Rd)2 |log((d /dµ)(x, y))| d(x, y) < +. Hence, (d /dµ) > 0, -almost surely. Using this result we have for any A  B(Rd)

[A] = Rd 1A(x)(d /dµ)(x)(d /dµ)(x)-1d(x) = Rd 1A(x)(d /dµ)(x)(d /dµ)(x)-1(d/dµ)(x)dµ(x) = Rd 1A(x)(d /dµ)(x)-1(d/dµ)(x)d (x) .

(372) (373) (374)

Hence we get that d/d = (d/dµ)(d /dµ)-1. We get that

KL(| ) = Rd log((d/dµ)(d /dµ)-1(x, y))d(x, y) = KL(|µ) - KL( |µ) . (375)
Hence, KL(|µ)  KL( |µ) with equality if and only if  = . Therefore,  is the Schrödinger bridge.

The following proposition is an alternative to Proposition 39.
Proposition 42. Assume B1. Then there exists a solution  to the Schrödinger bridge and the IPF sequence (n)nN satisfies limn+ n -  TV = 0 with   P2. If KL(|µ) < + and L1(0)  L1(1) is closed in L1() then  =  .

Proof. Since limn+ n -  TV = 0 by Theorem 36 and KL(|µ) < +, there exist A with µ(A) = 1 and  : (Rd)2  [0, +) such that, up to extraction, for any x, y  A

lim
n+

an(x)bn(y)

=

(x,

y)

,

(376)

and (d/dµ) = . Using Proposition 40, there exist a, b : Rd  [0, +) and B with (B) = 1 such that for any x, y  B, (d/dµ)(x, y) = a(x)b(y). We conclude upon using Lemma 41.

44

G Geometric convergence rates

In the following proposition we show that we recover a geometric convergence rate in a Gaussian

setting and derive intuition from this case study. We set N = 1 and assume that for any x0, xN  Rd

we have

p(x0, xN )  exp[- x0 2 + 2 x0, xN - xN 2] ,

(377)

with   [0, 1). In this case assume that there exists  > 0 such that the target marginals are given for any x0, xN  Rd by

pdata(x0)  exp[- x0 2] ,

pprior(xN )  exp[- xN 2] .

(378)

Proposition 43. Let   (0, 1) and  > 0. Then the Schrödinger bridge  exists and there exists C  0 (explicit in the proof) such that for any n  N, KL(n| )  C2n, with  < 1 given by  = /(1 + ) and  = 2/2. In addition,  admits a density w.r.t. the Lebesgue measure denoted
p and given for any x, y  Rd by

p (x, y) = exp[- x 2 + 2 x, y -  y 2]/ Rd exp[- x 2 + 2 x, y -  y 2]dxdy , (379)
with  = (2/2)(1 + (1 + 42/2)1/2).

Remark that if 2 = 1 - 2 then  and p = p, i.e. the IPF leaves µ invariant. Note that the performance of the IPF improves if  is close to 0, i.e. if  = 2/2 is close to 0. This is the case if   0 (the marginals are almost independent) or if   + (the target distribution is close to 0). This behavior is in accordance with the limit case where the marginals are independent or one of the target distribution is a Dirac mass in which case the IPF converges in two iterations.
Also, note that the convergence rate does not depend on the dimension but only on the constants of the problem.
In what follows we first derive the IPF sequence for this Gaussian problem and establish that  controls the amount of information shared by the marginals. Then we prove Proposition 43
In this section, we let µ  P2 with density p w.r.t. the Lebesgue measure p such that for any x0, x1  Rd
p(x0, x1) = exp[- x0 2 + 2 x0, x1 - x1 2]/ Rd exp[- x0 2 + 2 x0, x1 - x1 2]dx0dx1 . (380)
We have that µ is the Gaussian distribution with zero mean and covariance matrix  such that

 = (2(1 - 2))-1

Id  Id

 Id Id

.

(381)

Hence we get that for any x0, x1  Rd

p(x0, x1) = -d(1 - 2)d/2 exp[- x0 2 + 2 x0, x1 - x1 2] .

(382)

In what follows, we denote C = d(1 - 2)-d/2. Similarly, we get that µ0 = µ1 and that they admit the density p0 w.r.t. the Lebesgue measure given for any x  Rd by

p0(x) = -d/2(1 - 2)d/2 exp[- x 2 (1 - 2)] .

(383)

In what follows, we denote C0 = d/2(1 - 2)-d/2. In this case note that µ admits a density w.r.t. µ0  µ1 given for any x0, x1  Rd by

h(x0, x1) = (dµ/d(µ0  µ1))(x0, x1) = (1 - 2)-d/2 exp[-2 x0 2 - 2 x0, x1 - 2 x1 2] .
(384) Remark that pprior = pdata = q with for any x  Rd, q(x) = -d/2d/2 exp[- x 2]. We have for any x1, x0  Rd

p1|0(x1|x0) = p(x0, x1)/p0(x0) = -d/2(1 - 2)d/2 exp[-2 x0 2 + 2 x0, x1 - x1 2] . (385)
Hence, we have that A1 holds and the IPF sequence is well-defined and converges using Proposition 5. In what follows we start to show that  controls the amount of information by the two marginals µ0 and µ1. More precisely we have the following result.

45

Proposition 44. For any   (0, 1) we have KL(µ|µ0  µ1) = -(d/2) log(1 - 2).

Proof. For any x, y  Rd we have

(dµ/(dµ0  dµ1))(x, y) = exp[-2 x 2 + 2 x, y - 2 y 2](1 - )-d/2 .

We have that

Rd×Rd (-2 x 2 - 2 y 2 + 2 x, y )dµ(x, y) = 0 .

Hence, KL(µ|µ0  µ1) = -(d/2) log(1 - 2), which concludes the proof.

(386) (387)

In what follows, we denote by (n)nN the IPFP sequence, defined for any n  N we have for any x, y  Rd

(d2n/dµ)(x, y) = an(x)bn(y)h(x, y) , where for any x, y  Rd

(d2n+1/dµ)(x, y) = an+1(x)bn(y)h(x, y) , (388)

an+1(x) = (d0/dµ0)(x) Rd h(x, y)bn(y)dµ1(y) -1 , bn+1(x) = (d1/dµ1)(y) Rd h(x, y)an+1(x)dµ0(x) -1 .
We now turn to the proof of the Proposition 43.

(389) (390)

Proof. Let   (0, 1) and  > 1. We have for any x, y  Rd

(d0/dµ0)(x) = exp[(1 - 2 - 2) x 2]/C2 , (d1/dµ1)(y) = exp[(1 - 2 - 2) y 2]/C2 ,
(391) with C2 = C1/C0 with C1 = d/2d/2. For any x  Rd and   0 we have

(d0/dµ0)(x) Rd exp[- y 2]h(x, y)dµ1(y) -1 = (C0C2)-1C exp[(1 - 2 - 2) x 2] Rd exp[- y 2 - y - x 2]dy -1 = (C0C2)-1C exp[(1 - 2 - 2) x 2] × Rd exp[-( + 1) y - /( + 1)x 2 - 2(1 - 1/( + 1)) x 2]dy -1 = (C0C2)-1C exp[(1 - 2 - 2 + 2/( + 1)) x 2] × Rd exp[-( + 1) y - /( + 1)x 2]dy -1 = (C0C2C~)-1C exp[(1 - 2 - 2/( + 1)) x 2] ,

(392) (393) (394) (395) (396) (397) (398)

with C~ = d/2(1 + )-d/2. Note that a0 = b0 = 1. Let n  N and assume that for any y  Rd bn(y) = exp[-2n y 2]/C2n with 2n  0 and C2n > 0 then we have for any x  Rd

an+1(x) = (C0C2C~2n )-1CC2n exp[-(1-2-2/(2n+1)) x 2] = exp[-2n+1 x 2]/C2n+1 , (399)

with

2n+1 = 2 - 1 + 2/(2n + 1) ,

(C0C2C~2n )/(CC2n) = C2n+1 .

(400)

Similarly, if we assume that for any x  Rd an+1(x) = exp[-2n+1 x 2]/C2n+1 with 2n+1  0 and C2n+1 > 0 then we have for any y  Rd

bn+1(y) = (C0C2C~2n+1 )-1(CC2n+1) exp[-(1 - 2 - 2/(2n+1 + 1)) y 2] = exp[-2n+2 y 2]/C2n+2 ,

(401) (402)

with

2n+2 = 2 - 1 + 2/(2n+1 + 1) ,

(C0C2C~2n+1 )/(CC2n+1) = C2n+2 .

(403)

46

Combining this result, (400) and using the recursion principle we get that for any n  N

an+1(x) = exp[-2n+1 x 2]/C2n+1 ,

bn+1(y) = exp[-2n+2 y 2]/C2n+2 . (404)

The recursion can be extended to a0 and b0 by setting -1 = 0 = 0 and C-1 = C0 = 1. Therefore,

for any n  N we have

n+1 = 2 - 1 + 2/(n + 1) .

(405)

We now study the convergence of the sequence (n)nN. By recursion, we have that for any k,  N, if k   then for any m  N with m even we have m+k  m+ and for any m  N with m odd we have m+k  m+ . We have 0 = 0 and

1 = 2 + 2 - 1 , 2 = 2 - 1 + 2/(2 + 2) .

(406)

We divide the rest of the proof into two parts.

(a) First assume that 2 > 1 - 2. Using (406) we have that 1 > 0 and 2 > 0. Therefore, we obtain that (2n)nN is non-decreasing, that (2n+1)nN is non-increasing and that for any n  N, 0  2n  2n+1  1. Therefore, (n)nN converges and we denote  its limit. We have  = 2 - 1 + 2/( + 1). Hence,  is a root of X2 + (2 - 2)X + 1 - 2 - 2. We get that
 = 0 or  = 1 with

0 = 2/2 - 1 - (1/2)(4 + 42)1/2 , 1 = 2/2 - 1 + (1/2)(4 + 42)1/2 , (407)

0 , 1 are non-decreasing function of . We get that for any   0 such that 2  1 - 2, 0  0. In addition, we have 1 = 0 for 2 = 1 - 2, hence for any   0 such that 2  1 - 2, 1  0.

Since   0 we have

 = -1 + 2/2 + (1/2)(4 + 42)1/2 .

(408)

Denote n = n -  and  =  + 1. Let  > 0. Since limn+ n = 0, there exists n0  N such that |n| /  . Using (405), we obtain that for any n  N

|n+1| = 2|1/(n + 1) -  -1| = (2/ )|1 - (n/ + 1)-1|  (/ )2|n|/(1 - ) . (409)

Hence, we get that for any   (0, 1), there exists C > 0 such that for any n  N

|n|  Cn ,  = (/( (1 - )1/2))2 .

(410)

Note that  >  using (408) and   (0, 1) if  < 1 - / .

For any n  N and x, y  Rd we have

n(x, y) = an+1(x)bn+1(y) = exp[-2n+1 x 2 - 2n+2 y 2]/(C2n+1C2n+2) = exp[-2n+1 x 2 - 2n+2 y 2]/(C~C~2n+1 ) ,

(411) (412)

with C~ = C0C2/C. Therefore we obtain that for any x, y  Rd,  (x, y) = limn+ n(x, y)

exists and we have

 (x, y) = exp[- x 2 -  y 2]/(C~C~ ) .

(413)

Using this result we get that for any x, y  Rd

(d2n/d )(x, y) = exp[-2n+1 x 2 - 2n+2 y 2]C /C2n+1 = exp[-2n+1 x 2 - 2n+2 y 2] {(1 + 2n+1)/(1 +  )}-d/2 = exp[-2n+1 x 2 - 2n+2 y 2] {1 + 2n+1/(1 +  )}-d/2 .

(414) (415) (416)

Therefore we have for any x, y  Rd

log (d2n/d )(x, y)  |2n+1| x 2 + |2n+2| y 2 + (d/2) |log (1 + 2n+1/(1 +  ))| (417)

 |2n+1| x 2 + |2n+2| y 2 + (d/2) |2n+1| .

(418)

Therefore we obtain that for any n  N

KL(2n| )  (d/2)(-2 |2n+1| + |2n+2| + |2n+1|) .

(419)

47

A similar inequality holds for KL(2n+1| ). Therefore we get that for any   (0, 1 - / ) there exists C  0 such that for any n  N we have

KL(n| )  C2n ,

(420)

with

 = /( (1 - )1/2) = (2)/((2 + (4 + 42)1/2)(1 - )1/2)  /((1 + (1 + 2)1/2)(1 - )1/2) .

(421) (422)

Let  < 1 - (1 + )/(1 + (1 + 2)1/2). Then we get that    which concludes the first part of the proof.
(b) If 2 = 1 - 2 then the IPF is stationary since the IPF leaves µ invariant.
(c) Finally we assume that 2 < 1 - 2. Using (406) we have that 1 < 0 and 2 < 0 since 2 < 1 - 2. Therefore, we obtain that (2n)nN is non-increasing, that (2n+1)nN is non-decreasing and that for any n  N, 0  2n  2n+1  1. Therefore, (n)nN converges and we denote  its limit. We have  = 2 - 1 + 2/( + 1). Hence,  is a root of X2 + (2 - 2)X + 1 - 2 - 2. We recall that the two roots of this polynomial are given by

0 = 2/2 - 1 - (1/2)(4 + 42)1/2 ,

1 = 2/2 - 1 + (1/2)(4 + 42)1/2 .

We have

1 - 0 = 2 + 2 - 1 - 2/2 + 1 - (1/2)(4 + 42)1/2 = (1/2)(2 + 22 - (4 + 42)1/2)  0 .

(423)
(424) (425)

Since 3 > 1 we get that for any n  N with n  3, n  3 > 0 . Therefore  > 0 and then  = 1 . The rest of the proof is similar to the case where 2 > 1 - 2.

H Continuous-time Schrödinger bridges

We recall the continuous Schrödinger problem is given by

 = arg min {KL(|P) :   P(C), 0 = pdata, T = pprior} ,

T=

N -1 k=0

k+1.

(426)

In this section, we prove Proposition 6. We start with the following property which can be found in (Léonard, 2014b, Proposition 2.3, Proposition 2.10) and establishes basic properties of dynamic continuous Schrödinger bridges.
Proposition 45. The solution to (426) exists if and only if the solution to the static Schrödinger bridge exists. In addition, if the solution exists and P is Markov then the Schrödinger bridge is Markov.

We now turn to the proof of Proposition 6. First we highlight that (n)nN is well-defined since
its static counterpart (n)nN is well-defined using Proposition 32. We only prove that for any n  N, (2n+1)R is the path measure associated with the process (Yt2n+1)t[0,T ] such that Y02n+1 has distribution pprior and satisfies

dYt2n+1

=

bnT -t(X2t n+1)dt

+

 2dBt

.

(427)

The proof for 2n+2 is similar. Let n  N and assume that 2n is the path measure associated with

the process (X2t n)t[0,T ] such that X20n has distribution pdata and satisfies

 dX2t n = ftn(X2t n)dt + 2dBt .

(428)

We have that

2n+1 = arg min KL(|2n) :   P(C), T = pprior .

(429)

48

Let  = projT such that for any   C, projT () = T . Using Proposition 24 we get that for any   P(C) we have

KL(|2n) = KL(T |2Tn) + Rd KL(K(x, ·)|K2n(x, ·))dT (x) ,

(430)

where K and K2n are the disintegrations of  and 2n with respect to . Therefore, we get that

2n+1 = ppriorK2n. Since KL(2n|Q) < + and 2n is Markov, Using (Cattiaux et al., 2021,

Theorem 4.9) we get that (2n)R = T K2n satisfies the martingale problem associated with the

diffusion

 dYt2n = -fTn-t(Yt2n) + 2 log pnT -t(Yt2n) dt + 2dBt .

(431)

Since 2n+1 = ppriorK2n we get that 2n+1 also satisfies the martingale problem associated with (431) and is Markov which concludes the proof by recursion.

I Training Techniques

In this section we present practical guidelines for the implementation of DSB, based on Algorithm 1.

We emphasize that, contrarily to previous approaches Song et al. (2021); Song and Ermon (2020); Ho

et al. (2020); Dhariwal and Nichol (2021), we do not weight the loss functions as we do not notice

any

improvement.

Let

I



{0, N

- 1}

×

{1, M }.

We

define

the

generalized

losses

^b
n,I

and

^f
n,I

given by

^b
n,I

()

=

M -1

(k,j)I B (k + 1, Xkj+1) - (Xkj+1 + Fkn(Xkj+1) - Fkn(Xkj )) 2 ,

^f
n+1,I

()

=

M -1

(k,j)I F(k, Xkj ) - (Xkj + Bkn+1(Xkj+1) - Bkn+1(Xkj )) 2 .

Technique 1. Simulated Trajectory

(432) (433)

The losses (432) and (433) may be computed by simulating diffusion trajectories as described in Algorithm 1.
Technique 2. Closed Form Sampling

Since f0(x) = -x with  it is not necessary to compute full trajectories for the first IPF iteration and one may simply sample from a Gaussian distribution with appropriate mean and covariance matrix. This technique also improves the computational speed of the first DSB iteration.
Technique 3. Cached Trajectory

It is not possible to used closed form sampling for later DSB iterations, and simulating the full diffusion trajectory is both expensive and wasteful as for each particle j {Xkj}k will be correlated, hence only a single time-step per particle is used per gradient step . In order to obtain a speed-up we consider a cached-version of Algorithm 1 given in Algorithm 3 whereby the diffusion trajectory is stored and then resampled. Resampled terms are then used to compute losses (432) and (433). One may tune the number of time-steps and particles to cache, and the frequency in which to refresh the cache by simulation the diffusion. This trades-off bias and iteration speed.
This modification allows for significant speed-up as the trajectories are not resampled at each training iteration. Let ncache be the number of passes for one cache. The batch size is given by |I|. As a rule of thumb we find that, in order to avoid overfitting the cache, it should be refreshed once ncache × |I|  m × N × M where m  2.
Technique 4. Tune Gaussian Prior mean/ variance

The convergence of the IPF is also affected by the mean and covariance matrix of the target Gaussian. In Appendix J.1 we investigate different possible choices for these values. In practice we recommend to choose the variance of the Gaussian prior pprior to be slightly larger than the one of the target dataset and to choose the mean of pprior to be equal to the one of the target dataset. This remark is in accordance with (Song and Ermon, 2020, Technique 1).
Technique 5. Network Refinement / Fine Tuning

49

Algorithm 3 Cached Diffusion Schrödinger Bridge

1: for n  {0, . . . , L} do

2: while not converged do

3:

Sample {Xkj Xkj+1 = Xkj

}Nk,,jM=0 where X0j  + k+1fn (k, Xkj )

pdata and + 2k+1

Zkj+1

4: while not refreshed do

5:

sample I (uniform in {0, N - 1} × {1, M })

6:

Compute

^b
n,I

(n

)

using

(432)

7:

n

=

Gradient

Step(

^b
n,I

(

n))

8: end while

9: end while

10: while not converged do

11:

Sample {Xkj Xkj = Xkj+1

}Nk,,jM=0, + kb

nw(hke,rXe Xkj )Nj+p2prkio+r,1aZnkjd

12: while not refreshed do

13:

sample I (uniform in {0, N - 1} × {1, M })

14:

Compute

^f
n+1,I

(n+1)

using

(433)

15:

n+1 = Gradient Step(^fn+1,I (n+1))

16:

end while

17: end while

18: end for

19: Output: (L+1, L)

Training large networks from scratch, per DSB iteration, is very expensive. However, from (293)(294),

bnk+1(x) = bnk+-11(x) + 2 log pnk+1(x) - 2 log qkn-1(x) , fkn(x) = fkn-1(x) + 2 log qkn-1(x) - 2 log pnk+-11(x) .

(434) (435)

One may therefore initialize networks at DBS iteration n from n - 1 in order to reduce training time. There may be more sophisticated warm-start approaches through meta-learning.
Technique 6. Exponential Moving Average

Similar to (Song and Ermon, 2020, Technique 5), we found taking the exponential moving average of network parameters across training iterations, with rate 0.999, improved performance.

J Additional Experimental Results and Details
In this section, we provide additional examples in the case of two-dimensional toy examples in Appendix J.1. We then turn to generative modeling in Appendix J.2. Finally, we detail our dataset interpolation experiments in Appendix J.3.
J.1 Two-dimensional experiments
In the simple case of two-dimensional distribution we use a simple architecture for the networks f and b, see Figure 7. We use the variational formulation Appendix E.2.2. To optimize our networks we use ADAM Kingma and Ba (2014) with momentum 0.9 and learning rate 10-4. We refresh our optimizers and reinitialize the networks f and b at each step of the IPF.
50

x

MLPBlock (1a)

Concatenate

MLPBlock (2)

Output

k

PositionalEncoding MLPBlock (1b)

Figure 7: Architecture of the networks used in the two-dimensional setting. Each MLP Block is a Multilayer perceptron network. The "PositionalEncoding" block applies the sine transform described in Vaswani et al. (2017). MLPBlock (1a) has shape (2, 16, 32), MLPBlock (1b) has shape (1, 16, 32) and MLPBlock has shape (64, 128, 128, 2). The total number of parameters is 26498.

In all of our two-dimensional experiments k = 10-2 and we use a batch size of 512. The mean and variance of pprior are fixed to fit the ones of pdata. The cache contains 104 samples and is refreshed every 103 iterations. We train each DSB step for 104 iterations. All two-dimensional experiments are run on Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz CPUs.
In Figure 8 we present additional two-dimensional experiments.

Figure 8: The first row corresponds to iteration 1 of DSB, the second to iteration 3 of DSB, the third to iteration 5 of DSB and the last to iteration 20 of DSB.
We found that the variance of pprior has an impact on the convergence speed of DSB, see Appendix J.3 for an illustration. This remark is in accordance with (Song and Ermon, 2020, Technique 1). In practice we recommend to set the variance to be larger than the variance of the target dataset, see Technique 4 in Appendix I.

(a) pprior

(b) DSB iteration 5

(c) pprior

(d) DSB iteration 5

Figure 9: Effect of the variance of pprior on the convergence of DSB. If pprior has a small variance 2 (here 2 = 5 in (a) and (b)) then DSB converges more slowly. If 2  d2ata, where d2ata is the variance of pdata then we observe more diversity in the samples obtained using DSB even for few
iterations.

Finally, since DSB does not require the number of Langevin iterations N to be large, one can wonder why we do not use N = 1 in order to derive a feed-forward. In practice this choice of N is not

51

desirable for two reasons. (a) First, since pN is not a good approximation of pprior, theoretical results such as (Léger, 2020, Corollary 1) indicates that more IPF iterations are needed. (b) Second, in our experiments we observe that in order to obtain similar results to N = 10 with N = 1 we need to increase substantially the size of the networks, even for a large number of IPF iterations, see Figure 10.
Figure 10: Failure of DSB for low N . On the left the best result obtained using DSB (DSB iteration 3) with N = 2 and 30000 training steps at each DSB iterations. The results deteriorate after 5 iterations of the algorithm.
J.2 Generative Modeling
Implementation details We use a reduced version of the U-net architecture from Nichol and Dhariwal (2021) for F and B, where we set the number of channels at 64 rather than 128 for computational resource purposes. We tried the architecture of Song and Ermon (2020), however we observed worse results for our setting. Although we observed improvement using the corrector scheme of Song et al. (2021), this improvement was similar to the one obtained augmenting the number of steps in the Langevin scheme. We therefore chose to avoid using such techniques altogether because of the increase in computing time when sampling (often by doubling the number of passes through the network). We chose the sequence {k}Nk=0 to be invariant by time reversal, i.e. for any k  {0, . . . , N }, k = N-k. In practice, we assume that N is even and let k = 0 + (1 - 2k/N )(¯ - 0) for k  {0, . . . , N/2} with 0 = 10-5 and ¯ = 10-1. The remainder of the sequence is obtained by symmetry. In the case of the MNIST dataset we set the batch size to 128, the number of samples in the cache to 5 × 104 with 10 samples of the trajectories for each sample of pdata. We end up with an effective cache of size 5 × 105. The cache is refreshed each 103 iterations and the networks are trained for 5 × 103 iterations. Again we use the ADAM optimizer with momentum 0.9 and learning rate 10-4. pprior is a Gaussian density with zero mean and identity covariance matrix. In the case of the CelebA dataset (32 × 32) we set the batch size to 256, the number of samples in the cache to 250 with 1 sample of the trajectory for each sample of pdata. The cache is refreshed each 102 iterations and the networks are trained for 5 × 103 iterations. Again we use the ADAM optimizer with momentum 0.9 and learning rate 10-4. pprior is a Gaussian density with zero mean and identity covariance matrix. Our results on MNIST and CelebA were computed using 4 NVIDIA Tesla V100 from the Google Cloud Platform.
Additional examples In this section we present additional examples for our high dimensional generative modeling experiments. In Figure 11 we perform interpolation in the latent space. More precisely we let X00 and X01 be two samples from pprior. We then compute X0 = (1 - )X00 + X01 for different values of   [0, 1]. For each value of   [0, 1] we associate XN which corresponds to the output sample obtained using the generative model given by DSB with initial condition X0. Note that in order to obtain a deterministic embedding we fixed the Gaussian random variables used in the sampling. One could also have used the deterministic embedding used by Song et al. (2021), i.e. a neural ordinary differential equation that admits the same marginals as the diffusion thus enabling exact likelihood computation. In Figure 12 we present high quality samples for MNIST. In order to obtain these high quality samples we consider our baseline MNIST configuration but instead of choosing N = 10 time steps
52

Figure 11: Interpolation in the latent space for MNIST. we consider N = 30. In addition, we train the networks for 15 × 103 iterations instead of 5 × 103. The number of particles in the cache is M = 500
Figure 12: MNIST samples: original dataset (left) and generated MNIST samples (right) after 12 DSB iterations In Figure 13 we present a temperature scaling exploration of the embedding obtained for CelebA. Similarly to the interpolation experiment we fix the Gaussian random variables in order to obtain a deterministic mapping from the latent space to the image space.
Figure 13: Temperature scaling in the latent space. In Figure 14 we explore the latent space of our embedding of CelebA. To do so, we obtain samples using a Ornstein-Ulhenbeck process targeting pprior. We refer to our project page project webpage for an animated version of this latent space exploration.
53

Figure 14: Exploration of the latent space. Samples are generated using a Ornstein-Ulhenbeck process targeting pprior to obtain the initial condition then using the generative model given by DSB. From left to right to right: samples at time t = 0, 1.3, 3.6, 8.6. J.3 Dataset interpolation For the dataset interpolation task we keep the same parameters and architecture as before except that the number of Langevin steps is increased to 50 steps in the two-dimensional examples and to 30 steps in the EMNIST/MNIST interpolation task. We also change the reference dynamics which is chosen to be the one obtained with the DSB where pprior is a Gaussian. This choice allows us to speed up the training of DSB in this setting. Animated plots are available at project webpage. Two dimensional examples First, we present examples of dataset interpolation in the case for two-dimensional datasets.
Figure 15: Dataset interpolation (DSB iteration 9). From left to right: t = 0, 0.15, 0.30, 0.5.
EMNIST/MNIST In order to perform translation between the dataset of handwritten letters (EMNIST) and handwritten digits (MNIST) we reduce EMNIST to 5 letters so that it contains as many classes as MNIST (we distinguish upper-case and lower-case letters), see Cohen et al. (2017) for the original dataset.
54

Figure 16: Iteration 10 of the IPF with T = 1.5 (30 diffusions steps). From left to right: t = 0, 0.4, 1.25, 1.5.
55

