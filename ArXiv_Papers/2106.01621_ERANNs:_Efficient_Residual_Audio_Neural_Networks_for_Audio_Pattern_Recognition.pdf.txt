ERANNs: Efficient Residual Audio Neural Networks for Audio Pattern Recognition

Sergey Verbitskiy
Deepsound Novosibirsk State University
Novosibirsk, Russia s.verbitskii@g.nsu.ru

Viacheslav Vyshegorodtsev
Deepsound Novosibirsk, Russia partners@deepsound.ai

arXiv:2106.01621v1 [cs.SD] 3 Jun 2021

ABSTRACT
We present a new architecture of convolutional neural networks (CNNs) based on ResNet for audio pattern recognition tasks. The main modification is introducing a new hyper-parameter for decreasing temporal sizes of tensors with increased stride sizes which we call "the decreasing temporal size parameter". Optimal values of this parameter decrease the number of multi-adds that make the system faster. This approach not only decreases computational complexity but it can save and even increase (for the AudioSet dataset) the performance for audio pattern recognition tasks. This observation can be confirmed by experiments on three datasets: the AudioSet dataset, the ESC-50 dataset, and RAVDESS. Our best system achieves the state-of-the-art performance on the AudioSet dataset with mAP of 0.450. We also transfer a model pre-trained on the AudioSet dataset to the ESC-50 dataset and RAVDESS and obtain the state-of-the-art results with accuracies of 0.961 and 0.748, respectively. We call our system "ERANN" (Efficient Residual Audio Neural Network).
Index Terms-- audio pattern recognition, audio tagging, sound classification, residual convolutional neural networks.
1. INTRODUCTION
Audio pattern recognition is a growing research topic and audio pattern recognition systems can be used in many areas. Audio pattern recognition task include several subtasks. One of the subtasks is an environmental sound and an acoustic scene classification task such as ESC-50 [1]. The second subtask is a musical genre classification task [2]. The third subtask is sound event detection [3]. The fourth subtask is audio tagging, for example, the AudioSet tagging [4]. Audio pattern recognition systems also can be applied for speech emotion recognition task, for example, RAVDESS [5].
The large-scale AudioSet dataset [4] contains over two million audio recordings with 527 sound classes. The state-of-the-art system achieves mean average precision (mAP) of 0.442 [6] on this dataset. This result is low compared to results in image classification tasks with similar large-scale datasets such as the ImageNet dataset because audio pattern recognition tasks are more complex for computer vision.
Approaches with CNNs outperformed other approaches (RNNs, for example) for audio pattern recognition tasks [7], [8], [9], [10], [11]. These systems use different 2D audio features such as the log mel spectrogram as input. These 2D features can well describe audio signals in time-frequency representation.
Residual neural network (ResNet) [12] is a CNN with shortcut connections between convolutional layers. Introducing shortcut

connections is led to partially avoid the vanishing gradient problem. WideResNet [13] which based on ResNet has an additional hyper-parameter ­ the widening factor W for the width of the convolutional layers. Changing this parameter, the computational complexity of systems can be simply changed.
But there are some problems with transferring systems that were developed for image classification tasks to audio pattern recognition tasks. These systems can achieve optimal performance in given tasks, for AudioSet tagging [6], for example. But the computational complexity of these systems is very high for audio signals with long duration because the number of multi-adds depends linearly on input size. Systems with CNNs for image classification tasks are used for images with small sizes but the log mel spectrogram for long-duration signals has high width (the number of time frames).
In this work, we propose a new architecture of convolutional neural networks based on ResNet with the simple technique for decreasing the number of multi adds. We evaluate the performance of our systems on three different benchmark audio datasets for tagging and classification tasks: ESC-50 [1], AudioSet [4], and RAVDESS [5]. Our systems are able to achieve the state-of-the-art results on all three datasets, outperforming previous systems. Transfer learning is used for small datasets. We fine-tune our best systems pre-trained on the AudioSet dataset for the other two tasks.
The best system achieves a mAP of 0.450 on AudioSet tagging, while the best system as a trade-off between computational complexity and the system perfomance achieves a mAP of 0.446, outperforming the previous state-of-the-art system with a mAP of 0.442 [6] (CNN14, 128 mel bins). Also our system with mAP of 0.446 is 2x smaller than CNN14. On the ESC-50 dataset and RAVDESS our best systems can achieve accuracies of 0.961 and 0.748, respectively. Previous best systems achieved accuracies of 0.947 [6] and 0.721 [6], respectively.
2. PROPOSED SYSTEM
2.1. Data Augmentation Techniques
We use several data augmentation techniques to prevent systems from overfitting during training. The first technique is temporal cropping. Our systems use tc -second sections of audio recordings during training. Sections are cut from random places. During evaluating systems full audio recordings are used as input to get the predictions. For all of our experiments, we use SpecAgment [14]. We apply modified mixup [11] which takes into account the sound pressure level of two recordings that are mixed. We use mixup with the hyper-parameter  = 1.0 for any experiment. Two waveforms

are mixed as in [11] but not log mel spectrograms as in [6]. We ran some experiments on the AudioSet dataset and found out that systems with the mixing of log mel spectrograms achieve worse performance. Pitch shifting [15] are used only for RAVDESS during training because this data augmentation technique is effective for speech.

2.2. Parameters for Feature Extraction

We adopt the number of mel bins M = 128. We suppose that

M = 128 is the best choice because in [6] was shown that it is

better than M = 32 and M = 64 for the AudioSet dataset. But

large values of M can highly increase the computational complexity

of our systems.

We also use audio recordings with a sampling rate (sr) of 44.1

kHz for all tasks, in contrast with many other approaches where the

sampling rate was 22.05 kHz or 32 kHz. This is due to the fact

that modern devices record at 44.1 kHz or 48 kHz in particular.

Downsampling decreases the sound quality.

The hop size H is chosen so that the number of frames T0 has

value:

T0 =

sr · tc H

+1

T0 = 128 · tc

(1)

Hence, log mel spectrogram has a quality of 128 frames per second.

We use a Hann window of size 4·H for short time Fourier transform

(STFT).

Thus, log mel spectrograms for each audio recording are repre-

sented as a 2D tensors with a shape of M × T0, where M = 128

and T0 = 128 · tc. These tensors are used as inputs to the first 2D

convolutional layer of our systems.

2.3. Model
Each tensor after convolutional blocks has a shape of Fi × Ti × Ci, where Fi is "i-th frequency size", Ti is "i-th temporal size" and Ci is the number of channels. All input tensors after feature extraction have a shape of F0 × T0 × C0, where F0 = M , T0 = 128 · tc and C0 = 1. We modify sizes of kernel, stride, and padding for the temporal dimension in some convolutional layers to make architecture faster. The number of frames T0 is higher for tc > 1 than the number of frequency bins M from Equation 1. Therefore, we introduce "the decreasing temporal size parameter" tm in our models in order to use it to decrease temporal sizes Ti in proportion to the duration of audio recordings. This can decrease the number of multi-adds because tensors before convolutional layers have fewer sizes.
As in ResNet, we use convolutional layers with an increased stride to decrease Fi and Ti, but not pooling layers. In [16] is described that this approach leads to improve the performance. We also apply the Leaky ReLU [17] with parameter 0.01 as a replacement for ReLU.
Proposed architecture of ERANN is described in Table 1, where W is the widening factor as in WideResNet [13], tm is the decreasing temporal size parameter and tc is the temporal cropping length. tm = 2k, where k = 0, ..., 4 is a fixed value.
The input of our system is a raw audio recording (waveform). Data augmentation for training data is implemented at first for each mini-batch before the feature extractor. Only SpecAugment is applied after the feature extractor. The feature extractor is the block for extracting the log mel spectrogram from the raw audio recording. Parameters for the log mel spectrogram extraction are described in the previous subsection.

Table 1: Proposed architecture of ERANN

Block name

Output size

Layers

Extractor

128 × T0 × 1

Augmentation Feature extractor
SpecAugment BatchNorm

Block 1

128 × T1 × 8 · W TFB(1, 1, 8 · W ) × 4

Block 2

64 × T2 × 16 · W

TFB(2, t1, 16 · W ) × 1 TFB(1, 1, 16 · W ) × 3

Block 3

32 × T3 × 32 · W

TFB(2, t2, 32 · W ) × 1 TFB(1, 1, 32 · W ) × 3

Block 4

16 × T4 × 64 · W

TFB(2, t3, 64 · W ) × 1 TFB(1, 1, 64 · W ) × 3

Block 5

8 × T5 × 128 · W

TFB(2, t4, 128 · W ) × 1 TFB(1, 1, 128 · W ) × 3

1 × 1 × 128 · W 128 · W N

Global pooling FC1, Leaky ReLU FC2, sigmoid / softmax

TFB(x, y, c) is the Time Frequency Block, where x is a stride size for the frequency dimension, y is a stride size for the temporal dimension and c is the number of output channels. Time Frequency Block is an alternative to the basic block in a ResNet V2 [18] (with full pre-activation) but it is slightly modified. The architecture of the Time Frequency Block is described in Table 2 and in Figure 1.

Table 2: Proposed architecture of TFB(x,y,c)

Kernel size Stride Padding

BatchNorm1 Leaky ReLU Conv2D1 BatchNorm2 Leaky ReLU Conv2D2 Conv2DRES

- - K1(x) × K1(y) - - K2(x) × K2(y) 1×1

- - x×y - - 1×1 x×y

- - 1×1 - - P (x) × P (y) 0×0

It is worth noting that we do not use dimension reduction as in bottleneck blocks. In [6] was shown that on the AudioSet dataset a ResNet-22 with basic blocks performs better than a ResNet-54 with bottleneck blocks.
Stride sizes ti for i = 1, ..., 4 for different blocks are natural numbers and depend on hyper-parameter tm = 2k and can be calculated by formulas:

ti  {2, 4},

4
tj = 16 · tm = 2k+4, ti+1  ti (2)

j=1

Temporal sizes Ti of tensors for i = 1, ..., 5 are calculated as:

T1 = T0 = 128 · tc,

Ti =

T1
i-1 j=1

tj

,

i>1

(3)

Functions K1(x), K2(x), P (x) for TFBs (where x is a stride

c = cin  x=y=1

Input Hin x Win x cin

BatchNorm1 Leaky ReLU

Conv2D1 K1(x) x K2(y), c

BatchNorm2 Leaky ReLU

Conv2D2 K2(x) x K2(y), c

+

c  cin  x 1  y 1
Conv2DRes 1 x 1, c

Ouput Hout x Wout x c
Figure 1: Scheme of TFB(x,y,c)

size) are defined as:

(3, 3, 1) x = 1  x = 2

(K1(x), K2(x), P (x)) = (6, 5, 2) x = 4

(4)

It should be noted that the first TFB doesn't have BatchNorm and Leaky ReLU at the start. BatchNorm from the extractor is used as a replacement for data standardization.
For global pooling we use a combination of average pooling and max pooling as in [6] to combine their advantages. After global pooling, we use two fully connected layers FC1 and FC2 as in [6] and we use softmax (for sound classification tasks) or sigmoid (for audio tagging tasks) nonlinearity at the end to obtain model predictions.

3. EXPERIMENTS AND RESULTS
We train and evaluate our systems on three datasets: the AudioSet dataset [4], the ESC-50 dataset, [1] and RAVDESS [5]. Systems pre-trained on the AudioSet dataset are transferred to the other two datasets to achieve the best performance. In this section, we also demonstrate the comparison of the performance and the computational complexity of ERANNs with different hyper-parameters. We show that our systems achieve the state-of-the-art performance on all used datasets.
Each system with different hyper-parameters is abbreviated denoted as ERANN-tcxtm-W , where tc is the temporal cropping length, tm is the decreasing temporal size parameter and W is the widening factor.

3.1. Experimental Setup
Systems are optimised by minimizing a categorical cross-entropy loss with the Adam optimizer [19] with a batch size of 32. We evaluate systems using an exponential moving average of model parameters with a decay rate of 0.999. We also use a one-cycle learning rate policy [20] for the AudioSet dataset with a max learning rate

of 0.001. For ESC-50 and RAVDESS we use a learning rate of 0.0002 without a scheduler for training from scratch and learning rate of 0.0001 for fine-tuning. It is also worth noting that we use a balanced sampling strategy for the AudioSet dataset during training as in [6] because this dataset is highly unbalanced. It was shown in [6] that this approach improves the system performance on the evaluation set.

3.2. AudioSet
AudioSet [4] is a large-scale audio dataset. The dataset consists of over two million audio recordings with 527 sound classes. Audio recordings are extracted from Youtube videos and almost all audio clips have 10 seconds duration. We successfully download 87.4% of audio clips of the training set and 88.2% of audio clips of the evaluation set. All audio recordings are converted to monophonic with a sampling rate of 44.1 kHz format.
It should be noted that the AudioSet dataset is a multi-label dataset (tagging task), unlike other used single-label datasets. Therefore, sigmoid is used to obtain model predictions.
For evaluating and the comparison of systems on the evaluation set we use only macro metrics: mean average precision (mAP), mean area under ROC curve (mAUC), and d-prime (this metric entirely depends on mAUC). These metrics are used as official evaluation metrics for AudioSet tagging [4] and were used in many previous works.

3.2.1. Hyper-parameters
We compare three different values of W with tc = 8 and tm = 4 and different six values for pairs of hyper-parameters tc and tm with W = 6. Experiments are demonstrated with W = {4, 5, 6}. We stop at W = 6 in order to the model is not too large and stop at W = 4 in order to the model is not too simple. Tensor shapes before global pooling are denoted as F × T , where F is the frequency size and T is the temporal size (for 10-sec. duration audio clips). Results are shown in Table 3.

Table 3: Comparison of ERANNs with different hyper-parameters for the AudioSet tagging

System

F × T mAP AUC d-prime

ERANN-8x4-4 ERANN-8x4-5 ERANN-8x1-6 ERANN-8x2-6 ERANN-4x4-6 ERANN-8x4-6 ERANN-10x4-6 ERANN-8x8-6

8 × 20 8 × 20 8 × 80 8 × 40 8 × 10 8 × 20 8 × 10 8 × 10

0.430 0.446 0.447 0.450 0.417 0.448 0.440 0.436

0.976 0.976 0.976 0.976 0.975 0.976 0.975 0.975

2.793 2.801 2.799 2.804 2.763 2.800 2.767 2.780

ERANN with tc = 8, tm = 2 and W = 6 achieves the best mAP on the evaluation set. We can conclude that increased values of tm can improve the system performance (tm = 2 and tm = 4). Using of the temporal cropping as data augmentation technique increases the performance of systems (the comparison of tc = 8 and tc = 10 for tm = 4) but the temporal cropping should be not too strong (tc = 4).

3.2.2. Computational Complexity
Table 4 shows the comparison of the number of parameters, the performance, and the number of multi-adds (for single 10-second duration clip) for proposed systems with different hyper-parameters. The comparison of the number of parameters and the performance with previous systems is also demonstrated.
We do not compare the number of multi-adds with previous systems for correctness because everyone calculates this number differently (with feature extractor or not etc.). There is no universal method for calculating the number of multi-adds and accounting for all operations in PyTorch. To compare multi-adds of our systems we use torchsummaryX 1.

Table 4: Comparison of the computational complexity and the performance of different systems

System

mAP Parameters Multi-Adds

Previous systems

CNN14 [6]

0.431 80,753,615

­

ResNet38 [6]

0.434 73,783,247

­

Wave.-L.-CNN [6] 0.439 81,065,487

­

CNN14 (128) [6] 0.442 80,753,615

­

Our systems ERANN-8x4-4 ERANN-8x4-5 ERANN-8x1-6 ERANN-8x2-6 ERANN-8x4-6 ERANN-8x8-6

0.430 0.446 0.447 0.450 0.448 0.436

24,504,849 38,198,545 54,435,473 54,532,241 54,919,313 56,467,601

26.21 × 109 40.93 × 109 126.56 × 109 77.18 × 109 58.92 × 109 52.99 × 109

Increasing tm decreases the number of multi-adds because tensors before convolutional layers have fewer sizes. ERANN-8x4-6 has >2x fewer multi-adds than ERANN-8x1-6 but the performance of ERANN-8x4-6 is higher. Our best system as a trade-off between computational complexity and the system performance ERANN8x4-5 with mAP of 0.446 has >2x fewer parameters than the previous best system CNN14 [6] with mAP of 0.442 but the performance of our system is higher.

3.2.3. Results
The final results of our systems and the comparison with previous state-of-the-art systems are shown in Table 5.
Our best systems ERANN-8x2-6 and ERANN-8x4-5 achieve mAPs of 0.450 and 0.446, respectively, and outperform the best previous system with a mAP of 0.442.

3.3. ESC-50
ESC-50 (Environmental Sound Classification) [1] is the dataset with environmental sounds consisting of 50 sound classes and 2000 5second audio recordings. This dataset is balanced with 40 audio recordings per sound class. ESC-50 is one of the most popular datasets for sound classification. This dataset is divided into five parts for the 5-fold cross-validation.
We compare systems trained from scratch and fine-tuned systems pre-trained on the AudioSet dataset. For each experiment we
1https://github.com/nmhkahn/torchsummaryX

Table 5: Comparison of our systems with previous systems for the AudioSet tagging

System

mAP AUC d-prime

Previous systems Google CNN (VGGish) [4] TAL Net [21] DeepRes [22] SUSTAIN [23] Wavegram-Logmel-CNN [6] CNN14 (128 mel bins) [6]

0.314 0.362 0.392 0.398 0.439 0.442

0.959 0.965 0.971 0.972 0.973 0.973

2.452 2.554 2.682
­ 2.720 2.735

Our systems ERANN-8x4-5 ERANN-8x2-6

0.446 0.976 2.801 0.450 0.976 2.804

train and evaluate systems three times and calculate the average score. We use fine-tuning without freezing of parameters. The comparison of our proposed ERANNs with different parameters and with previous systems is shown in Table 6.

Table 6: Comparison of systems on ESC-50

System

Accuracy

Scratch CNN14 [6] ADCNN-5 [24] CNN + Searched Augment [25] ERANN-4x2-3 ERANN-4x1-4 ERANN-4x2-4 ERANN-4x4-4 ERANN-4x4-5

0.833 0.885 0.890 0.892 0.892 0.891 0.880 0.879

Fine-tuned SUSTAIN [23] CNN14 [6] ERANN-4x4-4 ERANN-4x4-5 ERANN-4x1-6 ERANN-4x2-6 ERANN-4x4-6

0.941 0.947 0.949 0.961 0.953 0.954 0.959

Systems with W  5 are large for the small dataset for training from scratch. High values of W increase a fast overfitting risk. Our best system trained from scratch is able to achieve an accuracy of 0.892 which is close to an accuracy of 0.89 of the best system trained from scratch [25]. ERANN-4x2-3 has 2.9x fewer multi-adds and 1.8x fewer parameters than ERANN-4x1-4, but ERANN-4x2-3 has the same performance. And our best fine-tuned system RANN4x4-5 achieves an accuracy of 0.961, outperforming previous the state-of-the-art fine-tuned system with an accuracy of 0.947 [6].

3.4. RAVDESS
The Ryerson Audio-Visual Database of Emotional Speech and Song [5] is the dataset containing speech and song recordings from 24 professional actors with various 8 emotions. We use only the speech set from the dataset which consists of 4-second 1440 audio recordings with a sampling rate of 48 kHz. We resample recordings to a

sampling rate of 44.1 kHz. We evaluate our ERANNs with 4-fold cross-validation. Folds are formed so that the specific actor was in only one fold for robust results.
The comparison of our systems with different parameters and with previous systems is shown in Table 7 (the average accuracy of three training and evaluating tries).

Table 7: Comparison of systems on RAVDESS

System

Accuracy

Scratch CNN14 [6] RAVDESS model [26] ERANN-3x2-3 ERANN-3x1-4 ERANN-3x2-4 ERANN-3x4-4 ERANN-3x4-5
Fine-tuned CNN14 [6] ERANN-3x4-4 ERANN-3x4-5 ERANN-3x1-6 ERANN-3x2-6 ERANN-3x4-6

0.692 0.716 0.731 0.748 0.741 0.722 0.720
0.721 0.730 0.737 0.743 0.735 0.734

Pre-trained systems on the Audioset dataset does not significantly increase the performance on RAVDESS. Systems with increased tm also has worse performance. ERANN-3x2-4 has 1.7x fewer multi-adds than ERANN-3x1-4, but the accuracy of ERANN3x2-4 (0.741) is close to the accuracy of ERANN-3x1-4 (0.748). All our systems outperform previous best system with an accuracy of 0.721 [6].

4. CONCLUSION
We have proposed ERANNs based on ResNet and WideResNet with the simple technique to decrease computational complexity for audio pattern recognition tasks. We have shown that applying of this technique does not decrease systems performance on the AudioSet dataset and on the ESC-50 dataset. Our systems are able to achieve the state-of-the-art performance on three datasets, outperforming previous best audio pattern recognition systems.

5. REFERENCES
[1] K. J. Piczak, "ESC: Dataset for Environmental Sound Classification," Proceedings of the 23rd ACM International Conference on Multimedia, p. 1015­1018, 2015.
[2] G. Tzanetakis and P. Cook, "Musical Genre Classification of Audio Signals," IEEE Transactions on Speech and Audio Processing, vol. 10, pp. 293 ­ 302, 2002.
[3] N. Turpault, R. Serizel, A. Parag Shah, and J. Salamon, "Sound event detection in domestic environments with weakly labeled data and soundscape synthesis," in Workshop on Detection and Classification of Acoustic Scenes and Events, 2019.
[4] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, "Audio

Set: An ontology and human-labeled dataset for audio events," IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), p. 776 ­ 780, 2017.
[5] S. Livingstone and F. Russo, "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English," PLoS ONE, vol. 13, 2018.
[6] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880­2894, 2020.
[7] Y. Tokozume and T. Harada, "Learning environmental sounds with end-to-end convolutional neural network," 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2721­2725, 2017.
[8] Z. Zhang, S. Xu, S. Cao, and S. Zhang, "Deep Convolutional Neural Network with Mixup for Environmental Sound Classification," arXiv preprint arXiv:1808.08405, 2018.
[9] J. Salamon and J. P. Bello, "Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification," IEEE Signal Processing Letters, vol. 24, p. 279­283, Mar 2017.
[10] S. Abdoli, P. Cardinal, and A. L. Koerich, "End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network," arXiv preprint arXiv:1904.08990, 2019.
[11] Y. Tokozume, Y. Ushiku, and T. Harada, "Learning from Between-class Examples for Deep Sound Recognition," arXiv preprint arXiv:1711.10282, 2017.
[12] K. He, X. Zhang, S. Ren, and J. Sun, "Deep Residual Learning for Image Recognition," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770­778, 2016.
[13] S. Zagoruyko and N. Komodakis, "Wide residual networks," arXiv preprint arXiv:1605.07146, 2016.
[14] W. C. Daniel S. Park, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition," INTERSPEECH, 2019.
[15] J. Laroche and M. Dolson, "New phase-vocoder techniques for pitch-shifting, harmonizing and other exotic effects," Proceedings of the 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. WASPAA'99 (Cat. No.99TH8452), pp. 91­94, 1999.
[16] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, "Striving for Simplicity: The All Convolutional Net," arXiv preprint arXiv:1412.6806, 2015.
[17] B. Xu, N. Wang, T. Chen, and M. Li, "Empirical Evaluation of Rectified Activations in Convolutional Network," arXiv preprint arXiv:1505.00853, 2015.
[18] K. He, X. Zhang, S. Ren, and J. Sun, "Identity Mappings in Deep Residual Networks," arXiv preprint arXiv:1603.05027 2016.
[19] D. P. Kingma and J. Ba, "Adam: A Method for Stochastic Optimization," arXiv preprint arXiv:1412.6980 2017.
[20] L. N. Smith and N. Topin, "Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates," arXiv preprint arXiv:1708.07120, 2018.

[21] Y. Wang, J. Li, and F. Metze, "A Comparison of Five Multiple Instance Learning Pooling Functions for Sound Event Detection with Weak Labeling," ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 31­35, 2019.
[22] L. Ford, H. Tang, F. Grondin, and J. Glass, "A Deep Residual Network for Large-Scale Acoustic Scene Analysis," Proc. Interspeech 2019, pp. 2568­2572, 2019.
[23] A. Kumar and V. K. Ithapu, "A Sequential Self Teaching Approach for Improving Generalization in Sound Event Recognition," 2020.
[24] J. Sharma, O.-C. Granmo, and M. Goodwin, "Environment Sound Classification using Multiple Feature Channels and Attention based Deep Convolutional Neural Network," arXiv preprint arXiv:1908.11219, 2020.
[25] J. Park, T. Kumar, and S.-H. Bae, "Search of an Optimal Sound Augmentation Policy for Environmental Sound Classification with Deep Neural Networks," Proceedings of the Korean Society of Broadcast Engineers Conference, pp. 18­21, 2020.
[26] D. Issa, M. Fatih Demirci, and A. Yazici, "Speech emotion recognition with deep convolutional neural networks," Biomedical Signal Processing and Control, vol. 59, 2020.

