arXiv:2106.00827v1 [cs.LG] 1 Jun 2021

Weighting vectors for machine learning: numerical harmonic analysis applied to boundary detection

Eric Bunch American Family Insurance
Madison, WI 53783 ebunch@amfam.com

Jeffery Kline American Family Insurance
Madison, WI 53783 jklin1@amfam.com

Daniel Dickinson American Family Insurance
Madison, WI 53783 ddickins@amfam.com

Suhaas Bhat American Family Insurance
Madison, WI 53783 sbhat@amfam.com

Glenn Fung American Family Insurance
Madison, WI 53783 gfung@amfam.com

Abstract
Metric space magnitude, an active field of research in algebraic topology, is a scalar quantity that summarizes the effective number of distinct points that live in a general metric space. The weighting vector is a closely-related concept that captures, in a nontrivial way, much of the underlying geometry of the original metric space. Recent work has demonstrated that when the metric space is Euclidean, the weighting vector serves as an effective tool for boundary detection. We recast this result and show the weighting vector may be viewed as a solution to a kernelized SVM. As one consequence, we apply this new insight to the task of outlier detection, and we demonstrate performance that is competitive or exceeds performance of state-of-the-art techniques on benchmark data sets. Under mild assumptions, we show the weighting vector, which has computational cost of matrix inversion, can be efficiently approximated in linear time. We show how nearest neighbor methods can approximate solutions to the minimization problems defined by SVMs.
1 Introduction

Figure 1: A visualization of two weighting vectors. The set in the left-hand figure is supported within four disjoint components, and they live in R2. The set in the right-hand figure is supported on an embedding of Möbius strip, and it lives in R3. In both images, the weight of each point is represented using color and point size.
Preprint. Under review.

Magnitude is a scalar quantity that has meaning for many different kinds of data, and as with other scalar quantities such as rank, diameter, and measure, it has wide applicability, an intuitive interpretation and a solid theoretical foundation. Magnitude has been discovered, and rediscovered multiple times in both practical and theoretical contexts. In this paper, our goal is to apply recent developments drawn from magnitude theory to machine learning, and to empirically demonstrate characteristics of magnitude that, while implicitly described by abstract theoretical results, have not, to our knowledge, been explicitly stated before, nor have they been leveraged for practical purpose.
Informally, magnitude aims to quantify the effective number of points in a space. Our aim is more subtle: we wish to identify which points are considered "effective" and "important." We do this using the weighting vector. The weighting vector appears naturally in the definition of magnitude, and we find that the weighting vector, under appropriate conditions, serves as an effective boundary detector. It is this behavior that makes the weighting vector especially well suited for machine learning tasks.

1.1 Background, notation and examples

We now define magnitude and the weighting vector, we present examples, and we state several central theorems of the field. While our focus is largely on subsets of Rn, we note that the concept magnitude and weighting vector can be defined for far more general types of sets.
Definition 1. Let X be a finite metric space with metric d. Denote the number of points in X by |X|. The similarity matrix of X is defined to be X pi, jq :" expp´dpxi, xjqq for 1  i, j  |X|. Whenever the inverse of X exists, we define the weighting vector of X to be
wX :" X´11, where 1 is the |X| ^ 1 column vector of all ones. The magnitude of X is defined to be the quantity
MagpXq :" 1T wX " 1T X´11.
That is, MagpXq is the sum of all the entries of the weighting vector wX .
Example. When X is a finite subset of Euclidean space, X is a symmetric positive definite matrix [Theorem 2.5.3, [16]]. In particular, X´1 is guaranteed to exist. Hence, the weighting vector and magnitude exist for finite subsets of Rn.
Example. Given an undirected, unweighted graph G, one can define a metric space whose points are given by the vertices of G, and whose metric is taken to be the length of the shortest path between two vertices. The weighting vector of this metric space is not guaranteed to exist. Definition 2. For an arbitrary subset X  Rn, the magnitude of X is defined as
MagpXq " suptMagpY q | Y is a finite subset of Xu.
Example. In 1 dimension, and for t  0, one has that Magpr0, tsq " 1 ` t{2. This was shown by Leinster in [16]. The magnitude of the ball with radius r in R2n`1 is a rational function of r; this was recently demonstrated by Barceló and Carbery [2].

For a finite metric space pX, dq, and any t P r0, 8s, we can define a new metric space ptX, tdq in the following way. The points of tX are the same as those of X, and the metric td is d scaled by t: tdpx, yq :" t ¨ dpx, yq. The magnitude function of X is the map t ÞÑ MagptXq, and it is well-defined whenever tX is invertible. Although the inverse of tX may not be defined in general, it has been shown in [Proposition 2.2.6 [16]] that for finite subsets of Rn, the magnitude function is analytic on p0, 8q. We also have the following:
Theorem 3 (Proposition 2.2.6 [16]). For X  Rn finite, limtÑ8 MagptXq " |X|.

The above proposition is one of the reasons underlying the informal interpretation of magnitude as quantifying the effective number of points in a space. The following very recent theorem gives a connection between the magnitude of X  Rn and the n-volume of X.
Theorem 4 (Theorem 1 [2]). For X  Rn nonempty and compact, we have

MagptXq VolpXq

lim MagptXq " 1, and lim

tÑ0`

tÑ8

tn

"

, n! VolpBnq

where Bn  Rn is the unit ball.

2

1.2 Properties of the weighting vector
The weighting vector plays a central role in the applications that are discussed below, but it is not clear by inspection of its definition that the individual entries of the weighting vector carry useful information. To provide intuition about the weighting vector, we now highlight some key features.
Let X  Rn and wX its weighting vector. The entries of wX may be indexed in a canonical way by x P X. We call wX pxq, the weight of x. Since the similarity matrix X is positive definite, its inverse
exists and is also positive definite. Thus, 11X´11 " 11wX  0. Although the average value of the
entries of wX is guaranteed to be positive, it may happen that wX pxq  0 holds for some x P X.
An heuristic argument led us to conjecture that the weighting vector ought to be useful as a device for boundary detection. Very recently, this heuristic was made rigorous with a formal theorem and proof [22]. Further discussion about this development is in Section 2.
It is not at all clear by inspection of the definition that a weighting vector might carry useful information about a set's boundary. There is early work empirically investigating the weighting vector for examples of finite metric spaces [34]. The theoretical foundation of using weighting vectors for boundary detection leverages the theory of Bessel potential functions and other machinery of harmonic analysis.
1.3 Related work, paper structure
An early reference to the concept of magnitude occurs in [30], where it was introduced as a way to measure biological diversity. However, the mathematical motivations were not divulged in this paper. Two decades later, Leinster placed the magnitude of a metric space within a formal mathematical framework using category theory [16]. This highly abstract perspective lead to the current era, where it is being explored through many different lenses, including functional analysis [20, 2], harmonic analysis [21] and homology theory [17], where it has been shown to be equivalent to an Euler characteristic. Much of the prior emphasis has been on a set's magnitude, and this focus has overshadowed the potential utility of the weighting vector.
Recently, topological data analysis has emerged as an approach to the problem of describing the shape of high-dimensional data [10, 29, 36]. One particularly popular topic within this field is persistent homology [10]. Recent efforts have realized magnitude as the Euler characteristic of a homology theory, called magnitude homology [17]. It has also been shown that there is a direct relationship between magnitude homology and persistent homology [23]; however, the current paper is the first known application of magnitude directly to machine learning.
We now describe the remaining sections of this paper. Section 2 presents theoretical and heuristic justification as to why the weighting vector is a boundary detector. Section 3 details how the weighting vector can be seen as the solution to a generalized SVM. Section 4 presents methods to approximate the weighting vector with nearest neighbor methods. Section 5 presents results of experiments run with a weight-based anomaly detection algorithm. We end with concluding remarks in Section 6.

2 Boundary detection

The purpose of this section is to state the theorem in [22], which contributes rigor to the above discussion about boundary detection. We begin by offering informal comments about how the finite, discrete sets of the applications relate to the infinite, continuous objects of the theorem. The background required for this initial part of the discussion is limited to basic familiarity with the Fourier transform. These comments also serve to present the moral case that weighting vectors ought to be useful as boundary detectors. The background required to interpret the theorem's statement includes substantial familiarity with tempered distributions and related theory, which is beyond the scope of what can be presented here.

In the sequel, let Fpf q denote the Fourier transform of a smooth function f : Rn Ñ C. Recall that

under suitable conditions on f one has that

p2iqk pF f q pq " `F `Bkf  pq.

(1)

Now suppose X  r0, ts is a finite set of equispaced points selected from the interval r0, ts (the equispacing condition may be relaxed to instead be a uniform random sample). The linear equation

3

that defines the weighting vector is X w " 1. This statement has, via Riemann summation, a
continuous analogue expression for a "weighting function" v for the entire interval, r0, ts. Let h :" h1{2. This analogue has the form,





h ° vpxq " hpx ´ yqvpyq dy " e´|x´y|vpyq dy " Ir0,tspxq.

R

R

By [12] Chapter 8,

`F Ir0,ts pq

"

pF ph

° vqq pq

"

pFhq pq pFvq pq

"

1`

2 422

pFvq pq,

or

1 2

p1

`

422qF

`Ir0,ts

pq

"

pF

vq

pq.

(2)

Applying Eq. 1 and the operator F ´1 to Eq. 2, one has

1 2

`Ir0,ts

´

B2Ir0,ts

"

v.

(3)

In Eq. 3, the term B2Ir0,ts vanishes everywhere except at the points 0 and t, where it behaves as secondorder derivative operator. Informally, v is constant on the open interior p0, tq, and it approximates a discrete second-order derivative operator at the boundary points 0 and t. This informal argument may be adapted to n  1 dimensions, where Ir0,ts is replaced by more general A  Rn.
We now turn to the theorem statement. The Bessel potential space is the Hilbert space of tempered distributions

Hs

:"

! 

P

S1pRnq

:

p1

`

}¨}2qs{2F 

P

) L2pRnq

that is equipped with norm

^ }}Hs :"

´ 1

`

}}2¯s

|pF q pq|2

1{2 d

.

Rn

For for compact A  Rn, the definition of a weighting, as well as necessary and sufficient conditions for A to have a weighting, can be found in Definition 3.3 and Theorem 4.1 of [21].

Theorem 5. ([22]) Let n be odd, A  Rn compact with weighting z P H´pn`1q{2pRnq, and let A denote Lebesgue measure restricted to int A. Then

1

z " n!n A ` 

(4)

for some  P H´pn`1q{2pRnq that is supported on BA. The constant n :" n{2{pn{2 ` 1q is the volume of the unit n-ball.

The decomposition of z stated in Eq. 4 agrees with informally-derived expression for v stated in Eq. 3. Numerically, we find that n odd does not seem to be required. Finally, we note that under extra regularity assumptions, a similar result follows from Theorem 5 of [2].

2.1 Interpreting Transformer Language Models with Weighting Vectors
In the sequel, we use the theory from section 2 to establish relationships between magnitude and wellknown tools in the machine learning community and develop novel uses for the boundary-detection properties of the weighting vector. Before doing so, however, we begin with a straightforward application of the weighting vector: interpreting the internal workings of modern transformer-based language models such as BERT [9]. Using a DistilBERT [28] pre-trained extractive questionanswering model [35], we compute the weighting vector of the tokens in the 768-dimensional embedding space for each layer.
As a concrete example, shown in figure 2, the question and context fed to the model are "What is a good example of a question answering dataset?", and "Extractive question answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the

4

Figure 2: The layers of a DistilBERT QA model manipulate the token-vector space so that the start and end tokens of the answer span are on a boundary.
SQuAD dataset, which is based on that task.", respectively. The desired answer, of course is "SQuAD dataset". In the final embedding layer of the transformer, the tokens that correspond to the beginning and end of "SQuAD dataset" have the largest weighting vector components. Notably, these are the tokens that the model has been trained to return. This implies that under the hood of the transformer, the vector embeddings of the correct tokens are being pushed towards a boundary of a region within the embedding space, and the remaining tokens remain in the interior of that boundary. Our analysis shows that this behavior holds in general. Using 1000 random question-context pairs from the SQuAD v2 dataset [25] and a value of t " 0.01, the start and end tokens in the last embedding layer on average each have weights that are in the 96th percentile of token weights for the pairing. We also note that as we trace the progress of the [CLS] token through the different layers, it's weighting vector component goes from very negative to quite positive, and this transition only begins around layer 3. This hints that in the DistilBERT model, many layers are necessary in order to imbue the embeddings with a solid semantic understanding of the input tokens.
3 Relation to generalized support vector machines
Figure 3: Classification of discrete sets in the plane using a kernelized SVM with the Laplacian kernel, Kpx1, yq :" exp p´t }x ´ y}qq. Point size is based on the modulus of the SVM solution. One of the classes is equi-distributed on a circle of radius r " 6 (left) and r " 8 (right). The solution of the SVM used in the image at right is equivalent the solution of a one-class SVM, and since the kernel is the Laplacian, this solution also equals the weighting vector of the cluster. In this section, we recast the weighting vector as a solution to a one-class support vector machine. We begin with some background on support vector machines (SVM) [3, 4, 6, 33, 19]. The task of a linear support vector machine aims to classify m points in Rn as belonging to one of two classes, either A` or A´. If we represent the data by the m ^ n matrix A, and the class membership as specified by a
5

diagonal matrix D, with `1 or ´1 along the diagonal, the linear support vector machine has the form

min
w,,y

11y ` }w}1

subject to DpAw ´ 1q ` y  1

y  0,

where

}w}1

:"


i

|wi|

denotes

the

1-norm

of

w.

The

variable

y

is

a

vector

of

slack

variables,

and

the optimal pw, q characterize a separating boundary between A` and A´. The decision boundary

consists of x P Rn that satisfies the linear equality w1x " .

In [19], it is shown that a very general kernelized support vector machine may be formulated. The
formulation established there, and which we use here, is more general than the common approach
that employs the kernel-trick to specify a SVM's nonlinear decision boundary. A kernel function, KpA, Bq, is any map from Rm^n ^ Rn^l Ñ Rm^l. If x, y P Rn are column vectors, then Kpx1, A1q is a row vector in Rm, and Kpx1, yq is a scalar. In the established notation, the kernelized support
vector machine program is

min
u,,y

11y ` s }u}1

subject to DpKpA, A1qDu ´ 1q ` y  1

y  0.

If pu, q is a solution to this program, then the decision boundary consists of x P Rn that satisfy Kpx1, A1qDu " .

If this program has kernel function Kpx1, yq :" expp´t }x ´ y}q, regularization parameter s " 0, and a classification task with only one class, then KpA, A1q " A and the program reduces to

min
u,

}minpA

u

´

p1

`



q1,

0q}1

(5)

For any pair pu, q, one has that
}minpAu ´ p1 ` q1, 0}  }Au ´ p1 ` q1}1 .
When A is a finite subset of Rn, the matrix inverse of A exists, and so the term on the right, which
is an upper bound, vanishes when u " p1 ` qA´11, which is (up to normalization) equal to the
weighting vector. Since this is an upper bound, the term on the left, which is nonnegative, must also vanish, and this shows the weighting vector is an optimal solution to the generalized SVM program.

This allows us to reinterpret the weighting vector as a support vector of a generalized SVM, and it also aligns with the empirical and theoretical observations made elsewhere that the entries of the weighting vector can be used as an effective boundary detector.

This view also provides an explanation for the empirical observation that when t  0 is very small, the weighting vector assigns larger weights to the extremal points of the convex hull of a metric space. Consider two labeled sets in an arrangement like that illustrated on the left in Figure 3. In the left image, one cluster of points is surrounded by a set of points distributed on a circle. This pair of sets was used to train a two-class SVM with kernel function Kpx1, yq :" exp p´t }x ´ y}qq. Letting pw, q represent the solution to this SVM, the image uses |w| to represent the point sizes in this image. In the image on the left, the largest entries of the solution act as supporting vectors of the maximum margin decision boundary, and those points are nearest to the opposite class. On the right, point size is determined by the weighting vector applied to the just the clustered class, which we have just seen, is a one-class SVM. The one-class SVM can formally be interpreted as a 2-class SVM, except that the points belonging to a second class surround the first image and live on a circle with infinite radius.

4 Approximation by nearest neighbors

In this section show that by using the work of Leinster on magnitude [16] we can give new insight

to the solution of generalized SVMs induced by a kernel K. For certain kernels K, w can be

approximated

by

1{f ,

where

f

is

the

kernel

density

estimator:

f pxq

"

1 n

n
i"1

Kpx, xiq.

Boundary detection has shown up in the context of approximating viability kernels in viability theory

[1]. In [8] it was shown that the solutions to kernelized SVMs, and specifically an SVM with a

6

Gaussian kernel, perform well when approximating the boundary of a viability kernel. There is also work done in [27] to use the k-d tree structure to find the boundary of a subset of Rd via oracle-labeled sample points, as well as to compute the volume of the enclosed region. After an exchange of the
Gaussian kernel of [8] for a Laplacian kernel, the result of this section will allow us to see the two
approaches of [8] and [27] to be closely related.

Definition 6. ([16], Def. 2.1.2) A dataset X is scattered if e´



1 n´1

where

is the smallest

distance between distinct points in X.

Theorem 7. For X scattered, and x P X,


wpxq ´ 

1

 

nf

pxq

 



npn ´ 1q2e´2 ` npn ´ 1qe´ 1 ´ pn ´ 1qe´

,

(6)

where f is the kernel density associated with the Laplacian kernel X .
In particular, consider the metric space family tX for some finite subset X of Rd, the weighting vectors wt, and kernel density estimators ft associated to tX . As t Ñ 8, the RHS of Eqn. 6 converges to zero. Theorem 7 shows that for sufficiently large t, the solution to the one class SVM induced by tX is approximated with concrete bounds by 1{nf . Naïvely, w can be obtained by finding the inverse t´X1. Assuming tX has been calculated, this can take Opnq, where currently 2    3; however 1{nf can be calculated in Opn2q, with much smaller constant multipliers in practice.
For sufficiently nice kernels, Theorem 1.1 and Corollary 1.1 in [5] give a way to estimate the kernel density function f in an asyptotically unbiased way via a normalized count of points falling in an appropriately small rectangular region, assuming the points are sampled from the probability distribution defined by the kernel. This provides a further estimate of the kernel density f by

f~pxq

"

|Rhpxq X X| np2hqd

(7)

where X  Rd a dataset with |X| " n, and Rhpxq is a d-cube of side length 2h centered at x P Rd.

|Rhpxq X X| denotes the number of points in X that lie in the set Rhpxq. This gives us a final

approximation of w by

1 nf~pxq

"

p2hqd |Rh pxqXX

|

.

Employing a k-d tree structure for efficient nearest

neighbor search, the quantity |Rhpxq X X| can be calculated on average in Oplogpnqq time, after

an average build time cost of Opnlogpnqq. An empirical demonstration of these approximations is

given in the supplementary material. It should be noted that while the bound on the approximation in

Thm. 7 is guaranteed when the space is scattered­that is, when the scale parameter t is chosen to

be large enough­in practice, we have seen that t can be chosen to be much smaller, with reasonable

approximation error. The data set depicted in the supplementary material requires a t value of

approximately 5.8 ^ 104 to be scattered, whereas t " 50 was used. In practice, it was seen that the

approximation

1 |Rh pxqXX |

has

lower

error

than

the

normalized

version

with

numerator

p2hqd.

5 Outlier detection

It is possible to apply weighting vectors to anomaly detection. Our algorithm, detailed below, is either competitive with or better than state-of-the-art techniques at outlier detection on benchmark data sets. The broad idea is as follows. Recall that for a given set of m points X  Rn, the weighting vector assigns a real-valued scalar value to each x P X. We call this value the weighting score of x. The weighting score of x is typically large when x is a large distance from all other points of X. By definition, outliers of X differ significantly from most other observations, and as a result, it is natural to expect the weighting score of such points to be large.
The foregoing can be used as the foundation of a simple outlier detection algorithm. Given a set of inlier data Y  Rn:
1. Normalize Y to have mean 0 and unit variance in each feature. Denote the normalizer operation by .

7

Name

Feat Prec@10 Rec@10 F1@10 AUC AUC RDOS [32]

breastw

9

0.97

0.97

0.97 0.99

cardio

21 0.94

0.66

0.77 0.98

glass

9

0.33

0.25

0.19 0.71

0.89

httpKDD 3

0.94

1.00

0.97 0.99

0.97

ionosphere 33 0.93

0.85

0.89 0.95

0.94

lympho

18 0.38

1.00

0.55 0.98

1.00

pendigits 16 0.75

0.94

0.83 0.99

0.97

pima

8

0.85

0.13

0.22 0.66

0.73

shuttle

9

0.97

1.00

0.98 1.00

0.98

vowels

12 0.80

0.71

0.75 0.97

wbc

30 0.73

1.00

0.84 0.98

0.98

wine

13 0.60

1.00

0.75 0.92

Table 1: Performance metrics of our anomaly detection algorithm applied to the benchmark data sets of [26]. We report the best AUC that was achieved by RDOS [32]. In each case, optimal RDOS performance exceeded the performance of prior work.

2. For any point x P Rn with x R Y , compute the weighting vector w of pY Y txuq.
3. If the weighting score of pxq is among the k largest entries in w, then classify x as an outlier.
We apply this algorithm to 12 data sets that have been reported in prior work, and we compare our results to those of [32] in Table 1. The data sets reported here have inliers and outliers labeled. The weighting vector w depends on a parameter, t, which must be chosen according to some objective metric. To find an optimal choice of t, we perform a search. First, we split the inlier data into training, validation and testing sets, and then we randomly distribute the labeled outliers into the validation and testing sets with equal probability. The training set serves the role of the set Y in step 1 in the above algorithm. Using the training and validation sets, we perform a parameter search over values of t P 1 ^ 10j, 5 ^ 10j : ´5  j  1(. The model identifies x as an outlier if its weighting score is among the k largest values. For several metrics reported in Table 1, we fixed the threshold k " 10 and indicated this with an "@10" suffix. This is a somewhat arbitrary choice, but it seems to work well in practice.
The optimal choice of t is found by selecting the value of t that maximizes the area under the receiver operating characteristic curve (AUC), which summarizes the results of varying the rank-order threshold k that is used to identify outliers. We select the value of t with largest AUC, and then report results for k " 10 applied to the testing set. Computing the weighting vector in step 2 for the space pY Y txuq can be done efficiently, using the standard block matrix inversion formula for 2 ^ 2 matrices, and pre-computing the matrix inverse Z´p1Y q one time. The computational cost of this is one matrix inverse operation, followed by a matrix-vector multiplication for each element in the testing set. In practice, a sample of at most 1000 inliers effectively serve as a training set.
6 Conclusion and future work
We have introduced the concept of magnitude and weighting vector into the machine learning community, and present theoretical as well as empirical evidence that the weighting vector can be effectively used as a boundary detector of a densely, uniformly sampled data set in Euclidean space. We detail how the weighting vector can be seen as the solution to a generalized SVM, and give methods to efficiently approximate it via nearest neighbor methods. We define an anomaly detection method based on the weighting vector, and show that it is competitive with state of the art.
The weighting vector has solid, although abstract, topological and geometric foundations, and sits in connection to SVMs, nearest neighbor methods, and machine learning. In future work, we intend to explore further the connection of SVMs to weighting and magnitude, as well as the connection with nearest neighbor methods. In particular, the approximation given in 7 seems to empirically hold for t smaller than the range specified in the theorem. We also have preliminary, ongoing work on a neural
8

network layer inspired by the weighting vector. Finally, the potential for applicability to graphs is touched on briefly in the supplementary materials, and is of interest for further investigation.
Acknowledgments and Disclosure of Funding
The authors would like thank Mark Meckes for reading through early manuscripts of this paper and providing extremely useful feedback and discussions.
References
[1] J.-P. Aubin, A. Bayen, and P. Saint-Pierre. Viability Theory: New Directions. Springer, 01 2011.
[2] J. Barceló and A. Carbery. On the magnitudes of compact sets in Euclidean spaces. American Journal of Mathematics, 140(2):449­494, 2018.
[3] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classifiers. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144­152, 1992.
[4] P. S. Bradley and O. L. Mangasarian. Feature selection via concave minimization and support vector machines. In Proceedings of the Fifteenth International Conference on Machine Learning, ICML '98, page 82­90, San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc.
[5] T. Cacoullos. Estimation of a multivariate density. Annals of the Institute of Statistical Mathematics, 18:179­189, 1966.
[6] V. Cherkassky and F. Mulier. Learning from data: Concepts, theory, and methods. In Learning from Data: Concepts, Theory, and Methods, 1998.
[7] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, 2000.
[8] G. Deffuant, L. Chapel, and S. Martin. Approximating viability kernels with support vector machines. IEEE Transactions on Automatic Control, 52(5):933­937, 2007.
[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171­4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
[10] H. Edelsbrunner, D. Letscher, and A. Zomorodian. Topological persistence and simplification. Discrete & Computational Geometry, 28(4):511­533, Nov 2002.
[11] P. Erdös and A. Rényi. On random graphs i. Publicationes Mathematicae Debrecen, 6:290, 1959.
[12] G. Folland. Real analysis: modern techniques and their applications. Pure and applied mathematics. Wiley, 1999.
[13] G. Fung and O. L. Mangasarian. Proximal support vector machine classifiers. In KDD '01, 2001.
[14] A. A. Hagberg, D. A. Schult, and P. J. Swart. Exploring network structure, dynamics, and function using networkx. In G. Varoquaux, T. Vaught, and J. Millman, editors, Proceedings of the 7th Python in Science Conference, pages 11 ­ 15, Pasadena, CA USA, 2008.
[15] D. Klein and M. Randic. Resistance distance. Journal of Mathematical Chemistry, 12:81­95, 12 1993.
[16] T. Leinster. The magnitude of metric spaces. Documenta Mathematica, 18:857­905, 2013.
9

[17] T. Leinster and M. Shulman. Magnitude homology of enriched categories and metric spaces, 2017.
[18] D. D. Lewis and W. A. Gale. A sequential algorithm for training text classifiers. CoRR, abs/cmp-lg/9407020, 1994.
[19] O. L. Mangasarian. Generalized support vector machines. In Advances in Large Margin Classifiers, pages 135­146. MIT Press, 1998.
[20] M. Meckes. Positive definite metric spaces. Positivity, 17:733­757, Sept 2013.
[21] M. W. Meckes. Magnitude, diversity, capacities, and dimensions of metric spaces. Potential Analysis, 42(2):549­572, 2015.
[22] M. W. Meckes. Personal communication, September 2020.
[23] N. Otter. Magnitude meets persistence. Homology theories for filtered simplicial sets, 2018.
[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python . Journal of Machine Learning Research, 12:2825­2830, 2011.
[25] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv e-prints, page arXiv:1606.05250, 2016.
[26] S. Rayana. ODDS library, 2016.
[27] J.-B. Rouquier, I. Alvarez, R. Reuillon, and P.-H. Wuillemin. A kd-tree algorithm to discover the boundary of a black box hypervolume or how to peel potatoes by recursively cutting them in halves. Annals of Mathematics and Artificial Intelligence, 75(3):335­350, Dec. 2015. 11 pages.
[28] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2020.
[29] R. Scopigno, D. Zorin, G. Carlsson, A. Zomorodian, A. Collins, and L. Guibas. Persistence barcodes for shapes, 2004.
[30] A. R. Solow and S. Polasky. Measuring biological diversity. Environmental and Ecological Statistics, 1(2):95­103, Jun 1994.
[31] J. Suykens and J. Vandewalle. Least squares support vector machine classifiers. Neural Processing Letters, 9:293­300, 06 1999.
[32] B. Tang and H. He. A local density-based approach for outlier detection. Neurocomputing, 241:171­180, 2017.
[33] V. Vapnik. The Nature of Statistical Learning Theory, volume 8, pages 1­15. Springer, 01 2000.
[34] S. Willerton. Heuristic and computer calculations for the magnitude of metric spaces, 2009.
[35] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38­45, Online, Oct. 2020. Association for Computational Linguistics.
[36] A. Zomorodian and G. Carlsson. Computing persistent homology. In Proceedings of the Twentieth Annual Symposium on Computational Geometry, SCG '04, page 347­356, New York, NY, USA, 2004. Association for Computing Machinery.
10

Figure 4: Active learning results comparing the weighting vector query strategy vs the uncertainty sampling strategy. Average over 100 runs.

A Appendix
A.1 Active learning Experiments
Let L (the labeled dataset) and U the (unlabeled dataset) be two subsets of the available pool of training data X, with X " U Y L and U X L " H. An iteration of the algorithm will pick some points in U to be labeled by an oracle (transferring them to L). The current model will be then updated using the new updated dataset L and its corresponding labels. For simplicity we will state the algorithm for a binary classification problem i.e. when L " tL0, L1u, however it can be trivially extended to a multi-class problem.
The intuition behind the algorithm is simple: at each iteration i, we assign every training data point to one of the sets X~0 or X~1 according to its predicted label by the current classifier fi. We will calculate the corresponding weight vectors wX~0 and wX~1 . Then, we choose to label the point with the minimum value (interior point) and the with the maximum value (likely to be in the boundary) for both sets X~0 and X~1. By choosing this way we are aiming to: (a) reinforce, validate and refine high confidence classifier information (labels) acquired in prior iterations (exploitation) and (b) to acquire labels in the predicted class boundaries where our classifier confidence is potentially lower (exploration). The proposed active learning algorithm is stated in Algorithm 1.

Algorithm 1 Active learning via weighting vector.

input: Data set X

L " H; U " X

initialize L ; U " X ´ L; with it's corresponding YL

f " train_classifierpL,YL)

while (not converged) or (labeling budget not reached) do

X~i " tx P X | f pxq " iu for i " 0, 1.

calculate weighting vectors wX~i

Qmin,i

"

arg min
U

abspwX~i q

for

i

"

0,

1

Qmax,i

"

arg max
U

abspwX~i q

for

i

"

0,

1

YQ=query_labels(Qmin,0,Qmax,0,Qmin,1,Qmax,1)

L " L Y tQmin,0,Qmax,0,Qmin,1,Qmax,1u

YL " YL Y YQ

U " X ´ L;

f " train_classifierpL, YL)

output: f

11

In order to assess the effectiveness of the weighting-vector-based active learning (AL) algorithm proposed, we compared Algorithm 1 to the simplest but highly effective and most commonly used query AL framework: uncertainty sampling [18]. In this framework, the AL algorithm queries the instances for which it is least certain about how to label (i.e. for many algorithms pplabel}xq « 0.5 or where the decision function is close to 0). For simplicity we used a kernelized Ridge regression model [7] (also refer as to LS-SVM [31] or proximal SVM [13]). Laplacian kernels were used both as magnitude to calculate the weighting vector and as classification kernel (kpx, yq " expp´}x ´ y}1q with  " 0.1. At each iteration of Algorithm 1 the classifier learned after obtained labels from the oracle has the form f pxq " Kpx, Lq1w ´ w0, where w0 is the bias term.
We performed experiments on five classic benchmark datasets from the UCI repository taking 67% of the data as training pool and the remaining 33% as a testing set. Note that the weighing-vectorinspired algorithm chooses 4 points per iterations so we picked the four more uncertain points for the uncertainty sampling algorithm to be fair.
Figure 4 shows average performance curves over 100 runs. The performance from the weighting vector algorithm seems to perform better in four out of the five datasets and slightly worse on the Galaxy dim. and Checkerboard datasets.

B Useful properties of magnitude

In this section, we offer some techniques that are useful when working with weighting vectors. We discuss how the computation of the weighting vector may be effectively computed by breaking the computation into smaller pieces and "gluing" the results together.

B.1 Inclusion-Exclusion for Weight and Magnitude

We demonstrate a practical way to calculate the weighting vector for a set Z :" X Y Y that is the union of two finite X, Y  Rn. To approach this, first we investigate the case when X and Y are disjoint. Then we will look at the case Y  X, and show how to calculate either wX or wY when one knows the other. Finally we will state an inclusion-exclusion principle for magnitude, as well as
the weighting vector.

Before proceeding, we recall the definition of the Schur complement.

Definition 8.

Let M

:"

,,A C

B D

be

the

block

matrix

where

the

matrices

A,

B,

C,

D

are

of

dimen-

sions n ^ n, n ^ m, m ^ n, and m ^ m respectively. If D is invertible, then the Schur complement of D in M is the n ^ n matrix

M {D " A ´ BD´1C. Similarly, if A is invertible, then the Schur complement of A in M is the m ^ m matrix

M {A " D ´ CA´1B.

Let H  Y  X  Rn be finite sets. Without loss of generality, we can index the points of X such that the first |Y | of them correspond to those points in Y . Then we can see that X can be written as a block matrix

X

"

,, Y YT,Y¯

Y,Y¯ Y¯



,

(8)

where Y¯ " XzY , and Y,Y¯ denotes the submatrix of X formed by taking the rows corresponding to
Y and columns corresponding to Y¯ . We can now rewrite the formula X w " 1 using equation 8 as
the system of equations

Y

wX

 Y

`

Y,Y¯

wX

 Y¯

" 1Y

YT,Y¯

wX

 Y

`

Y¯

wX

 Y¯

" 1Y¯ ,

12

where 1Y

and 1Y¯

are

respectively

the

|Y

|

^

1

and

Y¯

 

^

1

column

vectors

of

all

ones.

Since both Y

and Y¯ are invertible, we can form both of the Schur complements X {Y and X {Y¯ . With these in

hand, we can write

wX

 Y

" pX {Y¯ q´1p1Y

´ Y,Y¯ wY¯ q

(9)

wX

 Y¯

" pX {Y q´1p1Y¯

´ YT,Y¯ wY q,

(10)

where wY

and wY¯

are the weight vectors for Y

and Y¯

respectively,

and

wX

 Y

is the weight vector of

X, restricted to those indices corresponding to Y . Thus if we know wY and wY¯ , equations 9 and 10

give a way to compute wX .

Next, for finite sets Y  X  Rn we wish to calculate either the weight vector wX or wY given the

other.

Definition 9.

For a block matrix M

:"

,,A C

B D

with

A

invertible,

define

M A

:"

,,A´1BpM {Aq´1CA´1 ´pM {Aq´1CA´1

´A´1BpM {Aq´1 pM {Aq´1



.

Recall that for a block matrix M as in Definition 9,

M ´1

"

,,A´1 0

0 0

`

M

A

.

(11)

Definition 10. For Y  X  Rn finite sets, assume X is in block matrix format as in Equation 8. Define the matrix
XY " X Y
where XY is taken to be the zero matrix when Y " X, and XY is taken to be X when Y " H. Lemma 11. For finite sets Y  X  Rn, let PXY be a permutation matrix such that

Then

PXY X PXY

"

,, Y YT Y¯

Y Y¯  Y¯ .

wX

"

PXY

,,wY 0



`

PXY

XY

1,

and

MagpXq " MagpY q ` 1T XY 1.

Proof. Set M " PXY X PXY , employ Eqn. 11, and multiply on the right by 1.

We can now calculate the weight vector of X Y Y where X and Y are not necessarily disjoint. This can be viewed as an inclusion-exclusion principle that applies to weight vectors as well as magnitude.
Theorem 12. For finite sets X, Y  Rn, set Z " X Y Y . Then we have

wZ

"

PZX

^,,wX  0

`


ZX 1

`

PZY

^,,wY 0



`

ZY


1

´ PZXXY

^,,wXXY 0



´

Z

X

XY


1

,

and

MagpZq " MagpXq ` MagpY q ´ MagpX X Y q
` 1T ZX 1 ` 1T ZY 1 ´ 1T ZXXY 1.

Proof. This follows by applying Lemma 11 to each subset considered, e.g.

wZ

"

PZX

,,wX  0

`

PZX ZX 1.

13

Figure 5: Each image depicts the weighting vector or an approximation computed for the moons
[24] data set consisting of 10,000 points. Error computed is the l2 norm of the difference between the approximating vector and w. The left image shows the full weighting vector w with t " 50. The center image shows the approximation by 1{nf with again t " 50. The right image shows approximation by 1{nf~ with ball search radius 0.03 using the l8 norm.

B.2 Proof of theorem 7 Theorem. For X scattered, and x P X,


wpxq ´ 

1

 

nf

pxq

 



npn ´ 1q2e´2 ` npn ´ 1qe´ 1 ´ pn ´ 1qe´

,

(12)

where f is the kernel density associated with the Laplacian kernel X .

Proof. For a, b P X, write µpa, bq :" X´1pa, bq. Let denote the smallest distance between distinct points of X. Proposition 2.1.3 in [16] gives that for X scattered we can write

where

8
µpa, bq " ÿ p´1qkµkpa, bq
k"0

ÿ

µkpa, bq "

X pa, a1qX pa1, a2q ¨ ¨ ¨ X pak´1, bq

a"a0 ¨¨¨ak "b

where the sum is over all a0, . . . , ak P X with a0 " a, ak " b, and aj´1  aj for all 1  j  k. In particular, µ0 is the identity matrix. The proof of Prop. 2.1.3 in [16] gives that µkpa, bq  ppn ´ 1qe´ qk for all a, b P X. This gives that

8

8

8

µpa, bq " ÿ p´1qkµkpa, bq  ÿ µkpa, bq  ÿ ppn ´ 1qe´

k"0

k"0

k"0

qk

"

1 1 ´ pn ´ 1qe´

.

The final equality due to employing geometric series, since pn ´ 1qe´  1 by assumption of X being scattered. Let w be the weighting vector of X, and note that for a P X

8

8

wpaq " ÿ ÿ p´1qkµkpa, bq " ÿ µ0pa, bq ` ÿ ÿ p´1qkµkpa, bq

b k"0

b

b k"1

8

" 1 ` ÿ ÿ p´1qkµkpa, bq.

(13)

b k"0

14

Then


wpaq 

´

1

 

 nf paq 

"

   

1 nf paq

pnwpaqf

paq

´


1q 



 

1

 

  nf paq 

|nwpaqf paq

´

1|









1ÿ



ÿ



 |nwpaqf paq ´ 1| " nwpaq n

X

pa,

bq

´

1 

"

wpaq

`

wpaq

X

pa,

bq

´

1 



b



ba





|wpaq

´

1|

`

  wpaq 

ÿ
ba

X

pa,

  bq 



  1  

`

ÿ
b

8
ÿ p´1qkµkpa,
k"0

bq

´

  1  

`

   

npn ´ 1qe´ 1 ´ pn ´ 1qe´

   





ÿ ppn ´ 1qe´ q2  npn ´ 1qe´

  b

1 ´ pn ´ 1qe´

  

`

1

´

pn

´

1qe´

"

npn ´ 1q2e´2 ` npn ´ 1qe´ 1 ´ pn ´ 1qe´

.

(14)

For a positive definite kernel K on X having Kpx, xq " 1 for all x P X, denote by Kmax the largest

value

that

K

takes

on

distinct

points

of

X.

Say

X

is

K -scattered

if

Kmax



1 n´1

.

Then

Thm.

7

holds for K with e´ replaced by Kmax. Theorem 7 heavily leverages Prop. 2.1.3 in [16], which in

fact can be modified slightly to apply to any real valued matrix whose diagonal entries are 1, and

whose

non-diagonal

entries

have

absolute

value

strictly

less

than

1 n´1

,

giving

a

nice

formula

for

the

entries of the inverse matrix.

Figure 5 shows the complete weighting vector and two approximation methods outlined in Section 4 for the moons data set [24].

B.3 Weighting vector on graphs

When computing the weighting vector of a graph, one typically constructs a metric space whose points are indexed by the nodes of the graph, and distance derived from the structure of the edges; e.g. by taking the shortest path length between two nodes. We have observed empirically that the resistance distance [15] seems to behave better than the distance given by shortest path length when computing the weighting vector.

We can visualize the inverse relationship between wpq for  some node in the graph, and the number of nearest neighbors to  in the corresponding metric space. In Figure 6, the order of each vertex is plotted with respect to its magnitude. Although order is not exactly proportional to number of nearest neighbors in the metric space, under most reasonable metrics and with a large value of t, it serves as a very good proxy.

Given suitably large t, the graph is scattered­namely, we need t  logpn´1q , where n is the number

of nodes in the graph, and is the minimum distance between any two nodes. As seen in Figures 6

and

7,

the

inverse

relationship

wpxq

«

1 nf pxq

is

clear,

as

predicted

by

Theorem

7.

15

Figure 6: This image depicts number of neighbors (x-axis) vs weighting vector value (y-axis) for each node in the graph. The graph here is an Erdos-Renyi graph [11] with 50 nodes, and 15% connected. The distance metric is resistance distance [15], and we have t " 6, " 0.182. It's generated using the NetworkX [14] package.
Figure 7: A visualization of the same Erdos-Renyi graph [11], where the size of node x is defined by ep4.1qpwpxq´minpwqq. The inverse relationship between magnitude and number of neighbors can clearly be seen.
16

