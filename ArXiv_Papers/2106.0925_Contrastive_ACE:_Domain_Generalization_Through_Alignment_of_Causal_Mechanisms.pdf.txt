Contrastive ACE : Domain Generalization Through Alignment of Causal Mechanisms
Yunqi Wang1, Furui Liu* 2, Zhitang Chen2, Qing Lian3, Shoubo Hu2, Jianye Hao2, and Yik-Chung Wu1 1University of Hong Kong 2Huawei Noah's Ark Lab
3Hong Kong University of Science and Technology

arXiv:2106.00925v1 [cs.LG] 2 Jun 2021

Abstract
Domain generalization aims to learn knowledge invariant across different distributions while semantically meaningful for downstream tasks from multiple source domains, to improve the model's generalization ability on unseen target domains. The fundamental objective is to understand the underlying "invariance" behind these observational distributions and such invariance has been shown to have a close connection to causality. While many existing approaches make use of the property that causal features are invariant across domains, we consider the causal invariance of the average causal effect of the features to the labels. This invariance regularizes our training approach in which interventions are performed on features to enforce stability of the causal prediction by the classifier across domains. Our work thus sheds some light on the domain generalization problem by introducing invariance of the mechanisms into the learning process. Experiments on several benchmark datasets demonstrate the performance of the proposed method against SOTAs.
1. Introduction
The past decades have witnessed the remarkable success of machine learning, especially deep learning models in solving different problems in various fields. However, the performance guarantee of models is under the assumption that the training and testing data are independent and identically distributed, which can be easily violated in realworld applications since the data-generating processes are usually affected by time, environment, experimental conditions, etc. As a result, models that work well on training data may perform poorly on new data unseen in the model
*Furui Liu is the corresponding author

training stage, and thus restrain their deployment for further applications. It is of great interest to learn a domain-robust model that can be generalized to the domains beyond source data. To this end, researchers proposed the domain generalization problem [4], which aims at improving the robustness of models on unseen data (i.e., target domain) by learning from several training datasets (i.e., source domains).
The most straightforward domain generalization approach is the leave-one-out strategy, which defines one as the target domain for testing and the rest as source domains for training. To tackle the challenging problem that no data from target domain is available in training, efforts have been made to extract "invariance" from source domains. A natural idea is to frame the network to extract stable features which yield invariant predictions across domains [19, 36, 24, 37, 43, 2]. Methods under this branch either enforce consistency of the distributions of latent features across source domains [24, 19], or minimize the gradients of the classification loss with respect to latent features [2].
The underlying principle behind these approaches is the postulate that causal features are with certain stability across domains. Approaches try to recover these features utilizing the invariance property. However, recent studies show that the causal mechanism, rather than the distribution of features, is stable across domains [30]. Exploring features by only applying a regularizer to enforce invariance, one may find spurious causal features that are in fact only correlated with the label, leading to instability of the trained models [21]. Besides, by incorporating cross-domain mechanism invariance, one is able to recover the causal mechanism as well as causal features, with good interpretability of the contributions of individual features on the task at hand. This also benefits tasks like troubleshooting and identification of important features.
To this end, we tackle the problem of domain general-

ization from a causal perspective by treating machine learning models as Structural Causal Models (SCM). Instead of aligning the distributions of latent features across domains, we propose a novel constraint based on the causal attributions in networks measured by Average Causal Effect (ACE) [23]. By viewing samples of the same class from different domains as positive pairs and those of different classes as negative pairs, ACE contrastive loss is introduced to regularize the learning procedure and encourage domainindependent attributions of extracted features. The superior experimental results on several benchmark datasets demonstrate the effectiveness of the proposed approach in model generalization compared with several baseline methods and thus show the importance of involving causal attributions in the training of the models.
2. Related Work
2.1. Domain Generalization
Domain generalization remains a challenging yet important problem that has been investigated by many studies in the literature. The classic way of learning models with good generalization ability is to train feature extractors that can generate invariant representations across different source domains. Various methods have been proposed including naive approaches where a single network is trained by directly aggregating all data from source domains together [16], with a designed structure for more robust performance on data of multi-domain distributions [13] or modified optimization algorithms which minimize dissimilarity of features between different domains [17]. Specifically, domain invariant component analysis has been proposed to train models under distribution variations resulted from domain shift [24]. In [11], they leverage the maximum mean discrepancy as the measure to guide training on multi-task auto-encoders, under the principle of aligning source data across domains. Some other works [8, 17, 18] have introduced meta-learning with adaptive regularizers to improve generalization ability. By employing Model-Agnostic Meta-Learning or similar strategies in domain generalization, domain-specific gradients have been normalized [17, 18] and models are encouraged to extract features respecting inter-class relationships [8]. Data augmentation, as utilized in various applications, has been also demonstrated to be effective in domain generalization [25, 45, 46]. Several attempts have been made to enlarge the support of the distributions in training data such as mixing up or blending data points from different domains [42, 44, 45]. Moreover, adversarial data augmentation, as well as several alternatives based on GANs, has also been investigated and shows improvements in addressing domain generalization [5, 29, 39].
To better interpret domain generalization, literature aim-

ing at capturing invariant relations under the structure of causality has emerged. It is argued that causal features with respect to the task, such as shape for classifying objects, are stable and invariant features one wants to learn. However, simply enforcing invariance to train feature extractors without causal considerations, one may obtain only correlated but non-causal features, that are spurious invariant representations for the task. Consider the image classification as an example. A dataset contains a lot of cows on the grass. Feeding them to a model, the grass may also be learned as "invariant" representation, but it is not the causal feature for identifying the cow [32]. To avoid this, the Invariant Causal Prediction (ICP) is first proposed in [28]. It tries to exploit the invariant property of feature set in causality, in the sense that a structural causal model, as well as the invariant distribution of features, are considered. Several latter studies then made improvements by adding intervention on the target variable and attempt to learn invariant predictors or classifiers [22, 35]. By reformulating the optimization problems, Arjovsky et al. [2] introduce invariant risk minimization to distinguish between spurious correlations and the causal ones, which is then extended to nonlinear settings by [1].
2.2. Causal Neural Network Attribution
Causal neural network attribution refers to the causal effect of a specific input feature on output prediction in neural networks, which aims to quantify inherent causal influences in machine learning [23]. Most attribution-based studies [3, 23, 34] have focused on applying overall functional values to define the contribution of input features, while some other methods leverage the gradients or perturbations with occlusion maps to identify the effect of different features [33]. However, these types of methods are prone to artifacts, which are unlikely to be measured accurately due to nonidentifiability of the errors. Specifically, they can be treated as approaches for estimating individual causal effect, which fail to consider the complicated interactions among neurons and thus result in a biased measure of the importance of the input feature.
In a recent work [6], a new unbiased attribution method, called Average Causal Effect (ACE), has been proposed to calculate causal attribution. This metric is derived based on the first principles of causality [27]. Specifically, structural causal models are leveraged by interpreting the original networks as acyclic graphs where higher layers are generated through a hierarchy of interactions on nodes from lower layers and the operator do(), known as do-calculus, is also used [14]. Do-calculus or intervention in causality literature refers to the artificial perturbation on some variables of the system, expecting to measure their causal influences on others. When one applies an intervention to a variable, it is set to the fixed value. Tracking the system under this condition,

the distributions of other variables belonging to the system then are called interventional distribution. The causal effect of the intervened variable on others is defined mathematically based on the interventional distributions [27]. In a similar way, ACE is defined as the subtraction between the expectation of the output when a particular input feature is under intervention, and a baseline output when the same feature is uniformly perturbed in a fixed interval of values.

3. Methods

A domain is defined as a joint distribution over space X × Y, where X and Y denote the input and label space, respectively. The training data D in domain generalization consists of several data sets, each of which contains independent and identically distributed instances sampled from one domain. A na¨ive way to tackle the distribution shift across domains is to aggregate instances from all domains and conduct model training. Suppose there are m instances in total after combining all source data, written as D = {(xi, yi)}m i=1. The corresponding Empirical Risk Minimization (ERM) loss is

m

L(D; , ) =

(g (f(xi)) , yi)

(1)

i=1

where is an appropriate loss function, f : X  Z de-

notes an encoding model parameterized by  that maps the

raw input (image) to a latent feature vector, and g : Z  Y denotes a model parameterized by  that maps the latent

feature vector to the output label. Z is the space of latent

features and

zi = f(xi),

(2)

is the encoding of the observation xi. To avoid over-fit to the training domains, several differ-
ent regularizers have been used in addition to the loss as penalty for reducing domain gaps in the space of latent features [24, 19]. Unlike minimizing the cross-domain distance directly in the space of latent features, we provide a novel perspective from causality and impose the invariance on the mechanism for all environments. The basic idea is that the true underlying causal mechanisms that map features to labels are cross-domain invariant. It only depends on class but does not depend on the domain index. For samples in the same class, the true causal mechanism from features to label is similar. However, when the sample is with another class, the mechanism shifts. We design a quantification of the mechanism and use a contrastive loss to enforce this principle in structure learning. Our framework does not rely on domain labels, similar to ERM that collects samples from multiple domains and aggregates them together as the training data. This is an advantage over most of the domain generalization methods, where the domain indexes are essential for representation learning. Thus, our method

is applicable to improve the model's generalization ability to unseen domains even when all training data is within a single domain.
Some theoretical ties are linking the neural networks and the causal models [6]. By viewing the domain generalization problem from the causal perspective, one deems all datasets as generated from a typical causal framework, known as Structural Causal Model (SCM). Denote N (l1, l2, . . . , lT ) by a network of l-layers and lt  L = {l1, l2, . . . , lT } be the set of neurons in the tth layer. For neuron L, the set of functions defining causal mechanisms is represented by , and the set of exogenous random variables often considered as unobserved common causes are represented by U . The corresponding SCM thus can be expressed as function fSCM(L, U, , PU ) with PU referring to the probability distribution of exogenous random variables in set U . By interpreting the network N (l1, l2, . . . , lT ) as directed acyclic graphs, SCM constructs a hierarchical model which generates outputs of interactions between nodes from lower layers [6]. The flexibility of neural networks also raises confidence in the success of the task of using the neural model to capture the causal mechanism from observational data.

3.1. Average Causal Effect
To identify the causal mechanisms of the task, it is of need to quantify the causal effect of each input feature to the output. Recall that the f(xi) is an n dimensional vector. Correspondingly, g is a neural network with n input neurons and an output neuron for each class label. We use g as the causal quantification model, which consists of input neurons {zj}nj=1 and output neuron y. The causal attribution of the neuron zj, corresponding to the jth feature, on the output y is defined as the average causal effect cydo(zj=) with value  , which can be calculated as the subtraction between interventional expectation of y when zj =  and a baseline of zj [6]
E E E cydo(zj=) = y|do(zj = ) - zj [y|do(zj = )] .
(3) The interventional value  can be set to any value in the input domain of zj as

[lowj, highj].

(4)

When not intervened, the input neuron zj is assumed to be uniformly distributed between lowj and highj. Specifically,
E the term [y|do(zj = )], known as the interventional ex-
pectation of output neuron y condition on the intervention
operation do(zj = ), is defined as

E[y|do(zj = )] = y · p(y|do(zj = ))dy. (5)
y

Figure 1: The framework of our method. For each observational image xi, it first generates the features by the featurizer (or encoder), implemented by residual neural nets as zi = f(xi). g takes the features to generate the label. The ACE vector ci can be computed given the model g, and the contrastive ACE loss for xi is obtained from the distance between its ACE vector ci, and the one (cq(Pi) or cq(Ni)) computed using random samples from its positive and negative sets.

The average interventional expectation of y with respect to
E E zj, zj [ [y|do(zj = )]] is used as the baseline value of
zj, i.e.

E E zj [ [y|do(zj = )]] =

highj

p(zj) · y · p(y|do(zj = ))dydzj, (6)

lowj

y

which has been demonstrated to be unbiased [6]. Hence, the causal attribution of a feature neuron zj to an output label y can be quantified by ACE cydo(zj=).
3.2. Contrastive ACE

Inspired by the contrastive representation learning [7], we propose a new objective function named contrastive ACE loss, to evaluate the difference between the ACE values of the input feature on output y across domains. We first introduce the ACE vector as a quantification of the causal influences of all features on the label of the ith sample. The feature of the ith sample is a n-dimensional vector as

zi = f(xi) = [zi1, zi2, ..., zin].

(7)

The ACE of its jth feature on the label y is defined as

cji

=

cy
do(zj

=zij

)

.

(8)

Going through all dimensions, we get a ACE vector of the

ith sample

ci = [c1i , c2i , ...cni ].

(9)

Figure 2: Triplet generation: for one observational image xi with label yi, its positive set Pi consists of images that are in the same class as yi, and its negative set Ni consists of images that are with a class different from yi.
An illustration of obtaining the ACE vector is in Fig. 3. To match the ACE of an input to an output for all instances of the same class across domains, the contrastive ACE loss is optimized by minimizing the distance between inputs of the same class and maximizing those from different classes. We treat the inputs of the same class as the positive matches

Algorithm 1: ACE contrastive learning
Input: Data D = {(xi, yi)}m i=1, parameter  and . Output: Optimal Network. 1 Initialize f, g; 2 for i = 1 to m do 3 Construct Pi and Ni as Eq. 10 and 11
respectively. 4 end 5 while Not converged do 6 Compute the loss as Eq. 14; 7 Update  and  by gradient descent;
8 end 9 return f, g;

Figure 3: Computation of the ACE vector. Given the fea-
tures of a sample zi = f(xi), the ACE quantification of the classifier g with n input neurons {zj}nj=1 is a vector ci, which is generated by treating the jth neuron intervened as do(zj = zij).

of ith sample, and the ones of different classes as negative samples. The positive sets and negative sets for the ith sample are

Pi = {k|yi = yk, k = i},

(10)

Ni = {k|yi = yk, k = i},

(11)

respectively. An intuitive example is also depicted in Fig. 2, taking rotated MNIST dataset as an example.
A direct way to measure the overall pairwise distance between ACE vectors is to calculate a distance averaged over all samples in the whole set, which is under extremely heavy computational workloads when the number of samples is large. Thus, we use an efficient random sampling technique. Denote q(Pi) an index sampled uniformly at random from the set Pi, and q(Ni) an index sampled uniformly at random from the set Ni. The contrastive ACE loss A,(xi) is then defined as

max dist(ci, cq(Pi)) - dist(ci, cq(Ni)) + , 0 , (12)
and  > 0 is a small margin variable [38]. The loss A,(xi) becomes large when dist(ci, cq(Pi)) is large, or dist(ci, cq(Pi)) is very small. Thus, it penalizes the interclass dissimilarity and intra-class similarity. The margin variable here is used to reduce the non-robustness brought by the max operation, avoiding cases that the dist(ci, cq(Pi)) - dist(ci, cq(Ni)) is always below 0 and

never penalized. The distance we use is the Manhattan Distance between the pair of vectors as

n

dist(ci, cj) = |ci - cj|M = |cri - crj |.

(13)

r=1

Combined with the ERM original loss in Eq. 1, our loss with weighting parameter  can be written as

m

m

LA(D; , ) =

(g(f(xi)), yi) +  A,(xi).

i=1

i=1

(14)

We show the whole training framework in Fig. 1, and the

pseudo-code of the method in Alg. 1. Intuitively, our struc-

tural loss originates from the principle that the causal mech-

anism or structural causal model from features to labels is

class-dependent, but domain-independent, or cross-domain

stable. Given an observation, the ACE vector is a quantifica-

tion of its features' influence on its label. For samples that

are within the same class, we minimize the gap between

their quantification vectors; but for samples that are with

different classes, a larger gap is preferred. The loss that ex-

plicitly addresses the principle is designed in a contrastive

way that positive and negative pairs are used. Incorporating

this in the training procedure, we expect our model to re-

cover the true invariant causal structure, which can achieve

stable performance in the presence of domain shifts.

4. Experiments
In this section, we perform experiments to test the performance of our methods on several benchmark datasets, including simulated dataset (Rotated MNIST [11]) and realworld datasets (PACS [16], VLCS [9]). We compare our method with a set of domain generalization approaches. Out of them, ERM is without using the domain indexes, and all other methods take use of the domain indexes. Accuracy is the main metric being compared.

Table 1: Accuracy of Rotated MNIST dataset on target domains from 0 to 75. *This result is obtained from normalized MNIST dataset.

Method

0

15

30

45

60

75

Avg Domain Label

IRM [2]

96.0 ± 0.2 98.9 ± 0.0 99.0 ± 0.0 98.8 ± 0.1 98.9 ± 0.1 95.7 ± 0.3 97.9

DRO [31]

96.2 ± 0.1 98.9 ± 0.0 99.0 ± 0.1 98.7 ± 0.1 99.1 ± 0.0 96.8 ± 0.1 98.1

Mixup [43]

95.8 ± 0.3 98.9 ± 0.1 99.0 ± 0.1 99.0 ± 0.1 98.9 ± 0.1 96.5 ± 0.1 98.0

MLDG [40] CORAL [36]

96.2 ± 0.1 96.4 ± 0.1

99.0 ± 0.0 99.0 ± 0.0

99.0 ± 0.1 99.0 ± 0.1

98.9 ± 0.1 99.0 ± 0.0

99.0 ± 0.1 98.9 ± 0.1

96.1 ± 0.2 96.8 ± 0.2

98.0 98.2

required

MMD [19]

95.7 ± 0.4 98.8 ± 0.1 98.9 ± 0.1 98.8 ± 0.1 99.0 ± 0.0 96.3 ± 0.2 97.9

DANN [10]

96.0 ± 0.1 98.8 ± 0.1 98.6 ± 0.1 98.7 ± 0.1 98.8 ± 0.1 96.4 ± 0.1 97.9

CDANN [20]

95.8 ± 0.2 98.8 ± 0.0 98.9 ± 0.0 98.6 ± 0.1 98.8 ± 0.1 96.1 ± 0.2 97.8

ERM [37]

96.0 ± 0.2 98.8 ± 0.1 98.8 ± 0.1 99.0 ± 0.0 99.0 ± 0.0 96.8 ± 0.1 98.1

Contrastive-ACE (ours) 96.4 ± 0.3 98.7 ± 0.2 99.4 ± 0.1 99.4 ± 0.1 99.4 ± 0.1 97.4 ± 0.2 98.5 not required

Contrastive-ACE*

97.4 ± 0.2 99.5 ± 0.1 99.6 ± 0.1 99.6 ± 0.1 99.5 ± 0.1 98.0 ± 0.1 98.9

4.1. Experimental Settings
We mostly follow the model set up in the paper [12]. The models are trained on source domains that are generated from training dataset and evaluated on the target domain which is generated from testing dataset. Source and target domains are generated by the leave-one-out strategy, that one domain is the test and others are as training domains. When image data is the input, the domain generalization models contain the encoder f and the classifier g. For fair comparisons, all models are with the same f and g as the DomainBed [12]. f is a 4-layer CNN with residual network and pooling layers (small net for Rotated MNIST and big net for other two datasets), and g is a linear FC net. For model selection, we use the test-domain-validation-set, where a validation set that follows the distribution of the test domain is used to select the best model. The training epoch is fixed to be 100 with batch size 64. Adam optimizer is used without weight decay, with a learning rate to be 0.001. The hyperparameters of our methods, namely the weight of the ACE regularizer and the margin variable, are  = 1 and  = 0.05. The experiments are run 2 times for each dataset, and the average performance, as well as its statistical variation, are reported.
4.2. Experiments on Rotated MNIST
This dataset is an artificial dataset constructed from the popular MNIST handwritten digit sets. It contains grayscale MNIST handwritten digits with different rotations, with a degree from 0 to 75, with 15 as one step interval. The images that are with the same degree of rotation thus naturally form one domain, so that each domain is indexed by the rotation angle.
As reported in Table 1, we obtain an average accuracy of 98.5%, which is the best among all other approaches. An interesting observation is that the approaches without the need for domain labels are, in general, with better performance

Figure 4: Performance on target domain. The orange curve records the historical model accuracy when the training algorithm is ERM, and the blue curve records our results.
compared to the ones that need domain labels. When the domain of 0 rotation is used as the testing domains, methods, in general, perform slightly worse than other settings. Recently, debating about the role of normalization emerges [15] and we also explore its effect on the performance of our approaches. With a simple mean-std normalization of the data, we find that our ACE-based approach achieves a higher accuracy of 98.9%. This is possibly because that the normalized data is with a more stable range, which is less sensitive to additive noises and thus with a ground for making better ACE estimation and recovery of the causal mechanism.
To make an in-depth analysis of the training procedure, we plot the accuracy on testing domains of the models trained by ERM and ACE in Fig. 4. We observe that in the first 10 training epochs, ERM performs much similar to ours, with no obvious difference in-between. Our model clearly outperforms ERM as the training proceeds. This is because that in the initial exploration stage, the structure of neural networks are unstable, and the causal mechanism and features are not recovered to a satisfactory degree. However, when it approaches a relatively mature stage, ACE takes its effect to help guide the model to discover causal

Table 2: Model accuracy on VLCS dataset on target domains.

Method

C

L

S

V

Avg Domain Label

IRM [2]

97.6 ± 0.5 64.7 ± 1.1 69.7 ± 0.5 76.6 ± 0.7 77.2

DRO [31]

97.8 ± 0.0 66.4 ± 0.5 68.7 ± 1.2 76.8 ± 1.0 77.4

Mixup [43]

98.3 ± 0.3 66.7 ± 0.5 73.3 ± 1.1 76.3 ± 0.8 78.7

MLDG [40] CORAL[36]

98.4 ± 0.2 98.1 ± 0.1

65.9 ± 0.5 67.1 ± 0.8

70.7 ± 0.8 70.1 ± 0.6

76.1 ± 0.6 75.8 ± 0.5

77.8 77.8

required

MMD [19]

98.1 ± 0.3 66.2 ± 0.2 70.5 ± 1.0 77.2 ± 0.6 78.0

DANN [10]

98.2 ± 0.3 67.8 ± 1.1 74.2 ± 0.7 80.1 ± 0.6 80.1

CDANN [20]

98.9 ± 0.3 68.8 ± 0.6 73.7 ± 0.6 79.3 ± 0.6 80.2

ERM [37] Contrastive-ACE (ours)

97.7 ± 0.3 98.5 ± 0.5

65.2 ± 0.4 65.1 ± 0.3

73.2 ± 0.7 70.9 ± 1.2

75.2 ± 0.4 77.7 ± 0.6

77.8 78.1

not required

(a) Domain C

(b) Domain L

(c) Domain S

(d) Domain V

Figure 5: Four samples of the class "bird" from C, L, S, V four domains. Notice the difference in the proportion of space that the objects occupy in whole the image.

features so that stable performance is achieved.
4.3. Experiments on VLCS
As one of the classic benchmark datasets for domain generalization, VLCS collects natural images from four datasets, i.e. PASCAL VOC2007 (V), LabelMe (L), Caltech (C), and SUN09 (S), and contains a total of five classes for recognition task (bird, car, chair, dog and person). The images in VLCS are all collected from the real world, which have larger intra-class variance and significantly higher domain shift compared to the simulated datset such as Rotated

MNIST. The task of domain generalization thus becomes much more challenging.
As reported in Table 2, we achieve an average classification accuracy of 78.1%, which is the best among the approaches that do not require domain labels. We observe a huge difference in the performance across different domains (from 98.5% to 65.1%), which indicates the large distribution shift across domains. As opposed to Rotated MNIST, the approaches that require domain labels perform generally better than those do not in VLCS dataset. It is in accordance with natural intuition that domain information brought by labels makes much more critical impact when handling datasets of larger distribution shifts. Compared with the increase in accuracy of ERM (+0.4%) in Rotated MNIST, the improvement of aligning causal mechanism in VLCS is diminished. As complex real-world images contain more complicated and diverse features than simulated images, it is more difficult to infer the causal relationship between features and predictions, despite the presence of the contrastive ACE penalty.
4.4. Experiments on PACS
PACS dataset has recently been widely adopted as a benchmark dataset for domain generalization, which is even more challenging than VLCS. A total 7 classes of images (dog, elephant, giraffe, guitar, house, horse, and person) from 4 different domains (art painting, cartoon, photo, and sketch) are included. PACS is considered to have a significantly higher domain shift than VLCS, which attributes to the large difference in style. As shown in Figure 6, the objects in PACS dataset are better positioned by taking a large portion of the image and well centralized compared to those in VLCS dataset.
As reported in Table 3, we achieve an average classification accuracy of 87.3%, which is the best among all the methods, including the ones that use domain labels. It is interesting to observe that, although the inter-source domain divergence in PACS dataset is considered to be larger than

Table 3: Model accuracy on PACS dataset on target domains.

Method

A

C

P

S

Avg Domain Label

IRM[2]

85.7 ± 1.0 79.3 ± 1.1 97.6 ± 0.4 75.9 ± 1.0 84.6

DRO [31]

88.2 ± 0.7 82.4 ± 0.8 97.7 ± 0.2 80.6 ± 0.9 87.2

Mixup [43]

87.4 ± 1.0 80.7 ± 1.0 97.9 ± 0.2 79.7 ± 1.0 86.4

MLDG [40] CORAL [36]

87.1 ± 0.9 87.4 ± 0.6

81.3 ± 1.5 82.2 ± 0.3

97.6 ± 0.4 97.6 ± 0.1

81.2 ± 1.0 80.2 ± 0.4

86.8 86.9

required

MMD [19]

87.6 ± 1.2 83.0 ± 0.4 97.8 ± 0.1 80.1 ± 1.0 87.1

DANN [10]

86.4 ± 1.4 80.6 ± 1.0 97.7 ± 0.2 77.1 ± 1.3 85.5

CDANN [20]

87.0 ± 1.2 80.8 ± 0.9 97.4 ± 0.5 77.6 ± 0.1 85.7

ERM [37] Contrastive-ACE (ours)

87.8 ± 0.4 88.8 ± 1.3

82.8 ± 0.5 81.9 ± 1.2

97.6 ± 0.4 97.7 ± 0.2

80.4 ± 0.6 80.6 ± 0.3

87.2 87.3

not required

(a) Domain A

(b) Domain C

(c) Domain P

(d) Domain S

Figure 6: Four samples of the class "dog" from A, C, P, S four domains. The objects in four domains all occupy a large proportion in the image and are well centralized. The distribution shift across domains attributes to the completely style shifts.

that in VLCS dataset [41] [26], the difference in the model's generalization performance is relatively small compared to VLCS dataset. This is because the representations extracted by the featurizer contain more information related to the object rather than from the background. For example, a large proportion of features from Figure 5c might contain patterns depicting the sun and the sea while neglecting the bird object. On the contrary, the featurizer may be easier to extract causal features from the dog in Figure 6b.

Model's performance when domain P is used for testing and others for training are superior compared to other settings, owing to the featurizer realized by the backbone of ResNet-50 pretrained on ImageNet, which is comprised of real-world photos from domain P. The bias brought by featurizer (the pretrained backbone), still influences the generalization capability of the domain models. Although reduced, this bias cannot be completely removed even we use ACE contrastive learning. It remains a challenging problem that hasn't been well addressed in existing literatures.
Methods that impose cross-domain invariant representations aim at extracting features that contain domainindependent information while eliminating domain-specific information like styles. However, simply enforcing invariance, one may obtain "over-fixed" patterns without crossdomain flexibility that is beneficial for classification. This leads to inferior classification accuracies of their methods. Instead of enforcing learning domain-invariant features, the proposed contrastive ACE aligns the causal mechanism quantified by the ACE of latent representations to predictions, with room for reasonable variations among patterns. This is possibly the reason that we get better features for the task at hand.
5. Conclusion
In this paper, we provide a novel perspective on domain generalization by making use of the causal invariance between the average causal effect of the latent representations to the labels. By assuming the mechanism to be label-dependent but domain-independent, we align causal quantification vectors of samples. A novel contrastive ACE loss is introduced into the training to enforce cross-domain stability in predictions. Without using domain labels, our method still achieves good performance on benchmark datasets compared to SOTAs. The feasibility and effectiveness are demonstrated by extensive experiments. To the best of our knowledge, this work presents the first investigation on aligning causal mechanisms across do-

mains in the learning process to address domain generalization. We expect that it can motivate researchers to explore along this research line.
References
[1] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In International Conference on Machine Learning, pages 145­ 155. PMLR, 2020.
[2] Martin Arjovsky, Le´on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.
[3] Sebastian Bach, Alexander Binder, Gre´goire Montavon, Frederick Klauschen, Klaus-Robert Mu¨ller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.
[4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24:2178­2186, 2011.
[5] Fabio M Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2229­2238, 2019.
[6] Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, and Vineeth N Balasubramanian. Neural network attributions: A causal perspective. arXiv preprint arXiv:1902.02302, 2019.
[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, Febrary 2020.
[8] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. Advances in Neural Information Processing Systems, 32:6450­6461, 2019.
[9] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657­1664, 2013.
[10] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096­2030, 2016.
[11] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pages 2551­2559, 2015.
[12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.

[13] Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A Efros, and Antonio Torralba. Undoing the damage of dataset bias. In European Conference on Computer Vision, pages 158­171. Springer, 2012.
[14] Murat Kocaoglu, Christopher Snyder, Alexandros G Dimakis, and Sriram Vishwanath. Causalgan: Learning causal implicit generative models with adversarial training. arXiv preprint arXiv:1709.02023, 2017.
[15] Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, and Kilian Q Weinberger. On feature normalization and data augmentation. arXiv preprint arXiv:2002.11102, 2020.
[16] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542­5550, 2017.
[17] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for domain generalization. arXiv preprint arXiv:1710.03463, 2017.
[18] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In Proceedings of the IEEE International Conference on Computer Vision, pages 1446­1455, 2019.
[19] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5400­5409, 2018.
[20] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 624­639, 2018.
[21] Chaochao Lu, Yuhuai Wu, Jos´e Miguel Herna´ndez-Lobato, and Bernhard Scho¨lkopf. Nonlinear invariant risk minimization: A causal approach. arXiv preprint arXiv:2102.12353, 2021.
[22] Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions. arXiv preprint arXiv:1707.06422, 2017.
[23] Gre´goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert Mu¨ller. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition, 65:211­222, 2017.
[24] Krikamol Muandet, David Balduzzi, and Bernhard Scho¨lkopf. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pages 10­18, 2013.
[25] Oren Nuriel, Sagie Benaim, and Lior Wolf. Permuted adain: Enhancing the representation of local cues in image classifiers. arXiv preprint arXiv:2010.05785, 2020.
[26] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh AP. Discrepancy minimization in domain generalization with generative nearest neighbors. arXiv preprint arXiv:2007.14284, 2020.
[27] Judea Pearl. Causality. Cambridge university press, 2009.

[28] Jonas Peters, Peter Bu¨hlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical Methodology), pages 947­1012, 2016.
[29] Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, and Sridha Sridharan. Multi-component image translation for deep domain generalization. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 579­588. IEEE, 2019.
[30] Mateo Rojas-Carulla, Bernhard Scho¨lkopf, Richard Turner, and Jonas Peters. Invariant models for causal transfer learning. The Journal of Machine Learning Research, 19(1):1309­1342, 2018.
[31] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2019.
[32] Zheyan Shen, Peng Cui, Kun Kuang, Bo Li, and Peixuan Chen. Causally regularized learning with agnostic data selection bias. In Proceedings of the 26th ACM international conference on Multimedia, pages 411­419, 2018.
[33] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
[34] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vie´gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
[35] Adarsh Subbaswamy, Bryant Chen, and Suchi Saria. A universal hierarchy of shift-stable distributions and the tradeoff between stability and performance. arXiv preprint arXiv:1905.11374, 2019.
[36] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443­450. Springer, 2016.
[37] Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural information processing systems, pages 831­838, 1992.
[38] Daniel Ponsa Vassileios Balntas, Edgar Riba and Krystian Mikolajczyk. Learning local feature descriptors with triplets and shallow convolutional neural networks. In Edwin R. Hancock Richard C. Wilson and William A. P. Smith, editors, Proceedings of the British Machine Vision Conference (BMVC), pages 119.1­119.11. BMVA Press, September 2016.
[39] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. arXiv preprint arXiv:1805.12018, 2018.
[40] Bailin Wang, Mirella Lapata, and Ivan Titov. Meta-learning for domain generalization in semantic parsing. arXiv preprint arXiv:2010.11988, 2020.
[41] Shujun Wang, Lequan Yu, Caizi Li, Chi-Wing Fu, and Pheng-Ann Heng. Learning from extrinsic and intrinsic supervisions for domain generalization. In European Conference on Computer Vision, pages 159­176. Springer, 2020.

[42] Yufei Wang, Haoliang Li, and Alex C Kot. Heterogeneous domain generalization via domain mixup. In ICASSP 20202020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3622­3626. IEEE, 2020.
[43] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with domain mixup. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6502­6509, 2020.
[44] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020.
[45] Ling Zhang, Xiaosong Wang, Dong Yang, Thomas Sanford, Stephanie Harmon, Baris Turkbey, Holger Roth, Andriy Myronenko, Daguang Xu, and Ziyue Xu. When unseen domain generalization is unnecessary? rethinking data augmentation. arXiv preprint arXiv:1906.03347, 2019.
[46] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization. In European Conference on Computer Vision, pages 561­578. Springer, 2020.

