arXiv:2106.00515v1 [cs.CV] 28 May 2021

KVT: k-NN Attention for Boosting Vision Transformers
Pichao Wang, Xue Wang, Fan Wang, Ming Lin, Shuning Chang, Wen Xie, Hao Li, Rong Jin Alibaba Group
{pichao.wang,xue.w}@alibaba-inc.com {fan.w,ming.l,shuning.csn,ross.xw,lihao.lh,jinrong.jr}@alibaba-inc.com
Abstract
Convolutional Neural Networks (CNNs) have dominated computer vision for years, due to its ability in capturing locality and translation invariance. Recently, many vision transformer architectures have been proposed and they show promising performance. A key component in vision transformers is the fully-connected selfattention which is more powerful than CNNs in modelling long range dependencies. However, since the current dense self-attention uses all image patches (tokens) to compute attention matrix, it may neglect locality of images patches and involve noisy tokens (e.g., clutter background and occlusion), leading to a slow training process and potential degradation of performance. To address these problems, we propose a sparse attention scheme, dubbed k-NN attention, for boosting vision transformers. Specifically, instead of involving all the tokens for attention matrix calculation, we only select the top-k similar tokens from the keys for each query to compute the attention map. The proposed k-NN attention naturally inherits the local bias of CNNs without introducing convolutional operations, as nearby tokens tend to be more similar than others. In addition, the k-NN attention allows for the exploration of long range correlation and at the same time filters out irrelevant tokens by choosing the most similar tokens from the entire image. Despite its simplicity, we verify, both theoretically and empirically, that k-NN attention is powerful in distilling noise from input tokens and in speeding up training. Extensive experiments are conducted by using ten different vision transformer architectures to verify that the proposed k-NN attention can work with any existing transformer architectures to improve its prediction performance.
1 Introduction
Traditional CNNs provide state of the art performance in vision tasks, due to its ability in capturing locality and translation invariance, while transformer [47] is the de-facto standard for natural language processing (NLP) tasks thanks to its advantages in modelling long-range dependencies. Recently, various vision transformers [16, 45, 56, 48, 20, 43, 14, 61, 55, 35, 50, 24, 46, 53, 27] have been proposed by building pure or hybrid transformer models for visual tasks. Inspired by the transformer scaling success in NLP tasks, vision transformer converts an image into a sequence of image patches (tokens), with each patch encoded into a vector. Since self-attention in the transformer is position agnostic, different positional encoding methods [16, 11, 14] have been developed, and in [50, 8] their roles have been replaced by convolutions. Afterwards, all tokens are fed into stacked transformer encoders for feature learning, with an extra CLS token [16, 45, 14] or global average pooling (GAP) [35, 8] for final feature representation. Compared with CNNs, transformer-based models explicitly exploit global dependencies and demonstrate comparable, sometimes even better, results than highly optimised CNNs [22, 41].
The first two authors contribute equally.
Preprint. Under review.

Albeit achieving its initial success, vision transformers suffer from slow training. One of the key culprits is the fully-connected self-attention, which takes all the tokens to calculate the attention map. The dense attention not only neglects the locality of images patches, an important feature of CNNS, but also involves noisy tokens into the computation of self-attention, especially in the situations of cluttered background and occlusion. Both issues can slow down the training significantly [12, 14]. Recent works [55, 50, 8] try to mitigate this problem by introducing convolutional operators into vision transformers. Despite encouraging results, these studies fail to resolve the problem fundamentally from the transformer structure itself, limiting their success. In this study, we address the challenge by directly attacking its root cause, i.e. the fully-connected self-attention.
To this end, we propose a sparse attention, dubbed k-NN attention, to replace the fully-connected attention. Specifically, we do not use all the tokens for attention matrix calculation, but only select the top-k similar tokens from the sequence for each query token to compute the attention map. The proposed k-NN attention not only naturally inherits the local bias of CNNs as the nearby tokens tend to be more similar than others, but also builds the long range dependency by choosing the most similar tokens from the entire image. Compared with convolution operator which is an aggregation operation built on Ising model [37] and the feature of each node is aggregated from nearby pixels, in the k-NN attention, the aggregation graph is no longer limited by the spatial location of nodes but is adaptively computed via attention maps, thus, the k-NN attention can be regarded as a relieved version of local bias. Despite its simplicity, we verify, both theoretically and empirically, that k-NN attention is effective in distilling noisy tokens and speeding up training of vision transformers. Ten different available vision transformer architectures are adopted to verify the effectiveness of the proposed k-NN attention.
2 Related Work
2.1 Sparse Attention
Self-attention [47] has demonstrated promising results on NLP related tasks, and is making breakthroughs in speech and computer vision. For time series modeling, self-attention operates over sequences in a step-wise manner. Specifically, at every time-step, self-attention assigns an attention weight to each previous input element and uses these weights to compute the representation of the current time-step as a weighted sum of the past inputs. Even though promising results have been achieved, it comes at a price: the time and space complexity is quadratic in the length of the sequence. To deal with this problem, many efficient transformers [44] have been proposed. Among these efficient transformers, sparse attention and local attention are one of the main streams, which are highly related to our work. Sparse attention can be further categorized into data independent (fixed) sparse attention [9, 25, 2, 57] and content-based sparse attention [13, 38, 28, 42]. Local attention [36, 34, 35] mainly considers attending only to a local window size. Our work is also content-based sparse attention, but compared with previous works [13, 38, 28, 42], our k-NN attention has its merits for vision domain. For example, compared with routing transformer [38] that clusters both queries and keys, our k-NN attention equals only clustering keys by assigning each query as the cluster center, making the quantization more continuous which is a better fitting of image domain; compared with reformer [28] which adopts complex hashing attention that cannot guarantee each bucket contain both queries and keys, our k-NN attention can guarantee that each query has number k keys for attention computing. In addition, our k-NN attention is also a generalized local attention, but compared with local attention, our k-NN attention not only enjoys the locality but also empowers the ability of global relation mining.
2.2 Transformer for Vision
Transformer [47] is an effective sequence-to-sequence modeling network, and it has achieved stateof-the-art results in NLP tasks with the success of BERT [15]. Due to its great success, it has also be exploited in computer vision community, and the `CNN + Transformer' becomes a popular paradigm [3, 49, 7, 62, 31, 32, 21]. ViT [16] leads the other trend to use pure transformer for vision tasks [23, 30, 54] by dividing the images into patch embedding sequences and feeding them into standard transformers. Even though ViT has been proved compelling in vision recognition, it has several drawbacks when compared with CNNs: large training data, fixed position embedding, rigid patch division, coarse modeling of inner patch feature, single scale, unstable training process,
2

slow speed training, easily fitting data and poor generalization, shallow & narrow architecture, and quadratic complexity. To deal with these problems, many variants have been proposed. For example, DeiT [45] adopts several training techniques and uses distillation to extend ViT to a data-efficient version; CPVT [11] proposes a conditional positional encoding that is adaptable to arbitrary input sizes; CvT [50], CoaT [53] and Visformer [8] safely remove the position embedding by introducing convolution operations; T2T ViT [56], CeiT [55], and CvT [50] try to deal with the rigid patch division by introducing convolution operation for patch sequence generation; TNT [20] proposes the pixel embedding to model the inner patch feature; PVT [48], Swin Transformer [35], ViL [58], CvT [50], PiT [24], LeViT [19], CoaT [53], and Twins [10] adopt multi-scale technique for rich feature learning; DeepViT [61], CaiT [46], and PatchViT [18] investigate the unstable training problem, and propose the re-attention, re-scale and anti-over-smoothing techniques respectively for stable training; to accelerate the convergence of training, ConViT [14], PiT [24], CeiT [55], LocalViT [33] and Visformer [8] introduce convolutional bias to speedup the training; LV-ViT [27] adopts several techniques including MixToken and Token Labeling for better training and feature generation; T2T ViT [56], DeepViT [61] and CaiT [46] try to train deeper vision transformer models; T2T ViT [56], ViL [58] and CoaT [53] adopt efficient transformers [44] to deal with the quadratic complexity. To further exploit the capacities of vision transformer, OmniNet [43], CrossViT [6] and So-ViT [51] propose the dense omnidirectional representations, coarse-fine-grained patch fusion and cross co-variance pooling of visual tokens, respectively. However, all of these works adopt the fully-connected self-attention which will bring the noise or irrelevant tokens for computing and slow down the training of networks. In this paper, we propose an efficient sparse attention, called k-NN attention, for boosting vision transformers. The proposed k-NN attention not only inherits the local bias of CNNs but also achieves the ability of global feature exploitation. It can also speed up the training and achieve better performance.

3 k-NN Attention

3.1 Vanilla Attention

For any sequence of length n, the vanilla attention in the transformer is the dot product attention [47].
Following the standard notation, the attention matrix A  Rn×n is defined as:

QK

A = softmax  ,

(1)

d

where Q  Rn×d denotes the queries while K  Rn×d denotes the keys, and d represents the

dimension. By multiplying the attention weights A with the values V  Rn×d, the new values V^

are calculated as:

V^ = AV .

(2)

The intuitive understanding of the attention is the weighted average over the old ones, where the weights are defined by the attention matrix A. In this paper, we consider the Q, K and V are generated via the linear projection of the input token matrix X:
Q = XWQ, K = XWK , V = XWV ,
where X  Rn×dm , WQ, WK , WV  Rdm×d and dm is the input token dimension.
One shortcoming with fully-connected self-attention is that irrelevant tokens, even though assigned with smaller weights, are still taken into consideration when updating the representation V , making it less resilient to noises in V . This shortcoming motivates us to develop the k-NN attention.

3.2 k-NN Attention
Instead of computing the attention matrix for all the query-key pairs as in vanilla attention, we select the top-k most similar keys and values for each query in the k-NN attention. There are two versions of k-NN attention, as described below.
Slow Version: For the i-th query, we first compute the Euclidean distance against all the keys and values, and then obtain its k-nearest neighbors Nik and Niv from keys and values, and lastly calculate

3

the scaled dot product attention as:

aij = softmax

qikj d

, kj  Nik.

(3)

The shape of final attention matrix is Aknn  Rn×k, and the new values V^knn is the same size of
values V^ . The slow version is the exact definition of k-NN attention, but it is extremely slow because
for each query it has to select different k keys and values.

Fast Version: As the computation of Euclidean distance against all the keys and values for each query is slow, we propose a fast version of k-NN attention. The key idea is to take advantage of matrix multiplication operations. Same as vanilla attention, all the queries and keys are calculated by the dot product, and then row-wise top-k elements are selected for softmax computing. The procedure can be formulated as:

V^ knn = softmax Tk QK

·V,

(4)

d

where Tk (·) denotes the row-wise top-k selection operator:

[Tk(A)]ij =

Aij small enough constant

if Aij is within the top-k largest elements in row j (5) otherwise.

The computational overhead in the fast version can be ignored, and it does not increase the model size. The source codes (four lines) of fast version k-NN attention in Pytorch are shown in Algorithm 1 in supplementary, and the speed comparisons between slow and fast versions are illustrated in section A.2 of supplementary.

3.3 Theoretical Analysis on k-NN Attention

In this section, we will show theoretically that despite its simplicity, k-NN attention is powerful in speeding up network training and in distilling noisy tokens. All the proof of the Lemmas are provided in the supplementary.

Training speed-up and stabilization Compared to CNNs, the fully-connected self-attention is able to capture long range dependency. However, the price to pay is that the dense self-attention model requires to mix each image patch with every other patch in the image, which has potential to mix irrelevant information together, e.g. the foreground patches may be mixed with background patches through the self-attention. This defect could significantly slow down the training process as the goal of visual object recognition is to identify key visual patches relevant to a given class. It is particularly problematic at the beginning of training. This is because, due to the random initialization, we expect a relatively small difference in similarities between patches, which essentially makes self-attention behave like "global average". It will take multiple iterations for the learning algorithm to turn the "global average" into the real function of self-attention.

The k-NN attention resolves this problem by only averaging patterns of most similar ones, making it

stay away from "global average" from the very beginning of training. To see this, we can introduce

the weighted covariate matrix of input patches to further characterize the difference. Let's denote

Varal (x) as the covariate matrix on patches {x1, ..., xn} with probability from l-th row of the attention matrix. The detailed definition of Varal (x) can be found in Appendix. Since k-NN attention only uses patches with large similarity, its Varal (x) will be smaller than that computed from the
fully-connected attention. As indicated in the following Lemma, the gradient of the attention layer

is proportional to variance Varal (x). This implies that fully-connected attention in general will have larger gradients than a k-NN attention, and as a result, it will take a longer iteration for the

fully-connected attention to reduce its initially large gradients to zero, leading to a slower convergence.

In Table 2 and Figure 2, we numerically verify the training efficiency of k-NN attention as opposed

to the fully-connected attention.

Lemma 1. (Informal) Let V^lknn be the l-th row of the V^ knn. We have

V^lknn WQ



Varal (x) and

V^lknn WK



Varal (x).

(6)

The same is true for V^ of the fully-connected self-attention.

4

Noisy patch distillation. As already mentioned before, the fully-connected self-attention model may mix irrelevant patches with relevant ones, particularly at the beginning of training when similarities between relevant patches are not significantly larger than those for irrelevant patches. k-NN attention is more effective in identifying noisy patches by only considering the top k most similar patches. To formally justify this point, we consider a simple scenario where all the patches are divided into two groups, the group of relevant patches and the group of noisy patches. All the patches are sampled independently from unknown distributions. We assume that all relevant patches are sampled from distributions with the same shared mean, which is different from the means of distributions for noisy patches. It is important to know that although distributions for the relevant patches share the mean, those relevant patches can look quite differently, due to the large variance in stochastic sampling. In the following Lemma, we will show that the k-NN attention is more effective in distilling noises for the relevant patches than the fully-connected attention.
Lemma 2 (informal). We consider the self-attention for query patch l. Let's assume the patch xi are bounded with mean µi for i = 1, 2, ..., n, and k is the ratio of the noisy patches in all selected patches. Under mild conditions, the follow inequality holds with high probability:

V^lknn - µlWV

 O(k-1/2 + c1k),


(7)

where c1 is a positive number.

In the above Lemma, the quantity V^lknn - µlWV  measures the distance between V^lknn, represention vector updated by the k-NN attention, and its mean µlWv. We now consider two cases: the normal k-NN attention with appropriately chosen k, and fully-connected attention with k = n.

In the first case, with appropriately chosen k, we should have most of the selected patches coming

from the relevant group, implying a small k. By combining with the fact that k is decently large, we

expect a small upper bound for the distance

V^lknn - µlWV

, indicating that k-NN attention is


powerful in distilling noise. For the case of fully-connected attention model, i.e. k = n, it is clearly

that   1, leading to a large distance between transformed representation V^l and its mean, indicating that fully-connected attention model is not very effective in distilling noisy patches, particularly when

noise is large.

Besides the instance with low signal-noise-ratio, the instance with a large volume of backgrounds can

also be hard. In the next lemma, we show that under a proper choice of k, with a high probability the

k-NN attention will be able to select all meaningful patches.

Lemma 3 (informal). Let M = {i : the patch i is relevant to the query patch xl.}. Under mild conditions, there exist c2  (0, 1) such that with high probability, we have

n

1(qlki



min
jM

qlkj

)



O(nd-c2 ).

(8)

i=1

The above lemma shows that if we select the top O(nd-c2 ) elements, with high probability, we will be able to eliminate almost all the irrelevant noisy patches, without losing any relevant patches. Numerically, we verify the proper k gains better performance (e.g., Figure 1) and for the hard instance k-NN gives more accurate attention regions. (e.g., Figure 3 and Figure 4).

4 Experiments for Vision Transformers
In this section, we replace the dense attention with k-NN attention on the existing vision transformers for image classification to verify the effectiveness of the proposed method. The recent DeiT [45] and its variants, including T2T ViT [56], TNT [20], PiT [24], Swin [35], CvT [50], So-ViT [51], Visformer [8], Twins [10], and Dino [4], are adopted for evaluation. These methods include both supervised methods [45, 56, 20, 24, 35, 50, 51, 8, 10] and self-supervised method [4]. Ablation studies are provided to further analyze the properties of k-NN attention.
4.1 Experimental Settings
We perform image classification on the standard ILSVRC-2012 ImageNet dataset [39]. It contains 1.3 million images in the training set and 50K images in the validation set, covering 1000 object

5

Top-1 Accuracy (%) Top-1 Accuracy (%) Top-1 Accuracy (%) Top-1 Accuracy (%)

73.0

72.5

72.0

71.5

71.0

DeiT-Tiny

25

50

75

100 125 150

k

(a) DeiT-Tiny

79.0

78.8

78.6

78.4

78.2 100/25

100/45

150/45

k

Visformer-Tiny
180/45

(c) Visformer-Tiny

81.8

81.7

81.6

81.5

81.4

81.3

CvT-13

100/100/100 500/200/100 1600/400/100 2500/600/150
k

(b) CvT-13

82.55

82.50

82.45

100/100/25

360/100/25 360/100/45
k

PiT-Base
360/150/45

(d) PiT-Base

Figure 1: The impact of k on DeiT-Tiny, Visformer-Tiny, CvT-13 and PiT-Base.

classes. The images are cropped to 224×224. In our experiments, we follow the experimental setting

of original official released codes [45, 56, 24, 35, 51, 51, 8, 4], except for CvT [50] and TNT [20].

As there are no official released codes for CvT and TNT, we re-implement them and follow the

training setting of DeiT [45] for their training and evaluation. For fair comparison, we only replace

the vanilla attention with proposed k-NN attention. Unless otherwise specified, the fast version of

k-NN attention is adopted for evaluation. To speed up the slow version, we develop the CUDA

version k-NN attention. As for the value k, different architectures are assigned with different values.

For DeiT [45], So-ViT [51], Dino [4], CvT [50], TNT [20] and PiT [24], as they directly split an

input image into rigid tokens and there is no information exchange in the token generation stage, we

suppose the irrelevant tokens are easy to filter, and tend to assign a smaller k compared with these

complicated

token

generation

methods

[56,

35,

8,

10].

Specifically,

we

assign

k

to

approximate

n 2

at

each scale stage; for these complicated token generation methods [56, 35, 8, 10], we assign a larger k

which

is

approximately

2 3

n

or

4 5

n

at

each

scale

stage.

4.2 Results on ImageNet

Table 1 shows top-1 accuracy results on the ImageNet-1K validation set by replacing the dense attention with k-NN attention using ten different vision transformer architectures. Both ConvNets and Transformers are listed for comparison. With the budget constraint at around 5M parameters, k-NN attention reports 0.8% improvements on DeiT-Tiny, for both fast version and slow version; on So-ViT-7, it also improves 0.8%; under the 10M constraints, Visformer-Tiny gains 0.4%; with the budget constraint at 40M parameters, the k-NN attention improves the performance by 0.3% on CvT-13 and DeiT-Small, 0.4% on TNT-Small, and 0.5% on T2T-ViT-t-19; on Swin-Tiny, kNN attention still improves the performance a little bit even though the local attention is already adopted in Swin transformers; k-NN also has a positive effect on Dino-small, a self-supervised vision transformer architecture; under the 80M constraints, it gets 0.2% and 0.6% increase in performance on Twins-SVT-Base and PiT-Base, respectively.

4.3 The Impact of Number k

The only parameter for k-NN attention is k, and its impact is analyzed in Figure 1. As shown in

the figure, for DeiT-Tiny, k = 100 is the best, where the total number of tokens n = 196 (14 × 14),

meaning that k approximates half of n; for CvT-13, there are three scale stages with the number of

tokens n1 = 3136, n2 = 784 and n3 = 196, and the best results are achieved when the k in each stage is assigned to 1600/400/100, which also approximate half of n in each stage; for Visformer-Tiny,

there are two scale stages with the number of tokens n1 = 196 and n2 = 49, and the best results are

achieved when k in each stage is assigned to 150/45, as there are more than 21 conv layers for token

generation and the information in each token are already mixed, making it hard to distinguish the

irrelevant tokens, thus larger values of k are desired; for PiT-Base, there are three scale stages with

the number of tokens n1 = 961, n2 = 256 and n3 = 64, and the optimal values of k also approximate

the half of n. Please note that, we do not perform exhaustive search for the optimal choice of k,

instead,

a

general

rule

as

below

is

sufficient:

k



n 2

at

each

scale

stage

for

simple

token

generation

methods

and

k



2 3

n

or

4 5

n

for

complicated

token

generation

methods

at

each

scale

stage.

4.4 Training Speed of k-NN Attention

In Table 2, we investigate the training speed of k-NN attention. Three methods are included for comparison, i.e. DeiT-Small [45], CvT-13 [50] and T2T-ViT-t-19 [56]. From the Table we can see

6

Arch. ConvNets Transformers
ConvNets Transformers ConvNets
Transformers
Transformer (Self-supervised) ConvNets Transformers

Model
MnasNet-A3 [40] EfficientNet-B0 [41] ShuffleNet [59] MoblieNet [26]
DeiT-Tiny [45] DeiT-Tiny [45]  k-NN Attn DeiT-Tiny [45]  k-NN Attn-slow So-ViT-7 [51] So-ViT-7 [51]  k-NN Attn
EfficientNet-B2 [41] SAN10 [60] ResNet-18 [22] LambdaNets [1]
Visformer-Tiny [8] Visformer-Tiny [8]  k-NN Attn
EfficientNet-B4 [41] ResNet-50 [22] ResNeXt50-32x4d [52] REDNet-101 [29] REDNet-152 [29] ResNet-101 [22] ResNeXt101-32x4d [52]
CvT-13 [50]* CvT-13 [50]*  k-NN Attn DeiT-Small [45] DeiT-Small [45]  k-NN Attn TNT-Small [20]* TNT-Small [20]*  k-NN Attn Swin-Tiny [35] Swin-Tiny [35]  k-NN Attn T2T-ViT-t-19 [56] T2T-ViT-t-19 [56]  k-NN Attn
Dino-Small [4]! Dino-Small [4]!  k-NN Attn
ResNet-152 [22] ResNeXt101-64x4d [52]
Twins-SVT-Base [10] Twins-SVT-Base [10]  k-NN Attn PiT-Base [24] PiT-Base [24]  k-NN Attn

Input
2242 2242 2242 2242
2242 2242 2242 2242 2242
2242 2242 2242 2242
2242 2242
2242 2242 2242 2242 2242 2242 2242
2242 2242 2242 2242 2242 2242 2242 2242 2242 2242
2242 2242
2242 2242
2242 2242 2242 2242

Params
5.2M 5.3M 5.4M 6.9M
5.7M 5.7M 5.7M 5.5M 5.5M
9M 11M 12M 15M
10M 10M
19M 25M 25M 25M 34M 45M 45M
20M 20M 22M 22M 24M 24M 28M 28M 39M 39M
22M 22M
60M 84M
56M 56M 74M 74M

GFLOPs
0.4 0.4 0.5 0.6
1.3 1.3 1.3 1.3 1.3
1.0 2.2 1.8 -
1.3 1.3
4.2 4.1 4.3 4.7 6.8 7.9 8.0
4.6 4.6 4.6 4.6 5.2 5.2 4.5 4.5 9.8 9.8
4.6 4.6
11.6 15.6
8.3 8.3 12.5 12.5

Top-1 (%)
76.7% 77.1% 73.7% 74.7%
72.2% 73.0% 73.0% 76.2% 77.0%
80.1% 77.1% 69.8% 78.4%
78.6% 79.0%
82.9% 79.1% 79.5% 79.1% 79.3% 79.9% 80.6%
81.5% 81.8% 79.8% 80.1% 80.9% 81.3% 81.2% 81.3% 82.2% 82.7%
76.0% 76.2%
80.8% 81.5%
83.2% 83.4% 82.0% 82.6%

Table 1: The k-NN attention performance on ImageNet-1K validation set. "*" indicates our own implementation and training setting; "!" means we pretrain the model with 300 epochs and finetune the pretrained model for 100 epoch for linear eval, following the instructions of Dino training and evaluation; " k-NN Attn" represents replacing the vanilla attention with proposed k-NN attention; k-NN Attn-slow means adopting the slow version.

7

Epoch
10 30 50 70 90 120 150 200 300

Top-1 accuracy DeiT-S DeiT-S  k CvT-13 CvT-13  k T2T-ViT-t-19 T2T-ViT-t-19  k

29.1% 54.4% 60.9% 65.0% 67.7% 69.9% 72.4% 75.5% 79.8%

31.3% 55.4% 62.0% 65.8% 68.2% 70.7% 72.4% 75.7% 80.0%

46.8% 66.7% 70.4% 72.4% 73.4% 75.1% 76.4% 78.8% 81.3%

49.5% 68.2% 71.1% 72.9% 74.0% 75.3% 76.7% 79.3% 81.6%

0.52% 63.0% 73.8% 76.9% 78.4% 79.7% 80.7% 82.0% 81.3%

Table 2: Ablation study on the training speed of k-NN attention.

0.68% 63.2% 74.4% 77.3% 78.6% 80.0% 80.9% 82.3% 81.7%

Avg. cosine similarity Avg. standard deviation
Residual/Main-Branch (attn) Residual/Main-Branch (ffn)

0.40 0.35

DeiT-Tiny w k-NN attention

0.030 0.025

DeiT-Tiny

DeiT-Tiny DeiT-Tiny w k-NN attention

0.5

0.4

DeiT-Tiny

DeiT-Tiny DeiT-Tiny w k-NN attention
DeiT-Tiny w k-NN attention

1.0 0.8

DeiT-Tiny DeiT-Tiny w k-NN attention

0.30

DeiT-Tiny

0.25

DeiT-Tiny DeiT-Tiny w k-NN attention

2

4

6

8

10

12

Layer depth

(a) Layer-wise cosine

similarity of tokens

0.020 0.015

DeiT-Tiny w k-NN attention

0.3 0.2

0.6

DeiT-Tiny

0.4

0.2

DeiT-Tiny w k-NN attention

2

4

6

8

10

12

Layer depth

2

4

6

8

10

12

Layer depth

2

4

6

8

10

12

Layer depth

(b) Layer-wise s.t.d of (c) Ratio of residual and (d) Ratio of residual and

attention weights main branch for attn main branch for ffn

Figure 2: The properties of k-NN attention. Blue and red dotted lines represent the metrics for k-NN attetion and the original fully-connected self-attention, respectively.

that the training speed of k-NN attention is faster than full-connected attention, especially in the early stage of training. These observations reflect that removing the irrelevant tokens benefits the convergence of neural networks training.

4.5 Other properties of k-NN attention

To analyze other properties of k-NN attention, three quantitative metrics are defined as follows. Layer-wise cosine similarity between tokens: following [18] this metric is defined as:

1 CosSim(t) =

tTi tj

(9)

n(n - 1)
i=j

ti

tj

where ti represents the i-th token in each layer and · denotes the Euclidean norm. This metric implies the convergence speed of the network.
Layer-wise standard deviation of attention weights: Given a token ti and its softmax attention weight sfm(ti), the standard deviation of the softmax attention weight std(sfm(ti)) is defined as the second metric. For multi-head attention, the standard deviations over all heads are averaged. This metric represents the degree of training stability.
Ratio between the norms of residual activations and main branch: The ratio between the norm of the residual activations and the norm of the activations of the main branch in each layer is defined as fl(t) / t , where fl(t) can be the attention layer or the FFN layer. This metric denotes the information preservation ability of the network.
Comparisons of the three metrics on DeiT-tiny are shown in Figure 2. From Figure 2 (a) we can see that by using k-NN attention, the averaged cosine similarity is larger than that of using dense self-attention, which reflects that the convergence speed is faster for k-NN attention. Figure 2 (b) shows that the averaged standard deviation of k-NN attention is smoother than that of fully-connected self-attention, and the smoothness will help make the training more stable. Figure 2 (c) and (d) show the ratio between the norms of residual activations and main branch are consistent with each other for k-NN attention and dense attention, which indicates that there is nearly no information lost in k-NN attention by removing the irrelevant tokens.

8

k-NN attention dense attention

(a) input

(b) head0 (c) head1 (d) head2 (e) head3 (f) head4 Figure 3: Self-attention heads from the last layer.

(g) head5

k-NN attn dense attn input

(a) dog

(b) wheel (c) ferret (d) monitor (e) hornbill (f) chain Figure 4: Visualization using Transformer Attribution [5].

(g) crib

4.6 Visualization
We visualize the self-attention heads from the last layer on Dino-Small [4] as shown in Figure 3. We can see that different heads attend to different semantic regions of an image. Compared with dense attention, the k-NN attention filters out most irrelevant information from background regions which are similar to the foreground, and successfully concentrates on the most informative foreground regions. Images from different classes are visualized in Figure 4 using Transformer Attribution method [5] on DeiT-Tiny. It can be seen that the k-NN attention is more concentrated and accurate, especially in the situations of cluttered background and occlusion.
5 Conclusion
In this paper, we propose an effective k-NN attention for boosting vision transformers. By selecting the most similar keys for each query to calculate the attention, it screens out the most ineffective tokens. The removal of irrelevant tokens speeds up the training and makes the training more stable. The k-NN attention not only inherits the local bias of CNNs but also achieves the ability of global feature exploitation. We theoretically prove its properties in speeding up the network training, distilling noises without losing information, and increasing the performance by choosing a proper k. Ten vision transformer architectures are adopted to verify the effectiveness of the proposed k-NN attention.
9

References
[1] I. Bello. Lambdanetworks: Modeling long-range interactions without attention. In International Conference on Learning Representations, 2020.
[2] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
[3] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213­229. Springer, 2020.
[4] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021.
[5] H. Chefer, S. Gur, and L. Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.
[6] C.-F. Chen, Q. Fan, and R. Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. arXiv preprint arXiv:2103.14899, 2021.
[7] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu. Transformer tracking. In Computer Vision and Pattern Recognition, 2021.
[8] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian. Visformer: The vision-friendly transformer. arXiv preprint arXiv:2104.12533, 2021.
[9] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.
[10] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting spatial attention design in vision transformers. arXiv preprint arXiv:2104.13840, 2021.
[11] X. Chu, B. Zhang, Z. Tian, X. Wei, and H. Xia. Do we really need explicit position encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021.
[12] J.-B. Cordonnier, A. Loukas, and M. Jaggi. On the relationship between self-attention and convolutional layers. In International Conference on Learning Representations, 2019.
[13] G. M. Correia, V. Niculae, and A. F. Martins. Adaptively sparse transformers. In EMNLP, pages 2174­2184, 2019.
[14] S. d'Ascoli, H. Touvron, M. Leavitt, A. Morcos, G. Biroli, and L. Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697, 2021.
[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[17] J. Fan and J. Lv. Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849­911, 2008.
[18] C. Gong, D. Wang, M. Li, V. Chandra, and Q. Liu. Improve vision transformers training by suppressing over-smoothing. arXiv preprint arXiv:2104.12753, 2021.
[19] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. Jégou, and M. Douze. Levit: a vision transformer in convnet's clothing for faster inference. arXiv preprint arXiv:2104.01136, 2021.
[20] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.
[21] L. Han, P. Wang, Z. Yin, F. Wang, and H. Li. Exploiting better feature aggregation for video object detection. In Proceedings of the 28th ACM International Conference on Multimedia, pages 1469­1477, 2020.
[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­ 778, 2016.
10

[23] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang. Transreid: Transformer-based object re-identification. arXiv preprint arXiv:2102.04378, 2021.
[24] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh. Rethinking spatial dimensions of vision transformers. arXiv preprint arXiv:2103.16302, 2021.
[25] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.
[26] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
[27] Z. Jiang, Q. Hou, L. Yuan, D. Zhou, X. Jin, A. Wang, and J. Feng. Token labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on imagenet. arXiv preprint arXiv:2104.10858, 2021.
[28] N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.
[29] D. Li, J. Hu, C. Wang, X. Li, Q. She, L. Zhu, T. Zhang, and Q. Chen. Involution: Inverting the inherence of convolution for visual recognition. arXiv preprint arXiv:2103.06255, 2021.
[30] W. Li, H. Liu, R. Ding, M. Liu, and P. Wang. Lifting transformer for 3d human pose estimation in video. arXiv preprint arXiv:2103.14304, 2021.
[31] X. Li, Y. Hou, P. Wang, Z. Gao, M. Xu, and W. Li. Transformer guided geometry model for flow-based unsupervised visual odometry. Neural Computing and Applications, pages 1­12, 2021.
[32] X. Li, Y. Hou, P. Wang, Z. Gao, M. Xu, and W. Li. Trear: Transformer-based rgb-d egocentric action recognition. arXiv preprint arXiv:2101.03904, 2021.
[33] Y. Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool. Localvit: Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707, 2021.
[34] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018.
[35] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.
[36] M.-T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412­1421, 2015.
[37] S. Pai. Convolutional neural networks arise from ising models and restricted boltzmann machines.
[38] A. Roy, M. Saffar, A. Vaswani, and D. Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53­68, 2021.
[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211­252, 2015.
[40] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820­2828, 2019.
[41] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105­6114. PMLR, 2019.
[42] Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438­9447. PMLR, 2020.
[43] Y. Tay, M. Dehghani, V. Aribandi, J. Gupta, P. Pham, Z. Qin, D. Bahri, D.-C. Juan, and D. Metzler. Omninet: Omnidirectional representations from transformers. arXiv preprint arXiv:2103.01075, 2021.
11

[44] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler. Efficient transformers: A survey. arXiv preprint arXiv:2009.06732, 2020.
[45] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.
[46] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jégou. Going deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.
[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In NIPS, 2017.
[48] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.
[49] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia. End-to-end video instance segmentation with transformers. In Computer Vision and Pattern Recognition, 2021.
[50] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang. Cvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.
[51] J. Xie, R. Zeng, Q. Wang, Z. Zhou, and P. Li. So-vit: Mind visual tokens for vision transformer. arXiv preprint arXiv:2104.10935, 2021.
[52] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492­1500, 2017.
[53] W. Xu, Y. Xu, T. Chang, and Z. Tu. Co-scale conv-attentional image transformers. arXiv preprint arXiv:2104.06399, 2021.
[54] Z. Yu, X. Li, P. Wang, and G. Zhao. Transrppg: Remote photoplethysmography transformer for 3d mask face presentation attack detection. arXiv preprint arXiv:2104.07419, 2021.
[55] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu. Incorporating convolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021.
[56] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and S. Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.
[57] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2020.
[58] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint arXiv:2103.15358, 2021.
[59] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6848­6856, 2018.
[60] H. Zhao, J. Jia, and V. Koltun. Exploring self-attention for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10076­10085, 2020.
[61] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Q. Hou, and J. Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.
[62] C. Zou, B. Wang, Y. Hu, J. Liu, Q. Wu, Y. Zhao, B. Li, C. Zhang, C. Zhang, Y. Wei, et al. End-to-end human object interaction detection with hoi transformer. In Computer Vision and Pattern Recognition, 2021.
A Appendix
A.1 Source codes of fast version k-NN attention in Pytorch
The source codes of fast version k-NN attention in Pytorch are shown in Algorithm 1, and we can see that the core codes of fast version k-NN attention is consisted of only four lines, and it can be easily imported to any architecture using fully-connected attention.
12

Algorithm 1 Codes of fast version k-NN attention in Pytorch.

1 class kNN -Attention(nn.Module):

2

def __init__(self ,dim ,num_heads=8,qkv_bias=False ,qk_scale=None ,attn_drop =0., proj_drop

=0.,topk =100):

3

super () . __init__ ()

4

self.num_heads=num_heads

5

head_dim = dim // num_heads

6

self.scale=qk_scale or head_dim **-0.5

7

self.topk=topk

8

9

self.qkv=nn.Linear(dim ,dim*3,bias=qkv_bias)

10

self . attn_drop = nn . Dropout ( attn_drop )

11

self.proj=nn.Linear(dim ,dim)

12

self . proj_drop = nn . Dropout ( proj_drop )

13

14

def forward(self ,x):

15

B,N,C=x.shape

16

qkv=self.qkv(x).reshape(B,N,3,self.num_heads ,C//self.num_heads).permute

(2 ,0 ,3 ,1 ,4)

17

q,k,v=qkv[0],qkv[1],qkv[2] #B,H,N,C

18

attn=(q@k.transpose(-2,-1))*self.scale #B,H,N,N

19

# the core code block

20

mask=torch.zeros(B,self.num_heads ,N,N,device=x.device ,requires_grad=False)

21

index=torch.topk(attn ,k=self.topk ,dim=-1,largest=True)[1]

22

mask.scatter_(-1,index ,1.)

23

attn=torch.where(mask >0,attn ,torch.full_like(attn ,float('-inf')))

24

# end of the core code block

25

attn=torch.softmax(attn ,dim=-1)

26

attn=self.attn_drop(attn)

27

x=(attn@v).transpose (1,2).reshape(B,N,C)

28

x=self.proj(x)

29

x=self.proj_drop(x)

30

31

return x

A.2 Comparisons between slow version and fast version

We develop two versions of k-NN attention, one slow version and one fast version. The k-NN attention is exactly defined by slow version, but its speed is extremely slow, as for each query it needs to select different k keys and values, and this procedure is very slow. To speedup, we developed the CUDA version, but the speed is still slower than fast version. The fast version takes advantages of matrix multiplication and greatly speedup the computing. The speed comparisons on DeiT-Tiny using 8 V100 are illustrated in Table 3.

Table 3: The speed comparisons on DeiT-tiny for slow and fast version

method slow version (pytorch) slow version (CUDA) fast version (pytorch)

time per iteration (second) 8192 1.55 0.45

A.3 Proof

Proof for Lemma 1

Proof. Let's first consider the derivative of V^l over WQ,ij. Via some algebraic computation, we have

V^l  WQ,ij

=

(alV )  WQ,ij

=

n
alt
t=1

Tlknn(k1)  WQ,ij

-

n
alk1
k1 =1

Tlknn(k2)  WQ,ij

xtWV ,

(10)

where we denote Tlknn(k) as follow for shorthand.

Tlknn(k1) =

xlWQWK xk1 , Small Enough Constant,

if patch k1 is selected in row l otherwise

(11)

13

Let denote set S =. {i : patch i is selected in row l} and then we consider the right-hand-side of (10).

n
(10) = alt
tS

 xiWQWK xk  WQ,ij



-

alk1

k1 S

xiWQWK xk1  WQ,ij

xtWV

n

= alt x1ixtWK,j -

alk1 xlixk1 WK,j xtWV

tS

k1 S

n

n

= altxlixtWK,j xtWV - altxtWV ·

alk1 xlixk1 WK,j .

tS

tS

k1 S

(a)

(b)

(c)

(12)

Since al is the l-th row of the attention matrix, we have alt  0 and t alt = 1. It is possible to treat terms (a), (b) and (c) as the expectation of some quantities over t replicates with probability alt. Then (12) can be further simplified as

(12) = (Eal [xlixtWK,j · xtWV ] - Eal [xtWK,j ] · Eal [xlixtWV ]) = xli Eal [WK,j xt · xtWV ] - Eal [WK,j xt ] · Eal [xtWV ] = xliWK,j Eal [xt xt] - Eal [xt ] · Eal [xt] WV

= xliWK,j Varal (xt)WV ,

(13)

where the second equality uses the fact that xtWK,j is a scaler.

Combing (10)-(13), we have

V^l  WQ,ij

= xliWK,j Varal (xt)WV

 Varal (xt).

(14)

Due the symmetric on Q and K, we can follow the similar procedure to show

V^l  WK,ij

= xliWQ,j Varal (xt)WV

 Varal (xt).

(15)

Proof for Lemma 2 Before given the formal statement of the Lemma 2, we first show the assumptions.
Assumption 2
1. The token xi is the sub-gaussian random vector with mean µi and variance (2/d)I for i = 1, 2, ..., n.
2. µ follows a discrete distribution with finite values µ  V. Moreover, there exist 0 < 1, 0 < 2 < 4 such that a) µi = 1, and b) µiWQWKT µi  [2, 4] for all i and |µiWQWK µj |  2 for all µi = µj  V.
3. WV and WQWK are element-wise bounded with 5 and 6 respectively, that is, |WV(ij)|  5 and |(WQWK )(ij)|  6, for all i, j from 1 to d.
In Assumption 2 we ensure that for a given query patch, the difference between the clustering center and noises are large enough to be distinguished. Lemma 4 (formal statement of Lemma 2). Let patch xi be 2-subgaussian random variable with mean µi and there are k1patches out of all k patches follows the same clustering center of query l. Per Assumption 2, when d  3((, d) + 2 + 4), then with probability 1 - 5, we have
14

k i=1

exp

1 d

xlWQWk

xi

xiWV

k j=1

exp

1 d

xlWQWK

xj

- µlWV

 4 exp


(, d)  d

5

2 log
dk

2d 

 8 exp 2 - 4+ (, d) - 7 + exp 2 - 4+ (, d)

d

d

k1 k

µ1WV ,

where (, d) = 216

2 log

1 

+ 226 log

d 

.

Proof. Without loss of generality, we assume the first k patch are the top-k selected patches. From Assumption 2.1, we can decompose xi = µi + hi, i = 1, 2, ..., k, where hi is the sub-gaussian random vector with zero mean. We then analyze the numerator part.

k

1

exp
i=1

 d

xl

WQWk

xi

xiWV

(a)

(b)

k

1

= exp
i=1

 d

µlWQ

WK

µi

k
µiWv + exp
i=1 (c)

1

 d

xl

WQ

WK

xi

hiWv

k

1

1

+ exp
i=1

 d

xl

WQWk

xi

- exp

 d

µl

WQWK

µi

µiWv .

(16)

Below we will bound (a), (b) and (c) separately.

Upper bound for (a). Let denote index set S1 = {i : µ1 = µi, i = 1, 2, ..., k}. We then have

1

(a) - exp
iS1

 d

x1WQWK

xi

µ1WV


1

(k - |S1|) max
i

exp

 d

x1WQ

WK

xi

µ1WV 

(k - k1) exp 2 µ1WV ,

(17)

d

where last inequality is from the Assumption 2.2.

Upper bound for (b). Since each dimension in hl is the i.i.d random vector with zero mean variance 2/d based on Assumption 2.1, we can use Hoeffding Inequality to derive the following result holds with probability 1 - .

k

1

exp
i=1

 d

x1WQWK

xi

hiWV  5


2(k1U12 + (k - k1)U22) log 2d ,

d



(18)

where U1 = maxiS1

exp

1 d

x1WQWK

xi

and U2 = maxi/S1

exp

1 d

x1

WQWK

xi

.

We then build the upper bound for U1 and U2. Since xi = µi + hi for i = 1, 2, ..., k, we have

|x1WQWK xi |  |µ1WQWK µi | + |µ1WQWK hi | + |h1WQWK µi | + |h1WQWK hi |

15

Via Assumption 2.3 and Hoeffding Inequality, with probability 1 - 4, the follow results hold.

1

|µ1WQWK hi |  16 2 log 

(19)

1

|h1WQWK µi |  16 2 log 

(20)

|h1WQWK hi |  226 log

d 

(21)

and we denote (, d) = 216

2 log

1 

+ 226 log

d 

for shorthand and then we have

1 U1  exp  (2 + (, d))
d 1 U2  exp  (4 + (, d)) d

As a result, with a probability 1 - 5, we have

(b)   5

2(n1 exp

2 d

(2

+

(,

d))

+ (k - k1) exp

2 d

(4 + (, d))

) log

2d

d



(, d) = 5 exp 
d

2(k1 exp

22 d

+ (k - k1) exp

24 ) d log

2d

.

d



(22)

Upper bound for (c).

k

1

(c)  

exp

i=1

 d

x1

WQ

WK

xi

1

- exp

 d

µ1

WQWK

µi

µ1WV 



(c) 

k


exp

µ1WV 

i=1

1

 d

µ1WQ

WK

µi

1

exp

 d

x1WQWK xi

- µ1WQWK µi

-1



1

exp

 d

x1WQWK xi

- µ1WQWK µi

-1

exp 2 

i/S1

+
iS1

1

exp

 d

x1WQWK xi

- µ1WQWK µi

-1

exp 5 

1



exp

 d

µ1WQWK hi

+ h1WQWK µi

+ h1WQWK hi

i/S1

1

+

exp

iS1

 d

µ1WQWK hi

+ h1WQWK µi

+ h1WQWK hi

- 1 exp 2 
- 1 exp 5 . 
(23)

Combine (23) with (19)-(21) and we have with probability 1 - 4:

(c)   exp

(, d) 

-1

(k - k1) exp

2

+ k1 exp 5

d

d

d

µ1WV . (24)

16

From (16), (17), (22) and (24), with probability 1 - 5, we have

k

1

exp
i=1

 d

x1WQWK

xi

1

xiWV - exp
iS1

 d

µ1

WQWK

µi

µ1WV


(, d)  exp 
d

1(k, k1) Wvµ1  + 5

22(k, k1) log d

2d 

,

(25)

where 1(k, k1) = (k - k1) exp

2 d

k1) exp

24 d

.

+ k1 exp

5 d

and 2(k, k1) = k1 exp

22 d

+ (k -

Now we consider the upper bound the denominator part.

k

1

exp
i=1

 d

x1WQ

WK

xi

k

1

- exp
i=1

 d

µ1

WQ

WK

µi

(g1 )

k

1

 exp
i=1

 d

µ1WQWK

µi

1

exp

 d

x1WQWK xi

- µ1WQWK µi

Via Assumption 2.2 and (19)-(21), with probability 1 - 5 the follow results hold.

- 1 . (26)

(g1)  exp 4 d

k

1

exp
i=1

 d

x1WQWK xi

- µ1WQWK µi

-1

 exp 4 d

k

1

exp
i=1

 d

µ1WQWK hi

+ h1WQWK µi

+ h1WQWK hi

 exp 4 d

k

(, d)

exp  - 1

i=1

d

 k exp 4 exp (, d) - 1 .

d

d

-1 (27)

Combining (26), (27) and Assumption 2.2, we have

k

1

exp
i=1

 d

x1

WQWK

xi

k

1

 exp
i=1

 d

µ1WQ

WK

µi

- k exp 4 d

(, d) exp  - 1
d

k exp - 2 - k exp 4

(, d) exp  - 1 .

(28)

d

d

d



When d  3((, d) + 2 + 4), one may verify

exp

(,

d)

+ 

2

+

4

- exp

2+ 4

 exp

1

- 1  0.39 < 1

d

d

3

 exp

(, d) 

- 1 - exp - 2+ 4

<0

d

d

 exp - 2 - exp 4

(, d) exp  - 1 > 0

d

d

d

k exp - 2 - k exp 4

(, d) exp  - 1 > 0.

(29)

d

d

d

17

Finally, via (25), we have

k i=1

exp

k j=1

exp

1 d

x1

WQWK

xi

1 d

x1WQ

WK

xj

xiWV - µ1WV


k
·  exp
j=1 

1

 d

x1WQ

WK

xj

 

k

1

= exp
j=1

 d

x1

WQWK

xi

k

1

 exp
i=1

 d

x1

WQWK

xi

k

1

xiWV - exp
i=1

 d

x1WQWK

xj

µ1WV


1

xiWV - exp
iS1

 d

µ1WQ

WK

µi

µ1WV


1

+

exp

 d

µ1

WQWK

µi

i/S1

µ1WV


(, d)  exp


21(k, k1) µ1WV  + 5

22(k, k1) log d

2d 

+(k - k1) exp 2 µ1WV . d

- k1 exp 4 d

µ1WV  (30)

Per (30) and (29), we have

k i=1

exp

k j=1

exp

1 d

x1

WQWK

xi

1 d

x1

WQWK

xj

xiWV - µ1WV


 k exp - 2 - k exp 4

d

d

(, d)

-1

exp  - 1

d

(, d) · exp 
d

21(k, k1) µ1WV  + 5

22(k, k1) log d

2d 

+(k - k1) exp 2 µ1WV  - k1 exp 4 µ1WV 

d

d

exp

(,d) d

5

exp -24
d

22 (k,k1 ) dk2

log

2d 



1 + exp - 4+2 - exp (,d)

d

d

exp (,d)-4

+

d

21 (k,k1 ) k

-

k1 k

exp

4 d

+

(1

-

k1 k

)

exp

4 d

1 + exp - 4+2 - exp (,d)

d

d

 8 exp 2 - 4+ (, d) - 7 + exp 2 - 4+ (, d)

d

d

· µ1WV 

k1 k

µ1WV 

(, d)

2

2d

+ 4 exp

 d

5

log dk



,

(31)

where second inequality uses 4  µ2 and last inequality uses the definition of 1(k, k1), 2(k, k1) and d  3((, d) + 2 + 4).

Proof of Lemma 3
Proof for Lemma 3 Without loss of generality, we only consider the case with first query patches. In the k-NN attention scheme, we first use the dot-product product to compute the similarity between query and each key-patches and then use the softmax to normalize the similarities. We make the

18

following assumption to facility our analysis.

Assumption 3.1 There exists   R1×n and    such that q1 = K + , where is filled
with random variable follows N (0, 2) for some  > 0.

To see the connection between the Assumption 3.1 with attention scheme, we consider the follow problem.

min K 


- q1

22,

(32)

If K is normalized with zero columns meanand we apply the exponential gradient method on the

initial

solution

0

=

1 n

e

with

step

length

1/

d, the one step updated solution 1 is

1 =

exp(

1 d

K q1

)

k i=1

exp(

1 d

kiq1

. )

(33)

The above equation (33) is just the attention scheme used standard transformer type model. Based on Assumption 3.1, we can treat 1 as an approximation of underlying true parameters .

On the other hand, it is commonly believed that only part of patches are correlated with the query patch (i.e., with non-zero similarity weights.) and it would be ideal if we could use a computational cheap method to eliminate the irrelevant patches. In this paper, we consider the top-k selection scheme. To see the rationality of the top-k selection, we consider augmenting (32) with L2 regularization on .

min K 


- q1

2 2

+

 2



2 2

,

K(K  + q1 ) +  + 1 + e2 = 0,  = (KK + I)-1 Kq1 + 1 + e2 ,
where we use the KKT optimal condition, and 1, 2 are Lagrange multipliers to make sure   . If  is large enough and  > 0, we will have

1



 

Kq1 + e2

 Kq1

(34)

The above result indicates that we may selection the important elements in  (e.t., with large

magnitude) by considering rankness in vector Kq1 .

We then discussion the correctness of the top-k selection with the following regularity assumptions on K and q1. Assumption 3.2

1. K is normalized with row zero mean. Let  = KK and Z = -1/2K . We assume there exist some c, c4 > 1 and C1 > 0 such that the following inequality

P

max (p~-1 Z~ Z~

) > c4 and max(p~-1Z~Z~

1 )<
c4

holds for any p~ × d submatrix Z~ of Z with cn < p~  n.

< exp(-C1d)

2. var(q1) = O(1) and for some   0 and c5, c6 > 0,

min
i:i >0

i



c5 d

and

min
i:i >0

cov(i-1

q1

, ki

)



c6

3. There exist some   [0, 1) and c7 > 0 such that

max()  c7d .

Lemma 5 (formal statement of Lemma ). Let's assume only be s keywords are relevant to the query l. Under Assumption 3.1 and 3.2, when 2 +  < 1, with probability 1 - O(s exp(-Cd1-2/ log d)),

we have

n

1(qlki



min
iM

ql

ki

)



cnd2+ -1 ,

(35)

i=1

where M = {i : keyword i is relevant to the query l.} , and  , , c and C are positive constants.

19

Proof. Our strategy is the similar to the proof of Theorem 1 in [17].

Based on equation (44) in [17], we have

KK = n1/2U~ diag(µ1, ..., µd)U~ 1/2,

(36)

where µ1,..., µd are d eigenvalues of p-1ZZ , U~ = (Id, 0)d×nU , and U is uniformally distributed
on the orthogonal group O(n). To facility our further analysis we denote  = qlK . By definition of  and per Assumption 3.1, we have

 = Kql = KK  + K =.  + .

(37)

We then separately study  and .

Analysis on . We first bound  from above. Since {µi} is the eigenvalues of n-1ZZ , we have

diag(µ21, ..., µ2d)  max(n-1ZZ ) 2 Id

(38)

and There lead to

U~ U~  max()Id

 2  n2max() max(n-1ZZ ) 2 ·  1/2U~ U~ 1/2.

(39)

Let Q belongs to the orthogonal group O(n) such that 1/2 = 1/2 Qe1. Then, it follows from Lemma 1 in [17] that

 1/2U~ U~ 1/2 = 1/2 Q SQe1, e1 (=d) 1/2 Se1, e1 ,

(40)

where we use the symbol (=d) to denote being identical in distribution. By part 3 in Assumption 3.2, 1/2 2 =    var(y) = O(1), and thus via Lemma 4 in [17], we have for some C > 0,

P  1/2U~ U~ 1/2 > (d/n)  O (exp(-Cd)) .

(41)

Combining with max() = O(d ) and P(max(n-1ZZ ) > c1)  exp (-C1d) by parts 1 and 3
in Assumption 3.2 along with union bound, we have

P  2  O(d1+ n)  O(exp(-Cd)).

(42)

We then consider the lower bound on i for i  M. By (36), we have

i = nei 1/2U~ diag(µ1, ..., µd)U~ 1/2.

(43)

Note that 1/2ei = some c > 0 such that

var(Xi) = 1, 1/2 = O(1). By part 2 of Assumption 3.2, there exists

1/2, 1/2ei

=

icov(i-1q1

, ki)



c d .

(44)

Thus, there exists Q in orthogonal group O(n) such that 1/2ei = Qe1 and

1/2 = 1/2, 1/2ei Qe1 + O(1)Qe2.

(45)

Since (µ1, ...., µd) is independent of U~ by Lemma 1 in [17] and the uniform distribution on the orthogonal group O(n) is invariant under itself, it follows that

i (=d) 1/2, 1/2ei R1 + O(n)R2 =. i,1 + i,2,

(46)

where R = (R1, R2, ..., Rn) = U~ diag(µ1, ..., µd)U~ e1. We will bound the above two terms i,1 and i,2 separately. One can verify

R1  e1 U~ min(n-1ZZ )IdU~ e = min(n-1ZZ ) Se1, e1 ,

(47)

and thus by part 1 of assumption 3.2, Lemma 4 in [17], and union bound, we have for some c, C > 0,

P(R1 < cd/n)  O(exp(-Cd)).

(48)

20

Therefore we have for some c > 0,

P(i,1  cd1-)  O(exp(-Cd))

(49)

Similarly, we can also show that

P( R 2  O(d/n))  O(exp(-Cd)).

(50)

Via some analysis, we can show that the distribution of R~ = (R2, ..., Rn) is invariant un-
der the orthogonal group O(n - 1). Then, it follows that R~ (=d) R~ W / W , where W = (W1, ..., Wn-1)  N (0, In-1), independent of R~ . Thus, we have

(d)
R2 =

R~

W1 . W

(51)

And via the Lemma 5 in [17], we can show



P(|i,2|  c d|W |)  O(exp(-Cd)),

(52)

where

W

is

N (0, 1)-distributed

random

variable.

We

then

pick

xd

=

 c 2C

d1-/log

d.

Then,

by

standard tail bound, we have



P(c d|W |  xd)  O(exp(-Cd1-2/ log d)),

(53)

Then

P(|i,2|  xd)  O(exp(-Cd1-2/ log d)).

(54)

It implies that for i with i > 0, we have

P i,1 - |i,2|  cd1-  O(exp(-Cd1-2/ log d)) P i  cd1-  O(exp(-Cd1-2/ log d)).

Next, we examine term  = (1, ..., n) = K . Clearly, we have

K K = ZZ  Zmax()InZ = nmax()max(n-1ZZ )Id.

(55)

Then, it follows that

 2 = K K  nmax()max(n-1ZZ ) 2.

(56)

From

Assumption

3.1,

we

know

that

{

2 i

/2}

are

i.i.d.

21-distributed

random

variables.

Thus

there

exist c, C > 0 such that

P( 2 > cd2)  exp(-Cd)

(57)

Along with parts 1 and 3 of Assumption 3.2, we have

P(  2 > O(d1+ n))  O(exp(-Cd)).

(58)

We then bound |i| from above. Given that  = K  N (0, 2KK ). Hence i|K=K  N (0, var(i|K=K )) with var(i|K = K) = 2ei Kei.

Let E be the event {var(i|K)  cd} for some c>0. Then, using the same argument in the previous proof. we can show that

P(Ec)  O(exp(-Cd)).

(59)

Condition on the event E, for all x > 0, we have



P(|i| > x|K)  P( cd|W | > x)

(60)

, where W is N (0, 1) random variable. Via union bound, we have



P(|i| > x)  O(exp(-Cd)) + P( cd|W | > x)

(61)

21

By

setting

x

=

 2cC

d1-/log

d,

we

have

P(cn|W | > x)  O(exp(-Cd1-2/ log d))

(62)

Then

P(|i| > o(d1-))  O(exp(-Cd1-2/ log d))

(63)

Finally, we could reach

P( min i:i >0

i

-

|i|



c1d)



O(s

exp(-C d1-2 /

log

d))



P( min i:i >0

i



c1d)



O(s exp(-Cd1-2/

log

d))

Therefore, with probability 1 - O(s exp(-Cd1-2/ log d)), the magnitudes of i with i > 0 are uniformly at least of order d1- and for some c > 0, we have

n

i=1

1(k



min
i:i >0

i

)



cp d1-2-

 cnd2+-1

n

 1(qlki
i=1



min
iM

ql

ki

)



cnd2+ -1 .

22

