arXiv:2106.00609v1 [cs.CV] 1 Jun 2021

Robust Mutual Learning for Semi-supervised Semantic Segmentation
Pan Zhang1, Bo Zhang2, Ting Zhang2, Dong Chen2, Fang Wen2 1University of Science and Technology of China, 2Microsoft Research Asia
zhangpan@mail.ustc.edu.cn {zhanbo, tinzhan, doch, fangwen}@microsoft.com
Abstract
Recent semi-supervised learning (SSL) methods are commonly based on pseudo labeling. Since the SSL performance is greatly influenced by the quality of pseudo labels, mutual learning has been proposed to effectively suppress the noises in the pseudo supervision. In this work, we propose robust mutual learning that improves the prior approach in two aspects. First, the vanilla mutual learners suffer from the coupling issue that models may converge to learn homogeneous knowledge. We resolve this issue by introducing mean teachers to generate mutual supervisions so that there is no direct interaction between the two students. We also show that strong data augmentations, model noises and heterogeneous network architectures are essential to alleviate the model coupling. Second, we notice that mutual learning fails to leverage the network's own ability for pseudo label refinement. Therefore, we introduce self-rectification that leverages the internal knowledge and explicitly rectifies the pseudo labels before the mutual teaching. Such self-rectification and mutual teaching collaboratively improve the pseudo label accuracy throughout the learning. The proposed robust mutual learning demonstrates state-of-the-art performance on semantic segmentation in low-data regime.
1 Introduction
Semi-supervised learning [1, 2, 3, 4, 5, 6] improves the data efficiency for low-data regime relying on limited amounts of labeled images and extra massive amounts of unlabeled images. Recent state-ofthe-art methods, e.g., MeanTeacher [4], FixMatch [7] and Noisy student [8], share a similar philosophy and enjoy the merit of both pseudo labeling [9, 8, 10] and consistency regularization [2, 3, 4, 1, 11]: the teacher model generates pseudo labels for weakly-augmented unlabeled data whereas the student model trains on the strong-augmented counterparts, as shown in Figure 1a. Such strategy can also be applied to semantic segmentation, and the resulting approaches [12, 13, 14, 15, 16, 17, 18, 19] have demonstrated outstanding performance.
However, pseudo labeling methods are plagued with confirmation bias [20], i.e., the student model is prone to overfit the erroneous pseudo labels. A few recent works set out to address this issue, either by estimating the pseudo label uncertainty [21, 22] or directly rectifying the pseudo labels [23, 24]. Recent work [10] offers breathtaking performance on ImageNet benchmark and the key is to inform the teacher of the feedback from the student model so that better pseudo labels can be generated (shown in Figure 1b). Mutual learning [18, 19], on the other hand, trains two models in parallel that play the dual role of teacher and student (shown in Figure 1c). The networks suppress the noisy knowledge of the counterpart and collaboratively improve in the course of learning. Nonetheless, both networks may end up with learning homogeneous knowledge and suffer from coupled noises that hinder the further improvement of the two learners. Besides, the pseudo label noises are suppressed
Work was done during the first author's internship at Microsoft Research Asia.
Preprint. Under review.

noise

noise

student

student

model A

model A

model B

pseudo label feedback

pseudo label

teacher

teacher

model B

mean teacher A

C

C

mean teacher B

(a)

(b)

(c)

(d)

Figure 1: Comparison of (a) pseudo labeling [9], (b) meta pseudo labeling [10], (c) mutual learning [18, 19] and (d) the proposed robust mutual learning. The symbol © denotes the pseudo label self-rectification that explicitly online refines the pseudo label supervisions.

by purely relying on the knowledge from the peer network, while totally overlooking the network's own ability to improve the pseudo labels.
In this work, we propose robust mutual learning that ameliorates the coupling issue of mutual learning and introduce self-rectification for the pseudo label refinement. First, we observe that it is crucial to ensure heterogeneous knowledge during training so that the model coupling can be reduced. We comprehensively examine the factors to alleviate the issue: 1) we compute the pseudo labels from the mean teacher [4] which not only yields more reliable pseudo labels but also reduces the coupling issue since the learners no longer interact directly; 2) we further explore factors that reduce the coupling of the mutual learners and find that strong data augmentations and model noises (e.g., dropout [25] and stochastic depth [26]) are essential to let models learn incoherent knowledge; 3) we also investigate heterogeneous network architectures (e.g., convolutional neural network versus transformer [27, 28]) and demonstrate its effectiveness.
Meanwhile, besides the refinement from the peer network, we propose to explicitly denoise the pseudo labels prior to the mutual learning so that only robust knowledge is propagated throughout the framework. Specifically, we draw inspiration from [24] and propose to refine pseudo labels according to the class-wise prediction confidence estimated based on the relative feature distances to the class centroids. Figure 1 shows the comparison of our framework and prior works. Our framework has two distinct schemes for label refinement: self-rectification leverages the model's own knowledge to refine the pseudo labels and mutual teaching filters the noisy knowledge with the help of the heterogeneous peer network. Both schemes collaboratively improve the pseudo labels during training, leading to the SSL models with narrowed gap to supervised learning.
Extensive experiments show that the proposed robust mutual learning outperforms prior competitive approaches. On the Cityscapes dataset, our method is best performed under all the SSL settings. In particular, our method shows much superiority on extreme low data regime. When using only 1/30 labeled images, our method outperforms the state-of-the-art method [19] by 3.45% mIoU. Our method also demonstrates performance advantage on the PASCAL VOC 2012 dataset. Notably, our method could offer performance on-par with supervised learning merely using 1/8 labeled data.
Our main contribution can be summarized as follows. 1) We notice the coupling issue of existing mutual learning works and exhaustively examine factors that can be used to ameliorate that issue, leading to improved performance thanks to the induced heterogeneous knowledge learning. 2) We propose to leverage the model's internal knowledge besides the external knowledge from peer networks and the pseudo labels are further rectified according to the feature distances to class centroids, boosting the performance one step further to a record high. 3) The proposed robust mutual learning could better online refine the pseudo labels and achieves state-of-the-art performance on several commonly used semantic segmentation datasets.
2 Related Works
The key of semi-supervised segmentation lies in how to leverage the large unlabeled data set to derive additional training signals. Along this direction, prior works can be roughly categorized into five classes: generative learning, contrastive learning, entropy minimization, consistency regularization
2

and pseudo labeling. We briefly review the last two categories which are mostly related and widely incorporated in current state-of-the-art methods.
Consistency regularization. Consistency regularization enforces the model to make consistent predictions with respect to various perturbations. The effectiveness of consistency regularization is based on the smoothness assumption or the cluster assumption that data points close to each other are likely to be from the same class, which is often held in classification tasks and has brought about a lot of research efforts [5, 7, 29, 2, 30, 31, 32, 33, 34]. As for semantic segmentation, it is observed in [13, 16] that the cluster assumption is violated in the semantic segmentation task. Therefore, Ouali et al. [16] propose to perturb the encoder's outputs where the cluster assumption is maintained and leverage multiple auxiliary decoders to give consistent predictions. French et al. [13] find that mask-based augmentation strategies are effective and introduce an adapted version of a popular technique CutMix [13]. The idea of CutMix is to mix samples by replacing the image region with a patch from another image, which can be regarded as an extension of Cutout [35] and Mixup [36], and is further extended in recent works [14, 37, 38]. Our approach also adopts the idea from CutMix [13] to enforce consistency between the mixed outputs and the prediction over the mixed inputs.
Pseudo labeling. Pseudo labeling or self-training is a typical technique in leveraging unlabeled data by alternating the pseudo label prediction and feature learning, which encourages the model to make confident predictions for unlabeled data. This technique has been successfully and widely applied to many tasks such as natural language processing [39, 40, 41, 42], image classification [9, 43, 44, 8, 45] as well as semantic segmentation [46, 47, 44]. The key of pseudo labeling relies on the quality of the pseudo labels. Most models [9, 4, 7, 8] refine the pseudo label from external guidance, e.g., a teacher model. However, the teacher model is usually fixed, making the student inherit some inaccurate predictions from the teacher. Recent efforts turn to update the teacher along with the student in order to generate better pseudo labels, e.g., co-teaching [48, 49], dual student [50], meta pseudo label [10], mutual training [18, 19] and so on. Such mutual learning strategy however may end up learning homogeneous knowledge and hence fail to provide complementary information for each other.
In this paper, we point out that learning heterogeneous knowledge in mutual learning is of great importance but is rarely noticed. A close related work [51] also adopts mean teaching for unsupervised domain adaptation in person re-identification. In contrast, our work differs in two aspects: 1) we observe the importance of learning heterogeneous knowledge in semi-supervised learning and mean teaching is one of the proposed potential solutions; 2) besides mutual pseudo label refinement, we also explore internal guidance to improve the quality of the pseudo label.

3 Preliminary

Semi-supervised learning (SSL) for semantic segmentation learns from the pixel-wise labeled dataset Dl = {xl, yl}nj=l 1 in conjunction with unlabeled data Du = {xu}nj=u1, where x  RH×W ×3 denotes the training images with resolution of H × W and yl  RH×W ×K is the ground truth corresponding to xl with pixels labeled by K classes. The resulting segmentation network h parameterized by  can be regarded as a composite of a feature extractor f and a linear classifier g, i.e., h = g f .
Recent state-of-the-art approaches follow a similar paradigm: the network is first trained on the labeled images with weak augmentations  by minimizing the standard cross-entropy, i.e.,

Ls() = H(yl, h((xl))).

(1)

Then, for unlabeled images the "hard" pseudo labels2 y^ can be generated, which is a one-hot vector converted from the soft prediction, i.e., y^ = one_hot(h((xu))). The prediction for the strongly augmented images should match the pseudo labels. Also, the unreliable pseudo labels are discounted when the confidence falls below a predefined threshold  , so the unsupervised objective for unlabeled images can be formulated as:

Lu() = 1(max h((xu)) >  ) · H(y^, h((xu))),

(2)

where  denotes the strong augmentations that make the student learn in a harder way.

2Recent works find hard pseudo labels can lead to stronger performance. Our experiment in the supplementary material also proves this.

3

To better suppress the erroneous pseudo labels, the mutual learning [18, 19] adopts two models h1 and h2 that are initialized differently and generate the pseudo labels y^1 and y^2 respectively. The
same supervised loss as Equation 1 is used whereas the unsupervised loss becomes:

Lu(1, 2) = H(y^2, h1 ((xu))) + H(y^1, h2 ((xu))).

(3)

The incorrect pseudo labels are gradually filtered by the peer network [49] so both of the networks collaboratively improve during training.

4 Robust Mutual Learning

In this section, we propose robust mutual learning that suffers less from the coupling issue. Also, we involve pseudo label self-rectification to further enhance the supervision quality for unlabeled data.

4.1 Techniques for Model Coupling Reduction

Indirect mutual learning with mean teachers. Mutual learners possess different learning abilities due to different initialization and can online refine the pseudo labels from the counterpart. However, they may quickly converge to the same state and thereby suffer from the coupled noises. The model coupling in the early training phase will reduce the mutual learning to self-training. We conjecture that the coupling issue is caused by the direct interaction between the mutual learners. Therefore, we propose to transfer the knowledge in an indirect manner. Specifically, we generate pseudo labels from the mean teachers [4], i.e., the exponential moving average (EMA) of the training models, so that each learner receives supervision from the peer mean teacher. Let h~1 and h~2 be the moving average model for h1 and h2 respectively, the one-hot mutual supervisions in Equation 3 become:

y^1 = one_hot h~1 ((xu)) and y^2 = one_hot h~2 ((xu)) .

(4)

Since each model learns from the temporal ensemble knowledge of the counterpart rather than

its immediate state, the mutual learners are kept diverged and reach a consensus more gradually

compared to the direct mutual learning. To

0.313

prove that indirect mutual learning can slow

0.273

down the speed of model coupling, we train mutual networks of 3 layers of MLPs on the

0.234

Total variation

MNIST dataset of which 1/60 images are used

0.195

as labeled data for the SSL setting. We mea- 0.156

sure the total variation distance of the softmax

0.117

Direct mutual learning Indirect mutual learning Indirect mutual learning w/ noise

outputs of two teacher networks during training. 0.078

Figure 2 shows that the indirect teaching using

0.039

mean teachers indeed leads to more divergent

00

1000

2000

3000

4000

5000

mutual learners whereas direct mutual learning

Iterations

causes model collapse (total variation distance

Figure 2: MNIST experiment.

approaches to zero) at the early training phase.

In practice, each learner in our framework simultaneously learns from two supervisions (Figure 1d): the peer-supervision given by the peer mean teacher and self-supervision provided by its own mean teacher. We empirically find that learning from such an ensemble knowledge further improves the SSL performance. Therefore, the loss function for training two models is defined as,

Lu(1, 2) = H(y^2, h1 ((xu))) + H(y^1, h1 ((xu))),

(5)

+ H(y^1, h2 ((xu))) + H(y^2, h2 ((xu))).

Data augmentation and model noises. Indirect mutual learning alleviates the model coupling to some extent but still requires different initialized mutual learners [18, 19]. In contrast, we inject noises including input noise (i.e., data augmentation) and model noises to perturb the mutual learners and ensure their divergency throughout the training process. The data augmentations applied for the student model include photometric augmentations in RandAugment [52]. On the other hand, the model noises, including dropout [25] and stochastic depth [26], are able to bring architectural perturbations. The benefit of the noise injection is two-fold. First, they force the student model to

4

learn in a hard way, which has been proven crucially important for self-training [8]. Second, these perturbations prevent the mutual learners from closely tracking the state of the counterpart, hence the chance of model coupling is greatly reduced. In addition, we adopt CutMix [11], a kind of data augmentation that stitches training images based on a binary mask, to improve consistency learning.
In particular, we study the effectiveness of model noise using the MNIST experiment. As shown in Figure 2, we see clearly improved divergency of the mutual learners during training. Hence, the students consistently obtain complementary knowledge and collaboratively attain higher performance.
Heterogeneous architecture. Last but not least, besides injecting model noises to bring architectural perturbations, another straightforward way to ensure the architectural difference is to directly adopt distinct architectures. As opposed to using convolutional neural networks (CNN) with dissimilar backbones, we investigate Transformer [53] which has shown tantalizing promise in the vision tasks. Since the Transformer is able to capture long-range dependency while CNN is good at modelling local context, it seems that a heterogeneous architecture with CNN and Transformer naturally fits the mutual learning framework as students learn complementary knowledge. To validate our hypothesis, we experiment with the mutual learning between a CNN-based Deeplabv2 model and SETR [28], a transformer-based segmentation network. The results in Section 5 show that both networks, even without any noise injection, achieve significant gain from mutual learning.

4.2 Pseudo Label Self-rectification

Mutual learning heavily relies on the peer network but neglects the fact that the network itself can also identify suspicious pseudo labels. Since the pseudo labels are generated by the supervised model whose prediction confidence can be measured using the maximum softmax probability, Monte-Carlo dropout or learned confidence estimates [54, 55], a natural idea is to cultivate the network's own ability to spot and suppress the unreliable pseudo labels.

Pseudo label correction. To this end, we do not rely on the above uncertainty estimation techniques

as they require a predefined threshold to remove the unreliable pseudo labels. Instead, we aim to online

rectify the pseudo labels that serve as progressively improved supervisions. Contrary to the peer-

rectification, the self-rectification does not rely on any external knowledge. Such self-rectification,

which stems from [24], rectifies the pseudo labels according to the class-wise confidence that is estimated online. Formally, let p  RH×W ×K be the soft pseudo label for xu, i.e., p = h(xu), whereas pk  RH×W is the kth class probability given by the network output. We denote the initial state of this class probability as p0k, which is dynamically re-weighted according to the class-wise confidence k  RH×W ×K . Hence, the hard pseudo label y~  RH×W ×K is online updated as:

y~ = one_hot {pt0, pt1, ..., ptK } , where ptk = k · p0k.

(6)

Such rectified pseudo labels are further used as supervisions when optimizing Equation 5, so that the

students can always learn from denoised knowledge.

Class-wise confidence estimation. Now the problem reduces to calculating the class-wise confidence k. To achieve this, we model the clustered feature space as the exponential family mixture:

K

K

p(z|) = kp(z|k) = k exp -d(z, k) ,

(7)

k=1

k=1

where the cluster center k  RC, or the prototype, serves as the representative for the cluster, and k is the weight for mixture distributions. Then, the posterior probability of the assignment y for a given z can be computed as,

p(y = k|z) = k exp -d(z, k) ,

(8)

k k exp -d(z, k )

which is exactly the class-wise confidence useful to the pseudo label correction. When the mixture distributions are equally weighted, i.e., 1 = 2, ..., = K , and Euclidean distance is adopted, the class-wise confidence is essentially the softmax of distances to different prototypes, i.e.:

k =

exp(- f~(x) - k ) k exp(- f~(x) - k

. )

(9)

5

Here we use the feature space extracted by the mean teacher. Intuitively, the prediction confidence of belonging to kth class is measured by how much it is more close to k relative to other prototypes.

Prototype calculation Ideally, prototypes should be computed using all the training images in each iteration, i.e.,

k

=

|Dl

1  Du|

meanHi W
x{Dl Du }

f~(x)i · 1(argmax(h~(x)i) == k)

,

(10)

where i denotes the position index. However, this is computationally prohibitive for the online training. As an approximation, we compute the moving average:

k  k + (1 - )k,

(11)

where k is the feature mean of kth class for the current batch, and the momentum  is set to 0.9999. During training, this moving average estimation converges to the true cluster center as the features gradually evolve. Note that our approach is similar to clustering-based unsupervised learning [56, 57] that online computes the cluster centers, but we use the prototypes to rectify the soft pseudo label rather than cluster assignment for the following representation learning. Another benefit of using prototypes is that all the classes are treated equally regardless of their occurrence, so the segmentation suffers less from the class-imbalance issue.

The maintained K prototypes serve as the network knowledge of the underlying feature clusters. The pseudo label rectification using such self-knowledge complements the denoising capability of the peer network, and both schemes can jointly refine the pseudo labels during training, leading to a significant performance boost for semi-supervised learning.

5 Experiments
5.1 Dataset and Evaluation Protocol
Datasets. We conduct experiments on two commonly-used benchmarks for semantic segmentation.
· Cityscapes. This is a finely annotated urban scene dataset with 19 categories and consists of 2975 training images. The original image size is 2048 × 1024. Following [13, 58], we resize images to 1024 × 512 and randomly crop them to 512 × 256 during training.
· PASCAL VOC 2012. The standard dataset contains 1464 images that comprise 21 classes including background. As a common practice, we also obtain 9118 augmented images using [58]. Images are randomly resized by 0.5  1.5 and cropped to 321 × 321 for training.
Evaluation protocol. We evaluate the SSL performance under the learning from a varied amount of labeled images on two datasets. For the Cityscapes dataset, we randomly sample 1/30, 1/8 and 1/4 images to construct the labeled data and regard the rest of the training images as unlabeled data. For PASCAL VOC, we use 1/100, 1/50, 1/20 and 1/8 images from the standard training set as labeled data while the remaining images in the standard training set together with the augmented images constitute the unlabeled data. For a fair comparison, we use the same data split as [59] when subsampling the labeled data. We measure the quantitative performance using mIoU, i.e., the mean of class-wise intersection over union.
5.2 Implementation Details
In our experiments, the CNN-based segmentation network adopts the Deeplabv2 [60] architecture with the ResNet101 [61] as the backbone. We apply photometric augmentations within RandAugment [52] as well as CutMix [11] for data augmentation. In terms of the model noises, we introduce dropout [25] only for the last FC layer with the dropout rate as 0.5, and apply stochastic depth [26] to the residual blocks of backbone with layer-wise survival probability as 0.8. The proposed framework is optimized with SGD solver. The learning rate is 1e-4 with polynomial decay. We adopt stage-wise training strategy, i.e., once the mutual learning converges we recompute the pseudo labels and then start the next training stage. In our experiments, we observe performance saturation after training two stages. For Cityscapes dataset, we train 90k iterations for each stage with the batchsize of 4; whereas each

6

Table 1: Quantitative comparisons on Cityscapes dataset. We use a varied amount of labeled images and report the mIoU score (in percentage). Models are initialized with the pretrained weights on ImageNet except that CowMix [59] and DMT [19] report the performance using the COCO initialization.

Cityscapes

# Labels

Methods (ImageNet init.) 1/30 (100) 1/8 (372) 1/4 (744) Full (2975)

Deeplabv2 [60]
AdvSSL [58]
S4GAN [66]
CutMix [13] CowMix [59]
ClassMix [14]
ECS [23] DMT [19]

51.20±2.29 49.01±2.58 54.07±1.61 54.8

59.3 57.1 59.3 60.34±1.24 60.53±0.29 61.35±0.62 60.26±0.84 63.0

61.9 60.5 61.9 63.87±0.71 64.10±0.82 63.63±0.33 63.77±0.65
-

66.0 66.2 65.8 67.68±0.37
68.2

Deeplabv2 (reimplement) 45.40±1.04 56.08±0.27 60.91±0.62 67.14±0.32

Ours

58.25±1.36 63.97±0.46 66.15±0.06

-

Image

CutMix [13]

Robust mutual learning

Ground truth

Figure 3: Qualitative comparison on Cityscapes dataset. Models are trained using 1/8 training images as labeled data.

training stage of PASCAL VOC lasts 60k iterations with the batchsize of 10. For both datasets, we initialize the backbone with ImageNet [62] pre-trained weights. Besides, we also use the pre-trained weights from the MS COCO dataset [63] to initialize the model specifically for PASCAL VOC as suggested by [64, 17]. The training takes around 1.5 days using 4 Tesla V100 GPUs. Our method is implemented with Pytorch [65] and we plan to make the code publicly available.
5.3 Comparisons with Previous Works
We comprehensively compare our method with state-of-the-art methods which can be categories as: 1) adversarial based methods, i.e., AdvSSL [58] and S4GAN [66]; 2) methods that encourage consistency, i.e., ICT [6], CutMix [13], CowMix [59] and ClassMix [14]; 3) two recent works that also collaboratively train networks, i.e., ECS [23] and DMT [19]. For fair comparisons, we report the performance reported by their original papers under the same SSL setting.
Table 1 shows the quantitative comparison on Cityscapes dataset. Our method outperforms the prior leading approach significantly. We can see that the performance advantage becomes more evident when using fewer labeled images for training. In particular, when using 1/30 labeled data, our method improves the mIoU by 3.45% over prior best approach. Notably, our training on 1/4 labeled images gives mIoU on par with supervised learning, proving the data efficiency of our approach. Besides, comparing with DMT that also adopts mutual learning, our robust mutual learning is more powerful to denoise the pseudo labels and hence yields higher performance. We visualize the qualitative results
7

Table 2: Quantitative comparisons on PASCAL VOC dataset. We use a varied amount of labeled images and report the mIoU scores (in percentage). ImageNet initialization is used.

PASCAL VOC

# Labels

Methods (ImageNet init.) 1/100 1/50 1/20 1/8 Full (10582)

Deeplabv2 [60] AdvSSL [58] S4GAN [66]
ICT [6] CutMix [13]

35.82 53.79

48.3 49.2 60.4 46.28 64.81

56.8 59.1 62.9 53.17 66.48

62.0 64.3 67.3 59.63 67.60

70.7 71.4 73.2 71.50 72.54

Deeplabv2 (reimplemtent) 36.25 46.30 55.92 63.73

Ours

53.77 64.63 68.82 70.34

72.62 -

Table 3: Quantitative comparisons on PASCAL VOC dataset. We use a varied amount of labeled images and report the mIoU score (in percentage). COCO initialization is used.

PASCAL VOC

# Labels

Methods (COCO init.) 1/100 1/50 1/20 1/8 Full (10582)

Deeplabv2 [60] AdvSSL [58] S4GAN [66]
DMT [19] ECS [23] ClassMix [14]
Ours

63.04 54.18
63.60

53.2 57.2 60.9 67.15
66.15
68.66

58.7 64.7 66.4 69.92
67.77
71.19

65.2 69.5 69.5 72.70 72.95 71.00
73.38

73.6 74.9 73.9 74.75
-
-

Table 4: Ablation study in terms of mIoU. We ablate proposed components on Cityscapes 1/8 labels. IML: indirect mutual learning. RML: robust mutual learning.

Supervised baseline

IML

IML

IML

(+model noises) (+ input noises)

IML (+ full noises)

RML

RML (two stages)

56.03 60.09

60.51

60.49

61.25

62.85

63.65

in Figure 3. Compared with CutMix, our method excels at discriminating tiny objects and resolving ambiguity, giving results more consistent with the ground truth.
When conducting experiments on PASCAL VOC, we compare different methods using two types of initializations. Table 2 shows the performance using the ImageNet initialization. We improve the mIoU by 2.34% and 2.74% for 1/20 and 1/8 labeled images respectively. For lower data regime, our method achieves almost the same performance as the prior leading approach. On the other hand, the COCO dataset contains images with more similar content as the PASCAL, so the COCO initialization generally leads to better SSL performance. As shown in Table 3, our method yields superior mIoU scores in all the data regimes.
5.4 Discussions
Ablation study. We quantify the effectiveness of each proposed component in Table 4. We see that indirect mutual learning (IML) improves the mIoU of supervised baseline by 4.06%. The model noises and inputs noises are essential to reduce the mutual coupling and can jointly contribute to the mIoU gain by 1.16%. Moreover, with the pseudo label self-rectification, the single-stage robust mutual learning (RML) obtains mIoU improvement by 1.6%, and the second training stage achieves even higher performance. Overall, the improvement of our two-stage RML is substantial with the mIoU gain over the supervised baseline by 7.62%.

8

Table 5: Heterogeneous architecture (e.g., CNN vs Transformer) fits the mutual learning framework due to less model coupling (results on Cityscapes using 1/8 labeled data).

Architecture

mIoU

Deeplabv2 baseline SETR [28] baseline

54.52 57.79

Deeplabv2-Deeplabv2 IML 61.25 / 61.53

SETR-SETR IML

64.15 / 63.74

Deeplabv2-SETR IML 63.96 / 64.35

The online refinement of pseudo labels. The key motivation of our robust mutual learning is to online refine the pseudo labels so that better supervisions can be obtained for selftraining. Figure 4 visualizes how the pseudo labels progress in the course of training. Comparing with the MeanTeacher [4], direct mutual learning produces better pseudo labels as noises can be gradually suppressed by the peer network. In comparison, indirect mutual learning reduces the mutual coupling and thus leads to more effective co-teaching. Finally, much cleaner pseudo labels are produced in the robust mutual learning thanks to the strong denoising capability of the self-rectification scheme.

mIoU (%)

66

64

62

Mean teacher (Val:59.81)

Direct mutual learning (Val:60.61)

Indirect mutual learning (Val:61.25)

60

Robust mutual learning (Val:62.85)

0

20

40

60

80

Iterations (k)

Figure 4: The pseudo label online refinement.

Heterogeneous architecture. To validate the hypothesis that heterogeneous architecture is effective to reduce the model coupling, we adopt CNN and Transformer network to the mutual learning framework. Specifically, the student models adopt Deeplabv2 and SETR [28] architecture respectively. We study such hybrid architecture without noise injection nor self-rectification. Table 5 gives the result and we highlight several interesting findings. First, comparing to the baseline, all the architectural combinations benefit largely from mutual learning, proving the universality of the method. Second, the mutual learning between Deeplabv2 and SETR demonstrates the best performance. Notably, Deeplabv2 improves the mIoU by 2.71% when mutually trained with SETR compared to the mutual training with the same architecture. On the other hand, instead of being pulled down by the Deeplabv2, Transformer-based SETR obtains useful peer-supervision and achieves the highest performance in such hybrid mutual learning.

Limitation. While our method has shown state-of-the-art performance, the performance advantage on PASCAL VOC dataset is not as obvious as Cityscapes. We conjecture that this is because PASCAL VOC dataset is not finely annotated where the background class is not clean. Thus, the prototypes computed for such class may not be accurate. Besides, it takes many iterations to update the class-wise prototypes since each image in this dataset only contains 2  3 classes. We believe there is still room for further improvement along with our RML, which we leave for future work.

6 Conclusion
In this paper, we identify the model coupling issue in the current mutual learning works and comprehensively explore factors including aggressive input augmentation, model perturbation and adopting heterogeneous architectures for the mutual learners to alleviate the issue. To enhance the accuracy of peer supervision, we propose a self-rectification scheme that explicitly refines the pseudo labels by leveraging the self-knowledge of the network. The resulting framework, termed robust mutual learning, is able to generate better pseudo labels when simultaneously conducting the self-training and mutual learning. Our approach is a step further to narrow the gap with supervised learning for low-data regime and hopefully will inspire more research on denoising pseudo labels for more effective semi-supervised learning.

9

References
[1] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979­1993, 2018. 1
[2] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. arXiv preprint arXiv:1606.04586, 2016. 1, 3
[3] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. 1
[4] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017. 1, 2, 3, 4, 9
[5] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848, 2019. 1, 3
[6] Vikas Verma, Kenji Kawaguchi, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. arXiv preprint arXiv:1903.03825, 2019. 1, 7, 8
[7] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020. 1, 3
[8] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10687­10698, 2020. 1, 3, 5
[9] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, 2013. 1, 2, 3
[10] Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V Le. Meta pseudo labels. arXiv preprint arXiv:2003.10580, 2020. 1, 2, 3
[11] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6023­6032, 2019. 1, 5, 6
[12] Yuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li, Xiao Bian, Jia-Bin Huang, and Tomas Pfister. Pseudoseg: Designing pseudo labels for semantic segmentation. arXiv preprint arXiv:2010.09713, 2020. 1
[13] Geoffrey French, Samuli Laine, Timo Aila, Michal Mackiewicz, and Graham Finlayson. Semi-supervised semantic segmentation needs strong, varied perturbations. In British Machine Vision Conference, 2020. 1, 3, 6, 7, 8
[14] Viktor Olsson, Wilhelm Tranheden, Juliano Pinto, and Lennart Svensson. Classmix: Segmentation-based data augmentation for semi-supervised learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1369­1378, 2021. 1, 3, 7, 8
[15] Jianlong Yuan, Yifan Liu, Chunhua Shen, Zhibin Wang, and Hao Li. A simple baseline for semi-supervised semantic segmentation with strong data augmentation. arXiv preprint arXiv:2104.07256, 2021. 1
[16] Yassine Ouali, Céline Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with crossconsistency training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12674­12684, 2020. 1, 3
[17] Rihuan Ke, Angelica Aviles-Rivero, Saurabh Pandey, Saikumar Reddy, and Carola-Bibiane Schönlieb. A three-stage self-training framework for semi-supervised semantic segmentation. arXiv preprint arXiv:2012.00827, 2020. 1, 7
[18] Zhanghan Ke, Kaican Li Di Qiu, Qiong Yan, and Rynson WH Lau. Guided collaborative training for pixel-wise semi-supervised learning. In ECCV, volume 2, page 6. Springer, 2020. 1, 2, 3, 4
[19] Zhengyang Fenga, Qianyu Zhoua, Qiqi Gua, Xin Tana, Guangliang Chengb, Xuequan Luc, Jianping Shib, and Lizhuang Maa. Dmt: Dynamic mutual training for semi-supervised learning. arXiv preprint arXiv:2004.08514, 3(6):7, 2020. 1, 2, 3, 4, 7, 8
10

[20] Eric Arazo, Diego Ortego, Paul Albert, Noel E O'Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1­8. IEEE, 2020. 1
[21] Zhedong Zheng and Yi Yang. Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation. International Journal of Computer Vision, 129(4):1106­1120, 2021. 1
[22] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudolabeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. arXiv preprint arXiv:2101.06329, 2021. 1
[23] Robert Mendel, Luis Antonio de Souza, David Rauber, João Paulo Papa, and Christoph Palm. Semisupervised segmentation based on error-correcting supervision. In European Conference on Computer Vision, pages 141­157. Springer, 2020. 1, 7, 8
[24] Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, and Fang Wen. Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation. arXiv preprint arXiv:2101.10979, 2, 2021. 1, 2, 5
[25] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1): 1929­1958, 2014. 2, 4, 6
[26] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European conference on computer vision, pages 646­661. Springer, 2016. 2, 4, 6
[27] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2
[28] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020. 2, 5, 9
[29] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. arXiv preprint arXiv:1911.09785, 2019. 3
[30] Antti Tarvainen and Harri Valpola. Weight-averaged, consistency targets improve semi-supervised deep learning results. CoRR,, vol. abs/1703, 2017, 1780. 3
[31] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv preprint arXiv:1905.02249, 2019. 3
[32] Byoungjip Kim, Jinho Choo, Yeong-Dae Kwon, Seongho Joe, Seungjai Min, and Youngjune Gwon. Selfmatch: Combining contrastive self-supervision and consistency for semi-supervised learning. arXiv preprint arXiv:2101.06480, 2021. 3
[33] Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, and Pheng-Ann Heng. Semi-supervised skin lesion segmentation via transformation consistent self-ensembling model. arXiv preprint arXiv:1808.03887, 2018. 3
[34] Christian S Perone and Julien Cohen-Adad. Deep semi-supervised segmentation with weight-averaged consistency targets. In Deep learning in medical image analysis and multimodal learning for clinical decision support, pages 12­19. Springer, 2018. 3
[35] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 3
[36] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 3
[37] Ying Chen, Xu Ouyang, Kaiyue Zhu, and Gady Agam. Mask-based data augmentation for semi-supervised semantic segmentation. arXiv preprint arXiv:2101.10156, 2021. 3
[38] Geoffrey French, Timo Aila, Samuli Laine, Michal Mackiewicz, and Graham Finlayson. Consistency regularization and cutmix for semi-supervised semantic segmentation. arXiv preprint arXiv:1906.01916, 2 (4):5, 2019. 3
11

[39] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd annual meeting of the association for computational linguistics, pages 189­196, 1995. 3
[40] Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. Revisiting self-training for neural sequence generation. arXiv preprint arXiv:1909.13788, 2019. 3
[41] Jacob Kahn, Ann Lee, and Awni Hannun. Self-training for end-to-end speech recognition. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7084­7088. IEEE, 2020. 3
[42] Daniel S Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, and Quoc V Le. Improved noisy student training for automatic speech recognition. arXiv preprint arXiv:2005.09629, 2020. 3
[43] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1476­1485, 2019. 3
[44] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training and self-training. arXiv preprint arXiv:2006.06882, 2020. 3
[45] I Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:1905.00546, 2019. 3
[46] Zhengyang Feng, Qianyu Zhou, Guangliang Cheng, Xin Tan, Jianping Shi, and Lizhuang Ma. Semisupervised semantic segmentation via dynamic self-training and classbalanced curriculum. arXiv preprint arXiv:2004.08514, 1(2):5, 2020. 3
[47] Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng, Maxwell D Collins, Ekin D Cubuk, Barret Zoph, Hartwig Adam, and Jonathon Shlens. Leveraging semi-supervised learning in video sequences for urban scene segmentation. arXiv preprint arXiv:2005.10266, 2020. 3
[48] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In International Conference on Machine Learning, pages 7164­7173. PMLR, 2019. 3
[49] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. arXiv preprint arXiv:1804.06872, 2018. 3, 4
[50] Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, and Rynson WH Lau. Dual student: Breaking the limits of the teacher in semi-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6728­6736, 2019. 3
[51] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification. arXiv preprint arXiv:2001.01526, 2020. 3
[52] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702­703, 2020. 4, 6
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. 5
[54] Terrance DeVries and Graham W Taylor. Leveraging uncertainty estimates for predicting segmentation quality. arXiv preprint arXiv:1807.00502, 2018. 5
[55] Matteo Poggi, Filippo Aleotti, Fabio Tosi, and Stefano Mattoccia. On the uncertainty of self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3227­3237, 2020. 5
[56] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pages 132­149, 2018. 6
[57] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020. 6
12

[58] Wei Chih Hung, Yi Hsuan Tsai, Yan Ting Liou, Yen-Yu Lin, and Ming Hsuan Yang. Adversarial learning for semi-supervised semantic segmentation. In 29th British Machine Vision Conference, BMVC 2018, 2019. 6, 7, 8
[59] Geoff French, Timo Aila, Samuli Laine, Michal Mackiewicz, and Graham Finlayson. Semi-supervised semantic segmentation needs strong, high-dimensional perturbations. arXiv preprint arXiv:1906.01916, 2019. 6, 7
[60] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834­848, 2017. 6, 7, 8
[61] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­778, 2016. 6
[62] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248­255. Ieee, 2009. 7
[63] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740­755. Springer, 2014. 7
[64] Wei-Chih Hung, Yi-Hsuan Tsai, Yan-Ting Liou, Yen-Yu Lin, and Ming-Hsuan Yang. Adversarial learning for semi-supervised semantic segmentation. arXiv preprint arXiv:1802.07934, 2018. 7
[65] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019. 7
[66] Sudhanshu Mittal, Maxim Tatarchenko, and Thomas Brox. Semi-supervised semantic segmentation with high-and low-level consistency. IEEE transactions on pattern analysis and machine intelligence, 2019. 7, 8
13

