arXiv:2106.00808v2 [cs.LG] 7 Jun 2021

Invariant Policy Learning: A Causal Perspective
Sorawit Saengkyongam, Nikolaj Thams, Jonas Peters, and Niklas Pfister
University of Copenhagen, Denmark
Emails: {ss, thams, jonas.peters, np}@math.ku.dk
Abstract
In the past decade, contextual bandit and reinforcement learning algorithms have been successfully used in various interactive learning systems such as online advertising, recommender systems, and dynamic pricing. However, they have yet to be widely adopted in high-stakes application domains, such as healthcare. One reason may be that existing approaches assume that the underlying mechanisms are static in the sense that they do not change over time or over different environments. In many real world systems, however, the mechanisms are subject to shifts across environments which may invalidate the static environment assumption. In this paper, we tackle the problem of environmental shifts under the framework of offline contextual bandits. We view the environmental shift problem through the lens of causality and propose multi-environment contextual bandits that allow for changes in the underlying mechanisms. We adopt the concept of invariance from the causality literature and introduce the notion of policy invariance. We argue that policy invariance is only relevant if unobserved confounders are present and show that, in that case, an optimal invariant policy is guaranteed, under certain assumptions, to generalize across environments. Our results do not only provide a solution to the environmental shift problem but also establish concrete connections among causality, invariance and contextual bandits.
1 Introduction
The problem of learning decision-making policies lies at the heart of learning systems. To adopt these learning systems in high-stakes application domains such as personalized medicine or autonomous driving, it is crucial that the learnt policies are reliable even in environments that have never been encountered before. In this paper, we consider the problem of learning policies that are robust with respect to shifts in the environments. We consider this question in the setup of offline contextual bandits, which provides a mathematical framework for tackling the above learning problems.
While recent studies in offline contextual bandits (Dudik et al., 2011; Bottou et al., 2013; Swaminathan and Joachims, 2015a,b; Kallus, 2018; Zhou et al., 2018; Athey and Wager, 2021) offer theoretical results and novel methodologies for policy learning from offline data, they primarily focus on an identically and independent distributed (i.i.d.) setting, in which the underlying mechanisms do not change over time or over different environments. In practice, however, shifts between environments often occur, possibly invalidating the i.i.d. assumption. In healthcare, for example, datasets from different hospitals may not come from the same underlying distribution. As a result, a learning agent that ignores environmental shifts may fail to generalize beyond the environment it was trained on.
In the supervised learning context, the environmental shift problem has been studied under different terms, such as domain generalization, distributional robustness or out-of-distribution generalization (Muandet et al., 2013; Volpi et al., 2018; Arjovsky et al., 2019; Christiansen et al., 2020). In domain generalization, the goal is to develop learning algorithms that are robust to changes in the test distribution. Thus a fundamental problem is how to characterize such changes. A promising direction relies on a causal framework to describe the changes through the concept of
1

interventions (Schölkopf et al., 2012; Rojas-Carulla et al., 2018; Arjovsky et al., 2019; Christiansen et al., 2020; Magliacane et al., 2018). A key insight is that while purely predictive methods perform best if test and training distributions coincide, causal models generalize to arbitrarily strong interventions on the covariates.
In real world applications, however, causal knowledge may not be available. In recent years, invariance-based methods have been exploited to learn causal structure from data (Peters et al., 2016; Pfister et al., 2018; Heinze-Deml et al., 2018). In invariant causal prediction (Peters et al., 2016), for example, one assumes that the data are collected from different environments, each of which describes different underlying mechanisms under which the data were generated and uses this heterogenity to learn the causal parents of an outcome variable Y . The underpinning assumption is the invariance assumption, which posits the existence of a set of predictors X in which the mechanism between X and Y remains constant. A model based on such invariant predictors is guaranteed to generalize to all unseen environments. Although some recent studies have explored the use of causality and invariance for tackling the environmental shifts problem in contextual bandit and, more generally, reinforcement learning problems (Zhang et al., 2020; Sonar et al., 2020), the actual roles and benefits of causality and invariance remain unclear and under-explored.
Our paper delineates an explicit connection among causality, invariance, and the environmental shift problem in the context of contextual bandits. We develop a causal framework for characterizing the environmental shift problem in contextual bandits, and provide a practical and theoretically sound solution based on the proposed framework. Our framework differs from the framework of causal bandits (Lee and Bareinboim, 2018; Lattimore et al., 2016; Yabe et al., 2018; de Kroon et al., 2020). While causal bandits exploit causal knowledge (either assumed to be known a priori or estimated from data) for improving the finite sample performance or the regret bound in a single environment, our framework focuses on modeling distributional shifts and the ability to generalize to new environments.
Our contributions are fourfold. First, we propose a multi-environment contextual bandit framework that represents mechanisms underlying a contextual bandit problem by structural causal models (SCMs; Pearl (2009)). The framework allows for changes in environments and thereby relaxes the i.i.d. assumption. We define environments as different perturbations on the underlying SCM, and we evaluate the policy according to its worst-case performance in all possible environments. Second, using the proposed framework, we generalize the invariance assumption used in methods such as invariant causal prediction and define an invariant policy that, under certain assumptions, is guaranteed to generalize across different environments. Third, we develop an offline method for testing invariance under distributional shifts, and provide an algorithm for finding an optimal invariant policy. Fourth, we highlight a setting in which causality and invariance are not necessary for solving the environmental shift problem. This insight takes us closer to understanding what causality can offer in contextual bandits.
The remainder of our paper is organized as follows. The rest of this section briefly reviews an offline contextual bandit problem. Section 2 formally defines a causal framework for multienvironment contextual bandits and highlights the roles of causality and invariance in this formulation. Drawing on the proposed framework, Section 3 introduces invariant policies and provides the main theoretical contributions underpinning our solution for the environmental shift problem. Section 4 presents an algorithm for learning an optimal invariant policy from offline data. Section 5 provides simulation experiments which empirically verify our theoretical results. Section 6 concludes and discusses implications for future research.
1.1 Offline Contextual Bandits
We briefly review the offline contextual bandit problem (Beygelzimer and Langford, 2009; Strehl et al., 2010), considering a setup in which part of the contexts are unobserved. More precisely, we assume that the context variables can be partitioned into observed and unobserved context variables X  X and U  U , respectively. As in the standard contextual bandit setup (Langford and Zhang, 2008), for each round we assume that the system generates a context (X, U ) and reveals only the observable X to an agent. From the observed context X, the agent selects an
2

action A  A according to a policy  : X - (A) that maps the observed context to the probability simplex (A) over the set of actions A. Adapting commonly used notation, we write, for all x  X and a  A, (x|a) := (x)(a). The agent then receives a reward R depending, first, on the chosen action A, and, second, on both the observed and unobserved contexts (X, U ).
In the classical setting, one assumes that the contexts in each round are drawn i.i.d. from a joint distribution PX,U (an assumption we will relax when introducing multi-environment contextual bandits in Section 2) and that the reward is drawn from a conditional distribution PR|X,U,A. The agent is evaluated based on the performance of its policy  which is measured by the policy value:
V () := E(X,U )PX,U EA(X) ERPR|X,U,A R .
The agent is now given a fixed training dataset that is collected offline: it consists of n rounds from one or more different policies, i.e., D := {(Xi, Ai, Ri, i(Xi))}ni=1, where Ai  i(Xi) for all i  {1, . . . , n}. The goal of the agent is then to find a policy  that maximizes the policy value over a given policy class , i.e.,   arg max V ().
This setting assumes that the context in each round is sampled i.i.d. from some fixed distribution; this implies that the environment in which we deploy the agent is identical to the environment in which the training dataset was collected. Section 2 introduces a causal framework for multi-environment contextual bandits, a framework that relaxes the i.i.d. assumption.

2 A Causal Framework for Multi-environment Contextual Bandits

Instead of having a fixed distribution over the context variables PX,U , we introduce a collection E of environments such that, in each round, the context and reward are drawn from an environment specific distribution PeX,U that depends on the environment e  E in that round.
In practice, the agent only observes part of the environments Eobs  E and aims to generalize to unseen test environments E \ Eobs. More specifically, in this paper we aim to maximize the worstcase expected reward. To formalize the problem, we introduce a framework that puts assumptions on how environments change the distributions of X, U and R. The assumptions are constructed via an underlying class of structural causal models (SCMs) indexed by the environment and policy.1
Setting 1 (Multi-environment Contextual Bandits). Let X = X 1 × . . . × X d, U = U 1 × . . . × U p and A = {a1, . . . , ak} be measureable spaces, let  := {X - (A)} be the set of all policies and let E be a collection of environments. For all    and all e  E we consider the following SCMs,

U 

:= s(X,

U)



S(, e) : X := he(X, U, X )

(1)

A := g(X, A)





R := f (X, U, A, R),

where (X, U, A, R)  X ×U ×A×R, s, (he)eE , and f are measurable functions, = ( U , X , A, R)

is a random vector with a distribution Q = Q U Q X Q A Q R whose A component is uniform on (0, 1) and g is a function such that for all x  X it holds that g(x, A) is a discrete distribution

on A with probabilities (x).

We assume there exists a probability measure µ on X × U × A × R such that for all    and all e  E the SCM S(, e) induces a unique distribution P,e over (X, U, A, R) (see Bongers

et al. (2016) for details) which is dominated by µ and has full support on X. We denote the

corresponding density by p,e and the corresponding expectations by E,e. Whenever a probability,

density

or

expectation

does

not

depend

on

,

we

omit



and

write

Ee [X ]

rather

than

,e
E

[X

],

for

example.

1Readers familiar with the standard notion of SCMs may think about an SCM with a source node E. S(, e) then corresponds to an intervention on the action variable (change of policy) and on some of the observed context variables (change of environment). Here, we consider fixed environments, so that we do not have to consider them as random draws from an underlying distribution; see also Dawid (2002).

3

U

e

X

R



A

(a) Summary graph C in Setting 1.

X1

U

e



R

X2



A

(b) Graph G induced by the data generating process in Example 1.

Figure 1: (a) The summary graph induced by the model S(, e). (b) The graph induced by
the model in Example 1. The white and grey circles represent observed and hidden variables, respectively. In this example, a strategy depending on X1 and X2 may perform worse in a test environment than an (invariant) strategy depending only on X2. In practice, we do not assume
that the structure is known.

Some remarks regarding these assumptions are in order: (1) We only use the SCMs as a flexible way of modeling the changes in the joint distribution with respect to the environment e and the policy . In particular, we do not use it to model any intervention distributions that do not correspond to a change of policy or environment. (2) In practice, the precise form of the SCM is unkown. In particular, we neither assume knowledge of the functions of the SCM nor of the structure of the SCM (which can be represented by a graph, see below). (3) The assumption of a dominating measure for all environments ensures that we can always assume the existence of densities while also switching across environments. In particular, this avoids any measure theoretic difficulties regarding conditional distributions. (4) The assumption that the induced distributions over X have full support in all environments ensures that the generalization problem when moving from Eobs to E does not involve any extrapolation (we comment on this in more detail below Corollary 1). Additionally, it ensures that conditional expectations such as E,e[R | X = x] are always well-defined for all x  X easing the overall presentation.
For all   , we further introduce a graph G over the variables (X1, . . . , Xd, U 1, . . . , U p, A, R) that visualizes the structure of the SCMs S(, e) for all e  E. The nodes are connected according to the structural assignments, that is, we draw a directed edge from each variable that appears at least for one e on the right-hand side of a structural assignments to the variable on the left-hand side. Let I  {1, . . . , d} index the variables Xj for which the structural assignment Xj := hje(X, U, X ) in (1) varies with e, i.e. where there exist e, f  E such that hje = hjf . The environments E correspond to perturbations on variables XI, which implies that for each e  E the distribution P,e(XI | U, X{1,...,d}\I ) can be different. We then augment the graph with a square node to represent an environment e and draw a directed edge from the node e to each of the perturbation targets XI. Furthermore, we mark all edges from the nodes in X to A with  to represent their dependence on the policy (see Figure 1(b) for an example). Finally, we construct a summary graph C in which we group all the nodes in X and U as single nodes (see Figure 1(a)).
In Setting 1, the environments are considered fixed (non-random). However, we could also treat the environments as random variables which can be seen as a special case of the fixed-environment setting (see Appendix B).
We are now ready to define contextual bandits with multiple environments.
Definition 1 (Multi-environment Contextual Bandits). Assume Setting 1, then in a multienvironment contextual bandit setup, before the beginning of each round, the system is in an environment e  E, while the agent deploys a policy  : X - (A). Then, the system generates a context (X, U ) and reveals only the observable X and the environment label e to the agent. Based on the observed context X, the agent selects an action A according to the policy . The agent then receives a reward R, depending on the chosen action A and on both the observed and unobserved contexts (X, U ).
More precisely, we assume for all i  {1, . . . , n} that (Xi, Ui, Ai, Ri) are sampled independently

4

according to PXi,,Uei,A,R (see Setting 1). When |E| = 1, the setup reduces to a standard contextual bandit setup.

In the multi-environment contextual bandit setup, the context variables on different rounds

are not identically distributed due to the environment perturbations, this permits us to consider

situations in which the training environments differ from the test environments. The assumptions

in Setting 1 constrain how the environments affect the context and reward variables. Specifically,

an environment e can only perturb the distribution of the reward R through altering the structural

assignments of X. This constraint makes it possible to generalize information learned from one

set of environments to another.

In

this

formulation,

even

though

the

conditional

distribution

of

reward

,e
PR|X,U,A

is

assumed

to

be

invariant

across

environments,

the

distribution

after

marginalizing

out

U,

,e
PR|X,A

,

is

not

necessarily invariant, and since the variables U are unobserved we only have partial information

about

,e
PR|X,A

in

practice.

Example 1 (Linear Confounded Multi-environment Contextual Bandits). Consider a linear confounded multi-environment contextual bandit with the following underlying SCMs

S(, e) :

U := 

U



X 1 

:=

eU

+

X1

 X

2

:=

e

+

X2

A := g(X1, X2, A)



   R := 

X2 + U + R, X2 - U + R,

if A = 0 if A = 1,

where R, A, X1 , X2 , X3 are jointly independent noise variables with zero mean, e, e  R for all e  E and A = {0, 1}. Figure 1(b) depicts the induced graph G. In this example, the environments influence the observable contexts in two ways: (a) they change the mean of X2 via e and (b) they change the conditional mean of X1 given U via e, while the rest of the components remain fixed across different environments. Here, the environment-specific coefficient e modifies the correlation between the observable X1 and the unobserved variable U , and consequently between X1 and the reward R. Thus, an agent that uses information from X1 to predict the reward R
in the training environments may fail to generalize to other environments. To see this, consider
a training environment e = 1 and a test environment e = 2 and let 1 = 1, 2 = -1 be the environment-specific coefficients in the training and test environment, respectively. In the training environment, we have a large positive correlation between X1 and U , and consequently the agent will learn that the action A = 0 yields higher reward when observing positive value of X1 (and A = 1 otherwise). However, the correlation between X1 and U becomes negative (and large in
absolute value) in the test environment, which means that the policy that the agent learnt from the
training environment will now be harmful. We will see in Section 3 that a policy which depends on invariant variables (X2 in this example) does not suffer from this generalization problem and
is guaranteed to generalize across different environments.

2.1 Distributionally Robust Policies
To evaluate the performance of an agent across different environments, we define a policy value that takes into account environments. In particular, we focus on the worst-case performance of the agent across environments.
Definition 2 (Robust Policy Value). For a fixed policy   , and a set of environments E, we define the robust policy value V E ()  R as the worst-case expected reward
V E () := inf E,e R .
eE
Intuitively, an agent that maximizes the robust policy value is expected to perform well regardless of the environment. We now assume that for several environments e  Eobs, we are

5

U

e

X

R



A

Figure 2: A summary graph C in Setting 2. Here, there is no unobserved confounder and that we cannot benefit from taking into account environment structure, see Corollary 1.

given an i.i.d. sample from a multi-environment contextual bandit, see Definition 1. More pre-

cisely, we assume to observe D := {(Xi, Ai, Ri, i(Xi), ei)}ni=1, where ei  Eobs, Ai  i(Xi),

(Xi, Ai, Ri) ind. PXi,,Aei,R for all i  {1, . . . , n}. Using only D, we aim to solve the following maximin

problem:

arg max V E ().

(2)



Directly solving the maximin problem (2) is not feasible if we do not observe all the environments. A naive approach to this problem is to pool the data from all training environments and learn a policy that maximizes the policy value ignoring the environment structure. We will see in Section 2.2 that this is indeed optimal if all relevant context variables have been observed. If, however, hidden confounding is present, the naive approach no longer guarantees to find an optimal policy and the learnt policy may fail to generalize to unseen test environments. In Section 3, we introduce the notion of policy invariance. We will show that under certain assumptions solving the maximin problem (2) amounts to finding an optimal invariant policy which is then guaranteed to generalize across environments.

2.2 Policy Learning without Unobserved Confounders
This section illustrates a setting in which it is not beneficial to explicitly take into account the environment structure. Here, the naive approach of pooling the data from all training environments and applying a standard value-based policy learning algorithm yields a solution to (2). This result sheds light on the role of causality and invariance in contextual bandits and reinforcement learning.

Setting 2 (Unconfounded Multi-environment Contextual Bandits). In this setting, we assume that there is no unobserved confounder in the underlying causal model. More precisely, we modify the model class S(, e) in Setting 1 and consider

S(, e) :

U 

:= s(X,

U)



X := he(X, X )

A := g(X, A)   R := f (X, U, A, R)

with the corresponding summary graph shown in Figure 2.

Based on Setting 2, the following theorem shows that an optimal policy at the population level does not depend on the environment perturbations. In particular, the optimal policy can be learned from data obtained in any environment subset Eobs  E.
Theorem 1. Assume Setting 2, let Eobs  E be a non-empty subset of observed environments and    the policy defined for all x  X and all a  A by

(a | x) := 1 a = arg max QEobs (x, a ) ,

(3)

a A

6

where

QEobs (x, a)

:=

1
|E obs |

eEobs Ea,e[R | X = x], 1[·] denotes the indicator function and a is

the policy that always selects a. Then, it holds that

  arg max V E (),

i.e.,  is a solution to the maximin problem (2).

Proof. See Appendix A.1.

The proof of Theorem 1 uses that the conditional expectation of Ea,e[R | X] does not depend on the environment. This, in particular, implies that QEobs (x, a) = Ea,e[R | X = x] for any e  E. Theorem 1 therefore suggests that we can estimate the optimal policy by pooling the data from training environments and applying a standard value-based policy learning algorithm. The following theorem shows that such an approach indeed yields a consistent estimate of the optimal policy given that we have a consistent estimator Qn of the conditional mean Ea [R | X]. We
assume that Qn is estimated based on n independent observations (Xi, Ai, Ri) from potentially different environments.
Corollary 1. Assume Setting 2 and let Qn be a uniformly consistent estimator of QEobs , that is, for all a  A it holds that

lim ED sup Qn(x, a) - QEobs (x, a) = 0,

n

xX

where ED is an expectation over the n observations (Xi, Ai, Ri) used to estimate Qn. Then, for
n := 1 a = arg maxa A Qn(x, a ) it holds that

lim
n

ED

V E (n) - V E ()

= 0,

where  is the optimal policy defined in (3).

Whether it is possible to construct a uniformly consistent estimator Qn depends on the model class that has been assumed in the structural assignment of R, and on the policy used in generating the observations. For example, in the case of additive confounding and noise such as f (X, U, A, R) = g(X, A) + h(U, R) with g  F for some function class F ) and a policy  that has full support, i.e., a  A, x  X : (a | x) > 0, one can consider a least squares estimator of the form

QE obs
n

:=

arg min
f F

1 n

n
(f (Xi, Ai) - Ri)2.
i=1

The assumptions of Corollary 1 are then satisfied under further constraints on the function class and noise distributions, such as linear functions, Gaussian noise and bounded domains.
Corollary 1 implies that without hidden confounders, we do not benefit from taking into account the environment structure and considering invariance. However, the following section shows that this is different when hidden confounders do exist.

3 Invariant Policy for Distributional Robustness

We now introduce invariant policies and show that the maximin problem (2) can, under certain

assumptions, be reduced to finding an optimal invariant policy. For all subsets S  {1, . . . , d}, let us denote the set of all policies that depend only on XS by S := {   | S : X S 

(A) s.t. x  X , (·|x) = S(·|xS)}.

Because

of

the

hidden

confounding,

,e
E

[R

|

X

=

x]

is,

unlike

in

Section

2.2,

not

ensured

to

be independent of the environments. We will see in Proposition 1 that our model described in

Setting 1 nevertheless ensures that parts of the conditional mean may be invariant. To make this

precise, we first define an invariant policy.

7

Definition 3 (Invariant Policies). A policy  is said to be invariant with respect to a subset S  {1, . . . , d} if   S and for all e, f  E and all x  X S it holds that

,e
E

R | XS = x

=

,f
E

R | XS = x .

(4)

The conditioning set S in the above definition is important and motivates the following definition.
Definition 4 (Invariant Sets). A subset S  {1, . . . , d} is said to be an invariant set if there exists   S that satisfies (4).
By the Markov condition, which holds in SCMs, a d-separation statement in a graph implies conditional independence (Pearl, 2009; Lauritzen et al., 1990; Peters et al., 2017). Faithfulness (Spirtes et al., 2000) assumes that the converse holds, too. We now introduce mean faithfulness, which requires a dependence that is observable in the conditional mean.
Assumption 1 (Mean Faithfulness). For all S  {1, . . . , d} and for all   S it holds that
e, f  E, x  X S : E,e[R | XS = x] = E,f [R | XS = x] = R G e | XS,
where the symbol G denotes d-separation in G.
By definitions, the invariance property of a subset S only implies the existence of an invariant policy. However, under Assumption 1, the following proposition shows that if a subset S is invariant then all policies in S are also invariant.
Proposition 1. Assume Setting 1 and Assumption 1. Then, for every subset S  {1, . . . , d} and for all policies , ~  S, it holds that

 satisfies (4)  ~ satisfies (4).

Proof. See Appendix A.3.
In other words, a policy   S is an invariant policy if and only if S is an invariant set. This allows us to define the set of all invariant policies in terms of the invariant subsets. Formally, we denote the set of all invariant policies by

inv := {   | S s.t.  is invariant w.r.t. S}.

(5)

Equivalently, this set can also be written as

inv =

S ,

SSinv

where Sinv := {S  {1, . . . , d} | S is invariant} is the set of all invariant subsets. Invariant sets and policies play a central role in solving the maximin problem (2). The invari-
ance condition (4) ensures that the conditional expectation of the reward under an invariant policy has to remain identical across different environments. Intuitively, this means that an invariant policy  that yields a high reward in the training environments is expected to also yield a high reward in the unseen test environments.

Proposition 2. Assume Setting 1 and Assumption 1. Let inv be the set of all invariant policies defined in (5). Furthermore, let Eobs  E be a set of training environments, we then have that

arg max V E () = arg max V Eobs ()

(6)

inv

inv

Proof. See Appendix A.4.

8

In general, this is not true for non-invariant policies. In fact, we will see in Theorem 2 that under certain assumptions on the set E of environments, the worst-case reward of any non-invariant policy that is optimal on the training environments will be upper bounded by the worst-case reward of the optimal invariant policy.
We now outline the assumptions on the set of environments E facilitation this result. As we will see in the proof of Theorem 2 the crucial difference between invariant and non-invariant policies is that non-invariant policies use information - related to variables confounded with the reward - that may change across environments. In cases where the environments do not change the system `too strongly' it can therefore happen that using such information is beneficial across all environments. In practice one might however not know how strong the test environments can change the system in which case such information can become useless or even harmful. Intuitively, this happens, for example, if environments exist where the non-invariant confounded variables no longer contain any information about the reward. (Christiansen et al. (2020) use confounding removing interventions in the setting of prediction.) Formally, we make the following definition.

Definition 5 (Confounding Removing Environments). Assume Setting 1. An environment e is

said to be a confounding removing environment if there exists    such that for all j  {1, . . . , d}

for which

S  {1, . . . , d} : R G e | XS{j}

it holds that

Xj G,e U,

where G,e is the graph corresponding to the SCM S(, e).

To give an example, consider the graph G in Example 1 (see Figure 1(b)). A confounding removing environment is an environment that `removes' the incoming edge from U to X1. In such an environment, the variable X1 does not contain any information about the reward R.
The existence of confounding removing environments implies that at least in some of the environments it is impossible to benefit from a non-invariant policy. However, this is not necessarily sufficient to ensure that one can not benefit in the worst-case. Therefore we make the following additional assumption.

Assumption 2 (Strong Environments). For all e  E, there exists f  E such that f is a

confounding

removing

environment

and

,e
PX

=

,f
PX

.

Theorem 2. Assume Setting 1, Assumptions 1 and 2, and that both inv and  \ inv are

non-empty. Given a set of training environments Eobs  E, let ¯  arg maxinv V Eobs (),

 ¯

 arg max\inv V Eobs ()

be

optimal

policies

under

the

training

environments.

Then

V E (¯)  V E (). ¯

Proof. See Appendix A.5.

The above results motivate a procedure to solve the maximin problem (2). Proposition 2 implies that if we consider a policy class containing only the invariant policies, the maximin problem will reduce to a standard policy optimization problem, while Theorem 2 shows that an optimal invariant policy obtained in (6) always yields better or equal performance in terms of the robust policy value compared to a non-invariant one. In other words, given a training dataset D, we seek to operationalize the following two steps: (a) using D find the set inv of all possible invariant policies, (b) use standard methods for offline contextual bandits to solve arg maxinv V () on the data set D.
One of the key components of this method is to test whether a policy , which may be different from the policy generating the data, is invariant with respect to the set Eobs of training environments. The following section proposes such a test, proves theoretical guarantees, and gives a detailed overview of the whole procedure.

9

X1

U

X1

U

e

{1,2}

R

e

R

X2

{1,2}

A

(a)

X2

{2}

A

(b)

Figure 3: The graphs induced by the SCM S(, e) 3(a) with a policy that takes both X1, X2 as inputs, 3(b) with a policy that takes only X2 as an input.

4 Learning an Optimal Invariant Policy

4.1 Learning Invariant Sets

In the previous section, we showed that if a subset S  {1, . . . , d} is invariant, policies S  S generalize to unseen environments under suitable assumptions. We now consider the task of testing from data whether a set S is invariant. Let therefore H0(S, ) be the hypothesis that  and S satisfy the invariance property

H0(S, ) :

e, f



E, x



XS

:

E,e[R

|

XS

=

x]

=

,f
E

[R

|

XS

=

x].

Consider now a fixed set S. By Definition 4, S is invariant if there exists a S  S, such that

H0(S, S) is true. In this paper, we assume that the offline data are sampled from an initial policy 0 that does not necessarily satisfy 0  S. Therefore, testing H0(S, 0) is not the right step forward because it may happen that S is invariant but H0(S, 0) is false. To see this, consider, e.g., the setting in Figure 3(b). By Lemma 1, the set S := {X2} is invariant, i.e., H0(S, S) is true. Yet, if the initial policy 0 depends on both {X1, X2}, then H0(S, {1,2}) is not true: In Figure 3(a), the path e  X1  A  R is open, and so (given Assumption 1) we do not have

invariance

of

{1,2}
E

,e

[R

|

X {2} ].

Thus, we cannot directly test the invariance of a set S by using the initial policy and the

observed data. Instead, we need to test H0(S, S) for a policy S  S that is different from 0.

As we detail in the following section, we can do so by applying an off-policy test for invariance by

resampling the data to mimick the policy under S.

4.2 Testing Invariance under Distributional Shifts

To test the hypothesis H0(S, S) for some set S  {1, . . . , d}, we apply the off-policy test from Thams et al. (2021), which proposes to draw a target sample from S by resampling the offline
data that were drawn from 0, and to test invariance in the target sample.
More formally, we assume that for every e  Eobs we observe a data set De consisting of ne observations2 Die = (Xie, Aei , Rie, 0(Aei |Xie)). For all e  Eobs and all i  {1, . . . , ne} define the relative weights as

r(Die)

:=

S (Aei |Xie) 0(Aei |Xie)

,

where S  S is a target policy. Then, for all e  Eobs, we draw a weighted resample De,S := (Die1 , . . . , Dieme ) of size me from De with weights



me =1

r(Die

)



e
w := 0 i1,...,ime

(j1,...,jme ) distinct

me =1

r(Dje

)

(i1, . . . , ime ) distinct otherwise.

(7)

2It is even possible to allow a different initial policy i0 at each observation i. One then needs to define the relative weights as r(Die) := S (Aei |Xie)/i0(Aei |Xie).

10

We then apply an invariance test to the resampled data De1,S , . . . , DeL,S . An invariance test S is a function that takes data from environments e1, . . . , eL, each of size mei , and tests whether S is invariant
S : Dme1 +...+meL  {0, 1}.
Here,  = 1 indicates that we reject the hypothesis of invariance, and such a test is said to have pointwise asymptotic level if for all S invariant and all   S it holds that

lim sup P(S(De1 , . . . , DeL ) = 1)  .
ne1 ,...,neL 

We detail a concrete test  below, but we first state that the overall procedure has asymptotic level
if the test  has asymptotic level under . For simplicity, we assume that ne1 = · · · = neL =: n and me1 = · · · = meL =: m. The following result follows directly from Thams et al. (2021).

Proposition 3. Suppose that for each environment e1, of n observations Die = (Xie, Aei , Rie, i0(Aei | Xie)). Let

... m

, eL,we observe a data set = o( n) and assume that

De for

consisting all e  E,

0
E

[r(Die

)2

]

<

.

Consider

S

 S

and

for

all

e,

let

De,

:= (Die1 , . . . , Diem )

be

a resample

of

De drawn with weights given by (7). Let  be a hypothesis test for invariance of the conditional

expectation E,e[R | XS] that has asymptotic level   (0, 1) when  is applied to data sampled

from . Applying  to the resampled data yields pointwise asymptotic level, that is,

lim sup P0 ((De1,, . . . , DeL,) = 1)  
n

if S is invariant.

In other words, we can test whether XS is invariant by resampling the data and applying an invariance test on the resampled data set. Proposition 3 states that this procedure holds level if the sample size goes to infinity.

4.3 Choice of Target Test
We now detail a test  to test invariance in the target sample. We first pool data from all environments into one data set and estimate the conditional E[R | XS] using any prediction method (such as linear regression or a neural network). We then test whether the residuals R - E[R | XS] are equally distributed over the environments e  E, i.e., we split the sample back into L groups (corresponding to the environments) and test whether the residuals in these groups are equally distributed (see also Peters et al. (2016), for example). We then define  to be the composition of these operations, that is,  returns 1 if the test for equal distribution of the residuals is rejected.
In the simulations in Section 5 below, we use an F -test to test whether the residuals have the same mean across environments; this test holds pointwise asymptotic level for all   (0, 1) (see Proposition 3). To obtain power against more alternatives, one could also use non-parametric tests, such as two-sample kernel tests using the maximum mean discrepancy (Gretton et al., 2012) and then correct for the multiple testing using Bonferroni-corrections (see also Rojas-Carulla et al. (2018), for example).

4.4 Optimizing Power
To check for the invariance property of a subset S, we only need to choose a single policy   S. This provides us with a degree of freedom that we can leverage. Intuitively, the non-invariance may be more easily detectable in some target policies compared to others. We can therefore try to find a policy that gives us the strongest signal for detecting non-invariance. As presented in Section 4.2, we obtain a target sample DS from a target policy S by resampling the sample D that was generated under the policy 0, and then test invariance in the target sample. The

11

probabilities for obtaining the reweighted sample conditioned on the original sample are given by (7). We optimize the ability to detect non-invariance over a parameterized subclass of S,

S := {S |   },

× where  = a Rd and S is a linear softmax policy, i.e.,

S(a|XS) =

exp a XS . a exp a XS

This is the parameterization we chose in the experiments below, but other choices work, too. To check for the invariance condition of a subset S, the idea is then to find a policy S 
S such that, in expectation, the test power is maximized, i.e., we need to solve the following optimization problem:
arg max E pw(DS ) | D ,

where pw is a function that takes as input the reweighted sample DS and outputs the power of the test. The expectation is only with respect to the resampling of DS (in practice one is given a fixed observation D). For many invariance tests, the test power pw(DS ) cannot be directly
obtained, but one can maximize the effect size instead. Furthermore, as the sample size m of the
reweighted sample is fixed, maximizing the effect size is often similar to minimizing the p-value of
the test. Thus, we can redefine our objective function as follows

arg min E pv(DS ) | D ,

(8)

S

where pv is a function that takes as input the reweighted sample DS and outputs the p-value of the test. We then employ gradient-based optimization algorithms to solve the above opti-
mization problem, where the gradient is derived using the log-derivative. More precisely, let J() := E pv(DS ) | D be our objective function which now depends on the parameters . The gradient of the objective function J() can be derived as follows

J () =  E pv(DS ) | D =  P(DS = d | D)pv(d)
d
= P(DS = d | D) log P(DS = d | D)pv(d)
d
= E  log P(DS | D)pv(DS ) .

This expectation can be estimated by drawing repeated resamples DS , where P(DS | D) is given by (7). In practice we apply Stochastic Gradient Descent, i.e. at each iteration of the optimization we compute the gradient only from a single resample. As we argue in Appendix C, we can further speed up the optimization process substantially by a minor modification to (7), corresponding to sampling with replacement instead of distinct weights.
The optimization procedure yields a policy  that approximately satisfies   arg minS J(). We can then use  as a target policy for testing invariance of S, (4). Lastly, to preserve the level of the statistical test, we split the original sample into two halves, perform the power optimization procedure on one half, and verify the invariance condition on the other half. The algorithm is presented in Algorithm 1. We only use the approximation of (7) for the power optimization and use the weights (7) for the final resampling, so the level guarantee of Theorem 3 still holds.

4.5 Algorithm for Invariant Policy Learning
The previous sections discuss finding invariant subsets S. We now discuss how to employ this in an algorithm that learns an optimal invariant policy. We assume that we are given an off-policy

12

Algorithm 1: Testing invariance of a set S

Function test_inv(data D = (De1 , . . . , DeL ), function pv yielding the p-value of an invariance test, target set S, resampling size (m1, . . . , mL) = ( |De1 |/2, . . . , |DeL |/2), learning rate =1e-3, significance level =5e-2 ):

// sample splitting

for e = e1, . . . , eL do ne,sp  ceil(|De|/2) ; De,1  {(xei , aei , rie, 0(aei |xei ))}ni=e,1sp ; De,2  {(xei , aei , rie, 0(aei |xei ))}|iD=ne|e,sp+1;
end

// optimizing power

Initialize policy parameters  ;

while not converged do

for e = e1, . . . , eL do

for i = 1 to ne,sp do

compute

weights:

rie



S (aei | xei ,S ) 0(aei | xei )

;

end

draw De,S := (Die1,1, . . . , Diem,1e ) with replacement from De,1 with probabilities  rie ;
end

D1,S  (De1,S , . . . , DeL,S );

compute p-value: pv(D1,S ) ;

compute gradient:  log P(D1,S ) update policy parameters:    - pv(D1,S ) log P(D1,S ) ;

end

// verifying invariance condition

for e = e1, . . . , eL do

for i = ne,sp + 1 to |De| do

compute

weights:

rie



S (aei | xei ,S ) 0(aei | xei )

;

end

draw De,S := (Die1,2, . . . , Diem,2e ) with replacement from De,2 with probabilities  rie end

D2,S  (De1,S , . . . , DeL,S );

is_invariant  pv(D2,S )   ;

return is_invariant

optimization algorithm off_opt that takes as input a sample D and a policy space , and returns an optimal policy  and its estimated expected reward E^ (R). We present the precise algorithm off_opt that we use in the experimental section in Appendix D; but our approach can be applied to any off-policy optimization algorithm. If a subset S is found to be invariant, we can use off_opt to learn an optimal policy that uses S. Between all invariant subsets, we can then use the one that has the highest estimated expected reward.
We summarize the overall procedure for learning an optimal invariant policy, see Algorithm 2. The algorithm iterates over all subsets S  {1, . . . , d} and checks the invariant condition using Algorithm 1. For each iteration, if the set S is invariant, we learn an optimal policy S within the policy space S and compute its estimated expected reward E^S (R) using off_opt. Then, the algorithm returns an optimal policy S such that the estimated expected reward E^S (R) is

13

Algorithm 2: Learning an optimal invariant policy Input: data D, off-policy optimization function off_opt, test function pv, initial policy 0 initialize maximum reward maxR  - ; initialize optimal invariant policy inv  null ; for S  P({1, . . . , d}) do is_invariant  test_inv(D, pv, S, m) ; if is_invariant then S , E^S (R)  off_opt(D, S) ; if maxR < E^S (R) then maxR  E^S (R) ; inv  S ; end end end Output: optimal invariant policy inv
maximized. Lastly, the algorithm returns null if no invariant sets are found.
5 Simulations
To verify our theoretical findings we perform two simulation experiments, where we consider a linear multi-environment contextual bandit setting with the following SCM S(, e):
U := µU + U X2 := µeX2 + X2 X1 := eU + X1 A | X1, X2  (A | X1, X2)
R := A,1X2 + A,2U + R,
where U , X2 , X1 , R  N (0, 1), A takes values in the space {a1, . . . , aL}, µeX2 , e are the parameters that depend on the environment e, and µU , a1,1, . . . , aL,1, a1,2, . . . , aL,2 are the parameters fixed across environments. Appendix D contains details on how the parameters are chosen in the experiments. Code for the simulated experiments will be made available online.
5.1 Generalization of Invariant Sets
Here, we first consider an oracle setting, where we know a priori which subsets are invariant. From our data generating process, it follows that {X2} is the the only invariant set. We then compare an invariant policy which depends only on X2 with a policy that uses both X1 and X2. We train both policies on a data set of size 10 000 obtained from multiple training environments under a fixed initial policy 0. In both cases, we employ a weighted least squares regression to estimate the expected reward E[R | A, XS], where S is the subset that the policy uses. The policy then takes a greedy action w.r.t. the estimated expected reward, i.e. arg maxa E^[R | A = a, XS]. Then we evaluate both policies on multiple unseen environments, where we compute the regret with respect to the policy that is optimal in each of the unseen environments. Figure 4 illustrates the experimental result. Each data point represents the evaluation on an unseen environment. The y-axes show the regret value and the x-axes display the distance from each unseen environment to the training environments. The distance is computed as the 2-norm between the average value of the pairs (e, µeX2 ) in the training environments and the pair (e, µeX2 ) in the unseen test environment. The plot shows that the worst-case behavior of the invariant policy is smaller than the non-invariant one. This empirically supports our result of Theorem 2.
14

Regret

#training envs: 2 5
4
3
2
1
0 10 20 30 40 Distance

#training envs: 6

Policy X2 X1,X2

10 20 30 40 Distance

Figure 4: The generalization performance (in terms of regret) of the policy based on an invariant set (X2) and the policy based on a non-invariant set (X1, X2). The left and the right plot show the results when the training environments consist of two and six different environments, respectively. In both cases, the worst-case regret for the invariant policy is evidently smaller than for the non-invariant policy.

Acceptance Rate

1.0 0.8 0.6 0.4 0.2 0.0
103

#training envs: 2 104
Sample Size

#training envs: 6

105 103

104 Sample Size

Policy X1 X2 X1,X2
105

Figure 5: Acceptance rates for the off-policy invariance test proposed in Section 4.2 for varying sample sizes. The left and the plot show that result when the training environments consist of two and six different environments, respectively. The proposed method correctly accepts the set {X2} with high chance, while gradually rejects other sets.

5.2 Learning an Invariant Policy
In practice, we do not know in advance which sets are invariant. We now aim to find an invariant policy from a data set generated under an initial policy 0 which takes both X1 and X2 as input. To do so, we employ the method proposed in Section 4.2 for testing invariance under distributional shifts. More precisely, we generate a data set of size n from multiple training environments under the initial policy 0 and apply the off-policy invariance test (see Section 4.3) to verify the invariance property of each subset in {, {X1}, {X2}, {X1, X2}}. We repeat the experiment 500 times and plot the acceptance rates at various sample sizes (n = 1 000, 3 000, 9 000, 27 000, 81 000). The resulting acceptance rates are shown in Figure 5. Our method yields high acceptance rates for the set {X2}, which is the correct invariant set, while the acceptance rates for other sets gradually decrease as the sample size increases. Furthermore, we can see that our testing method is more accurate when the number of training environments increases.
6 Conclusion
This paper tackles the problem of environmental shifts in offline contextual bandits using a causal perspective. We introduce a multi-environment contextual bandit framework based on structural causal models and frame the environmental shift problem as a distributionally robust objective un-

15

der arbitrary environmental perturbations. We prove that if there are no unobserved confounders, causality and invariance are not necessary in solving the distributionally robust objective. However, causality and invariance become particularly relevant when not all variables are observed. To tackle settings with unobserved confounders, we adapt invariance-based ideas from causal inference to the proposed framework and introduce invariant policies. Our theoretical results show that under certain assumptions an invariant policy that is optimal on the training environments is also optimal on all unseen environments, and this optimal invariant policy is a solution to the distributionally robust objective. We further provide a method for finding invariant policies based on an off-policy invariance test. It can be combined with any existing policy optimization algorithm to learn an optimal invariant policy, which is then guaranteed to generalize to unseen environments. We believe that our contributions shed some light on what causality can offer in contextual bandit and, more generally, in reinforcement learning problems.
For future work, there are several directions that would be interesting to investigate. One direction is to explore the use of the invariance-based ideas in the adaptive setting, which aims to learn an agent that can optimally adapt to a changing environment. We hypothesize that learning agents may require fewer and safer explorations in a new environment if they carry over invariance information from previous environments. Another direction is to extend the invariancebased ideas from the contextual bandit setting to the full reinforcement learning problem with long-term consequences and state dynamics. Although some previous works have explored this direction (Zhang et al., 2020; Sonar et al., 2020), we believe that the connections with respect to causality and invariance are not yet fully understood. We believe that the invariance-based ideas could become helpful for building safer and more robust adaptive learning systems.
Acknowledgments
SS, NT, and JP were supported by a research grant (18968) from VILLUM FONDEN and JP was, in addition, supported by the Carlsberg Foundation.
References
M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.
S. Athey and S. Wager. Policy learning with observational data. Econometrica, 89(1):133­161, 2021.
A. Beygelzimer and J. Langford. The offset tree for learning with partial labels. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 129­138, 2009.
S. Bongers, P. Forré, J. Peters, and J. M. Mooij. Foundations of structural causal models with cycles and latent variables. The Annals of Statistics (accepted), arXiv preprint arXiv:1611.06221, 2016.
L. Bottou, J. Peters, J. Quiñonero-Candela, D. X. Charles, D. M. Chickering, E. Portugaly, D. Ray, P. Simard, and E. Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14(65):3207­3260, 2013.
R. Christiansen, N. Pfister, M. E. Jakobsen, N. Gnecco, and J. Peters. A causal framework for distribution generalization. arXiv preprint arXiv:2006.07433, 2020.
A. P. Dawid. Influence diagrams for causal modelling and inference. International Statistical Review, 70(2):161­189, 2002.
A. A. de Kroon, D. Belgrave, and J. M. Mooij. Causal discovery for causal bandits utilizing separating sets. arXiv preprint arXiv:2009.07916, 2020.
16

M. Dudik, J. Langford, and L. Li. Doubly robust policy evaluation and learning. In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 1097­1104, New York, NY, USA, 2011. ACM.
A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(25):723­773, 2012.
C. Heinze-Deml, J. Peters, and N. Meinshausen. Invariant causal prediction for nonlinear models. Journal of Causal Inference, 6(2):1­35, 2018.
N. Kallus. Balanced policy evaluation and learning. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2008.
F. Lattimore, T. Lattimore, and M. D. Reid. Causal bandits: Learning good interventions via causal inference. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems (NeurIPS) 29. 2016.
S. L. Lauritzen, A. P. Dawid, B. N. Larsen, and H.-G. Leimer. Independence properties of directed Markov fields. Networks, 20:491­505, 1990.
S. Lee and E. Bareinboim. Structural causal bandits: Where to intervene? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems (NeurIPS) 31. 2018.
S. Magliacane, T. van Ommen, T. Claassen, S. Bongers, P. Versteeg, and J. M. Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
K. Muandet, D. Balduzzi, and B. Schölkopf. Domain generalization via invariant feature representation. In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 10­18. PMLR, 2013.
J. Pearl. Causality. Cambridge University Press, 2009.
J. Peters, P. Bühlmann, and N. Meinshausen. Causal inference using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society, Series B (Statistical Methodology) (with discussion), 78, 2016.
J. Peters, D. Janzing, and B. Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017.
N. Pfister, P. Bühlmann, and J. Peters. Invariant causal prediction for sequential data. Journal of the American Statistical Association, 114(527):1264­1276, 2018.
M. Rojas-Carulla, B. Schölkopf, R. Turner, and J. Peters. Causal transfer in machine learning. Journal of Machine Learning Research, 19(36):1­34, 2018.
B. Schölkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. M. Mooij. On causal and anticausal learning. In Proceedings of the 29th International Conference on Machine Learning (ICML), pages 1255­1262. Omnipress, 2012.
A. Sonar, V. Pacelli, and A. Majumdar. Invariant policy optimization: Towards stronger generalization in reinforcement learning. arXiv preprint arXiv:2006.01096, 2020.
P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT Press, 2nd edition, 2000.
17

A. Strehl, J. Langford, L. Li, and S. M. Kakade. Learning from logged implicit exploration data. In Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010.
A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 814­823, 2015a.
A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In Advances in Neural Information Processing Systems, volume 28, pages 3231­3239. Curran Associates, Inc., 2015b.
N. Thams, S. Saengkyongam, N. Pfister, and J. Peters. Statistical testing under distributional shifts. arXiv preprint arXiv:2105.10821, 2021.
R. Volpi, H. Namkoong, O. Sener, J. C. Duchi, V. Murino, and S. Savarese. Generalizing to unseen domains via adversarial data augmentation. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
A. Yabe, D. Hatano, H. Sumita, S. Ito, N. Kakimura, T. Fukunaga, and K.-i. Kawarabayashi. Causal bandits with propagating inference. In Proceedings of the 35th International Conference on Machine Learning, 2018.
A. Zhang, C. Lyle, S. Sodhani, A. Filos, M. Kwiatkowska, J. Pineau, Y. Gal, and D. Precup. Invariant causal prediction for block MDPs. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning (ICML), volume 119, pages 11214­ 11224. PMLR, 2020.
Z. Zhou, S. Athey, and S. Wager. Offline multi-action policy learning: Generalization and optimization. arXiv preprint arXiv:1810.04778, 2018.

A Proofs

A.1 Proof of Theorem 1
Proof. We begin by showing that the model class in Setting 2 satisfies an invariance property. Let e  E, a  A and x  X , then by using the explicit SCM structure from Setting 2, it holds that

Ea,e R | X = x = Ea,e f (X, s(X, U ), A, R) | X = x .

Since we assume there is no hidden confounding, it holds that U  X which implies that
Ea,e f (X, s(X, U ), A, R) | X = x = E U , R f (x, s(x, U ), a, R)
and hence Ea,e R | X = x does not depend on the environment. This in particular implies that for all e  E, all x  X and all a  A, it holds that

QEobs (x, a)

=

1
|E obs |

Ea,f [R | X = x]

f Eobs

= Ea,e[R | X = x].

(9)

We thus have for any policy    and x  X that,

max QEobs (x,

a)



,e
E

[R

|

X

=

x].

(10)

aA

18

Next, take the expectation over X on both sides to get

e
E

max

QE obs

(X,

a)]



e
E

E,e[R | X]

aA

=

,e
E

R.

Finally, taking the infimum over e  E leads to

inf

e
E

max QEobs (X, a)]



inf

,e
E

R.

eE

aA

eE

(11)

By definition, the policy

(a | x) := 1 a = arg max QEobs (x, a )
a A

satisfies

  ,e
E

R

=

e
E

maxaA QEobs (X, a)].

Therefore (11), implies that  is an optimal

policy, which completes the proof of Theorem 1.

A.2 Proof of Corollary 1

Proof. We begin by defining for all n  N the term c(n) = max sup |QEobs (x, a) - Qn(x, a)|.
aA xX

As A is assumed to be finite and because Qn is assumed to be uniformly convergent it holds that

lim ED[c(n)] = 0.

(12)

n

Moreover, as shown in (9), in the proof of Theorem 1, we know that for all e  E, all a  A and

all x  X it holds that

QEobs (x, a) = Ea,e[R | X = x].

This implies that for all x  X and all e  E it holds that

En,e[R | X = x]

= Ea,e[R | X = x]n(a|x)

aA

=

QEobs (x, a)n(a|x)

aA

= Qn(x, a)n(a|x) + (QEobs (x, a) - Qn(x, a))n(a|x).

(13)

aA

aA

Next, observe that

(QEobs (x, a) - Qn(x, a))n(a|x)

aA



QEobs (x, a) - Qn(x, a) n(a|x)

aA

 c(n)

(14)

and

Qn(x, a)n(a|x)
aA

= max Qn(x, a)
aA

= max QEobs (x, a) + (max Qn(x, a) - max QEobs (x, a)).

aA

aA

aA

(15)

19

Using (14), (15) and (13) together with the triangle inequality leads to

En,e[R | X = x] - max QEobs (x, a)
aA

= max Qn(x, a) - max QEobs (x, a)

aA

aA

+ (QEobs (x, a) - Qn(x, a))n(a|x)
aA

 2c(n).

This in particular implies that for all e  E and all x  X it holds that

max QEobs (x, a) - 2c(n)  En,e[R | X = x]
aA

and that

En,e[R | X = x]  max QEobs (x, a) + 2c(n).
aA

Taking the expectation over X and the infimum over E in both inequalities leads to

V E () - 2c(n)  V E (n)  V E () + 2c(n).

Finally, we use (12) and get that

lim
n

ED

|V E (n) - V E ()|



lim
n

ED [4c(n)]

=

0.

This completes the proof of Corollary 1.

A.3 Proof of Proposition 1
Proof. First, we show in the following Lemma that the opposite of the mean faithfulness holds true.
Lemma 1 (Extended Markov Property). For any fixed policy   , it holds for all subsets S  {1, . . . , d} and for all Z  {U, R} that

Z G e | XS

=

e, f



E, x



XS

:

E,e[Z

|

XS

=

x]

=

,f
E

[Z

|

XS

=

x],

where the symbol G denotes d-separation in the graph G.
Next, fix S  {1, . . . , p}, and let   S. Assume  is an invariant policy, i.e., satisfies (4). By mean faithfulness, we have that R G e | XS. Furthermore, since for all   S the graphs G and G are identical, this implies that the same d-separation also holds in G . Consequently, by Lemma 1, we then have for all   S it holds that

e, f  E, x  X S : E,e[Z | XS = x] = E,f [Z | XS = x],

which concludes the proof of Proposition 1.

A.3.1 Proof of Lemma 1
Proof. Lemma 1 corresponds to a global Markov property in the augmented graph (including the non-random environment index). Such results are well-established and heavily used in settings in which E is finite, for example in influence diagrams (Dawid, 2002). The result, however, also holds for more general E.

20

To prove this, we will first fix   , S  {1, . . . , d} and Z  {U, R}. Furthermore, let
e  E, let  be the discrete -algebra on E and let e :   [0, 1] be a probability measure that puts non-zero mass on {e}. Adding a random variable E with distribution e the class of SCMs (S(, f ))fE induces a joint distribution over (E, X, U, A, R) that is globally Markov with respect to the graph G. This implies that the d-separation Z G E | XS (which is implied by Z G e | XS) implies that the joint distribution (E, X, U, A, R) satisfies the following conditional independence
Z  E | XS.
By definition this implies for all x  X S and all f  E with e(f ) > 0 that

f
E

[Z

| XS

=

x]

=

E[Z

| XS

=

x, E

=

f]

=

E[Z

| XS

=

x],

where the expectations without superscript are taken with respect to the joint distribution including E. The function w(x) := E[Z, | XS = x] therefore no longer depends on the environment nor on e. Since e(e) > 0 this in particular implies that for all x  X S it holds that
Ee[Z | XS = x] = w(x).

As this construction works for all e  E, this completes the proof of Lemma 1.

A.4 Proof of Proposition 2
Proof. To begin, let us define a(a | x) := 1 a = a as the policy that always selects the action
a, Sinv as the collection of all invariant sets and QeS(x, a) := Ea,e R | XS = x . For any set S  Sinv it holds by definition of invariant sets that QeS does not depend on e,
we will therefore drop the superscript e in these cases. For all S  Sinv, all policies S  S, all x  X and all e  E it holds that,

max QS(x, a)



S
E

,e

[R

|

XS

=

x].

aA

Taking the expectation over XS on both sides shows that

e
E

max

QS (X,

a)]



e
E

S
E

,e

[R

|

XS]

aA

=

S ,e
E

R

and taking maximum over S  Sinv only on the LHS, we get that

max Ee

max

QS

(X,

a)]



S ,e
E

R.

SSinv

aA

Then, by letting S denote the maximizer of the LHS, we get

e
E

max
aA

QeS0 (X,

a)]



S ,e
E

R

Taking the infimum over e  E leads to

inf

e
E

max QS (X, a)]



inf

S ,e
E

R

= V E (S).

eE

aA

eE

(16)

The inequality (16) holds for all S  Sinv. We therefore have that

(a | x) := 1 a = arg max QS (x, a )  arg max V E ().

a A

inv

By taking the infimum over Eobs in (16) instead of E we also get that (a | x)  arg maxinv V Eobs (). This completes the proof of Proposition 2.

21

A.5 Proof of Theorem 2
Proof. To simplify notation we assume Xk  DE(Xk). We begin by defining the stable set

SNI := {1, . . . , d} \ {j  {1, . . .} | k  CI : j  DE(Xk)},

where CI are confounded and directly intervened on nodes (i.e., for k  CI there exists 
{1, . . . , p} such that e  Xk  U  R). Furthermore, let us denote a(a | x) := 1 a = a as
the policy that always selects the action a. Now we provide the following lemmas which are the
main parts of our proof.

Lemma 2 (Stable sets and Invariance). Assume Setting 1 and Assumption 1. If an invariant set exists, it holds that the stable set SNI is an invariant set.
Lemma 3 (Lower bound on V E (¯)). Assume Setting 1 and Assumption 1. Let ¯  arg maxinv V Eobs (). Then it holds that

V

E (¯)



inf
eE

e
EX SN I

max Ea R | XSNI
aA

.

Lemma 4 (Upper bound on V E ()). Assume Setting 1 and Assumptions be an arbitrary non-invariant po¯licy. Then it holds that it holds that

1

and

2.

Let





\inv

V

E ()



inf
eE

e
EX SN I

max Ea R | XSNI
aA

.

Then, based on these Lemmas, the proof proceeds as follows: Let ¯  arg maxinv V Eobs ()

and

 ¯



arg

max\inv

V

Eobs (),

then

by

Lemma

3

and

Lemma

4,

we

have

V E (¯)  V E (), ¯

which concludes the proof of Theorem 2.

A.5.1 Proof of Lemma 2
Proof. Recall our definition of the stable set
SNI := {1, . . . , d} \ {j  {1, . . .} | k  CI : j  DE(Xk)},
where CI are confounded and directly intervened on nodes (i.e., for k  CI there exists  {1, . . . , p} such that e  Xk  U  R).
Claim 1. If an invariant set exists, it holds that SR  SNI .
Proof. We prove the claim by contrapositive. Assume that there exists j  SR such that j / SNI . This implies that there exist k  {1, . . . , d} and  {1, . . . , p} such that e  Xk  U  R in G and j  DE(Xk). Now, we will show that there is no invariant set. Let S  {1, . . . , d} be an arbitrary set. There are two possibilities,
(a) j  S: Since j  DE(Xk), the path e  Xk  U  R is d-connected given S, and therefore R  G e | XS. By Assumption 1, this directly implies that S is not an invariant set.
(b) j / S: Since j  SR is a parent of R, S can only d-separates e from R if PA(Xj)  S. However, because j  DE(Xk), the path e  Xk  U  R is d-connected given PA(Xj). We thus have that R  G e | XS, which by Assumption 1 implies that S is not an invariant set.
The two possibilities conclude that there is no invariant set which completes the proof (by contrapositive).

22

Now, we prove the main result. By Lemma 1, it suffices to show that any path  from e to R is d-separated given SNI . First, by Claim 1, we have that SR  SNI . Therefore, if the path  enters R through a parent it will be d-separated given XSNI . So assume  enters R through a hidden variable U . Then, there are two cases:
(i)  : e  · · · Xk  Xj  U  R
(ii)  : e  · · · Xk  Xj  U  R
In case (i), since Xj is a collider,  can only be d-connected given XSBe(R) if k  SNI and DE(Xj)  SNI = . Consider two cases:
(a) k  SNI : This directly implies that  is d-separated given XSNI .
(b) k  SNI : By definition of SNI it holds that DE(Xk)  SNI = . Hence,  is d-separated given XSNI .
We have therefore shown that in case (i) the path  is d-separated given XSNI . Next, consider the case (ii) and let Xr be the collider closest to Xj on . The path  can only be d-connected given XSNI if j  SNI and DE(Xr)  SNI = . Consider the two cases:
(a) j  SNI : This directly implies that  is d-separated given XSNI .
(b) j  SNI : Since DE(Xr)  DE(Xj), this implies that DE(Xr)  SNI = . Hence, the path  is d-separated given XSNI .
From the above two cases, we have shown that the path  is d-separated by XSNI , which proves that SNI is an invariant set.

A.5.2 Proof of Lemma 3

Proof. Let inv be the set of all invariant policies defined in (5). First, from Proposition 2, we
have that inv  inv : V E (¯)  V E (inv).

Because SNI is an invariant set (by Lemma 2), the above inequality directly implies

V

E (¯)



inf
eE

e
EX SN I

max Ea R | XSNI
aA

,

which concludes the proof of Lemma 3.

A.5.3 Proof of Lemma 4
Proof. Recall our definition of the stable set
SNI := {1, . . . , d} \ {j  {1, . . .} | k  CI : j  DE(Xk)},
where CI are confounded and directly intervened on nodes (i.e., for k  CI there exists  {1, . . . , p} such that e  Xk  U  R). To begin with, we provide the following claim as part of the main proof.
Claim 2. If e  E is a confounding removing environment it holds that j  {1, . . . , d} \ SNI : R G Xj | XSNI .
Proof. Let j  {1, . . . , x} \ SNI , then by definition there exist k  {1, . . . , d} and  {1, . . . , p} such that e  Xk  U  R in G and j  DE(Xk). Since Xk is a collider this implies that for all S  {1, . . . , d} it holds that R G e | XS{j}. Therefore, by the definition of a confounding removing environment we know that Xj G,e U . Additionally, by Claim 1, we also know that SR  SNI . Thus, R and Xj can only be d-connected given XSNI if there exists u  {1, . . . , d} and v  {1, . . . , p} such that Xj  Xu  U v  R and SNI  DE(Xu) = . However, since u  DE(Xj), this implies that u  DE(Xk). Then, by the definition of SNI we have that SNI  DE(Xu) =  which then concludes that R G Xj | XSNI and completes the proof.

23

Now, we are ready to prove the main result. Let    \ inv be an arbitrary non-invariant policy, and XS be the variables that the policy  depends on, i.e., there exists S : X S  (A) such that for all x  X it holds that (·|x) = S(·|xS). We have

V E ()

=

inf

,e
E

R,

eE

by the tower property of conditional expectation,

=

inf
eE

e
EX SN I

,X S\SN I

,e
E

R | X SNI , X S\SNI

=

inf
eE

e
EX SN I

,X S\SN I

Ea,e R | XSNI , XS\SNI (a|XS ) µ(da) .

Now, we use Assumption 2. For each e  E we choose a confounding removing environment f (e)

such

that

,f (e)
PX

=

PX,e.

We

then

have

V E ()



inf
eE

e
EX SN I

,X S\SN I

Ea,f(e) R | XSNI , XS\SNI (a|XS ) µ(da) .

Next, we use the result from Claim 2. We have that j  {1, . . . , d} \ SNI : R G Xj | XSNI . Then, by the Markov property, we get

V E ()



inf
eE

e
EXSNI ,XS\SNI

Ea,f(e) R | XSNI (a|XS ) µ(da) ,

we can then omit f (e) since SNI is an invariant set (Lemma 2),

=

inf
eE

e
EXSNI ,XS\SNI

Ea R | XSNI (a|XS) µ(da)

=

inf
eE

e
EX SN I

Ea

R | X SNI

e
EX S\SN I

(a|XS) µ(da) ,

letting ~(a | XSNI ) := EXS\SNI [(a|XS )],

=

inf
eE

e
EX SN I

Ea R | XSNI ~(a | XSNI ) µ(da)



inf
eE

e
EX SN I

max Ea R | XSNI
aA

.

A.6 Proof of Theorem 3

Proof. We only show that our setting with environments can be cast as that in Thams et al.

(2021), which has no reference to environments. First we randomly permute the rows of each dataset De to obtain a set D~ e. Then construct an

auxiliary data set DE , where each observation Di of DE is the concatenation of the i'th observation (after permutation) from all environments, Di := (D~ie1 , . . . , D~ieL ).
We can now apply the resampling methodology from Thams et al. (2021) to draw a sequence

(Di1 , . . . , Dim ) with weights given by (7) where

r(Di)

:=

(Aei 1 | Xie1 ) 0(Aei 1 | Xie1 )

···

(Aei L | XieL ) 0(Aei L | XieL )

Because the observations are independent, both within and between environments, the probability

of of

the sequence drawing first

(Di1 , . . . , Dim ) = ((Die11 , m observations from e1,

.(.D. ,ie1D1 ,ie.1L.

), .,

.D. .iem ,1()D, aiem1n,d.

. . , DiemL )) is equal to then m from e2 etc.

the

probability

24

B Connection to Random Environments

It is possible to define multi-environment contextual bandits using random environments.

Setting 3 (Random Environment Contextual Bandits). Let X = (X1, . . . , Xd)  X = X 1 × . . . ×

X d, U = (U 1, . . . , U p)  U = U 1 × . . . × U p, A  A = {a1, . . . , ak}, R  R, E  E. For any

  { : X - (A)}, let g denote the function that ensures, for all x  X , g(x, A) equals (x)

in distribution for a uniformly distributed A. Now, consider functions s, h, and f , a factorizing

distribution P = P E × P U × P X × P A × P R whose A component is uniform, and a structural causal model S() given by

S() :

 E := E   U := s(X, U ) 
X := h(X, U, E, X )

A 

:=

g (X,

A)



R := f (X, U, A, R).

Assume further that for all , the SCM induces a unique distribution over (E, X, U, A, R), which we denote by P. The structure of the SCM S(, e) can be also visualized by a graph G which is constructed in a similar way to the graph in Setting 1, except that the environment becomes one
of the variable nodes in this graph.

Remark 1. Setting 3 is a special case of Setting 1 in the following sense: Assume, starting
from Setting 3, for all i  {1, . . . , n} that (Xi, Ui, Ai, Ri, Ei), are independent and distributed according to PXi,U,A,R,E. Then, defining he(·, ·) := h(·, e, ·), we have that, for all i  {1, . . . , n}, (Xi, Ui, Ai, Ri), are independent and distributed according to PXi,,UE,iA,R, using Setting 1.

C Faster power optimization

In Section 4.4, we show that we can optimize the power to detect non-invarince by gradient descent. In particular, the gradient is
J () = E  log P(DS | D)pv(DS ) ,

where DS is a resample of the data D and pv is a function returning a p-value of our invariance test. P(DS | D) is given by Equation (7), but as discussed in Thams et al. (2021), this may be infeasible to compute if n is very large.
As a computationally efficient alternative, Thams et al. (2021) proposes an approximate resam-
pling scheme, where a sequence (i1, . . . , ime ) (distinct or non-distinct) is sampled with replacement. That is, the weights are given by

w,(i1,...,ime ) := =

me =1

r (Die

)

(j1 ,...,jm )

me =1

r (Dje

)

me =1

r

(Die

ne j=1

r

(Dje)

)
me

.

This expression is much easier to compute than Equation (7), because the denominator is a sum over ne terms (instead of ne!/(ne - me)!). In particular, we get

 log P(DS | D) =  log w,(i1,...,ime )

me

ne

=  log r(Die ) - me log r(Dje).

=1

j=1

Algorithm 1 splits the data in two halves: we optimize power on the first half of the data and test for invariance on the second half. We only use the above approximation for the power

25

optimization, where we need to explicitly compute the normalization constant of the weights. In the second half of Algorithm 1, we use Equation (7) (i.e., we do not use the approximate weights), because Proposition 3 requires the weights to be those given in Equation (7). If n is so large that we cannot sample by explicitly computing the weights Equation (7), there are several options for sampling from the scheme without computing the denominator ­ see Thams et al. (2021) for a variety of approaches.

D Simulation Details
D.1 Data Generating Process
We generate data from the following SCM S(, e):
U := µU + U X2 := µeX2 + X2 X1 := eU + X1 A | X1, X2  (A | X1, X2)
R := A,1X2 + A,2U + R,
where U , X2 , X1 , R  N (0, 1), A takes values in the space {a1, . . . , aL}. In our experiments, we consider 3 possible actions (L = 3) and randomly draw the parameters µU , a1,1, . . . , a3,1, a1,2, . . . , a3,2 from N (0, 1), while the environment-specific parameters µeX2 , e are drawn from N (0, 4). These parameters are then fixed across all experiment runs.

D.2 Policy Optimization Algorithm

In Section 5.1, we consider the policy of the form

S(a | x) := 1 a = arg max f^S(x, a ) ,
a A

where f^S is an estimator of the conditional mean Ea (R | XS) and a is the policy that always

selects a.

Given observations {(Xi1, Xi2, Ai, Ri, ei)}ni=1 from an initial policy 0, we find an importance-

weighted regressor

f^S

:=

arg min
f F S

n i=1

1 0(Ai |

Xi) (f (Ai, XiS)

-

Ri)2,

(17)

where F S = {X S × A - R} is a class of functions. In our experiment, we consider a linear function class. Solving the objective (17) then reduces to fitting a weighted linear regression.

26

