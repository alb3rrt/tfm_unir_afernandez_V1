Rich dynamics caused by known biological brain network features resulting in stateful networks.

arXiv:2106.01683v1 [q-bio.NC] 3 Jun 2021

Udaya B. Rongala Department of Experimental Medical Science
Faculty of Medicine, Lund University Lund. Sweden.
udaya_bhaskar.rongala@med.lu.se

Henrik Jörntell Department of Experimental Medical Science
Faculty of Medicine, Lund University Lund. Sweden.
henrik.jorntell@med.lu.se

Abstract
The mammalian brain could contain dense and sparse network connectivity structures, including both excitatory and inhibitory neurons, but is without any clearly defined output layer. The neurons have time constants, which mean that the integrated network structure has state memory. The network structure contains complex mutual interactions between the neurons under different conditions, which depend on the internal state of the network. The internal state can be defined as the distribution of activity across all individual neurons across the network. Therefore, the state of a neuron/network becomes a defining factor for how information is represented within the network. Towards this study, we constructed a fully connected (with dense/sparse coding strategies) recurrent network comprising of both excitatory and inhibitory neurons, driven by pseudo-random inputs of varying frequencies. In this study we assessed the impact of varying specific intrinsic parameters of the neurons that enriched network state dynamics, such as initial neuron activity, amount of inhibition in combination with thresholded neurons and conduction delays. The impact was assessed by quantifying the changes in mutual interactions between the neurons within the network for each given input. We found such effects were more profound in sparsely connected networks than in densely connected networks. However, also densely connected networks could make use of such dynamic changes in the mutual interactions between neurons, as a given input could induce multiple different network states.
1 Introduction
Recurrent excitatory and inhibitory loops are known to exist in mammalian central nervous system [1­10]. These loops are known to be more complex than in traditional artificial neural networks. Furthermore, brain networks (neuronal networks) need not necessarily contain input and output neurons/layers, allowing every neuron in the network to contribute as an input and output function. The neurons were observed to surmise together to generate predictive models that helps in forming internal representing based on the external world inputs[11­15]. This theory makes every neuron within the brain networks an important component in representing the information, and hence the "networks internal state1" will be a defining factor in information representation. With this line of thinking, in this article we have explore the effect of input/neuron/network parameter on the internal state of the network. Towards this we have constructed a recurrent network comprising of both excitatory (ENs) and inhibitory neurons (INs) coupled using dense and sparse connectivity. We have chosen a fully connected network to have minimal assumptions. Both the ENs and INs
1Internal state of a network can be defined as distribution of activity across all individual neurons across the network.
Preprint. Under review.

are modelled based on similar neuron properties. A study conducted by Anton et. al. [16], based on a range of neuron types in the brain (in vivo), they indicated that a neuron spike activity is a probability density function of the neuron membrane potential. Therefore, we have used a simple non-spiking neuron model that is computationally efficient and incorporating fundamental properties of a conductance-based Hodgkin­Huxley model [17]. Such a neuron model will allow us to have minimal information loss withing the neurons in a network. We assessed the impact of varying specific intrinsic parameters of the neurons that enriched network state dynamics, such as input noise, initial neuron activity, amount of inhibition in combination with thresholded neurons and conduction delays. In biology, the state of a network was generally depicted as up and down states, which were characterized by high and low firing rates in the central nervous system [18­20] and the advantages of such dynamic states was well studied [21­24]. The effect of network configurations such as conduction delays, and their effect on network states was also previously studied [25­27]. Whereas in this article we try to present the state of a network as a dynamically evolving entity, that is a function of the intrinsic parameters and resulting co-activation of all the neurons present in the network. We try to create this understanding, by addressing how intrinsic parameters of a neuron/network could affect the neurons activity, and further having its influence on the other neurons within the network.

2 Methods

2.1 Neuron Model
In this study we use a non-spiking Linear Summation neuron Model (LSM). LSM was a computationally compact neuron model that was designed to capture the important characteristic in a H-H conductance-based model [17]. In LSM, the neuron membrane potential (A, Equation 1) was given by summation of weighted (w) input synaptic activity (a), that was normalized using static leak (kstatic) and dynamic leak (dyn) components. The static leak was given by the total number of synapses on the neuron, reflecting the total number of open ion channels contributing to the membrane potential. The dynamic leak component mimics the effect of an RC circuit, reflecting the ion channels and membrane capacitance of a neuron. In the LSM adopted within this study, the neuron membrane activity was thresholded at zero resting potential (Equation 2). The continuous output neuron activity of LSM was intended to reflect the mean firing rate of a traditional spiking neuron model. The total neuron activity of LSM is given by following equations 1 & 2,

dyn



dA dt

=

-A(t) +

kstatic

(wi +

 ai(t)) |wi  ai(t)|

(1)

if A < 0, thenA = 0

(2)

2.2 Network Connectivity
In this study we consider dense and sparse network connectivity configurations, comprising of both excitatory (ENs, blue marker) and inhibitory neurons (INs, red markers). In dense connectivity, we have a fully connected network where all the nodes within the network were bi-directionally connected (Figure 1A). In sparse connectivity, pseudo-randomly chosen 50% of connections within the fully connected network were removed (Figure 1B). In this study, we considered a total of 20 neurons (including ENs and INs), where two of the ENs (Neuron #1, #2) received the inputs (Figure 1A, B, black markers). We explored different percentage of inhibitory neurons in the network, during which we switched a neuron from excitatory to inhibitory without any change in the connectivity. When a neuron was shifted from being excitatory to inhibitory, the weights of all outgoing connections for that neuron were converted from positive to negative weights while keeping the same value for the weight. While scaling the percentage of inhibitory neurons in the network, the index of INs were chosen pseudo-randomly.

2.3 Conduction Delays
The network dynamics in the presence of conduction delays (axonal delays between any two neurons in the network) was also explored in this study (Figure 5). The conduction delays were generated as normal distribution with a distribution mean (µ) of 20 and standard deviation () of 10. These

2

Figure 1: The networks that were studied. The networks comprised both excitatory (ENs, blue markers) and inhibitory neurons (INs, red marker). Both the networks consisted of a total of 20 neurons and two of the ENs (Neuron #1, #2) received the external inputs (black markers). (A) The network with dense connectivity, where all the neurons in the network were fully connected. (B) The network with sparse connectivity where 50% of the connections were pseudo-randomly removed from the densely connected network.

conduction delays were fixed and not varied for different network simulation in this study. In simulations the value of conduction delay denotes the number of timesteps, that a signal is lagged between two neurons.

2.4 Pseudo-Random Inputs
The external inputs to the network, we generated pseudo-random spike trains of two different frequencies (5 and 50 Hz) with uniform normal distribution, using an inbuilt MATLAB function "randi". Further, these spikes were convoluted to resemble post-synaptic potentials using the following kernel equation [28],

ai

=

m d - r



exp

-t - l - t d

- exp

-t - l - t r

(3)

Where, t is the input spike time, d is the decay time (4 ms), r is the rise time (12.5 ms), m is the constant to calculate ratio between rise and decay times (21.3 ms) and l is the latency time which is zero in this case. These values were chosen based on the previous work from [29].

2.5 Noise Input
A random gaussian noise was added to the convoluted signal (ai), as input to the neuronal networks. The noise was generated as uniform distribution ranging between 0 and a given maximum noise value (noise, Figures 3 & 4). Further to analyze the effect of noise on the network dynamics, we have also explored different levels of input noise, ranging between 0 and 0.2 with a step of 0.05 (Figures 3 & 4). We have generated 25 uniform distribution for each noise level (25 repetitions) to assess the variance in responses for a given network/input configuration.

2.6 Initial neuron activity
The network dynamics were explored based on the initial activity of the individual LSM neuron within the network. The initial activity was generated as uniform distribution ranging between 0 and a given maximum value (initial neuron activity, Figures 3 & 4), across all the neurons in the network.

3

We have explored different level of the initial neuron activity, ranging between 0 and 0.2 with a step of 0.05 (Figures 3 & 4). We have generated 25 uniform distribution for each given initial neuron activity (25 repetitions) to assess the variance in responses for a given network/input configuration. 2.7 Synaptic Weights All connection in the network were weighted. The weights were generated as normal distribution with a distribution mean (µ) of 0.3 and standard deviation () of 0.1. The weights of INs outgoing connections were set negative and the ENs were made positive. 2.8 Statistical Analysis 2.8.1 Cross correlation The correlation index measure was used to compute the similarity between all the neuron output responses (Figure 3). The correlation index was computed across all the 20 neuron pairs, and the average across all pairs was reported. This measure was computed using an inbuilt MATLAB function "xcorr" (with zero lag), which produces values (cross-correlation index) from 0 (uncorrelated) to 1 (similar). 2.8.2 Covariance The covariance measure was used to determine how the neurons co-activate for a given input. The covariance measure was computed across all the 20 neurons activity. We have used an inbuilt MATLAB function "pca" and extracted the covariances between all the 20 neurons for first principal component (explained > 90% of information in the network). The pca was computed on the entire time window (1s) of neuron output responses for a given input. To assess the effect of a given network configuration (input noise, INA, Inhibition), we have computed the Euclidean distance as a function of covariance matrix, this was reported as "covariance index". The value of covariance index at zero indicate high similarity between the neuron covariances for given network configurations.
Figure 2: Neuron responses to the two given inputs for the two network configurations (dense and sparse). (A) The 5 Hz input, consisting of two pseudo-randomly generated input spike trains with an average frequency of 5 Hz that were convoluted using a kernel function (see Methods). The convoluted signals were fed as input to the Neurons #1 & #2. (B) Similar to A, but with input spike frequencies of 50 Hz. (C-D) The neuron responses in the densely connected network, for the two given input frequencies. For clarity, the activity of Neuron #2 is indicated with a dashed line. (E-F) The neuron responses in the sparsely connected network, for the two given inputs. In this Figure, both networks had 10% inhibition (2 INs), an input noise of 0.1 and the initial neuron activity was 0.
4

3 Results
3.1 Network Activity Dynamics
3.1.1 Effect of LSM in bi-directional networks
We first characterized the responses of all the neurons within the networks, with dense and sparse connectivity respectively (Figure 1A, B), for the two given pseudo-random inputs with average impulse frequencies of 5 Hz and 50 Hz (Input #1 & #2, Figure 2A, B). In Figure 2, both the densely and the sparsely connected networks have 10% INs (2 INs, as shown in Figure 1A, B) with an input noise level of 0.1 and initial neuron activity set to 0. During the initial phase of the input presentation, a gradual increase in the neuron activities was observed (shaded region, Figure 2A, E). Due to the recurrency within these networks, the input activity (only noise input in initial cycles (0 ­ 150 ms) for Input #1) integrated over time, which caused the activity increase. Remarkably, the static leak component of the LSM normalizes the neuron activity and bring the neurons to a "steady state"2 once a certain activity level is approached (in Figure 2C, for example, this occurred at a neuron activity just above 0.25). From Figure 2 we can observe that once a steady state neuron activity is reached, only the neurons that receive direct input (Neuron #1 & #2) display a sustained activity dynamics in relation to the input, whereas the other neurons displayed a substantially smaller amount of activity dynamics. Due to the density of the connections in both these network configurations, the activity dynamics of these other neurons was diminished, which was shown by the cross-correlation analysis across all the neuron responses (See Methods). The cross-correlation index (CCI), calculated across all neuron pairs for each of these configurations, was 0.98 (Figure A.1).
3.1.2 Effect of Inhibition
We next assessed the changes in network activity dynamics that could be caused by different amount of inhibition. With an increased inhibition to 30% (i.e., the networks were configured with 6 INs), the steady state activity across all neurons, in both sparse and dense networks, was substantially lower (Figure 3A, C) than in the networks with 10% inhibition (Figure 2). Increased inhibition would be expected to drive the neuron activity further towards zero. The LSM neuron is thresholded at zero and therefore increased inhibition would be expected to introduce non-linear dynamics into the network. This in turn would be expected to result in an increase in the activity dynamics across all the neurons of the networks, and this is also what we observed (Figure 3A-D). The CCI measure for each given configuration (Input noise = 0.01 & Neuron initial activity = 0, solid green trace in Figure 3E, G) also indicated that the dissimilarities between the individual neuron activities increased with increased inhibition This indicated that the inhibition preserved activity dynamics in both sparsely and densely connected networks. The effect of inhibition was more pronounced in networks with sparse connectivity than with dense connectivity.
3.1.3 Effect of Input/Neuron parameters
We further investigated the effect of input noise level and initial neuron activity (INA) on the network activity dynamics, for different amounts of inhibition (Figure 3E-H). We examined the networks for five different values of input noise (0, 0.05, 0.1, 0.15 and 0.2, Noise 1-5) and INA (0, 0.05, 0.1, 0.15, 0.2, INA 1-5). For each given value of input noise and INA, we generated 25 pseudo-random repetitions (see Methods). In Figure 3E-H, the solid lines indicate the average CCI (y-axis on left) across these repetitions and the dotted lines indicate the variance of CCI across these repetitions (y-axis on right). Note: The Figure 3E-F and Figure 4E-F show the results for input #1 (5 Hz), the results for inputs #2 (50 Hz) are presented in the supplementary materials Figure A.3.
Figure 3E & G shows that the input noise impacted the CCI for both networks. As the input noise increased, the steady state level of the neuron increased proportionally (Figure A.2), thus affecting the dynamics in the network activity, by increasing the requirement of the amount of inhibition to create an increase in the separation in the activity between the neurons (a decrease in CCI). Conversely, for a fixed inhibition level, a greater noise caused a greater CCI value and a drop in the CCI variance, in effect a loss of separation of neuron activities. Figure 3F & H illustrates the effect of INA on the activity across all the neurons in the networks. The addition of an initial neuron activity caused a change in the separation of the neuron output activity, but further changes in the INA had a negligible
2State of the neuron at which its output activity is not affected by trivial changes in the input activity.
5

Figure 3: Neuron activity dynamics in the presence of varied inhibition and varied intrinsic properties. (A-D) Neuron responses for both densely and sparsely connected networks, for both given inputs (5 & 50 Hz). These neuron responses are for network configuration with input noise = 0.1 and initial neuron activity = 0, related to the CCI index indicated by arrow in E, G respectively. (E) Cross-correlation between all neuron responses (dense connectivity, with Input #1 (5 Hz)) for different values of input noise and inhibition. The initial neuron activity value was 0. Note: The y-axis (left) indicates the average cross-correlation index value for 25 different repetitions of each setting (see Methods), y-axis (right) indicates the variance in cross-correlation index value for 25 repetitions. This was similar for subplots E-H. (F) Cross-correlation between all neuron responses (dense connectivity) for different values of initial neuron activity and inhibition. The input noise value was 0.1. (G) Similar to E, with sparse connectivity. (H) Similar to F, with sparse connectivity.
effect. However, increases in the INA caused a higher level of variance in CCI across a range of inhibitory amounts, indicating that the initial activity of individual neurons had a significant effect on defining the distribution of neuron output activity across the network.
3.1.4 Effect of Conduction Delays
We further explored if the effects of input noise, INA or inhibition could be made to alter if conduction delays between the neurons were introduced in the networks. Conduction delays seemingly induced dynamical discrepancies in activity across all the neurons in the network (Figure 4A-D) in contrast to the network without delays, where the activity across all the neurons converges to an attractor state based on the input signal (Figure 3A-D). This would be an effect of the fully connected network architecture with random weights. The effect of discrepancies across the neuron activities created by the conduction delays was expected to be more effective when the steady state activity of the neurons was near to the threshold of LSM, as this amplifies the relative differences between the neurons in the network and further results in inducing higher level of non-linearity into the network. This was reflected accordingly in the cross-correlation analysis where the CCI is lower for a given inhibition in the networks with conduction delays (Figure 4E-H) as compared to the networks without delays (Figure 3E-H). The introduction of conduction delays showed an increase in the CCI variance for densely connected networks at higher amount of inhibition (30-50% inhibition), for different noise levels (Figure 4E). This variance for different noise levels indicated that the temporal effect of noise had a higher impact on the neuron output activity in the networks with conduction delays, implying a higher degree of non-linearity in the network activity dynamics compared to the networks without delays (Figure 3E).
3.2 Network Dynamics
In the above section we explored the effect of various network configurations on the neuron activity for a given input. In this section we extend our analysis to understand if the co-activation of neurons changes for a given network configuration (input noise, INA, inhibition). We computed the covariance across all the neuron output activity and calculated the distance between covariances for different network configuration (see Methods). In Figure 5, the covariance index (CI) value (each data point,
6

Figure 4: Neuron activity dynamics in the presence of conduction delays, for a varied inhibition and varied intrinsic properties. (A-D) Neuron responses for both densely and sparsely connected networks, for both given inputs (5 & 50 Hz). These neuron responses are for network configuration with input noise = 0.1 and initial neuron activity = 0, related to the CCI index indicated by arrow in E, G respectively. (E) Cross-correlation between all neuron responses (dense connectivity, with input #1 (5 Hz)) for different values of input noise and inhibition. The initial neuron activity value was 0. Note: The y-axis (left) indicates the average cross-correlation index value for 25 different repetitions of each setting (see Methods), y-axis (right) indicates the variance in cross-correlation index value for 25 repetitions. This was similar for subplots E-H. (F) Cross-correlation between all neuron responses (dense connectivity) for different values of initial neuron activity and inhibition. The input noise value was 0.1. (G) Similar to E, with sparse connectivity. (H) Similar to F, with sparse connectivity.
in each subplot) indicates the distance between covariances of neurons in the two network types, given different values of input noise/initial neuron activity. Figure 5A-D shows how the co-activation between neurons changes based on their level of input noise/INA. This variation is high in the sparsely connected networks with high amount of inhibition (30%-50%). In the densely connected network, the co-activation between neurons changed based on the level of input noise/initial neuron activity, but only in the networks without conduction delays (Figure 5E, G). Hence, in networks with conduction delays (Figure 5F, H) the impact of varying the configurations on causing variation in the co-activation of the neurons, was very small. It is clear from Figure 5, that sparse connectivity had a higher effect on network dynamics than dense connectivity. This also supports the argument (as mentioned in above section) that in dense networks, the steady state neuron activity is high above the threshold of LSM, preventing the neuron activity to cross the threshold, which in turn affects the network dynamics (all neurons converge to an attractor state based on the input signal).
4 Conclusions
The linear summation neuron model (LSM) demonstrated to be effective in keeping the total network activity at a steady state, in a fully connected recurrent neuronal network, with continuous inputs. While continually keeping the neuron activity around steady state, rather than saturating, we also showed that LSM maintains sensitivity in capturing the evolution of the input activity (Figure 2). This property of the LSM reflects the resting membrane potential of biological neurons [17].
The function of the thresholding function in the LSM can be related to the spiking threshold in a biological neuron. We showed that this threshold function, along with inhibition and conduction delays, to play a beneficial role for sustaining the dynamic information across all the neurons within the network. Maintaining the steady-state activity near to the threshold allowed the inhibitory input into the neuron to drive the neuron activity towards its threshold (Equation 2), thereby generating non-linear dynamics in the network. Conduction delays on other hand induced discrepancies across the neurons in the network, adding an extra dimension of complexity to the non-linear dynamics in the network. Such mechanisms were demonstrated to help to avoid loss of activity dynamics in the neuron activity (and) avoiding the activity of all neurons to converge to a single attractor state (Figure
7

Figure 5: Analysis of network dynamics using covariance measure. The covariance index (CI) value (each data point, in each subplot) indicates the distance between covariances of neurons in a sparse/dense network, given different values of input noise/initial neuron activity.
3, 4). These mechanisms would also help in learning generalized representation of the inputs, leading to a robust learning architecture. We showed the type of network connectivity (with dense recurrency) to have significant impact on the neuron activity dynamics and network dynamics. The sparsely connected network was more sensitive to the dynamical activity changes within the input and the internal activity of the network. However, neurons in a sparsely connected network would have the disadvantage of limited integration of information across the network. The neurons in a densely connected network tend to encapsulate all the information within the network, allowing information sharing and learning generalized representations, but carrying shortcomings such as loss in information specificity. Therefore, we believe dense coding3 in a sparsely connected network will be potentially an advantageous tradeoff between information specificity and generalization of information representation [16]. Considering a dense coding scenario (where the activity across all neurons was used to learn a given context) the internal state of the neurons would play a crucial role. The combined internal state of neurons in a network will define the network state, which in turn become the reference by which the input would be learnt. In this work, with the covariance analysis (Figure 5), we showed that the covariance between neurons would vary depending on the specific input/neuron parameters and the network connectivity. Therefore, we argue that state of the neuron/network is dynamically changing and it will become a defining factor for how information is represented within the network.
Acknowledgments and Disclosure of Funding
This work was supported by the EU Grant FET 829186 ph-coding (Predictive Haptic COding Devices In Next Generation interfaces), the Swedish Research Council (Project Grant No. K2014-63X-1478012-3).
References
[1] Tom Binzegger, Rodney J Douglas, and Kevan AC Martin. A quantitative map of the circuit of cat primary visual cortex. Journal of Neuroscience, 24(39):8441­8453, 2004.
[2] Sen Song, Per Jesper Sjöström, Markus Reigl, Sacha Nelson, and Dmitri B Chklovskii. Highly nonrandom features of synaptic connectivity in local cortical circuits. PLoS Biol, 3(3):e68, 2005.
3Dense coding is when all the neurons in a network are active and their combined activity is used to encode each context.
8

[3] German Koestinger, Kevan AC Martin, and Elisha S Rusch. Translaminar circuits formed by the pyramidal cells in the superficial layers of cat visual cortex. Brain Structure and Function, 223(4):1811­1828, 2018.
[4] Kohitij Kar and James J DiCarlo. Fast recurrent processing via ventrolateral prefrontal cortex is needed by the primate ventral stream for robust core visual object recognition. Neuron, 109(1):164­176, 2021.
[5] J Julius Zhu and Fu-Sun Lo. Recurrent inhibitory circuitry in the deep layers of the rabbit superior colliculus. The Journal of Physiology, 523(3):731­740, 2000.
[6] Rodney J Douglas and Kevan AC Martin. Inhibition in cortical circuits. Current Biology, 19(10):R398­ R402, 2009.
[7] Joshua Obermayer, Tim S Heistek, Amber Kerkhofs, Natalia A Goriounova, Tim Kroon, Johannes C Baayen, Sander Idema, Guilherme Testa-Silva, Jonathan J Couey, and Huibert D Mansvelder. Lateral inhibition by martinotti interneurons is facilitated by cholinergic inputs in human and mouse neocortex. Nature communications, 9(1):1­14, 2018.
[8] Henrik Jörntell and Carl-Fredrik Ekerot. Receptive field plasticity profoundly alters the cutaneous parallel fiber synaptic input to cerebellar interneurons in vivo. Journal of Neuroscience, 23(29):9620­9631, 2003.
[9] Hyun-Jae Pi, Balázs Hangya, Duda Kvitsiani, Joshua I Sanders, Z Josh Huang, and Adam Kepecs. Cortical interneurons that specialize in disinhibitory control. Nature, 503(7477):521­524, 2013.
[10] Khadeejah T Sultan and Song-Hai Shi. Generation of diverse cortical inhibitory interneurons. Wiley Interdisciplinary Reviews: Developmental Biology, 7(2):e306, 2018.
[11] Mitsuo Kawato, Hideki Hayakawa, and Toshio Inui. A forward-inverse optics model of reciprocal connections between visual cortical areas. Network: Computation in Neural Systems, 4(4):415­422, 1993.
[12] Mitsuo Kawato, Kazunori Furukawa, and Ryoji Suzuki. A hierarchical neural-network model for control and learning of voluntary movement. Biological cybernetics, 57(3):169­185, 1987.
[13] Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J Friston. Canonical microcircuits for predictive coding. Neuron, 76(4):695­711, 2012.
[14] Aman B Saleem, Asli Ayaz, Kathryn J Jeffery, Kenneth D Harris, and Matteo Carandini. Integration of visual motion and locomotion in mouse visual cortex. Nature neuroscience, 16(12):1864­1869, 2013.
[15] Andy Clark. Whatever next? predictive brains, situated agents, and the future of cognitive science. Behavioral and brain sciences, 36(3):181­204, 2013.
[16] Anton Spanne and Henrik Jörntell. Questioning the role of sparse coding in the brain. Trends in neurosciences, 38(7):417­427, 2015.
[17] Udaya B. Rongala, Jonas M. D. Enander, Matthias Kohler, Gerald E. Loeb, and Henrik Jörntell. A non-spiking neuron model with dynamic leak to avoid instability in recurrent networks. Frontiers in Computational Neuroscience, 15:43, 2021.
[18] Mircea Steriade, Angel Nunez, and Florin Amzica. A novel slow (< 1 hz) oscillation of neocortical neurons in vivo: depolarizing and hyperpolarizing components. Journal of neuroscience, 13(8):3252­3265, 1993.
[19] Ronald L Cowan and Charles J Wilson. Spontaneous firing patterns and axonal projections of single corticostriatal neurons in the rat medial agranular cortex. Journal of neurophysiology, 71(1):17­32, 1994.
[20] Ilan Lampl, Iva Reichova, and David Ferster. Synchronous membrane potential fluctuations in neurons of the cat visual cortex. Neuron, 22(2):361­374, 1999.
[21] Nestor Parga and Larry F Abbott. Network model of spontaneous activity exhibiting synchronous transitions between up and down states. Frontiers in neuroscience, 1:4, 2007.
[22] David Holcman and Misha Tsodyks. The emergence of up and down states in cortical networks. PLoS Comput Biol, 2(3):e23, 2006.
[23] Alain Destexhe. Self-sustained asynchronous irregular states and up­down states in thalamic, cortical and thalamocortical networks of nonlinear integrate-and-fire neurons. Journal of computational neuroscience, 27(3):493­506, 2009.
[24] Maria V Sanchez-Vives and David A McCormick. Cellular and network mechanisms of rhythmic recurrent activity in neocortex. Nature neuroscience, 3(10):1027­1034, 2000.
9

[25] Pavel M Esir, Susan Yu Gordleeva, Alexander Yu Simonov, Alexander N Pisarchik, and Victor B Kazantsev. Conduction delays can enhance formation of up and down states in spiking neuronal networks. Physical Review E, 98(5):052401, 2018.
[26] Michael Marmaduke Woodman and Carmen C Canavier. Effects of conduction delays on the existence and stability of one to one phase locking between two pulse-coupled oscillators. Journal of computational neuroscience, 31(2):401­418, 2011.
[27] G Bard Ermentrout and Nancy Kopell. Fine structure of neural spiking and synchronization in the presence of conduction delays. Proceedings of the National Academy of Sciences, 95(3):1259­1264, 1998.
[28] Alberto Mazzoni, Stefano Panzeri, Nikos K Logothetis, and Nicolas Brunel. Encoding of naturalistic stimuli by local field potential spectra in networks of excitatory and inhibitory neurons. PLoS Comput Biol, 4(12):e1000239, 2008.
[29] Udaya B Rongala, Anton Spanne, Alberto Mazzoni, Fredrik Bengtsson, Calogero M Oddo, and Henrik Jörntell. Intracellular dynamics in cuneate nucleus neurons support self-stabilizing learning of generalizable tactile representations. Frontiers in cellular neuroscience, 12:210, 2018.
10

A Appendix
Figure A.1: (A) Cross-correlation between all neuron responses (dense connectivity) for different values of input noise, initial neuron activity and inhibition. The boxplot was made across the correlation measure for 25 different repetitions of the same network setting (input noise, initial neuron activity, inhibition). B Similar to A, with sparse connectivity.
11

Figure A.2: Average neuron activity. (A) Average neuron activity in a network with sparse connectivity, for different input noise and a given input of 5 Hz (input #1). (B) Similar to A, with densely connected network.
12

Figure A.3: (A) Cross-correlation between all neuron responses (dense connectivity, with input #2 (50 Hz)) for different values of input noise and inhibition. The initial neuron activity value was 0. Note: The y-axis (left) indicates the average cross-correlation index value for 25 different repetitions of each setting (see Methods), y-axis (right) indicates the variance in cross-correlation index value for 25 repetitions. This was similar for subplots A-H. (B) Cross-correlation between all neuron responses (dense connectivity) for different values of initial neuron activity and inhibition. The input noise value was 0.1. (C) Similar to A, with sparse connectivity. (D) Similar to B, with sparse connectivity. E-H Similar to A-D, with the networks in the presence of conduction delays.
13

