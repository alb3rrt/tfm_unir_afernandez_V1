Post-Contextual-Bandit Inference

arXiv:2106.00418v1 [stat.ML] 1 Jun 2021

Aure´lien Bibaut Netflix

Antoine Chambaz Universite´ Paris Descartes

Maria Dimakopoulou Netflix

Nathan Kallus Cornell University and Netflix

Mark van der Laan University of California, Berkeley

Abstract
Contextual bandit algorithms are increasingly replacing non-adaptive A/B tests in e-commerce, healthcare, and policymaking because they can both improve outcomes for study participants and increase the chance of identifying good or even best policies. To support credible inference on novel interventions at the end of the study, nonetheless, we still want to construct valid confidence intervals on average treatment effects, subgroup effects, or value of new policies. The adaptive nature of the data collected by contextual bandit algorithms, however, makes this difficult: standard estimators are no longer asymptotically normally distributed and classic confidence intervals fail to provide correct coverage. While this has been addressed in non-contextual settings by using stabilized estimators, the contextual setting poses unique challenges that we tackle for the first time in this paper. We propose the Contextual Adaptive Doubly Robust (CADR) estimator, the first estimator for policy value that is asymptotically normal under contextual adaptive data collection. The main technical challenge in constructing CADR is designing adaptive and consistent conditional standard deviation estimators for stabilization. Extensive numerical experiments using 57 OpenML datasets demonstrate that confidence intervals based on CADR uniquely provide correct coverage.
1 Introduction
Contextual bandits, where personalized decisions are made sequentially and simultaneously with data collection, are increasingly used to address important decision-making problems where data is limited and/or expensive to collect, with applications in product recommendation [Li et al., 2010], revenue management [Kallus and Udell, 2020, Qiang and Bayati, 2016], and personalized medicine [Tewari and Murphy, 2017]. Adaptive experiments, whether based on bandit algorithms or Bayesian optimization, are increasingly being considered in place of classic randomized trials in order to improve both the outcomes for study participants and the chance of identifying the best treatment allocations [Athey et al., 2018, Quinn et al., 2019, Kasy and Sautmann, 2021, Bakshy et al., 2018].
But, at the end of the study, we still want to construct valid confidence intervals on average treatment effects, subgroup effects, or the value of new personalized interventions. Such confidence intervals are, for example, crucial for enabling credible inference on the presence or absence of improvement of novel policies. However, due to the adaptive nature of the data collection, unlike classic randomized trials, standard estimates and their confidence intervals actually fail to provide correct coverage, that is, contain the true parameter with the desired confidence probability (e.g., 95%). A variety of recent work has recognized this and offered remedies [Hadad et al., 2019, Luedtke and van der Laan, 2016], but only for the case of non-contextual adaptive data collection. Like classic confidence intervals, when data comes from a contextual bandit ­ or any other context-dependent
Alphabetical order
Preprint. Under review.

adaptive data collection ­ these intervals also fail to provide correct coverage. In this paper, we propose the first asymptotically normal estimator for the value of a (possibly contextual) policy from context-dependent adaptively collected data. This asymptotic normality leads directly to the construction of valid confidence intervals.
Our estimator takes the form of a stabilized doubly robust estimator, that is, a weighted time average of an estimate of the so-called canonical gradient using plug in estimators for the outcome model, where each time point is inversely weighted by its estimated conditional standard deviation given the past. We term this the Contextual Adaptive Doubly Robust (CADR) estimator. We show that, given consistent conditional variance estimates which at each time point only depend on previous data, the CADR estimator is asymptotically normal, and as a result we can easily construct asymptotically valid confidence intervals. This normality is in fact robust to misspecifying the outcome model. A significant technical challenge is actually constructing such variance estimators. We resolve this using an adaptive variance estimator based on the importance-sampling ratio of current to past (adaptive) policies at each time point. We also show that we can reliably estimate outcome models from the adaptively-collected data so that we can plug them in. Extensive experiments using 57 OpenML datasets demonstrate the failure of previous approaches and the success of ours at constructing confidence intervals with correct coverage.
1.1 Problem Statement and Notation
The data. Our data consists of a sequence of observations indexed t = 1, . . . , T comprising of context X(t)  X , action A(t)  A, and outcome Y (t)  Y  R generated by an adaptive experiment, such as a contextual bandit algorithm. Roughly, at each round t = 1, 2, . . . , T , an agent formed a contextual policy gt(a | x) based on all past observations, then observed an independently drawn context vector X(t)  Q0,X , carried out an action A(t) drawn from its current policy gt(· | X(t)), and observed an outcome Y (t)  Q0,Y (· | A(t), X(t)) depending only on the present context and action. The action and context measurable spaces X , A are arbitrary, e.g., finite or continuous.
More formally, we let O(t) := (X(t), A(t), Y (t)) and make the following assumptions about the sequence O(1), . . . , O(T ) comprising our dataset. First, we assume X(t) is independent of all else given A(t) and has a time-independent marginal distribution that we denote by Q0,X . Second, we assume A(t) is independent of all else given O(1), . . . , O(t - 1), X(t) and we set gt(· | X(t)) to its (random) conditional distribution given O(1), . . . , O(t - 1), X(t). Third, we assume Y (t) is independent of all else given X(t), A(t) and has a time-independent conditional distribution given X(t) = x, A(t) = a that is denoted by Q0,Y (· | A, X). The distributions Q0,X and Q0,Y are unknown, while the policies gt(a | x) are known, as would be the case when running an adaptive experiment. To simplify presentation we endow A with a base measure µA (e.g., counting for finite actions or Lebesgue for continuous actions) and identify policies gt with conditional densities with respect to (w.r.t.) µA. In the case of K <  actions, policies are maps from X to the K-simplex.
Note that, as the agent updates its policy based on already collected observations, gt is a random O(1), . . . , O(t - 1)-measurable object. This is the major departure from the setting considered in other literature on off-policy evaluation, which only consider a fixed logging policy, gt = g, that is independent of the data. See Section 1.2.
The target parameter. We are interested in inference on a generalized average causal effect expressed as a functional of the unknown distributions above, 0 = (Q0,X , Q0,Y ), where for any distributions QX , QY , we define
(QX , QY ) := yQX (dx)g(a | x)dµA(a)QY (dy | a, x),
where g(a | x) : A × X  [-G, G] is a given fixed, bounded function. Two examples are: (a) when g is a policy (conditional density), then 0 is its value; (b) when g is the difference between two policies then 0 is the difference between their values. A prominent example of the latter is when A = {+1, -1} and g(a | x) = a, which is known as the average treatment effect. If we include an indicator for x being in some set, then we get the subgroup effect.
2

Defining the conditional mean outcome,
Q¯0(a, x) := EQ0,Y (·|x,a)[Y ] = yQ0,Y (dy | a, x),
we note that the target parameter only depends on Q0,Y via Q¯0, so we also overload notation and write (QX , Q¯) = Q¯(a, x)QX (dx)g(a | x)dµA(a) for any function Q¯ : A × X  Y. Note that when |A| <  and µA is the counting measure, the integral over a is a simple sum.

Canonical gradient. We will make repeated use of the following function: for any conditional density (a, x)  g(a | x), any probability distribution QX over the context space X , and any function Q¯ : A × X , we define the function D (g, Q¯) : O  R by

D

(g, Q¯)(x, a, y)

:=

g(a

|

x) (y

-

Q¯(a, x))

+

g(a | x)

Q¯(a , x)g(a | x)dµA(a ).

Further, define D(g, QX , Q¯) = D (g, QX , Q¯) - (QX , Q¯), which coincides with the so-called canonical gradient of the target parameter  w.r.t. the usual nonparametric statistical model comprising all joint distributions over O [van der Vaart, 2000, van der Laan and Robins, 2003].

Integration operator notation. For any policy g and distributions QX , QY , denote by PQ,g the induced distribution on O. For any function f : O  R, we use the integration operator notation

PQ,gf = f (x, a, y)QX (dx)g(a | x)dµA(a)QY (dy | a, x),

that is, the expectation w.r.t. PQ,g alone. Then, for example, for any O(1), . . . , O(s-1)-measurable random function f : O  R, we have that PQ0,gs f = EQ0,gs [f (O(s)) | O(1), . . . , O(s - 1)].

1.2 Related Literature and Challenges for Post-Contextual-Bandit Inference

Off-policy evaluation. In non-adaptive settings, where gt = g is fixed and does not depend on previous observations, common off-the shelf estimators for the mean outcome under g include the Inverse Propensity Scoring (IPS) estimator [Beygelzimer and Langford, 2009, Li et al., 2011] and and the Doubly Robust (DR) estimator [Dud´ik et al., 2011, Robins et al., 1994]:

IPS := 1 T D (g, 0), T
t=1

DR := 1 T D (g, Q¯) T
t=1

where Q¯ is an estimator of the outcome model Q¯0(a, x). If we use cross-fitting to estimate Q¯ [Chernozhukov et al., 2018], then both the IPS and DR estimators are unbiased and asymptotically normal, permitting straightforward inference using Wald confidence intervals (i.e., ±1.96 of the
estimated standard error). There also exist many variants of the IPS and DR estimators that, rather than plugging in the importance sampling (IS) ratios (g/gt)(A(t) | X(t)) and/or outcome-model estimators, instead choose them directly with the aim to minimize error [e.g. Kallus, 2018, Farajtabar
et al., 2018, Thomas and Brunskill, 2016, Wang et al., 2017, Kallus and Uehara, 2019b].

Inference challenges in adaptive settings. In the adaptive setting, it is easy to see that, if in the
tth term for DR we use an outcome model Q¯t-1 fit using only the observations O(1), . . . , O(t - 1), then both the IPS and DR estimators both remain unbiased. However, neither generally converges
to a normal distribution. One key difference between the non-adaptive and adaptive settings is that the IS ratios (g/gt)(A(t) | X(t)) can both diverge to infinity or converge to zero. As a result of this, the above two estimators may either be dominated by their first terms or their last terms. At a
more theoretical level, this violates the classical condition of martingale central limit theorems that
the conditional variance of the terms given previous observations stabilizes asymptotically.

Stabilized DR estimators in non-contextual settings. The issue for inference due to instability of the DR estimator terms was recognized by Luedtke and van der Laan [2016] in another setting. They work in the non-adaptive setting but consider the problem of inferring the maximum mean outcome over all policies when the optimal policy is non-unique. Their proposal is a so-called

3

stabilized estimator, in which each term is inversely weighted by an estimate of its conditional standard deviation given the previous terms. This stabilization trick has been also been reused for off-policy inference from non-contextual bandit data by Hadad et al. [2019], as the stabilized estimator remains asymptotically normal, permitting inference. In their non-contextual setting, an estimate of the conditional standard deviation of the terms can easily be obtained by the inverse square root propensities. In contrast, in our contextual setting, obtaining valid stabilization weights is more challenging and requires a construction involving adaptive training on past data.
1.3 Contributions
In this paper, we construct and analyze a stabilized estimator for policy evaluation from contextdependent adaptively collected data, such as the result of running a contextual bandit algorithm. This then immediately enables inference. After constructing a generic extension of the stabilization trick, the main technical challenge is to construct a sequence of estimators 1, . . . , T of the conditional standard deviations that are both consistent and such that for each t, t only uses the previous data points O(1), . . . , O(t - 1). We show in extensive experiments across a large set of contextual bandit environments that our confidence intervals uniquely achieve close to nominal coverage.

2 Construction and Analysis of the Generic Contextual Stabilized Estimator

In this section, we give a generic construction of a stabilized estimator in our contextual and adaptive setting. That is, given generic plug-ins for outcome model and conditional standard deviation. We then provide conditions under which the estimator is asymptotically normal, as desired. To develop CADR, we will then proceed to construct appropriate plug in estimators in the proceeding sections.

2.1 Construction of the Estimator
Outcome and variance estimators. Our estimator uses a sequence (Q¯t)t1 of estimators of the outcome model Q¯0, such that, for every t, Q¯t is O(1), . . . , O(t)-measurable, that is, is trained using only the data up to time t. A key part of our estimator are the conditional variance estimators.
Additionally, we require estimates of the conditional standard deviation of the canonical gradient. Define
0,t := 0,t(gt),
where 02,t(g) := VarQ0,g D (g, Q¯t-1)(O(t)) | O(1), . . . , O(t - 1) .
Let (t)t1 be a given sequence of estimates of 0,t such that t is O(1), . . . , O(t - 1)-measurable, that is, is estimated using only the data up to time t.

The generic form of the estimator. The generic contextual stabilized estimator is then defined as:

T :=

1 T

T

t-1

-1
1
T

T

t-1D (g, Q¯t-1).

(1)

t=1

t=1

2.2 Asymptotic normality guarantees

We next characterize the asymptotic distribution of T under some assumptions. Assumption 1 (Non degenerate efficiency bound). infg PQ0,gD2(g, Q¯0, Q0,X ) > 0.
Assumption 1 states that there is no fixed logging policy g such that the efficiency bound for estimation of (Q¯0, Q0,X ) in the nonparametric model, from i.i.d. draws of PQ0,g, is zero. If assumption 1 does not hold, there exists a logging policy g such that, if O = (X, A, Y )  PQ0,g, then (g(A | X)/g(A | X))Y equals (Q¯0, Q0,X ) with probability 1. In other words, if assumption 1 does not hold, there exists a logging policy g such that (Q¯0, Q0,X ) can be estimated with no error with probability 1 from a single draw of PQ0,g. Thus, it is very lax. An easy sufficient condition for Assumption 1 is that the outcome model has nontrivial variance in that VarQ0,X ( Q¯(a, X)g(a | x)dµA(a)) > 0.

4

Algorithm 1 The CADR Estimator and Confidence Interval

Input: Data O(1), . . . , O(T ), policies g1, . . . , gT , target g, outcome regression estimator for t = 1, 2, . . . , T do

Train Q¯t-1 on O(1), . . . , O(t - 1) using the outcome regression estimator

Set Dt,s = D(gs, Q¯t-1)(O(t)) for s = t, . . . , T // (note index order compared to next line)

Set t2

=

1 t-1

t-1 s=1

gt (A(s)|X (s)) gs (A(s)|X (s))

(Ds,t

)2

-

1 t-1

t-1 s=1

gt (A(s)|X (s)) gs (A(s)|X (s))

Ds,t

2

end for

Set T Return

=

1 T

estimate

T t=1

t-1

T

=

T T

-1
T t=1

t-1Dt,t

and

confidence

intervals

CI

=

[T

± 1-/2T

 / T]

Assumption 2 (Consistent standard deviation estimators.). t - 0,t -t-- 0 almost surely.

In the next section we will proceed to construct specific estimators t that satisfy Assumption 2, leading to our proposed CADR estimator and confidence intervals.
Assumption 3 (Exploration rate). For any t  1, we have that infaA,xX gt(a | x) t-1/2 almost surely.

Here, at bt means that for some constant c > 0, we have at  cbt for all t  1. Assumption 3 requires that the exploration rate of the adaptive experiment does not decay too quickly.

Based on these assumptions, we have the following asymptotic normality result:

Theorem 1. Denote T := T -1

T t=1

t-1

-1
. Under Assumptions 1 to 3, it holds that

 -T 1 T

T - 0

-d N (0, 1).

Remark 1. Theorem 1 does not require the outcome model estimator to converge at all. As we will see in Section 3, our conditional variance estimator does require that the outcome model converges to a fixed limit Q¯1, but this limit does not have to be the true outcome model Q¯0. In other words, consistency of the outcome model is not required at any point of our analysis.

3 Construction of the Conditional Variance Estimator and CADR

We now tackle the construction of t satisfying our assumptions; namely, they must be adaptively trained only on past data at each t and they must be consistent. Observe that 02,t = 02(gt, Q¯t-1), where we define
02(g, Q¯) := 0,1(g, Q¯) - (0,2(g, Q¯))2, 0,i(g, Q¯) := PQ0,g(D )i(g, Q¯), i = 1, 2.

Designing an O(1), . . . , O(t - 1)-measurable estimator of 02,t presents several challenges. First, while we can only use observations O(1), . . . , O(t - 1) to estimate it, 02,t is defined as a function
of integrals w.r.t. PQ0,gt , from which we have only one observation, namely O(t). Second, our
estimation target 02,t = 0(gt, Q¯t-1) is random as it depends on gt and Q¯t. Third, gt, Q¯t depend on the same observations O(1), . . . , O(t - 1) that we have at our disposal to estimate 02,t.

Representation via importance sampling. We can overcome the first difficulty via importance sampling, which allows us to write 0,i(g, Q¯), i = 1, 2 as integrals w.r.t. PQ0,gs , s = 1, . . . , t - 1, i.e., the conditional distributions of observations O(s), s = 1, . . . , t - 1 given their respective past.
Namely, for any s  1, i = 1, 2, we have that

0,i(g,

Q¯)

=

PQ0 ,gs

g gs

(D

)i(g,

Q¯).

(2)

5

Dealing with the randomness of the estimation target. We now turn to second challenge. Since 02,t can be written in terms of 0,i(gt, Q¯t-1) for i = 1, 2, Eq. (2) suggests perhaps an approach based on sample averages of (gt/gs)(D )i(gt, Q¯t-1) over s. However, whenever s < t, the latter is an O(1), . . . , O(t - 1)-measurable function due to the dependence on gt and Q¯t. Namely, PQ0,gs {(gt/gs)(D )i(gt, Q¯t-1)} does not coincide in general with the conditional expectation EQ0,gs [((gt/gs)(D )i(gt, Q¯t-1))(O(s)) | O¯(s - 1)], as would arise from a sample average. We now look at solutions to overcome this difficulty, considering first Q¯t-1 and then gt.

Dealing with the randomness of Q¯t-1. We propose an estimator of 02(g, Q¯t-1) for any fixed g.
While requiring that Q¯t-1 converges to the true outcome regression function Q¯0 is a strong requirement, most reasonable estimators will at least converge to some fixed limit Q¯1. As a result, under
an appropriate stochastic convergence condition on (Q¯t-1)t1, 0,i(g, Q¯t-1) can be reasonably approximated by the corresponding Cesaro averages, defined for i = 1, 2 as

¯ 0,i,t(g)

1 :=
t-

1

t-1
0,i(g, Q¯s-1)

=

1 t-

1

t

EQ0 ,gs

((g/gs)(D )i(g, Q¯s-1))(O(s)) | O¯(s - 1) .

s=1

s=1

These are easy to estimate from the corresponding sample averages, defined for i = 1, 2 as

1 i,t(g) := t - 1

t
((g/gs)(D )i(g, Q¯s-1))(O(s)),

s=1

since for each i = 1, 2, the difference i,t(g) - ¯ 0,i,t(g) is the average of a martingale difference sequence (MDS). We then define our estimator of 02(g, Q¯t-1) as

t(g) := 1,t(g) - (2,t(g))2.

(3)

From fixed g to random gt. So far, we have proposed and justified the construction of t(g) as
an estimator of 0,t(g, Q¯t-1) for a fixed g. We now discuss conditions under which t(gt) is valid
estimator of 0,t(gt, Q¯t-1). When g is fixed, for each i = 1, 2, the error i,t(g) - 0,i,t(g, Q¯t-1) decomposes as the sum of the MDS average i,t(g) - ¯ 0,i,t(g) and of the Cesaro approximation
error ¯ 0,i,t(g) - 0,i(g, Q¯t-1). Both differences are straightforward to bound. For a random gt, the term i,t(gt) - ¯ 0,i,t(gt) is no longer an MDS average. Fortunately, under a complexity condition on the logging policy class G, we can bound the supremum of the martingale empirical processes {|i,t(g) - ¯ 0,i,t(g)| : g  G}, which in turn gives us a bound on |i,t(gt) - ¯ 0,i,t(gt)|.

Consistency guarantee for t2. Our formal consistency result relies on the following assumptions. Assumption 4 (Outcome regression estimator convergence). There exists  > 0, and a fixed function Q¯1 : A × X  R such that Q¯t - Q¯1 1,Q0,X,g = O(t-) almost surely.

The next assumption is a bound on the bracketing entropy (see, e.g., [van der Vaart and Wellner, 1996] for definition) of the logging policy class.
Assumption 5 (Complexity of the logging policy class). There exists a class of conditional densities G such that gt  G t  1 almost surely, there exists G > 0 such that supgG g/gref   G, and for some p > 0

log N[ ]( , G/gref , · ) 2,Q0,X ,gref

-p,

where G/gref := {g/gref : g  G}.

Next, we require a condition on the exploration rate that is stronger than Assumption 3.
Assumption 6 (Exploration rate (stronger)). For ant t  1, we have that infaA,xX gt(a | x)/gref (a | x) t-(,p) almost surely, where (, p) := min(1/((3 + p)), 1/(1 + 2p), ).

6

Theorem 2. Suppose that Assumptions 4 to 6 hold. Then, t2 - 02,t = o(1) almost surely.
Remark 2. While we theoretically require the existence of a logging policy class G with controlled complexity, we do not actually need to know G to construct our estimator. Moreover, while we require a bound on the bracketing entropy of the logging policy class G, we impose no restriction on the outcome regression model complexity, permitting us to use flexible black-box regression methods.

Remark 3. Assumption 4 requires (Q¯t) to be a sequence of regression estimator, such that for every

t  1, Q¯t fixed limit

is fitted on O(1), . . . , O(t) and for which we Q¯1. Note that this can at first glance pose a

can guarantee a rate of convergence to some challenge since observations O(1), . . . , O(t)

are adaptively collected. In the appendix, we give guarantees for outcome regression estimation

over a nonparametric model using an importance sampling weighted empirical risk minimization.

CADR asymptotics. Our proposed CADR estimator is now given by plugging our estimates t from Eq. (3) into Eq. (1), as summarized in Algorithm 1 As an immediate corollary of Theorems 1 and 2 we have our main guarantee for this final estimator, showing CADR is asymptotically normal, whence we immediately obtain asymptotically valid confidence intervals.

Corollary 1 (CADR Asymptotics and Inference). Suppose that Assumptions 1 and 4 to 6 hold. Let

t be given as in Eq. (3). Denote T := T -1

T t=1

t-1

-1
. Then,

 -T 1 T

T - 0

-d N (0, 1).

Moreover, letting  denote the -quantile of the standard normal distribution,

 Pr (Q0,X , Q¯0)  T ± 1-/2T / T

-T- -- 1 - .

4 Empirical Evaluation

We next present computational results on public datasets that demonstrate the robustness of CADR confidence intervals using contextual bandit data with comparison to several baselines. Our experiments focus on the case of finitely-many actions, A = {1, . . . , K}.

4.1 Baseline Estimators

We compare CADR to several benchmarks. All take the following form for a choice of wt, t, Q¯t:





T =

1 T wt

-1 1 T

T

wtD~ t,

t=1

CI = T ± 1-/2 

T t=1

wt2(D~ t - )2

T t=1

wt

2

  

,

K
where D~t = t(Y (t) - Q¯t-1(A(t), X(t))) + Q¯t-1(a, X(t))g(a | X(t)).
a=1

The Direct Method (DM) sets wt = 1, t = 0 and fits Q^¯t-1(a, ·) by running some regression

method for each a on the data {(X(s), Y (s)) : 1  s  t - 1, A(s) = a}. We will use either

linear regression or decision-tree regression, both using default sklearn parameters. Note that

even in non-contextual settings, where Q^¯t-1 is a simple per-arm sample average, Q^¯t-1 may be

biased due to adaptive data collection [Xu et al., 2013, Luedtke and van der Laan, 2016, Bowden

and Trippa, 2017, Nie et al., 2018, Hadad et al., 2019, Shin et al., 2019]. Inverse Propensity Score

Weighting (IPW) sets wt = 1, t = (g/gt)(A(t) | X(t)), Q^¯t = 0. Doubly Robust (DR) sets

wt = 1, t = (g/gt)(A(t) | X(t)) and fits Q^¯t-1 as in DM. More Robust Doubly Robust (MRDR)

[Farajtabar et al., 2018] is the same as DR but when fitting Q^¯t-1 we reweight each data point

by

g

(A(s)|X (s))(1-gs (A(s)|X gs (A(s)|X (s))2

(s)))

.

None of the above are generally asymptotically normal under

adaptive data collection [Hadad et al., 2019]. Adaptive Doubly Robust (ADR; a.k.a. stabilized one-

step estimator for multi-armed bandit data) [Luedtke and van der Laan, 2016, Hadad et al., 2019] is

7

Samples < 1000  1000 and < 10000  10000

Count 17 30 10

Classes =2
> 2 and < 10  10

Count 31 17 9

Features  2 and < 10  10 and < 50  50 and  100

Count 14 34 9

Table 1: Characteristics of the 57 OpenML-CC18 datasets used for evaluation.

the same as DR but sets wt = gt-1/2(A(t)|X(t)). ADR is unbiased and asymptotically normal for multi-armed bandit logging policies but is biased for context-measurable adaptive logging policies, which is the focus of this paper. Finally, note that our proposal CADR takes the same form as DR but with wt = t-1 using our adaptive conditional standard deviation estimators t in Eq. (3).
4.2 Contextual Bandit Data from Multiclass Classification Data
To construct our data, we turn K-class classification tasks into a K-armed contextual bandit problems [Dud´ik et al., 2014, Dimakopoulou et al., 2017, Su et al., 2019], which has the benefits of reproducibility using public datasets and being able to make uncontroversial comparisons using actual ground truth data with counterfactuals. We use the public OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18; BSD 3-Clause license) [Bischl et al., 2017], which has datasets that vary in domain, number of observations, number of classes and number of features. Among these, we select the classification datasets which have less than 100 features. This results in 57 classification datasets from OpenML-CC18 used for evaluation and Table 1 summarizes the characteristics of these datasets.
Each dataset is a collection of pairs of covariates X and labels L  {1, . . . , K}. We transform each dataset to the contextual bandit problem as follows. At each round, we draw X(t), L(t) uniformly at random with replacement from the dataset. We reveal the context X(t) to the agent, and given an arm pull A(t), we draw and return the reward Y (t)  N (1{A(t) = L(t)}, 1). To generate our data, we set T = 10000 and use the following -greedy procedure. We pull arms uniformly at random until each arm has been pulled at least once. Then at each subsequent round t, we fit Q¯t-1 using the data up to that time in the same fashion as used for the DM estimator above using decision-tree regressions. We set A~x(t) = arg maxa=1,...,K Q¯t-1(a, X(t)) and t = 0.01 · t-1/3. We then let gt(a | x) = t/K for a = A~x(t) and gt(A~x(t) | x) = 1 - t + t/K. That is, with probability t we pull a random arm, and otherwise we pull A~X(t)(t).
We then consider four candidate policies to evaluate: (1) "arm 1 non-contextual": g(1 | x) = 1 and otherwise g(a | x) = 0 (note that the meaning of label "1" changes by dataset), (2) "arm 2 noncontextual": g(2 | x) = 1 and otherwise g(a | x) = 0, (3) "linear contextual": we sample a new dataset of size T using a uniform exploration policy, then fit Q¯T as above using linear regression, fix a = arg maxa{1,...,K} Q¯T (a, x), and set g(a | x) = 1 and otherwise g(a | x) = 0, (4) "tree contextual": same as "linear contextual" but fit Q¯T using decision-tree regression.
4.3 Results
Figure 1 shows the comparison of CADR estimator against DM, IPW, DR, ADR, and MRDR w.r.t. coverage, that is, the frequency over 64 replications of the 95% confidence interval covering the true 0, for each of the 57 OpenML-CC18 datasets and 4 target policies. In each subfigure, each dot represents a dataset, the y-axis corresponds to the coverage of the CADR estimator and the x-axis corresponds to the coverage of one of the baseline estimators. The lines represent one standard error over the 64 replications. The dot is depicted in blue if for that dataset CADR has significantly better coverage than the baseline estimator, in red if it has significantly worse coverage, and in black if the difference in coverage of both estimators is within one standard error. In Fig. 1, outcome models for CADR, DM, DR, ADR, and MRDR are fit using linear regression (with default sklearn parameters). In the appendix, we provide additional empirical results where we use decision-tree regressions, or where we use the MRDR outcome model for CADR, or where we use cross-fold estimation across time.
8

Figure 1: Comparison of CADR estimator against DM, IPW, DR, ADR and MRDR w.r.t. 95% confidence interval coverage on 57 OpenML-CC18 datasets and 4 target policies.
Across all of our experiments, we observe that the confidence interval of CADR has better coverage of the ground truth than any other baseline, which can be attributed to its asymptotic normality. The second best estimator in terms of coverage is DR. The advantages of CADR over DR are most pronounced when either (a) there is a mismatch between the logging policy and the target policy (e.g., compare the 1st and 2nd rows in Fig. 1; the tree target policy is most similar to the logging policy, which also uses trees) or (b) when the outcome model is bad (either due to model misspecification such as with a linear model on real data or due to small sample size).
5 Conclusions
Adaptive experiments hold great promise for better, more efficient, and even more ethical experiments. However, they complicate post-experiment inference, which is a cornerstone of drawing credible conclusions from controlled experiments. We provided here the first asymptotically normal estimator for policy value and causal effects when data were generated from a contextual adaptive experiment, such as a contextual bandit algorithm. This led to simple and effective confidence intervals given by adding and subtracting multiples of the standard error, making contextual adaptive experiments a more viable option for experimentation in practice.
6 Societal Impact and Limitations
Adaptive experiments hold particular promise in settings where experimentation is costly and/or dangerous, such as in medicine and policymaking. By adapting treatment allocation, harmful interventions can be avoided, outcomes for study participants improved, and smaller studies enabled. Being able to draw credible conclusions from such experiments make them viable replacements for classic randomized trials. Our confidence intervals offer one way to do so. At the same time, and
9

especially subject to our assumption of vanishing but nonzero exploration, these experiments must be subject to the same ethical guidelines as classic randomized experiments. Additionally, the usual caveats of frequentist confidence intervals hold here, such as its interpretation only as a guarantee over data collection, this guarantee only being approximate in finite samples when we rely on asymptotic normality, and the risks of multiple comparisons and of p-hacking. Finally, we note that our inference focused on an average quantity, as such it focuses on social welfare and need not capture the risk to individuals or groups. Subgroup analyses may therefore be helpful in complementing the analysis; these can be conducted by setting g(a | x) to zero for some x's. Future work may be necessary to further extend our results to conducting inference on risk metrics such as quantiles of outcomes.
References
Susan Athey, Sarah Baird, Julian Jamison, Craig McIntosh, Berk O¨ zler, and Dohbit Sama. A sequential and adaptive experiment to increase the uptake of long-acting reversible contraceptives in cameroon, 2018. URL http://pubdocs.worldbank.org/en/606341582906195532/ Study-Protocol-Adaptive-experiment-on-FP-counseling-and-uptake-of-MCs. pdf. Study protocol.
Eytan Bakshy, Lili Dworkin, Brian Karrer, Konstantin Kashin, Benjamin Letham, Ashwin Murthy, and Shaun Singh. Ae: A domain-agnostic platform for adaptive experimentation. In Workshop on System for ML, 2018.
Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 129­138, 2009.
Aurelien Bibaut, Maria Dimakopoulou, Antoine Chambaz, Nathan Kallus, and Mark van der Laan. Risk minimization from adaptively collected data: Guarantees for supervised and policy learning. 2021.
Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael G Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. arXiv preprint arXiv:1708.03731, 2017.
Jack Bowden and Lorenzo Trippa. Unbiased estimation for response adaptive clinical trials. Statistical methods in medical research, 26(5):2376­2388, 2017.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1­C68, 2018.
Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, and Guido Imbens. Estimation considerations in contextual bandits. arXiv preprint arXiv:1711.07077, 2017.
Miroslav Dud´ik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In Proceedings of the 28th International Conference on International Conference on Machine Learning, pages 1097­1104, 2011.
Miroslav Dud´ik, Dumitru Erhan, John Langford, Lihong Li, et al. Doubly robust policy evaluation and optimization. Statistical Science, 29(4):485­511, 2014.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust offpolicy evaluation. In International Conference on Machine Learning, pages 1447­1456. PMLR, 2018.
Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. arXiv preprint arXiv:1911.02768, 2019.
Nathan Kallus. Balanced policy evaluation and learning. In Advances in Neural Information Processing Systems, pages 8895­8906, 2018.
10

Nathan Kallus and Madeleine Udell. Dynamic assortment personalization in high dimensions. Operations Research, 68(4):1020­1037, 2020.
Nathan Kallus and Masatoshi Uehara. Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning. arXiv preprint arXiv:1909.05850, 2019a.
Nathan Kallus and Masatoshi Uehara. Intrinsically efficient, stable, and bounded off-policy evaluation for reinforcement learning. Advances in neural information processing systems, 32, 2019b.
Maximilian Kasy and Anja Sautmann. Adaptive treatment assignment in experiments for policy choice. Econometrica, 89(1):113­132, 2021.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661­670, 2010.
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 297­306, 2011.
Alexander R. Luedtke and Mark J. van der Laan. Statistical inference for the mean outcome under a possibly non-unique optimal treatment strategy. The Annals of Statistics, 44(2):713 ­ 742, 2016. doi: 10.1214/15-AOS1384. URL https://doi.org/10.1214/15-AOS1384.
Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou. Why adaptively collected data have negative bias and how to correct for it. In International Conference on Artificial Intelligence and Statistics, pages 1261­1269. PMLR, 2018.
Sheng Qiang and Mohsen Bayati. Dynamic pricing with demand covariates. arXiv preprint arXiv:1604.07463, 2016.
Simon Quinn, Alex Teytelboym, Maximilian Kasy, Grant Gordon, and Stefano Caria. A sequential and adaptive experiment to increase the uptake of long-acting reversible contraceptives in cameroon, 2019. URL https://www.socialscienceregistry.org/trials/3870. Study registration.
James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association, 89 (427):846­866, 1994.
Jaehyeok Shin, Aaditya Ramdas, and Alessandro Rinaldo. On the bias, risk and consistency of sample means in multi-armed bandits. arXiv preprint arXiv:1902.00746, 2019.
Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims. Cab: Continuous adaptive blending for policy evaluation and learning. In International Conference on Machine Learning, pages 6005­6014. PMLR, 2019.
Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In Mobile Health, pages 495­517. Springer, 2017.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139­2148. PMLR, 2016.
Mark J van der Laan and James M Robins. Unified methods for censored longitudinal data and causality. Springer Science & Business Media, 2003.
A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes. Springer-Verlag New York, 03 1996. ISBN 9781475725452.
Aad W van der Vaart. Asymptotic statistics. Cambridge university press, 2000.
R. van Handel. On the minimal penalty for Markov order estimation. Probability Theory and Related Fields, 150:709­738, 2011.
11

Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudik. Optimal and adaptive off-policy evaluation in contextual bandits. In International Conference on Machine Learning, pages 3589­3597. PMLR, 2017.
Min Xu, Tao Qin, and Tie-Yan Liu. Estimation bias in multi-armed bandit algorithms for search advertising. Advances in Neural Information Processing Systems, 26:2400­2408, 2013.
Checklist
1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [Yes] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] In the supplemental material with specifics in Section E.4 of the supplemental material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] In Section 4.2 (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] In all figures 1-7. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] In Section E.4 of supplemental material.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] In Section 4.2 (b) Did you mention the license of the assets? [Yes] In Section 4.2 (c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]
12

Supplementary Material for:
Post-Contextual-Bandit Inference
Anonymous Author(s)

A Proof of the asymptotic normality of CADR

Proof of theorem 1. Recalling the definition of our estimator, we have that  T (T - (Q¯0, Q0,X ))

1 =T 
T

T
t-1
t=1

(Q¯t-1, QX,t-1) - (Q¯0, Q0,X ) + D(gt, Q¯t-1, QX,t-1)(O(t))

1 =T 
T

T
t-1
t=1

D(gt, Q¯t-1)(O(t)) - PQ0,gt D(gt, Q¯t-1)(O(t))

1 =T 
T

T
(O(t) - PQ0,gt )t-1(D
t=1

)(gt, Q¯t-1),

where

(D )(g, Q¯) :=D(g, Q¯, Q0,X ) + (Q¯, Q0,X )

= g (y - Q¯) + g

g(a | ·)Q¯(a, ·)dµA(a).

Note that

VarQ0,gt ((D )(gt, Q¯t-1)(O(t)) | O¯(t - 1)) = VarQ0,gt (D(gt, Q¯t-1, QX,t-1)(O(t)) | O¯(t - 1)) =02,t.

Let Zt,T := T -1/2t-1(O(t) - PQ0,gt )(D )(gt, Q¯t-1).

Observe that {Zt,T : t = 1, . . . , T, T  1} is a martingale triangular array where, for every T  1, t  [T ], Zt,T is O¯(t)-measurable. We will apply a martingale central limit theorem for triangular

arrays to prove that

T t=1

Zt,T

-d

N (0, 1).

This will hold if we can check that

· the sum of conditional variances VT :=

T t=1

VarQ0,gt

(Zt,T

|

O¯(t - 1)) converges in

probability to 1,

· the Lindeberg condition is satisfied, that is, for any > 0,

T
E[Zt2,T 1(Zt,T > ) | O¯(t - 1)] -p 0.
t=1

Convergence of the sum of conditional variances. We have that

VT

:=

1 T

T
VarQ0,gt (ZT,t
t=1

| O¯(t - 1)) =

1 T

T t=1

02,t t2

=1+

1 T

T t=1

02,t

02,t - t2 + (02,t -

t2) .

We now show that the terms of the right-hand side of the last equality above are o(1) a.s. As 02,t - t2 = o(1) a.s. by assumption, it suffices to show that 0,t is lower bounded by a positive constant.
For any fixed QX , Q¯, g, we have that, D(g, Q¯, QX ) = D(g, Q¯0, Q0,X ) + (D(g, Q¯, QX ) - D(g, Q¯0, Q0,X )). It is straightforward to check that D(g, Q¯0, Q0,X ) lies in the Hilbert space

13

T1(Q0) := L02(Q0,Y )  L02(Q0,X ), where L02(Q0,Y ) := h : O  R : (x, a)  X × A,

h(x, a, y)dQ0,Y (y | a, x) = 0 ,

and L02(Q0,X ) := h : X  R : h(x)dQ0,X (x) = 0 , while D(g, Q¯, QX ) - D(g, Q¯0, Q0,X ) lies in the Hilbert space
T2(g) := L02(g) := h : X × A  R : x  X , h(x, a)g(a | x)dµA(a) = 0 .

It is straightforward to check that T1(Q0) and T2(g) are orthogonal subspaces of L2(PQ0,g). We have

02(g, Q¯) =

D(g, Q¯, QX )

2 2,Q0 ,g

-

PQ0,gD(g, Q¯, QX ) 2



D(g, Q¯, QX )

2 2,Q0 ,g

=

D(g, Q¯0, Q0,X )

2 2,Q0 ,g

+

D(g, Q¯, QX ) - D(g, Q¯0, Q0,X )

2 2,Q0 ,g



D(g, Q¯0, Q0,X )

2

.

2,Q0 ,g

where we have used in the third line above that D(g, Q¯0, Q0,X ) and D(g, Q¯, QX )-D(g, Q¯0, Q0,X ) lie in the orthogonal subspaces T1(Q0) and T2(g). Therefore,

inf
t1

02,t

:=

inf
t1

02(gt,

Q¯ t-1

)

 inf
g

D(g, Q¯0, Q0,X )

2 2,Q0 ,g

>0,

where the last inequality is exactly assumption 1.

Therefore,

02,t - t2 02,t + (02,t - t2)



02,t - t2 infs1 02,s + o(1)

=

o(1)

almost surely. Therefore, by Cesaro summation, VT - 1 = o(1) a.s.

Checking Lindeberg's condition. Let > 0. We want to show that

T
E[Zt2,T 1(Zt,T  )] -p 0.
t=1
Let t = aA,xX gt(a | x). From assumption 3, t t-1/2/ We have that Zt,T O(t-1T -1/2t-1). Notice that t-1 = (02,t + t2 - 02,t)-1/2 = O(1) a.s. since 02,t  C > 0 and t2 - 02,t = o(1). Therefore, Zt,T = O(t-1T -1/2) = o(1) a.s. since t-1 = o(t-1/2) a.s., and therefore, almost surely, there exists T0( ) such that, for any T  T0( ), all the terms in the sum of the Lindeberg condition, are zero, which implies the the sum converges to zero almost surely.

Therefore, from the central limit theorem for martingale triangular arrays,

 T -T 1(T

-

0)

-d

N (0, 1).

B Estimation of 02,t via sequential importance sampling.
B.1 Errors decomposition In the following lemma, we provide a useful decomposition of the IS-weighted integrands that appear in the expressions of 0,1(g, Q¯) and 0,2(g, Q¯).
14

Lemma 1. It holds that

g gs

D12

(g

,

Q¯)

gref =
gs

f1(g,

Q¯)

+

gref gs

f2(Q¯)

+

gref gs

f3(g,

Q¯)

and

g gs

D1

(g

,

Q¯)

gref =
gs

f4(Q¯)

+

gref gs

f5(g, Q¯),

where

f1(g,

Q¯)

:=

(g/gref (g/gref

)2 )

(y

-

Q¯)2,

f2(Q¯) :=2(g/gref )(y - Q¯) g(a | ·)Q¯(a, ·)dµA(a),

f3(g, Q¯) :=(g/gref )

2
g(a | ·)Q¯(a, ·)dµA(a) ,

f4(Q¯) :=(g/gref )(y - Q¯), f5(g, Q¯) :=(g/gref ) g(a | ·)Q¯(a, ·)dµA(a).

The decomposition above motivates the following definitions.

(11,t)(g)

:= t

1 -

1

t-1 s=1

O(s)

gref gs

f1(g,

Q¯ s-1 ),

(12,t)

:= t

1 -

1

t-1 s=1

gref O(s) gs

f2 (Q¯ s-1 ),

(13,t)(g)

:= t

1 -

1

t-1 s=1

O(s)

gref gs

f3(g,

Q¯ s-1 ),

(21,t)

:= t

1 -

1

t-1 s=1

gref O(s) gs

f4 (Q¯ s-1 ),

(22,t)(g)

:= t

1 -

1

t-1 s=1

O(s)

gref gs

f5(g,

Q¯ s-1 ),

and

¯ (01,1),t(g)

:= t

1 -

1

t-1 s=1

PQ0 ,gs

gref gs

f1(g,

Q¯ s-1 ),

¯ (02,1),t

:= t

1 -

1

t-1 s=1

PQ0 ,gs

gref gs

f2 (Q¯ s-1 ),

¯ (03,1),t(g)

:= t

1 -

1

t-1 s=1

PQ0 ,gs

gref gs

f3(g,

Q¯ s-1 ),

¯ (01,2),t

:= t

1 -

1

t-1 s=1

PQ0 ,gs

gref gs

f4 (Q¯ s-1 ),

¯ (02,2),t(g)

:= t

1 -

1

t-1 s=1

PQ0 ,gs

gref gs

f5(g,

Q¯ s-1 ),

15

and

(01,1) (g ,

Q¯ t-1 )

:=PQ0,gt

gref gs

f1(g,

Q¯ t-1 ),

(02,1) (Q¯ t-1 )

:=PQ0,gt

gref gs

f2 (Q¯ t-1 ),

(03,1) (g ,

Q¯ t-1 )

:=PQ0,gt

gref gs

f3(g,

Q¯ t-1 ),

(01,2) (Q¯ t-1 )

:=PQ0,gt

gref gs

f4 (Q¯ t-1 ),

(03,2) (g ,

Q¯ t-1 )

:=PQ0,gt

gref gs

f5(g,

Q¯ t-1 ),

We have that

1,t =(11,t) + (12,t) + (13,t), and 2,t = (21,t) + (22,t),

¯ 0,1,t(g) =¯ (01,1),t(g) + ¯ (02,1),t + ¯ (03,1),t(g), and ¯ 0,2,t(g) = ¯ (01,1),t + ¯ (02,2),t(g),

0,1(g, Q¯t-1) =(01,1)(g, Q¯t-1) + (02,1)(Q¯t-1) + (03,1)(Q¯t-1),

0,2(g, Q¯t-1) =(01,2)(Q¯t-1) + (02,2)(g, Q¯t-1).

We recall the decomposition of the errors i,t(gt) - 0,i(gt), Q¯t-1) in a martingale empirical process term and an approximation term:
i,t(gt) - 0,i(gt), Q¯t-1) =(i,t(gt) - ¯ 0,i,t(gt)) + (¯ 0,i,t(gt) - 0,i(gt, Q¯t-1)).
We treat the approximation terms in subsection B.4 further down. We further decompose the martingale empirical process terms here. We have that
1,t(gt) - ¯ 0,1,t(gt) =((11,t)(gt) - ¯ (01,1),t(gt)) + ((12,t) - ¯ (02,1),t) + ((13,t)(gt) - ¯ (03,1),t(gt)),
and 2,t(gt) - ¯ 0,2,t(gt) =((21,t) - ¯ (01,2),t) + ((22,t)(gt) - ¯ (02,2),t(gt)).
The two differences (12,t) - ¯ (02,1),t and (21,t) - ¯ (01,2),t are averages of martingale difference sequences, and can be analyzed with a martingale version of Bernstein's inequality. We bound the three other differences by the supremum of martingale empirical processes

B.2 Control of the martingale empirical processes
Let, for any  > 0, G() := {g  G : infa,x g(a | x)  }. In the following lemma, we bound the sequential bracketing entropy of the classes of sequences of functions
Fk,t() := (f1(g, Q¯s-1))st-=11 : g  G() , for k = 1, 3, 5. Lemma 2 (Sequential bracketing entropy bound). Suppose that assumption 5 holds. Then, for i = 3, 5,
N[ ]( , F1,t(), L2(PQ0,gref ))  N[ ](G-22 , G, L2(PQ0,gref )). Suppose in addition that assumption 6 also holds. For k = 3, 5, we then have that
N[ ]( , Fk,t(), L2(PQ0,g ))  N[ ]( , G, L2(PQ0,gref )).
Proof of lemma 2. Observe that
0  g(a | ·)Q¯(a, ·)dµA(a)  1, and 0 (g/gref )2(y - Q¯)2  G2.

16

Let {(lj, uj) : j  [N ]} be an -bracketing of G()/gref in L2(PQ0,gref ). Without loss of generality, we can assume that uj  lj  gref for every j. Let g  G(). There exists j such that lj  g  uj, and therefore,
f1(uj, Q¯) f1(g, Q¯)  f1(lj, Q¯) and fk(lj, Q¯) fk(g, Q¯)  fk(uj, Q¯), for k = 3, 5.

We have that

f1(lj , Q¯) - f1(uj , Q¯) 2,Q0,gref

=

(g/gref

)

(uj /gref ) (uj /gref

- (lj /gref )(lj /gref )

)

(y

-

Q¯)2

2,Q0 ,g ref

-2G2

and for k = 3, 5, denoting i3 := 2 and i5 := 1, we have that fk(uj , Q¯) - fk(lj , Q¯) 2,Q0,gref

= ((uj /gref ) - (lj /gref )) .

g(a | ·)Q¯(a, ·)dµA(a)
2,Q0 ,g ref

Therefore,

and, for k = 3, 5,

((f1(lj , Q¯s-1) - f1(uj , Q¯s-1))ts-=11)  -2G2 .

((fk(lj , Q¯s-1) - fk(uj , Q¯s-1))ts-=11)  .
We have thus shown that an -bracketing in L2(PQ0,X,gref ) norm of G/gref induces an (G2-1, L2(PQ0,gref )) sequential bracketing of F1,t(), and ( , L2(PQ0,gref )) sequential bracketings of F3,t() and F5,t(), which yields the claims.
Lemma 3 (Uniform convergence of the martingale empirical process). Suppose that assumptions 5 and 6 hold. Then, for any (i, j)  {(1, 1), (1, 3), (2, 2)}

sup |(i,jt)(g) - ¯ (0j,i),t(g)| = o(1) a.s.
gG

Proof. Let  := mins[t-1] inf(a,x)A×X gs(a | x). In this proof, we treat G as a constant, and we absorb it in the symbols , O, o, and O whenever we use them.
We treat the case (i, j) = (1, 1) and the case (i, j)  {(1, 3), (2, 2)} separately.

Case (i, j) = (1, 1). For any g  G, we have that s  [t - 1], f1(g, Q¯s-1)   G2-1. Therefore, from theorem 3, for any r-  (0, -1/2], it holds with probability at least 1 - 2e-x that

sup (11,t)(g) - ¯ (01,1),t(g)
gG

r-

+

1 

G2  -1

t r-

log(1 + N[ ]( , F1,t(), L2(PQ0,gref )))d

+ +

G2-1 t log N[
G2-3/2t-1/2

](G2-1,

 x

+

G2

F1,t(), L2 -2t-1x.

(PQ0

,gref

))

Let xt := (log t)2 and let Bt the right-hand side above where we set x to xt. From Borel-Cantelli, we have that supgG |(11,t)(g) - ¯ (01,1),t(g)| = o(Bt) almost surely. Let us make Bt explicit.

17

From lemma 2 and from assumption 5, we have that

G2-1 t

log(1

+

N[ ](G2-1, F1,t(), L2(PQ0,gref )))

G2-1 
t

log(1

+

N[ ](, G/gref , L2(PQ0,gref )))

G2-(2+p)t-1.

Let us now focus on the entropy integral. We have that

G2  -1

log(1 + N[ ]( , F1,t(), L2(PQ0,gref )))d
r-

G2  -1


r-

log(1 + N[ ](G-22 , G/gref , L2(PQ0,gref )))d


=G2-2
G-2  2 r-

log(1 + N[ ](u, G/gref , L2(PQ0,gref )))du


=G2-2

u-p/2du

G-2  2 r-

= G2-2 (1-p/2 - (G-22r-)1-p/2, 1 - p/2

for any p = 2.

We choose r- so as to minimize the rate of r- +

(t)-1/2

G2  -1 r-

log(1 + N[ ]( , F1,t(), L2(PQ0,gref )))d . We distinguish the cases p < 2

and p > 2.

Case p < 2. We just set r- = 0, and we obtain

G2  -1
r- + (t)-1/2
r-

log(1 + N[ ]( , F1,t(), L2(PQ0,gref )))d



-

1 2

(3+p)

t-

1 2

.

Collecting the other terms yields that Bt = O(-(3+p)/2t-1/2 + t-1-(2+p)). From assumption 6,  t-, with  < min(1/(3 + p), 1/(1 + 2p)), and we therefore have Bt = o(1).

Case p >

(t)-1/2

G2  -1 r-

2. We pick r- so as to balance both terms of r- +

log(1 + N[ ]( , F1,t(), L2(PQ0,gref )))d . , that is we pick r- such that

r-

=

t-1/2

Gp

-

1 2

(1+2p)



r-

=

G2

-

1 p

(1+2p)

t-

1 p

.

Collecting

the

other

terms

then

yields

Bt

=

O(-

1 p

(1+2p)

t-

1 p

+

-(2+p)t-1).

From

assumption

6,

 t-, with  < min(1/(3 + p), 1/(1 + 2p)), and we therefore have Bt = o(1).

Case (i, j)  {(1, 3), (2, 2)}. For any g  G, s  [t-1], k = 3, 5, we have that fk(g, Q¯s-1)   G. Therefore, from theorem 3, for any (i, j, k)  {(1, 3, 3), (2, 2, 5)}, for any x > 0, it holds with probability at least 1 - 2e-x that

sup (i,jt) - ¯ (0j,i),t
gG

r-

+

1 

G

t r-

log(1 + N[ ]( , Fk,t(), L2(PQ0,gref )))d

G + t log(1 + N[ ](G, Fk,t(), L2(PQ0,gref )))

xx +G +G
t t

r- +

1

1 

(G1-p/2 - (r-)1-p/2) + G1-p + G

xx +G ,

1 - p/2 t

t

t t

18

where we have used that, from lemma 2 and assumption 5, log(1 + N[ ]( , Fk,t(), L2(PQ0,gref ))) 

log(1 + N[ ]( , G/gref , L2(PQ0,gref ))

-p. Setting x to xt := (log t)2 in the bound

above and denote Bt the resulting quantity. Applying Borel-Cantelli's lemma yields that

supgG (i,jt) - ¯ (0j,i),t = o(Bt) almost surely. We now give an explicit bound on Bt.

Case p  (0, 2). We set r- = 0. We obtain Bt = O((t)-1/2 + (t)-1). Since from assumption 6,  t- with  < 1, we have that Bt = o(1).

Case p > 2. We set r- = (t)-1/p. We have Bt = O((t)-1/p + (t)-1). Since from assumption 6,  t- with  < 1, we have that Bt = o(1).

B.3 High probability bound for the martingale terms

Lemma 4. Suppose that there exists  > 0 such that g/gs   -1 for every s  [t - 1]. Then For (i, j)  {(1, 2), (2, 1)}, for any x > 0, it holds with probability 1 - 2e-x that

(i,jt) - ¯ (0j,i),t

xx +,
t t

(4)

and for (i, j)  {(1, 3), (2, 2)}, it holds with probability at least 1 - 2e-x that

(i,jt) - ¯ (0j,i),t

xx +.
tt

(5)

Proof of lemma 4. We have that

(12,t)

-

¯ (02,1),t

= t

1 -

1

t-1
(O(s)
s=1

-

PQ0

,gs

)

g gs

f2

(Q¯ s-1

)

and

(21,t)

-

¯ (01,2),t

= t

1 -

1

t-1
(O(s)
s=1

-

PQ0

,gs

)

g gs

f4

(Q¯ s-1

).

Therefore, both differences are the average of martingale difference sequences. For k = 2, 4, we

have that

g gs

fk

(Q¯ s-1





-1

and

g gs

fk

(Q¯ s-1

2,Q0 ,g 



-1/2.

Bernstein's inequality for

martingale difference sequences then yields (4).

Concerning the other two differences, we have that

(13,t)

- ¯ (03,1),t

=

1 t-1

t-1
Q0,X f3(Q¯s-1)2

s=1

and (22,t) - ¯ (02,2),t

=

1 t-1

t-1
Q0,X f3(Q¯s-1).

s=1

These two terms are the average of martingale sequences too, and since f3(Q¯s-1)   1, Bernstein's inequality for martingale difference sequences yields (5).

B.4 Approximation error lemma Lemma 5. For any Q¯, Q¯1 : A × X  R, it holds that
max (0j,i)(Q¯) - (0j,i)(Q¯1) : (i, j)  {(1, 2), (1, 3), (2, 1), (2, 2)}  4 Q¯ - Q¯1 2,Q0,g and for any conditional densities (a, x)  g(a | x), and (a, x)  g1(a | x) such that g1, g   for some  > 0, it holds that
(01,1)(Q¯) - (01,1)(Q¯1)  -2 g - g1 1,Q0,X ,g + -1 Q¯ - Q¯1 . 1,Q0,X ,g
Proof. We treat each case separately.

19

Case (i, j) = (1, 2).
(02,1)(Q¯) - (02,1)(Q¯1) =2 PQ0,g (y - Q¯) g, Q¯ - (y - Q¯1) g, Q¯1 =2 PQ0,g (Q¯1 - Q¯) g, Q¯ + (y - Q¯1) g, Q¯ - Q¯) 4 Q¯ - Q¯1 1,Q0,X ,g

Case (i, j) = (1, 3).

(03,1)(Q¯) - (03,1)(Q¯1)
= Q0,X g, Q¯ 2 - g, Q¯1 2 2Q0,X g, Q¯ - Q¯1 = Q¯ - Q¯1 1,Q0,X ,g

Case (i, j) = (2, 1).

(01,2)(Q¯) - (01,2)(Q¯1)
= PQ0,g (y - Q¯) - (y - Q¯1)  Q¯ - Q¯1 )1,Q0,X ,g

Case (i, j) = (2, 2).

(02,2)(Q¯) - (02,2)(Q¯1) = Q0,X g, Q¯ - g, Q¯1  Q¯ - Q¯1 1,Q0,X ,g

Case (i, j) = (1, 1).

(01,1)(g, Q¯) - (01,1)(g1, Q¯1)

= PQ0,g

g (y
g

-

Q¯)

-

g g

(y

-

Q¯1)

 PQ0,g

1 gg1

(g

-

g1)

+

1 (Q¯ g

-

Q¯1)

1  2

1 g - g1 1,Q0,X ,g + 

Q¯ - Q¯1 . 1,Q0,X ,g

B.5 Proof of theorem 2

Proof of theorem 2. As noted at the beginning of this section, the estimation error t2 - 02,t decomposes as

t2 - 02,t :=

(i,jt) - ¯ (0j,i),t

(6)

(i,j)S

+

¯ (0j,i),t - (0j,i)(gt, Q¯1)

(7)

(i,j)S

+

(0j,i)(gt, Q¯1) - (0j,i)(gt, Q¯t-1).

(8)

(i,j)S

20

The terms in line (6) are MDS averages or martingale empirical processes evaluated at gt. Setting xt := (log t)2 in lemma 4 and using Borel-Cantelli gives that the MDS averages are o(1) almost surely. Lemma 3 gives that the martigale empirical process terms evaluated at gt are o(1) almost surely as well.
From lemma 5, and assumptions 4 and 6,
(0j,i)(gt, Q¯1) - (0j,i)(gt, Q¯s-1)
(i,j)S
=O(s-) a.s. =o(1) a.s..
Therefore the third line above (8) is o(1) almost surely, and by Ceasro summation, the second line above (7) is o(1) almost surely as well.

C Maximal inequality for importance sampling weighted martingale empirical processes

In this section, we restate a maximal inequality for so-called importance sampling martingale empirical processes from Bibaut et al. [2021]. We include it for our reader's convenience.

Sequential a sequence

bracketing of functions

entropy. Let  O  R such that

be a set, for any t

and let  [T ],

T t()

1. is

For O¯(t

any   , let (t - 1)-measurable.

())Tt=1 be We denote

T := (t())Tt=1 :    .

Let gref be a fixed reference policy. For any O¯(t - 1)-measurable for any t, we introduce

sequence the norm

(ft)Tt=1

of

O



R

functions

such

that

ft

is

((ft)Tt=1) :=

1T T
t=1

f2
t 2,Q0,gref

1/2
.

Following the definition of van Handel [2011], we say that a collection of sequences of pairs of functions O  R of the form
((jt , tj ))Tt=1 : j  [N ]
forms an ( , L(PQ,gref )) sequential bracketing of T if
· for any t  [T ] and any j  [N ], jt and tj are O¯(t - 1)-measurable O  R functions, · for any   [], there exists j  [N ] such that, for any t  [T ], jt  t()  tj. · for any j  [N ], ((tj - jt )Tt=1)  .
We denote N[ ]( , T , L2(PQ,gref )) the cardinality of any ( , L2(PQ,gref ) sequential bracketing of T of minimal cardinality.

Importance sampling weighted martingale empirical process. We term importance sampling weighting martingale empirical processes stochastic processes of the form

1T

gref

T

(O(t) - PQ0,gt )
t=1

gt

t() :   

.

The result below is theorem 1 from iswerm. Theorem 3 (Maximal inequality for IS weighted martingale processes). Suppose that
· there exists  > 0 such that g/gt    for every t  [T ],

21

· there exists B > 0 such that sup t()   B for every t  [T ],
· there exists p > 0 such that log N[ ]( , T , L2(PQ,gref )) -p.

Then, for any r > 0, r-  [0, r/2] and x > 0,it holds with probability at least 1 - 2e-x that

sup
gG

1 T

T

gref

(O(t) - PQ0,gt )
t=1

gt

t() : 

 , ((t())Tt=1) 

r- +

r T r-

B log(1 + N[ ]( , T , PQ0,gref )d + T log(1 + N[ ](r, T , PQ0,gref ))

x Bx

+r

+

TT

D High probability bound for IS weighted nonparametric least squares from adaptively collected data

Suppose Y

  [- M , M ] for some M

>

0 and let Q¯ be a convex class of functions A × X

 Y.

For any Q¯ : A × X  R, and any o = (x, a, y)  O, let (Q¯, o) := (y - Q¯(a, x))2. Let

gref be a fixed (as opposed to random) density w.r.t. some dominating measure µ on A. For any

Q¯, define the corresponding population risk w.r.t. PQ0,gref as R0(Q¯) := PQ0,gref (Q¯, ·). Observe that the population risk can be rewritten in terms of the conditional distributions (PQ0,gs )ts=1 of observations (O(s))ts=1 given their respective past, via IS weighting:

R0(Q¯)

:=

1 t

t

gref

PQ0 ,gs
s=1

gs

(Q¯, ·).

We define the corresponding IS weighted empirical risk as

Rt(Q¯)

:=

1 t

t

gref

O(s)
s=1

gs

(Q¯, ·).

Let Q¯t  arg minQ¯Q¯ Rt(Q¯) be an empirical risk minimizer over Q¯. In the upcoming theorem, we provide a high probability bound on the excess risk R0(Q¯t) - R0(Q¯1). Our result requires the following assumptions.
Assumption 7 (Entropy of the loss class). There exists p > 0 such that log N[ ](M , (Q¯), L2(PQ0,gref )) -p, where (Q¯) := { (Q¯) : Q¯  Q¯}. Assumption 8 (Bounded IS ratios). There exists t > 0 such that g/gs   t for every s = 1, . . . , t.

Theorem 4 in Bibaut et al. [2021] gives a high probability excess risk bound on the least squares estimator. We restate it here under the current notation for our reader's convenience.

Theorem 4. Consider the setting of the current section, and suppose that 7 and 8 hold. Then, for any x > 0, it holds with proability 1 - 2e-x that

R(Q¯t)

-

inf
Q¯ Q¯

R(Q¯)

M

+ t

1 1+p/2

t

t x t

t t

1 p

+

t t

+

t x t

+

t x t

if p < 2, if p > 2.

E Additional Empirical Results

E.1 Sequential Sample Splitting vs. Cross-Time-Fitting
The approach we proposed in the main text estimates Q¯t-1 using only the data O(1), . . . , O(t - 1). This means that potentially few data are available for earlier estimates. In this section, we empirically explore an alternative strategy for fitting Q¯t-1 inspired by the cross-time-fitting procedure

22

proposed in Kallus and Uehara [2019a] and which would be theoretically justified under some sufficient mixing (which is not necessary for our sequential approach). Specifically, we split our data into F = 4 folds and train F outcome regression models, Q¯f , f = 1, 2, 3, 4, each to be used to make predictions on data in the corresponding fold. The model Q¯f is trained using observations in all folds except for folds f and min(f + 1, F ). As long as the data is sufficiently mixing, dropping fold f + 1 ensures sufficient independence from future data. At the same time, each model now uses an amount of data that grows linearly in T . Further, unlike sequential sample splitting, which requires training of T - 1 models, cross-time-fitting requires training only F models. Figures 2 and 3 establish parity in the conclusions w.r.t. CADR's coverage compared to all other baseline estimators on 57 OpenML-CC18 datasets, 4 target policies and linear outcome regression models for all estimators that use them when these models are trained with sequential sample splitting (as in Figure 1 of the Section 4.2 in the main text) and with time cross-fitting respectively.
Figure 2: Comparison of CADR estimator against DM, IPW, DR, ADR, MRDR w.r.t 95% confidence interval coverage on 57 OpenML-CC18 datasets and 4 target policies with sequential sample splitting for training the linear outcome regression model of all estimators that use them.
E.2 CADR in Misspecified vs. Well-Specified Outcome Regression Models Although CADR's advantage over DR is more pronounced when the off-policy estimator's outcome regression model is misspecified (e.g., using linear model on real data), this section establishes the advantage of CADR over all other estimators when they all use a well-specified outcome regression model (e.g., tree). Figure 4 shows CADR's coverage performance when the outcome regression model of DM, DR, MRDR and CADR is misspecified (linear regression model trained with the default sklearn parameters) and Fig. 5 shows CADR's coverage performance when the outcome regression model of DM, DR, MRDR and CADR is well-specified (decision tree regression model trained with the default sklearn parameters). Each dot represents each one of the 72 datasets and is colored blue when CADR has significantly better coverage than the corresponding baseline column estimator, in red when it has significantly worse coverage and in black when the two coverage are within standard error. Results are averaged over 64 simulations per dataset and standard errors are shown. CADR remains the best estimator in both cases but as expected, in the misspecified outcome regression model case there are more datasets where CADR has significantly better coverage than DR compared to the well-specified outcome regression model case where there are more datasets for which CADR's and DR's coverage are within standard error. This is because when the error is large and is multiplied by a potentially large inverse propensity score of the logging policy, the variance stabilization performed by CADR is the most effective.
23

Figure 3: Comparison of CADR estimator against DM, IPW, DR, ADR, MRDR w.r.t. 95% confidence interval coverage on 57 OpenML-CC18 datasets and 4 target policies with cross-fitting for training the linear outcome regression model of all estimators that use them.

Figure 4: Comparison of CADR estimator against DM, IPW, DR, ADR, MRDR w.r.t. 95% confidence interval coverage on all 72 OpenML-CC18 datasets and 4 target policies with linear outcome regression model (misspecified) trained with cross-fitting of all estimators that use them.

E.3 Importance Sampling Weighted Training of CADR Outcome Regression Model

Finally, we consider the effect of using weighted training in the outcome model fitting of CADR

akin to MRDR's outcome model fitting, where each training sample O(s) = (X(s), A(s), Y (s)) is

weighted

by

w(s)

=

g (A(s)|X (s)) gs (A(s)|X (s))

.

We

call

this

estimator

CAMRDR.

Figure

6

shows

CAMRDR's

coverage performance against baselines and CADR when the outcome regression model of DM,

24

Figure 5: Comparison of CADR estimator against DM, IPW, DR, ADR, MRDR w.r.t. 95% confidence interval coverage on all 72 OpenML-CC18 datasets and 4 target policies with tree outcome regression model (well-specified) trained with cross-fitting of all estimators that use them.
DR, MRDR, CADR and CAMRDR is misspecified (linear regression model trained with the default sklearn parameters). Figure 7 shows CAMRDR's coverage performance against baselines and CADR when the outcome regression model of DM, DR, MRDR, CADR and CAMRDR is wellspecified (decision tree regression model trained with the default sklearn parameters). Again, each dot represents each one of the 72 datasets and is colored blue when CAMRDR has significantly better coverage than the corresponding column estimator, in red when it has significantly worse coverage and in black when the two coverage are within standard error. Results are averaged over 64 simulations per dataset and standard errors are shown. Importance sampling weighted training makes a small positive difference compared to CADR in the well-specified case and a small negative difference compared to CADR in the mis-specified case. CAMRDR is better than all other baselines in both cases.
E.4 Execution Specifics of Experiment Code
The IPython notebook to reproduce the experimental results of the main paper and the appendix is included as an attachment in the supplemental materials. One needs to obtain an OpenML API key to run this code (instructions can be found at https://docs.openml.org/Python-guide/) and replace the string 'YOURKEY' in summarize_openmlcc18() and in download_openmlcc18() functions with it. After that, if the notebook is executed as is, it reproduces Figure 3 (1h 26min on a 64 CPU Intel Xeon). Changing variable ope_outcome_model_training from cross_fitting to sequential_sample_splitting reproduces Figures 1/2 (same) (22h 23min on a 64 CPU Intel Xeon). Changing variable task_min_samples from 1000 to 0 and variable task_max_contexts to np.inf reproduces Figure 4 (20h 20min on a 64 CPU Intel Xeon). Changing variable ope_outcome_model from LinearRegression() to DecisionTreeRegressor(), variable task_min_samples from 1000 to 0 and variable task_max_contexts to np.inf reproduces Figure 5 (26h 8min on a 64 CPU Intel Xeon). Figures 6 and 7 are from the same execution as Figures 4 and 5 but with adding 'CAMRDR' in the competitors variable of the visualize_coverage() function.
25

Figure 6: Comparison of CAMRDR estimator against DM, IPW, DR, ADR, MRDR and CADR (last column) w.r.t. 95% confidence interval coverage on all 72 OpenML-CC18 datasets and 4 target policies with linear outcome regression model (misspecified) trained with cross-fitting.
Figure 7: Comparison of CAMRDR estimator against DM, IPW, DR, ADR, MRDR and CADR (last column) w.r.t. 95% confidence interval coverage on all 72 OpenML-CC18 datasets and 4 target policies with tree outcome regression model (well-specified) trained with cross-fitting.
26

