arXiv:2106.00956v1 [math.LO] 2 Jun 2021

Smooth relaxation preserving Turing machines
Adrian K. Xu
February 2021
1 Introduction
In [CM19], Clift and Murfet, via ideas from differential linear logic, arrive at a particular means of propagating uncertainty through a Turing machine (TM), interpreted in terms of the state of belief of a naive Bayesian observer. The naive Bayesian observer is distinguished from a standard Bayesian observer by a number of independence assumptions­namely, the independence (in distribution) of the tape entries, the state, and the direction of the tape head movement. We henceforth use the phrase "naive Bayesian" whenever these assumptions are in effect. We refer to the smooth dynamical system obtained by propagating uncertainty through a TM via naive Bayesian probability as a smooth relaxation of a TM.
This leads naturally to a smooth relaxation of the space of programs by considering universal Turing machines (UTM) whose description codes may contain uncertainty, and which are designed in such a way that, when uncertainty is present, their simulation behaviour remains well-defined. Along these lines, in [JCW21], the authors introduce the staged pseudo-UTM, by which they endow the corresponding subset of Turing machines with a smooth manifold structure. A primary objective of such work is to extend the differential methods which have seen considerable success in modern machine learning to program spaces which more closely approximate the space of computable functions.
A number of technical issues regarding the smooth relaxation of TMs remain unclear, a few of which we now survey.
It is well known that the Turing model of computation is robust in the following sense. Given any two variants of the Turing machine model­we may for instance vary the number of tapes or the alphabet size­any partial function computable using one variant is also computable using the
1

other, suitable encodings allowed; typical proofs of results of this kind involve constructing step-wise simulations of arbitrary machines.
Suppose we generalise the associated partial functions to the smooth case. A formal attempt to do so must resolve the following technicality: what does is mean for the smooth relaxation to halt? Granted a satisfactory answer, we obtain a stronger criterion for the equivalence of two variants of the Turing model­namely, given a machine of one variant, there must exist a machine of the other variant which not only computes the same partial function, but propagates uncertainty in an equivalent manner. A further technicality arises in attempting to formalise this equivalence. It is not always obvious how uncertainty in the tape configuration of one machine can be translated into uncertainty in that of the other machine.
In this paper, we sidestep the halting technicality by restricting our discussion to step-wise simulations, and by comparing the propagation of uncertainty through two machines in a step-wise manner. For the second technicality, regarding the translation of uncertainty, we for the moment take a pedestrian route and assume uncertainty can be meaningfully translated between two machines using standard probability. These and related issues are elaborated in Sections 2.1 and 2.2. In section 3.1, we show that a multi-tape machine can be simulated on a single tape machine whilst preserving the smooth relaxation, and hence that these two variants are equivalent in the stronger sense just introduced. The equivalence between other variants, in particular, between machines of differing alphabet size, remains to be seen. We shall return to this in the closing remarks.
In section 3.2, we introduce a smooth relaxation preserving pseudo-UTM intended as an alternative to the design presented in [JCW21]. Conceptually, one would expect a relationship between the way a UTM propagates uncertainty from its simulated input to output, and the particular smooth relaxation of the space of programs it gives rise to. In this vein, we observe at the end of this section that the smooth relaxation preserving pseudo-UTM gives rise to a particularly natural smooth relaxation of the space of programs.
The deeper significance of these technical issues remains unclear.
The constructions in this paper can be understood given only a familiarity with Turing machines. However, the formulation and analysis of the smooth relaxation have been expressed in the language of tensor products and direct sums. Hence, at least a passing familiarity with these in the linear algebra context is necessary to appreciate the significance of the constructions.
2

2 Preliminaries

For the present paper, we adopt the following definition of a Turing machine. For our purposes, a given machine need not possess any explicit input-output behaviour, so we omit the initial and halting state from the definition.

Definition 2.1 (Turing machine). A (single tape) Turing machine (TM) is a triple, (Q, , ), where Q is a finite set of states,  is a finite tape alphabet containing a dedicated empty symbol , and  is a transition function Q ×  - Q ×  × {-1, 0, 1} sending a source state and read symbol to a target state, write symbol and move direction.
Denote by Z the set of functions Z   mapping all but finitely many i  Z to the blank symbol. Given a transition function (or any function with a Cartesian product codomain) , write i for proji  . Any given Turing machine (Q, , ) specifies a computation via an associated step function,
step : Q × Z - Q × Z

sending where

(q, (yi)iZ) - (q, (yi+d)iZ)

q := 1(q, y0),
 yi := yi
2(q, y0)

i=0 , and
i=0

d := 3(q, y0).

By the runtime of a TM on a given input y  Z we mean the sequence generated by iteration of the step function on (q0, y). We refer to an element of this sequence as a runtime configuration or just configuration and to the second component of such an element as a tape configuration.

2.1 Smooth relaxation
In the naive Bayesian probabilistic extension, we relax the state to a distribution over the states, and the symbols on the tape to distributions over symbols. We then specify a smooth step function which propagates uncertainty according to the independence assumptions discussed in [CM19]. The language of tensor products provides a natural setting for the constructions to follow, so we formulate
3

our definition in this language and verify (Claim 2.1) that we recover the probability update rules of [JCW21, Definition F.1].
Given finite set X, denote by RX the free vector space on X. Denote by X the standard ||simplex (or rather, its embedding into RX) given by the set xX xx  RX | xX x = 1 and x  0 . Similarly, when X is countably infinite, we denote by X the set of probability distributions over X with finite support. Given some function f : X  Y , we write f for the unique linear operator RX  RY sending each element of the standard basis x  X to f (x), and we refer to this as the linear operator induced by f , except where explicitly defined otherwise.
Note that by fi we mean (proji  f ). We denote by ·, · the standard inner product.
Definition 2.2 (Smooth relaxation of Turing machine). By the smooth relaxation of a Turing machine (Q, , ) we mean the Turing machine together with a smooth step function

step : Q × ()Z - Q × ()Z

sending where

1

(q, (yi)iZ) - (q, (

d, d

y

 i+d

)iZ

)

d=-1

q := 1(q  y0),

yi := yi 2(q  y0)

i=0 , and
i=0

d := 3(q  y0).

Here, we view a vector q  Q as a probability distribution over Q with the probability of a given state q encoded by the scalar projection q, q , and similarly for the tape symbols and move direction. As defined above, the operators induced by the transition components, i, have as their domain R(Q × ), into which the set of probability distributions (Q × ) over Q ×  embed. Recall there exists an isomorphism  : RQ  R - R(Q × ) determined by its operation on the standard bases, q    (q, ). In particular, if q  Q and   , then (q  )  (Q × ), and we view q   as the joint distribution arising when q and  are treated as the distributions of independent random variables. This framework is employed more generally, and for the most part implicitly, throughout the rest of this paper.

4

Compare the above definition to Definition 2.1. In the conventional situation, the step function would induce an operator R(Q × Z )  R(Q × Z ), and this would correspond to the propagation of uncertainty according to standard probability. However, here we have pushed in R (and restricted to the simplices) to obtain the domain and codomain appearing in the definition. This corresponds to the naive Bayesian assumption that the state and each symbol on the tape are conditionally independent at every step.
Once uncertainty has been propagated via i from the source state and read symbol to the target state, write symbol and move direction, the resultant tape configuration is computed by a superposition of the tape configuration following the write operation, with one copy for each alternative move direction. The superposition is weighted according to the distribution over the move directions. Note that, according to standard probability, the write symbol distribution y0+d appearing in each term of the superposition should be conditioned on the move direction. The absence of this conditioning corresponds to the further assumption that the move direction and write symbol are conditionally independent. The reader seeking a fuller explanation of this may wish to consult Section 6.2 of [CM19] and compare the equations with the lemma that follows.
Below, we write ½() for the indicator function, equal to one when the enclosed statement is true
and zero otherwise. The following lemmas follow via direct computation, using linearity of the relevant operators.

Lemma 2.1. In Definition 2.3, we have the following. Note that the summations are indexed over Q × .

1. q0, q = q, ½(1(q, ) = q0) q, q , y0 for q0  Q.

2. d0, d = q, ½(3(q, ) = d0) q, q , y0 for d0  {-1, 0, 1}.

3. 0, y0 = q, ½(2(q, ) = 0) q, q , y0 for 0  .

4. 0,

1 d=-1

d, d

yi+d

=

1 d=-1

d, d

[½(i

=

d)

0, y0

+ ½(i = d) 0, yi-d ] for 0  .

In order to implement the smooth relaxation in our simulator, it is necessary to re-express the tape configuration update rule in the following form.

Lemma 2.2. From Definition 2.3, we have
1
d, d yi+d = (d  yi-1  yi  yi+1)
d=-1

5

where  is the function {-1, 0, 1} × 3   sending


-1   
(d, -1, 0, 1)  0
   1

d = -1 d=0 . d=1

It follows that

1
d, d yi+d = (q  y0  yi-1  yi  yi+1)
d=-1

where  =   (3 × id).

One can extend all the above to multi-tape machines as follows. We assume all the tapes share the

same alphabet. Recall that an n-tape Turing machine is given as before by a quadruple, (Q, , , q0), however the transition function is now a map Q × n  Q × n × {-1, 0, 1}n, and the step function a map Q × (Z )n  Q × (Z )n defined in the obvious manner. In the smooth relaxation, the update

rules generalise to:

1

1

(q, (y1i )iZ, ..., (yni )iZ) - (q, (

d, d(1) yi(+1d) )iZ, ..., (

d, d(n) yi(+nd))iZ)

d=-1

d=-1

where

q := 1(q  y(01)  · · ·  y(0n)),



yi(j)

:=

yi(j) 1+j (q



y(01)



···



y(0n))

i=0 , for j = 1, ..., n, and
i=0

d(j) := 1+n+j (q  y(01)  · · ·  y(0n)), for j = 1, ..., n.

From Lemma 2.2 we now have for j = 1, ..., n

1

d, d(j)

yi(+jd)

=

(j)(q



y(01)



·

·

·



y0(n)



yi(-j)1



yi(j)



y

(j) i+1

)

d=-1

where (j) =   (1+n+j × id).

2.2 Smooth relaxation preserving simulations
Our objective in this section is to make more precise the sense in which our constructions will be smooth relaxation preserving.
6

Rogozhin [Rog96] defines the notion of simulation as an equivalence between the partial functions computed by each TM. In this paper, we adopt a more narrow viewpoint of a simulation as an equivalence between the dynamical systems specified by the step function of each TM. We will not attempt to formalise this viewpoint; rather, we define a necessary, though not sufficient, criteria for such a simulation, which will provide a minimal context in which the phrase "smooth relaxation preserving" has meaning.
Definition 2.3. Let M , M  be Turing machines, with states Q, Q, alphabets , , and step functions step, step, respectively. Suppose we have a set of encodings Enc  Q × Z , and a surjective decoder  : Enc  Q × Z such that the following hold.
1. When run on any input x  Q × Z , M passes through Enc infinitely many times.
2. For x  Enc, define step(x) = stept(x), where t is the smallest positive integer such that stept(x)  Enc. Then the following diagram commutes.

Q × Z
step

Enc
 step

Q × Z

Enc


Then we say that the triple (M, Enc, ) generates M . We shall use the phrase generating TM to refer variously to both M and its associated triple. By a cycle we mean loosely the sequence of configurations through which M passes when moving from one encoding to the next.

Remark. Let M, M ,  be TMs. If (M, Enc, ) generates M  and (M , Enc, ) generates N , then (M, -1(Enc),   ) generates N .
We expect our notion of a simulation to be a strict subset of the above notion. Indeed, for any TM M , there exists a trivial generating TM which stores the initial configuration of M on its tape, alongside a counter, to be incremented on every step; the decoder will then run M on the stored initial configuration for the number of steps recorded by the counter. Such a construction cannot reasonably be characterised as a simulation. Nevertheless, the notion of a generating TM will suffice for our purposes.

7

In the context of Definition 2.3, the smooth configurations Q×()Z sit inside the vector space

RQ×(R)Z . There exists an embedding,  : RQ×(R)Z  R(Q×Z ) defined as usual by taking the

tensor product of the components, such that smooth configurations are sent to their corresponding

distribution over classical configurations. That is, the embedding restricts to Q × ()Z 

(Q × Z ).

We wish to relax the set of classical encodings, Enc, to a set of smooth encodings, say, SmoothEnc.

At the very least, we would expect such smooth encodings to be distributions over classical encodings.

That is, we expect an inclusion (SmoothEnc)  Enc. Moreover, we would expect any distribution

over classical encodings to be a valid smooth encoding. That is, for any x such that (x)  Enc,

we expect that x  SmoothEnc. Thus, we are obliged to set SmoothEnc := -1(Enc).

A smooth decoder must then be a surjective mapping SmoothEnc  Q × ()Z , with

appropriate smoothness properties. For now, we shall assume that the uncertainty between the

tapes is propagated via standard probability. In this case, the decoder is fully determined by the

following composition.

SmoothEnc - Enc -- (Q × Z )

Refer to section 2.1 for notational conventions. For this to be a valid decoder, one must verify that its image is indeed Q × ()Z .
In generalising generating TMs to the smooth setting, we exclude the possibility that the generating TM enters into a configuration which superposes both valid and invalid encodings, hence ensure that the cycles remain cleanly demarcated.

Definition 2.4. Let (M, Enc, ) be the generating TM in the context of Definition 2.3. Henceforth, we write Enc to mean -1(Enc) and  to mean   . We say (M, Enc, ) is well-behaved with respect to the smooth relaxation if the following hold.

1. Given x  Enc, there exists a T  Z>0 such that (step)T (x)  Enc, and (step)t(x)  -1((Q × Z \ Enc)) for 0 < t < T .

2. Im  = Q × ()Z (relevant embeddings implied).

We then call (M, Enc, ) a smooth generating TM.

We are now ready to formalise the meaning of "smooth relaxation preserving".

Definition 2.5. Let (M, Enc, ) be a smooth generating TM as in Definition 2.4. For x  Enc, define step(x) = (step)t(x), where t is the smallest positive integer such that (step)t(x) 

8

Enc. We say that this smooth generating TM is smooth relaxation preserving if the following diagram commutes.

Q × ()Z
step



Enc

step

Q × ()Z



Enc

We will henceforth be content to speak semi-formally of smooth relaxation preserving simulations,

with the understanding that every smooth relaxation preserving simulation is at least a smooth re-

laxation preserving generating TM. We shall say that a simulation is well-behaved with respect to

the smooth relaxation if it is well-behaved as a generating TM. The generalisation to n-tape TMs

is fairly immediate, and we shall not bother here.

Indeed, the staged pseudo UTM given in Appendix F of [JCW21] is well-behaved with respect to the smooth relaxation, but is not smooth relaxation preserving. For instance, suppose it is initialised with the code for a single state machine which simply writes back to the tape whatever it reads. Suppose the simulation tape alphabet consists of two symbols, A and B, and suppose the simulation is initialised with the distribution 0.5A + 0.5B under the tape head. The code contains two tuples, namely, (A, q, A, q, S) and (B, q, B, q, S), where q is the one and only state of the machine. At the beginning of the simulation cycle, the write symbol square on the staging tape reads X. The UTM then scans one of the tuples first, say, the one corresponding to A. After scanning this tuple, the staged write symbol contains the distribution 0.5X + 0.5A. After scanning the second tuple, this distribution becomes 0.5(0.5X + 0.5A) + 0.5B, and the final distribution written to the working tape will be 0.375A + 0.625B. (The UTM interprets X by writing back the read symbol, hence X stands in effect for the read distribution 0.5A + 0.5B.) This asymmetry does not arise when the uncertainty is propagated directly through the simulated machine. Rather, as one would expect, the distribution remains unchanged.
Moreover, many routine constructions by which multi-tape machines and UTMs are simulated on single-tape machines break down with respect to the smooth relaxation. In general, any construction which relies on certain auxiliary symbols to demarcate segments of the tape and situate the tape head relative to those segments, and which, when passing to the smooth relaxation, experiences ambiguity in the direction of the tape head movement, will not be well-behaved with respect to the

9

smooth relaxation. Inspecting Definition 2.2, we see that any such ambiguity will cause every square along the simulator's tape to be "smudged" by the two adjacent squares. That is, uncertainty in a read symbol will, if propagated to uncertainty in the move direction, "contaminate" the entire tape. This will typically compromise the simulator's ability to cleanly situate its tape head, and irreversibly distort its working squares.
These considerations are the principle design constraints on the constructions to follow.

2.3 State partitions and contexts

In the forthcoming constructions, we view the states as being partitioned into sections, each in bijection with a set of local indices which we shall call the context. The typical context will be a Cartesian product involving the state set and alphabet of the simulated machine. Hence we interpret the simulator's transition function as encoding various components of the simulated machine's transition function.
We shall make this heuristic explicit in our notation, in order to render transparent the behaviour of the construction under its smooth relaxation, and hence its correctness as a smooth relaxation preserving simulation.
Suppose we have a TM with sections {Qi}iI and corresponding contexts {Xi}iI . That is, the TM has a set of states i Qi, with each Qi in bijection with a corresponding set of local indices, Xi. Suppose further that there is a family of transitions from Qi to Qj for some i, j  I, over some set of read symbols 0  . That is, we have 1(Qi × 0)  Qj. Let f be the function Xi × 0  Xj ×  × {-1, 0, 1} induced by the restriction |Qi×0 . We refer to |Qi×0 as the tract from i to j over 0, and write the following.

[Xi]i

0 f1 (x,a),f2 (x,a),f3 (x,a)

[Xj ]j

Here, the square braces indicate a section with index given by the subscript. The braces enclose the context associated to this section. Hence, states in our designs will never receive explicit names; rather, they will receive a name (an element of Xi) local to their section, in terms of which the transition function will be specified. The long arrow denotes the collection of transitions over the states in its source section and an indicated set of read symbols to the left of the smaller arrow (that is, the above mentioned tract). The three expressions to the right of the smaller arrow indicate the target local index in the target section, write symbol, and move direction of an arbitrary transition in the tract in terms of its source local index and read symbol, always denoted by x and a respectively.

10

We return now to the smooth relaxation. Earlier, we observed that the conditionally independent distributions embed into the set of all distributions over Q ×  via a map Q ×   (Q × ) sending (q, )  q  . Moreover, in Definition 2.2, the linear operator induced by each component of the transition function was treated separately.
By pre-composing with the above embedding the map (Q×)  Q××{-1, 0, 1} defined component-wise by the linear operator induced by each component of the transition function, we obtain a map Q×  Q××{-1, 0, 1} which we shall call the smooth transition function.
Consider the earlier described tract. Suppose the state is distributed over Qi, and the read symbol is distributed over 0. Then the state in the next time step is distributed over Qj. Hence the restriction of the smooth transition function Qi × 0  Qj ×  × {-1, 0, 1} induces a map Xi × 0  Xj ×  × {-1, 0, 1} (and vice versa).
In general, in analysing the smooth relaxation of our constructions, we shall view state distributions as distributions over contexts, transformed according the induced map just described. We shall refer to distributions over contexts as local state distributions.
3 Constructions
3.1 Multitape on single tape
Theorem 3.1. There exists a smooth relaxation preserving simulation of any n-tape TM on a single tape TM.
Proof. Let M be an n-tape TM with states Q, alphabet , and transition function . Let SIM be a single tape machine.
The transition function of SIM will be specified in four phases: the read phase, write phase, parallel move phase and state update phase. The final section in each phase correspond to the initial section of the next phase, with the final section of the state update phase cycling back to the initial section of the read phase. We do not bother to specify unreachable transitions.
The tape alphabet of SIM shall contain  along with auxiliary symbols #L, #R and #0. We now specify an encoding. Let y  [()Z ]n be a tape configuration of M . See the end of Section 2.1 for notational conventions. Then an encoding of q  y in SIM is as follows. SIM shall have a state distribution over section R1 (the first section of the read phase) with local distribution
11

q. Let R be any integer strictly greater than 1 such that any square on any tape of M with non-zero

probability of being non-empty has index less than or equal to R. Similarly for L. We exclude -1,

0 and 1 to avoid some edge cases later on. Then SIM shall have a tape configuration containing the sequences given by each row of the following table, interleaved such that y(01) is at index 0, y(02) at index 1 and so forth.

#L y(L1) · · · y(-11) #0 y(01) y(11) · · · y(R1) #R

#L y(L2) · · · y(-21) #0 y(02) y(12) · · · y(R2) #R

...

...

...

#L

y

(n) L

···

y(-n1) #0

y(0n)

y

(n) 1

···

y(Rn)

#R

Note that the rightward movement of the tape head amounts to moving down a column and

wrapping back up to the top of the next column when the bottom is reached.

Read phase

In the notation of Section 2.3:

[Q]R1 (x,a),a,R [Q × ]R2 (x,a),a,R · · · (x,a),a,R [Q × n-1]Rn (x,a),a,S [Q × n]W 1 At the beginning of each simulation cycle, SIM will be in the context Q, with local state
distribution q  Q mirroring the corresponding state distribution of M . The tape head will be over y(01). Applying the smooth step function, one sees that the tape head moves (unambiguously) right, scanning each encoded read symbol of M U LT I. The local state distribution transforms as
q  q  y(01)  · · ·  q  y(01)  · · ·  y(0n). In the last tract, the tape head stays put in anticipation of the write phase.
Write phase
Here we write ,k to denote the component corresponding to the write symbol of the k-th tape of M U LT I.

12

[Q × n]W 1

x,,n(x),L

···

x,,2(x),L

[Q × n]W n

x,,1(x),L

[Q × n]MLB1

SIM scans the read symbols in right-to-left, this time replacing them with the appropriate distributions, ,k(q  y(01)  · · ·  y(0n)). (Refer to the end of Section 2.1 for the smooth step function of multi-tape machines.)

Parallel move phase

This is the phase most sensitive to our requirement that the simulation be smooth relaxation preserving.
In order to compute the superposition at an encoded square of M U LT I, say at index i of tape k, resulting from the possibly ambiguous tape head movement, SIM must have loaded into its local state distribution the symbol distributions at indices i - 1, i and i + 1. Since the original symbol distribution on one side is erased by the update operation, SIM must "remember" the original symbol distribution of the most recently updated square, then discard it once all superpositions involving it have been computed and written to the tape. An alternative solution would be to introduce staging squares between the working squares to store copies of the original distributions.
We only give a partial construction, which computes the superpositions in the bottom row of the table on page 10. Extending the construction to the remaining rows is routine, and can be achieved by copy-pasting the partial construction with appropriate intermediate transitions and minor adjustments. Alternatively, one would loop a suitably modified partial construction over the rows and introduce new auxiliary symbols to cue the loop exit. As usual, there will be a trade-off between the size of the alphabet and the number of states used. As we have no intention of being economical in this regard, we proceed without further comment.
We divide the partial construction into four sub-phases: left border shift, left edge case, main loop, right edge case and right border shift. These will be indicated by prefixes M LB, M LE, M M L, M RE and M RB in the section indices.

Left border shift

13

{#0}x,a,L

[Q × n]MLB1

#Lx,#L,L

#Lx, ,L

[Q × n]MLB2

x,#L,R

[Q × n]MLB3

#Lx,#L,R

x, ,L

[Q × n]MLB4

x,a,L

...

x,a,L

[Q × n]MLB(n+2)

x,a,L

[Q × n]MLE1
Here, the objective is to shift the left border of the bottom row one column to the left, to make room for the outward flow of non-empty squares. This outward flow occurs at a rate no faster than one column per simulation step, so it is sufficient to displace the border one column outwards in this way each cycle. In a more typical construction, the borders will be shifted depending on whether the simulated tape head moves to the edge of the tape encoding, however such an approach may introduce ambiguity in the tape head movement of SIM , so we avoid it.
After executing the shift, the tape head of SIM returns to the square corresponding to y(Ln).

Left edge case

In the following, we refer to the function  as defined at the end of section 2.1.

14

[Q × n]MLE1

(x,a),a,R
[Q × n]MLE2

x,a,R

···

x,a,R

[Q × n × ]MLE(n+1)

(x,a),a,L

[Q × n × 2]MLE(2n+1)

x,a,L

···

x,a,L

[Q × n × 2]MLE(n+2)

x,(n)(x1,··· ,xn+1, ,xn+2,xn+3),R

[Q × n × 2]MML1

Here, SIM computes the superposition at the left most encoded square of the current row. Since

the square to the left of this on the corresponding tape of M U LT I is not explicitly encoded, but

assumed to be blank, this is an edge case. At section M LE1, SIM loads y(Ln) into its local state

distribution, which transforms as q  y0  q  y0  y(Ln). The tape head then moves right until it

is

over

y(Ln+)1,

which

is

also

loaded,

producing

the

local

state

distribution

q



y0



y

(n) L



y(Ln+)1.

The

tape head returns to y(Ln), and writes the superposition given by (q  y0   y(Ln)  y(Ln+)1).

Main loop

We again refer to the function . Note that the sections omitted in the ellipsis will also be equipped with the same loop transitions of their source and target sections. Most of these loops will be redundant, depending on the current row being computed.

#0x,a,R

[Q × n × 2]MML1

x,a,R

···

(x1,··· ,xn+1,xn+3,xn+4),(n)(x),R

[Q × n × 3]MML(3n)

x,a,L

···

#0x,a,L

x,a,R x,a,L

[Q × n × 2]MRB1
#0x,a,R

#Rx, ,R

[Q × n × 2]MML(2n)

(x,a),a,L

[Q × n × 3]MML(2n+1)

#0x,a,L

15

At M M L1, SIM has local state distribution q  y0  y(i-n)1  yi(n), for some L < i < R. From

M M L1

to

M M L(2n),

the

tape

head

moves

rightward

from

y(in)

until

y

(n) i1

is

reached,

whereupon

it

is loaded into the local state distribution. From M M L(2n + 1) to M M L(3n), the tape head returns

to y(in), where it writes the superposition as given by (q  y0  y(i-n)1  y(in)  yi(+n)1). At the same time, the distribution y(i-n)1 is no longer needed by SIM , so is dumped from the local state distribution, which transforms as q  y0  y(i-n)1  y(in)  y(i+n)1  q  y0  y(in)  yi(+n)1. The loop

exits when #R is encountered instead of a symbol from , triggering the right border shift.

Right border shift and right edge case

[Q × n × 2]MRB1

x, ,R

···

x, ,R

[Q × n × 2]MRBn

x,#R,L

[Q × n × 2]MRB(2n-1) {#R}x, ,L · · · {#R}x, ,L [Q × n × 2]MRB(n+1)
{#R}x, ,L

[Q × n × 2]MRE1

(x1,··· ,xn+1),(n)(x, ),L

[Q × n]MRE2

#0x,#0,L

···

x,a,L
The analysis here mirrors the left edge case and border shift, so we omit further explanation. The tape head returns to the #0 in the bottom row and the remaining rows are computed.

State update phase

We assume the tape has returned to the square formerly containing y(01). All that remains is to update the simulated state distribution, which is achieved as follows.

[Q × n]S

1 (x),a,S

[Q]R1

The local state distribution transforms as q y0  1(q y0). SIM returns to the first section

16

of the read phase, and the cycle is complete.

3.2 Universal Turing machine
In this section, we introduce a design for a smooth relaxation preserving pseudo-UTM. That is, a machine which simulates only machines with a maximal state count and tape alphabet size.
Our UTM will be a 2-tape machine. The notation generalises in the natural way as follows, with a and b denoting the read symbols of each tape and the components of f mirroring those of .

[Xi]i

(01) ×(02) f1(x,a,b),f2(x,a,b),f3(x,a,b),f4(x,a,b),f5(x,a,b)

[Xj ]j

We will also encounter a situation in which the state distribution is spread over two sections. In this case, the context will be a disjoint union of two contexts, say, X  Y . In analysing the smooth relaxation, we invoke the isomorphism RX  RY - R(X  Y ) sending x  0  x and 0  y  y, thus denote local state distributions as direct sums.

Theorem 3.2. There exists a smooth relaxation preserving psuedo-UTM.

Proof. Let M be a single tape TM with states Q, alphabet  and transition function . Let U be a 2-tape machine. The tape alphabet U of U shall contain Q   along with an auxiliary symbol #. Let yi be the symbol distribution at index i on the tape of M . The first tape will contain a sequence of tuples bordered by #, encoding M . Each tuple will be of the form (q, , 1(q, ), 2(q, ), 3(q, )), for a choice of q  Q and   . The tape head will be positioned over the left #. The second tape will be identical to the tape of M . We shall call the first tape the description tape and the second tape the working tape.
The full construction of U is given as follows.

17

U \{-1,0,1}×x,a,b,R,S
[Q × ]wait

\{x2}×x,a,b,R,S

Q\{x1}×x,a,b,R,S

{-1,0,1}×x,a,b,R,S

[Q × ]scan1 {x1}×x,a,b,R,S [Q × ]scan2

{x2}×x,a,b,R,S

[Q × ]load1

Q×a,a,b,R,S

#(x,b),a,b,R,S

[Q]load2
×(x,a),a,b,R,S

[Q × ]load3
{-1,0,1}×(x,a),a,b,R,S

[Q]read
U \{#}×x,a,b,L,S

#x1,a,x2,L,x3

[Q ×  × {-1, 0, 1}]update
U \{#}×x,a,b,R,S

At the beginning of a simulation cycle, U has a state distributed over section read, with local distribution q  Q mirroring the state distribution of M . Via the upwards tract, the read symbol distribution is loaded, yielding a local distribution q  y0. We may decompose this as
q, q , y0 q  ,
q,
where q and  range over the standard bases of RQ and R. Henceforth, by "term", we mean a summand in this decomposition. Meanwhile, the description tape head moves onto the beginning of the first tuple. From here, the objective of U is to transport each term in the state distribution over section scan1 to an appropriate term in the target state distribution over section update. This will be clarified shortly. A term corresponding to local index (q, ) will arrive at local index
18

(1(q, ), 2(q, ), 3(q, ). Note that The resulting local state distribution will not encode a conditionally independent joint distribution over Q ×  × {-1, 0, 1}, however any dependence will be erased by the smooth relaxation in the leftward tract, during execution of which the working tape is updated.
We now verify that the transportation of the state distribution from section scan1 to update behaves as desired. Let the first tuple correspond to the pair (q1, 1). As U scans the initial state and read symbol on the tuple, the two rightward tracts have the effect of filtering out the correct term, with section wait serving as a kind of sieve. The residue terms (left side of the final disjoint union below) remain in section wait until U has finished scanning the given tuple, upon which they return to scan1. When the first tuple is scanned, the local state distribution transforms as follows.
q, q, q , y0 q  
q=q1, q, q , y0 q    q=q1, q, q , y0 q  
(q,)=(q1,1) q, q , y0 q    q1, q 1, y0 q1  1 The three downward tracts of load1, load2 and load3 distribute the right term over the target context, as U loads the target state, write symbol and move directions. The right term transforms as follows (whilst the left term is stagnant).
q1, q 1, y0 q1  1  q1, q 1, y0 1(q1, 1)  q1, q 1, y0 1(q1, 1)  2(q1, 1)  q1, q 1, y0 1(q1, 1)  2(q1, 1)  3(q1, 1)
The remaining terms transform similarly. Once all terms have been transported, the tract bridging update and read executes the write and parallel move operation, and the tape head on the description tape returns to the left # before the cycle repeats.
We briefly consider the behaviour of the above pseudo-UTM when the codes themselves are allowed to contain uncertainty. That is, suppose we allow the target state, write symbol and move direction in each tuple to take distributions in Q,  and {-1, 0, 1} respectively. This amounts to a generalisation of the transition function to a map,  : Q ×   Q ×  × {-1, 0, 1}.
19

Extending the above analysis reveals that the step function for codes with uncertainty is obtained from Definition 2.2 by replacing all instances of the classical transition function with this generalised version. Indeed, this is a simplification of the behaviour exhibited by the staged pseudo UTM of [JCW21].
4 Closing remarks
As alluded to in the introduction, it is not clear whether similar results exist for simulating Turing machines of arbitrary alphabet size using alphabets of size 2. According to condition 2 of Definition 2.4, one would need to devise a means of encoding distributions over finite sets of arbitrary size as sequences of Bernoulli distributions. One possibility is to employ a one-hot style encoding, with the probability of a given element in the sample space given by a single bit with uncertainty. However, a distribution encoded in this way cannot be loaded into the state distribution using the techniques demonstrated in section 3. If a certain kind of asymmetry is introduced into the encoding, it becomes possible to load the distribution, however (is seems) this is at the expense of being able to write the distributions encoded with the correct asymmetry back onto the tape.
Constructing smooth relaxation preserving simulations using a size 2 alphabet is closely related to constructing smooth relaxation preserving (true) universal Turing machine, since the latter will inevitably involve variable length encodings of distributions over sets of arbitrary size.
5 Acknowledgements
The author is in gratitude to Dan Murfet for his patience, open-mindedness and invaluable feedback during the writing of this manuscript.
References
[CM19] James Clift and Daniel Murfet. Derivatives of turing machines in linear logic. 2019. [JCW21] Daniel Murfet James Clift and James Wallbridge. Geometry of program synthesis. 2021. [Rog96] Yurii Rogozhin. Small universal turing machines. 1996.
20

