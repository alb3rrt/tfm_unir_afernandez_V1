
# Towards Deeper Deep Reinforcement Learning

[arXiv](https://arxiv.org/abs/2106.01151), [PDF](https://arxiv.org/pdf/2106.01151.pdf)

## Authors

- Johan Bjorck
- Carla P. Gomes
- Kilian Q. Weinberger

## Abstract

In computer vision and natural language processing, innovations in model architecture that lead to increases in model capacity have reliably translated into gains in performance. In stark contrast with this trend, state-of-the-art reinforcement learning (RL) algorithms often use only small MLPs, and gains in performance typically originate from algorithmic innovations. It is natural to hypothesize that small datasets in RL necessitate simple models to avoid overfitting; however, this hypothesis is untested. In this paper we investigate how RL agents are affected by exchanging the small MLPs with larger modern networks with skip connections and normalization, focusing specifically on soft actor-critic (SAC) algorithms. We verify, empirically, that na√Øvely adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, we show that dataset size is not the limiting factor, and instead argue that intrinsic instability from the actor in SAC taking gradients through the critic is the culprit. We demonstrate that a simple smoothing method can mitigate this issue, which enables stable training with large modern architectures. After smoothing, larger models yield dramatic performance improvements for state-of-the-art agents -- suggesting that more "easy" gains may be had by focusing on model architectures in addition to algorithmic innovations.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/towards-deeper-deep-reinforcement-learning](https://paperswithcode.com/paper/towards-deeper-deep-reinforcement-learning)

## Bibtex

```tex
@misc{bjorck2021deeper,
      title={Towards Deeper Deep Reinforcement Learning}, 
      author={Johan Bjorck and Carla P. Gomes and Kilian Q. Weinberger},
      year={2021},
      eprint={2106.01151},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

