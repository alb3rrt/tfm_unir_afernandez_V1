Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer
Ziqing Yang1, Wentao Ma1, Yiming Cui2,1, Jiani Ye1, Wanxiang Che2, Shijin Wang3,4 1Joint Laboratory of HIT and iFLYTEK (HFL), iFLYTEK Research, China 2 Research Center for SCIR, Harbin Institute of Technology, Harbin, China 3iFLYTEK AI Research (Hebei), Langfang, China 4State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China 1,3,4{zqyang5,wtma,ymcui,jnye,sjwang3}@iflytek.com 2{ymcui,car}@ir.hit.edu.cn

arXiv:2106.01732v1 [cs.CL] 3 Jun 2021

Abstract
Multilingual pre-trained models have achieved remarkable transfer performance by pretrained on rich kinds of languages. Most of the models such as mBERT are pre-trained on unlabeled corpora. The static and contextual embeddings from the models could not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by aligning the embeddings better. We propose a pre-training task named Alignment Language Model (AlignLM), which uses the statistical alignment information as the prior knowledge to guide bilingual word prediction. We evaluate our method on multilingual machine reading comprehension and natural language interface tasks. The results show AlignLM can improve the zero-shot performance significantly on MLQA and XNLI datasets.
1 Introduction
Large-scale pre-trained language models like GPT (Radford et al., 2019) and BERT (Devlin et al., 2019) have proved to be effective in many natural language processing tasks, which contributes to the development of multilingual language models, such as mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2020). Those models achieve significant improvement in various cross-lingual tasks like text classification and question answering.
The pre-training method of the multilingual language models can be divided into two groups: unsupervised pre-training like Multilingual Masked Language Model (MMLM) (Devlin et al., 2019; Conneau et al., 2020), and supervised pre-training like Translation Language Model (TLM) (Conneau and Lample, 2019) or its variations (Huang et al., 2019; Chi et al., 2020). The key difference is that
Equal contribution.

MMLM predicts the mask tokens with monolingual context, while TLM replaces the context with bilingual parallel sentences.
On the other hand, embedding alignment has been widely studied for enabling cross-lingual transfer, where the bilingual word embeddings have similar representations for word translation pairs (Mikolov et al., 2013; Smith et al., 2017), e.g. `apple' and `Apfel'.
Schuster et al. (2019) proposed a dictionary supervision method to align ELMo (Peters et al., 2018) with rotational transform; Cao et al. (2020) proposed a bilingual pre-training objective for mBERT to align contextual embeddings with parallel data.
The previous works focus on aligning the contextual embeddings for the pre-trained models. We argue that it is not enough for zero-shot cross-lingual transfer because the static embeddings from the embedding layer may not be aligned. In this paper, we propose a new pre-training task called Alignment Language Model (AlignLM), to enhance the cross-lingual transferability of pre-trained models like mBERT. Different from previous works, we align both the static embeddings and contextual embeddings of pre-trained models.
Specifically, we take word alignment pairs from fastAlign (Dyer et al., 2013) as the prior knowledge in the pre-training. For each masked token, AlignLM constructs the language model by monolingual prediction and bilingual prediction. The monolingual prediction uses the token's representation to predict the original word just like TLM, while bilingual prediction uses the alignment representation from the other language. For example, if the word `apple' is masked in the English­German parallel sentences, the representations of `apple' and `Apfel' are taken for monolingual prediction and bilingual prediction respectively. Through the two ways of prediction, both the contextual em-

Bilingual Prediction

special

T'4
$XVQDKPH

T1

Wir

We

Monolingual Prediction

alignment

special

T4

T1

T'4

T'1

Wir

Tcls T1

T2

T3

Tsep

T'1

T'2

T'3

T'4

T'5

Tsep

Multilingual Pre-trained Language Model

Ecls E1

E2

E3

Esep

E'1

E'2

E'3

E'4

E'5

Esep

[CLS] We are [MASK] [SEP] [MASK] Sind ein Ausnahme ##fall [SEP]

Figure 1: Alignment language model pretraining. The monolingual prediction is similar to TLM in (Conneau and Lample, 2019). The bilingual prediction uses statistical word alignment from fastAlign to predict the tokens of the other language.

beddings from the last layer and the static embeddings from the embedding layer can be aligned. We evaluate our method in the sentence-level task XNLI (Conneau et al., 2018) and the word-level task MLQA (Lewis et al., 2019). The results show AlignLM can significantly improve the crosslingual transfer performance based on mBERT.
2 Methodology
2.1 Translation Language Model
We first briefly describe Translation Language Model (TLM) (Conneau and Lample, 2019). Like MMLM in (Devlin et al., 2019), TLM performs the masked word prediction task, where it random masks some words and predicts the original ones within a parallel sentence pair. For each masked word, the model can either attend to the surrounding words or the translation word in the other language, encouraging the model to align the words in different languages.
2.2 Alignment Language Model
We can see that TLM aligns the words in a soft way, relying on the attention within the parallel sentence pair. In our proposed method AlignLM, we align the words in both soft and hard ways, called monolingual prediction and bilingual prediction, shown in Figure 1. Monolingual Prediction. In the monolingual prediction, we predict the original words by the masked token's representation. Unlike TLM, we did not change the position embeddings or add the

language embeddings to ensure the transferability of AlignLM. Take mBERT as example, we construct the inputs and representations for a sourcetarget sentence pair S, T by

X = [CLS]S[SEP]T [SEP] (1)

H = mBERT(X)

(2)

where H  Rm×h is the output from the last transformer layer of mBERT, m is the max sequence length and h is hidden size. For the masked token Xi, we predict the original word by the corresponding representation

Hi = (W1 · Hi + b1)

(3)

p(Wj|Hi) =

exp(linear(Hi)·Wj )

S k=1

exp(linear(Hi

)·Wk

)

(4)

where  is the activation function GELU

(Hendrycks and Gimpel, 2016), linear(·) is a linear layer, Hi is the token representation for Xi, S is the size of vocabulary.

Bilingual prediction. In the bilingual prediction,

we align the words in a hard way based on the

statistical word alignment results. For the paral-

lel sentence pair S, T , we use fastAlign to construct the alignment word set, denoted d(s, t) = {(i1, j1), ..., (in, jn)}, where i, j  Rm, i is the word index of source language in the input sequence, j is the index of the target one, n is the

number of word alignment pairs. Then we convert

the alignment set to a word alignment matrix by

1, if {(i, j) or (j, i)}  d A(i, j) =
0, else

Model

en

es

de

zh

Avg

Translate-Train
mBERT  mBERT (ours)

77.7/65.2 53.9/37.4 62.0/47.5 61.4/39.5 63.8/47.4 80.3/67.3 67.1/48.4 63.5/49.1 63.6/42.8 68.6/51.9

Zero-Shot
mBERT  mBERT+TLM mBERT+AlignLM

77.7/65.2 80.0/66.8 79.7/66.7

64.3/46.6 65.7/47.7 67.8/49.6

57.9/44.3 63.1/48.4 64.3/49.7

57.5/37.3 62.0/40.1 63.7/41.7

64.4/48.4 67.7/50.7 68.9/51.7

Table 1: Experimental results on the test set of MLQA dataset. We report F1 and exact match (EM) scores. The results with  are taken from (Lewis et al., 2019).

The i-th row in A  Rm×m contains the alignment words for the i-th word in X. We construct the cross-lingual alignment representation for each word by

H = AT · H

(5)

H~ = W2 · H + b2

(6)

where H~ is the alignment representation from the other language, W2 and b2 are parameters for language transformation. At last, we predict the original words by the cross-lingual representation similar to monolingual prediction

H~ = (W3 · H~i + b3)

(7)

p~(Wj|H~i) =

exp(linear(H~i)·Wj )

S k=1

exp(linear(H~ i

)·Wk

)

(8)

Pre-training Objective. Given a bilingual parallel

corpus D, we train the multilingual model using

cross-entropy loss. Let  denotes the parameters

of the model, then the objective function L(D, )

of pre-training can be formulated as

M

Lmp = - i=1 log(P (xi))

(9)

Lbp = -

M i=1

log(P~

(xi))

(10)

N

L(D, ) = (Lmp + Lbp) (11)

where M is the number of masked tokens in the
instance, N is the number of instances in D, P (xi) and P~(xi) are the predicted probability of the masked token xi over the vocabulary size,  is a hyper-parameter, which varies according to the
language pairs.

3 Experiment

3.1 Expriment setup
The pre-training procedure is primarily based on the weights of mBERT released by Google. For

Model

en es de zh Avg

Translate-Train mBERT

82.1 77.8 75.9 75.7 77.9

Zero-Shot

mBERT 

82.1 74.3 71.1 69.3 74.2

Cao et al. (2020) 80.1 75.5 73.1 -

-

mBERT+TLM

82.0 75.0 73.5 73.1 75.9

mBERT+AlignLM 82.6 76.4 74.5 74.4 77.0

Table 2: Experimental results on XNLI dataset. We report test accuracy scores, where the results with  are taken from (Conneau et al., 2020).

the pre-training parallel corpus, we consider the source language English and three target languages: Chinese 1, German and Spanish 2. We pre-train three models for the target languages separately to avoid alignment interference among different language pairs.
During the pre-training step, we empirically set the mask probability as 0.3, and the other setting for masking is the same as the MLM (Devlin et al., 2019). For the parameter setting, we set the learning rate as 5e-5, batch size as 32, max sequence length as 12, and pre-train our models for 2 epochs.
3.2 Baselines
We use mBERT (Devlin et al., 2019) as our main baseline, where the model consists of 12 transformer layers, with a hidden size of 768 and 12 attention heads. For the fair comparison, we also consider a baseline mBERT+TLM, which has the same pre-training settings as our methods but uses TLM as the pre-training task based on mBERT. We compare with an additional baseline Aligned BERT from Cao et al. (2020) for the XNLI dataset.
1We use the corpus from (Xu, 2019) 2http://www.statmt.org/europarl/

Figure 2: The visualization of the static embeddings from mBERT before and after AlignLM pre-training. We select 20 English-German word alignment pairs, which appear most frequently in the pre-training corpus. Each word alignment pair is generated by fastAlign in Section 2.2 and connected by a blue dotted line.

3.3 Results on MLQA
The results on MLQA dataset are reported in Table 1. We can find that TLM can improve the crosslingual transfer performance of mBERT by a large margin, but is worse than the model trained by the translate training set. AlignLM further improve the scores based on TLM significantly and achieve a litter better performance than the model with supervised training. That means it is possible for us to use pre-training instead of translation. Please note that we pre-train a separate model for each target language both in TLM and AlignLM pretraining steps. We report the average performance of the three models for English.
3.4 Results on XNLI
The results are reported in Table 2. We can find that mBERT+TLM and Align mBERT have similar performance, which may be because both of them align the words in the soft way. mBERT with AlignLM has significantly better performance, which shows the ability of the bilingual prediction. Please note that all of them are pre-trained with the same parallel corpus. Compared with the translatetrain, the model with AlignLM pre-training has a much closer performance but fails to exceed it. That is different from results in MLQA, which may be because the examples in XNLI have shorter input sequences and less translation noise.
4 Visualization
The alignment of contextual embeddings is useful in improving the transferability of mBERT (Cao et al., 2020), but the effect of the alignment of the static embeddings in the embedding layer is

still unclear. To further explore this problem, we use t-SNE (Maaten and Hinton, 2008) to visualize the embeddings of word alignment pairs with the highest frequency (excluding stop words), which can be seen in Figure 2.
We can find that the embeddings in the embedding layer of mBERT are partly aligned. For example the pairs `today-heute,' `Council-Rat' are aligned well, but `Beriche-report,' `Mr-Herr' are not aligned. Furthermore, most of the word pairs are aligned better after AlignLM pre-training except for a few word pairs like `our-uns', which may be because they are not the exact translation pair (`us-uns'). In general, the embeddings are aligned much better after AlignLM pre-training procedure, which indicates that the alignment of static embeddings may be useful to improve the transfer performance of multilingual models.
5 Conclusion
In this paper, we propose a new pre-training task named AlignLM to align the contextual and static word embeddings from mBERT, including monolingual prediction and bilingual prediction. As a supplement of previous works MMLM or TLM, AlignLM introduces the statistic alignment information as prior knowledge to guide the bilingual prediction. Through the experiments on MLQA and XNLI, we find that AlignLM can improve the transfer performance significantly and align the static embeddings within the models better. In the future, we will extend our method to other multilingual models like XLM-R.

References
Steven Cao, Nikita Kitaev, and Dan Klein. 2020. Multilingual alignment of contextual word representations. arXiv preprint arXiv:2002.03518.
Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, XianLing Mao, Heyan Huang, and Ming Zhou. 2020. Infoxlm: An information-theoretic framework for cross-lingual language model pre-training. arXiv preprint arXiv:2007.07834.

Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2019. Mlqa: Evaluating cross-lingual extractive question answering. arXiv preprint arXiv:1910.07475.
Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579­2605.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzma´n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440­ 8451, Online. Association for Computational Linguistics.
Alexis Conneau and Guillaume Lample. 2019. Crosslingual language model pretraining. In Advances in Neural Information Processing Systems, pages 7059­7069.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475­2485.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171­4186.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT-2018, pages 2227­2237. Association for Computational Linguistics.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2019. Improving language understanding by generative pre-training.
Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. 2019. Cross-lingual transfer learning for multilingual task oriented dialog. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3795­3805.
Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. arXiv preprint arXiv:1702.03859.
Bright Xu. 2019. Nlp chinese corpus: Large scale chinese corpus for nlp.

Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644­648.

Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.

Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019. Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2485­2494, Hong Kong, China. Association for Computational Linguistics.

