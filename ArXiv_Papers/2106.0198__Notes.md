
# Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence

[arXiv](https://arxiv.org/abs/2106.0198), [PDF](https://arxiv.org/pdf/2106.0198.pdf)

## Authors

- Runyu Zhang
- Zhaolin Ren
- Na Li

## Abstract

We study the performance of the gradient play algorithm for multi-agent tabular Markov decision processes (MDPs), which are also known as stochastic games (SGs), where each agent tries to maximize its own total discounted reward by making decisions independently based on current state information which is shared between agents. Policies are directly parameterized by the probability of choosing a certain action at a given state. We show that Nash equilibria (NEs) and first order stationary policies are equivalent in this setting, and give a non-asymptotic global convergence rate analysis to an $\epsilon$-NE for a subclass of multi-agent MDPs called Markov potential games, which includes the cooperative setting with identical rewards among agents as an important special case. Our result shows that the number of iterations to reach an $\epsilon$-NE scales linearly, instead of exponentially, with the number of agents. Local geometry and local stability are also considered. For Markov potential games, we prove that strict NEs are local maxima of the total potential function and fully-mixed NEs are saddle points. We also give a local convergence rate around strict NEs for more general settings.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{zhang2021gradient,
      title={Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence}, 
      author={Runyu Zhang and Zhaolin Ren and Na Li},
      year={2021},
      eprint={2106.00198},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

