arXiv:2106.01214v1 [stat.ME] 2 Jun 2021

General Bayesian Loss Function Selection and the use of
Improper Models
Jack Jewson1,2 and David Rossell1,2
1Department of Business and Economics, Universitat Pompeu Fabra, Barcelona, Spain 2Data Science Center, Barcelona Graduate School of Economics, Spain jack.jewson@upf.edu, david.rossell@upf.edu
June 2021
Abstract Statisticians often face the choice between using probability models or a paradigm defined by minimising a loss function. Both approaches are useful and, if the loss can be re-cast into a proper probability model, there are many tools to decide which model or loss is more appropriate for the observed data, in the sense of explaining the data's nature. However, when the loss leads to an improper model, there are no principled ways to guide this choice. We address this task by combining the Hyv¨arinen score, which naturally targets infinitesimal relative probabilities, and general Bayesian updating, which provides a unifying framework for inference on losses and models. Specifically we propose the H-score, a general Bayesian selection criterion and prove that it consistently selects the (possibly improper) model closest to the data-generating truth in Fisher's divergence. We also prove that an associated H-posterior consistently learns optimal hyper-parameters featuring in loss functions, including a challenging tempering parameter in generalised Bayesian inference. As salient examples, we consider robust regression and non-parametric density estimation where popular loss functions define improper models for the data and hence cannot be dealt with using standard model selection tools. These examples illustrate advantages in robustness-efficiency tradeoffs and provide a Bayesian implementation for kernel density estimation, opening a new avenue for Bayesian non-parametrics.
Keywords: Loss functions; Improper models; General Bayes; Hyv¨arinen score; Robust regression; Kernel density estimation.
1

1 Introduction
A common task in Statistics is selecting which amongst a set of models is most appropriate for an observed data set y. Tools to address this problem include a variety of penalised likelihood, shrinkage prior, and Bayesian model selection methods. Under suitable conditions, these approaches possess the important property of consistently selecting the model closest to the data-generating truth in Kullback-Leibler divergence (for example, see Rossell (2021) and references therein for a recent discussion). However, many data analysis methods are not defined in terms of probability models but as minimising a given loss function, for example to gain robustness or flexibility. It is then no longer clear how to use the data to guide the choice of the most appropriate loss function, or any associated hyper-parameters. A key observation is that while the likelihood fk(y; k) of a model k with parameters k always defines a loss function k(y; k) = - log fk(y; k) (Good, 1952), the converse is not true. The exponential of an arbitrary loss fk(y; l) = exp{- k(y; k)} may not integrate to a finite constant and therefore, defines an improper model on y. For example, this occurs in robust regression with Tukey's loss (Figure 1) and in kernel density estimation. In these scenarios traditional model selection tools are not applicable to choose the more appropriate loss. Neither are methods to evaluate predictive performance such as cross-validation, since they require specifying a loss or criterion to evaluate performance in the first place, and do not attain consistent model selection even in simple settings (Shao, 1997). Methods to tackle intractable but finite normalisation constants, such as approximate Bayesian computation (Beaumont et al., 2002; Robert, 2016), also do not apply since they require simulating from a proper model.
We propose methodology to evaluate how well each given model and loss captures the unknown data-generating distribution g(y). The main idea is viewing each loss k as defining a (possibly improper) model exp{- k(y, k)}, and then measuring how well it approximates g(y) via Fisher's divergence. As we shall see, Fisher's divergence and its related Hyv¨arinen score (Hyv¨arinen, 2005) do not depend on normalising constants, and hence give a strategy to compare improper models. Note, that one could conceivably define the likelihood in ways other than exp{- k(y, k)}. Conversely, defining losses as negative log-likelihoods provides the only smooth, local, proper scoring rule (Bernardo, 1979), and also the only transformation that leads to consistent parameter estimation for certain general class of likelihoods (Bissiri and Walker, 2012). Further, it seems reasonable that the loss should be additive over independent pieces of information, and that the likelihood of an improper model should factorize under such independence, and for both properties to hold one must take the exponent of the negative
2

loss. Our framework consistently selects the best model in Fisher's divergence, and in particular the

(proper) data-generating model if it is under consideration. We also show how, after a model is chosen,

one can learn important hyper-parameters such as the likelihood tempering in generalised Bayes and

PAC-Bayes, robustness-efficiency trade-offs in regression and the level of smoothing in kernel density

estimation.

The use of probability models versus algorithms is one of the most fundamental, long-standing

debates in Statistics. In an influential piece, Breiman et al. (2001) argued that models are not realistic

enough to represent reality in any useful manner, nor flexible enough to predict accurately complex

real-world phenomena. Despite advances in flexible and non-parametric models, this view remains in

the current era where predictive machine learning proliferates, and shows ample potential to tackle

large and/or complex data. However, their limitations notwithstanding, probability models remain a

fundamental tool for research. Paraphrasing Efron (2020): "Abandoning mathematical models comes

close to abandoning the historic scientific goal of understanding nature." We agree with the view that

there are many situations where models facilitate describing the phenomenon under study. We seek

to enrich this paradigm by noting that loss functions define improper models that also lead to natural

interpretations, in terms of relative probabilities, and proposing a strategy to learn which loss gives a

better description of the process underlying the data.

Our strategy is to view fk as expressing relative (as opposed to absolute) probabilities, for example fk(y0, k)/fk(y1, k) describes how much more likely is it to observe y = y0 than y = y1. A convenient

manner to describe such ratios is by comparing the gradient of log fk(y, k) to the gradient of the log

data-generating density log g(y). This can be achieved by minimising Fisher's divergence

DF

(g||fk)

:=

1 2

||y log g(y) - y log fk(y; k)||22g(y)dy,

(1)

where y is the gradient operator. Minimising Fisher's divergence is equivalent to minimising the

Hyv¨arinen score (Hyv¨arinen, 2005). The Hyv¨arinen score has been used for models with intractable,

but finite normalising constants (Hyv¨arinen, 2005) and more recently to conduct Bayesian model

selection using improper priors (Dawid and Musio, 2015; Shao et al., 2019). We consider for the first

time its use to select between possibly improper models, and associated hyper-parameters.

The paper proceeds as follows. Section 2 reviews recent developments in Bayesian updating with

loss functions, discusses our motivating examples and some failures of standard methodology. Section

3 explains how we interpret the inference provided by an improper model in terms of relative probabil-

ities, and their relation to Fisher's divergence and the H-score. It also outlines our methodology: the

3

definition of an H-posterior, a n-consistency result to learn parameters and hyper-parameters, and the definition of the integrated H-score and H-Bayes factors as a criterion to choose among possibly improper models. Section 4 gives consistency rates for H-Bayes factors, including important nonstandard cases where optimal hyper-parameters lie at the boundary, as can happen when considering nested models. Section 5 applies our procedure to robust regression. Section 6 produces a Bayesian implementation of kernel density estimation, which cannot be tackled by standard Bayesian methods, since kernel densities define an improper model for the observed data. All proofs and some additional technical results are in the supplementary material. Code to reproduce the examples of Sections 5 and 6 can be found in the repository https://github.com/jejewson/HyvarinenImproperModels.

2 Problem formulation
We define the problem and notation and then provide the necessary foundations by reviewing general Bayesian updating, providing two motivating examples, identifying the shortcomings of standard approaches, and finally introducing Fisher's divergence and the Hyv¨arinen score.
Denote by y = (y1, . . . , yn) an observed continuous outcome, where yi  R are independent draws from an unknown data-generating distribution with density g(·). One is given a set of M probability models and L loss functions which, in general, may or may not include g. As usual each model k = 1, . . . , M is associated to a density fk(y; k, k), where k are parameters of interest and k are hyperparameters, (k, k)  k × k  Rdk . Any such density defines a loss k(y; k, k) = - log fk(y; k, k). Similarly, denote by k(y; k, k) for k = M + 1, . . . , M + L the given loss functions. For k > M , we refer to fk(y; k, k) = exp{- k(y; k, k)} as the (possibly improper) density associated to k. In general such fk need not integrate to a finite number with respect to y, i.e. fk may define an improper model on y. Our goal is to choose which among f1, . . . , fL+M provides a better representation of g, in a sense made precise below.

2.1 General Bayesian updating
In the frequentist paradigm it is natural to infer parameters by minimising loss functions, a classical example being M -estimation (Huber and Ronchetti, 1981). Loss functions are also used in the PACBayes paradigm, where one considers the posterior distribution on the parameters

k(k|y, k)  k(k | k) exp{- k(y; k, k)}

(2)

4

where k(k | k) is a given prior distribution and  denotes "proportional to". See Guedj (2019) for a review on PAC-Bayes, and Gru¨nwald (2012) for the safe-Bayes paradigm, which can be seen as a particular case where k is a tempered negative log-likelihood. At this stage we consider inference for k for a given hyper-parameter k, we discuss learning k later. As a key result supporting the interpretation of (2) as conditional probabilities akin to Bayes rule, Bissiri et al. (2016) showed that (2) leads to a principled updating of beliefs, and referred to the framework as general Bayesian updating.
These results allow Bayesian inference on parameters k based on loss functions. The properties of the general Bayesian posterior have been well-studied, for example under suitable regularity conditions Chernozhukov and Hong (2003) and Lyddon et al. (2019) showed that it is asymptotically normal. However, the emphasis of prior work is on inference for k. To our knowledge viewing exp{- k(y; k, k)} as an improper density has not been considered, which is critical for interpretation and posterior predictive inference, nor has the problem of choosing which loss best represents the data.

2.2 Motivating applications
We introduce two problems which, despite being classical, cannot be tackled with standard inference. We first consider robust regression where one contemplates a parametric model and a robust loss, and wishes to assess which represents the data best. To our knowledge there are no solutions for this problem. We next consider learning the bandwidth in kernel density estimation, where the goal is predictive inference on future data. While there are many frequentist solutions, Bayesian methods are hampered by the associated loss defining an unnormalisable model for the observed data.

2.2.1 Robust regression with Tukey's loss

Consider the linear regression of yi on an p-dimensional vector xi,

yi = xTi  + i, with E( i) = 0, V ( i) = 2 for i = 1, . . . , n.

Consider first a Gaussian model, denoted k = 1, so that f1(yi; 1) = N (yi; xTi , 2), where 1 =

{, 2}  Rp × R+, and there are no hyper-parameters (1 = ). The negative log-likelihood gives the

least-squares loss 1(y; 1) =

n i=1

1(yi; 1), where

1(yi;

1)

=

-

log

f1(yi; ,

2)

=

1 2

log

22

+

(yi

- xTi 22



)2

.

(3)

5

(y; , 2) 01234
f~(y; , 2) 0.0 0.1 0.2 0.3 0.4

2 =  2 = 4 2 = 3 2 = 2

-4

-2

0

2

4

(y - µ)/2

-4

-2

0

2

4

(y - µ)/2

Figure 1: Squared-error loss and Tukey's loss (left) and corresponding (improper) densities (right). The improper densities for Tukey's loss are scaled to match the mode of the Gaussian density.

Since the least-squares loss is non-robust to outliers, one may consider alternatives. A classical choice is Tukey's loss (Beaton and Tukey, 1974), which we denote k = 2, given by



2(yi;

2,

2)

=

 

1 2

log

 

1 2

log

22 22

+

(yi-xTi )2 22

-

(yi-xTi )4 2 4 22

+

(yi-xTi  6 2 42

)6

,

+

22 6

,

if |yi - xi|  2 otherwise

(4)

where 2 = {, 2}  Rp ×R+ and 2  R+ is a cut-off hyper-parameter. Note that (4) is parametrised

such that the units of the cut-off parameter 2 are standard deviations away from the mean.

The density f2(y; 2, 2) = exp{-

n i=1

2(yi; 2, 2)} integrates to infinity, defining an improper

model. Figure 1 plots Tukey's loss for several 2 and their corresponding densities. (4) is similar to

(3) when |yi - xTi | is close to 0, while for large |yi - xi| it becomes flat, bounding the influence of

outliers. The Gaussian model is recovered from Tukey's loss when 2 = . As we shall see, such

nested comparisons pose methodological challenges that motivated our developments. As a technical

remark, in robust statistics 2 is typically estimated separately from , either as part of a two-stage

procedure (see e.g. Chang et al., 2018) or using S-estimation (Rousseeuw and Yohai, 1984). Instead,

our framework allows to jointly estimate (, 2).

We note that one can add more losses into our framework, for example those in Black and Ran-

garajan (1996); Basu et al. (1998) or Wang et al. (2020). Also, our framework is not limited to

linear regression. One may replace xTi  by a non-linear function, for example from a deep learning or Gaussian process regression.

Setting 2 in Tukey's loss is related to the so-called robustness-efficiency trade-off, an at least

6

60-year old unsettled issue; see Box (1953) and Tukey (1960). While 2 =  gives the most efficient parameter estimates if the data are Gaussian, they are least robust outliers. Decreasing 2 increases robustness, but can significantly reduce estimation efficiency if 2 is set too small. We address this issue by learning from the data whether or not to estimate parameters in a robust fashion (i.e. selecting between 1 and 2) and when appropriate, exactly how robust to be (i.e. estimating 2).
Despite the importance of these questions, we are aware of limited work setting 2 in a principled, data-driven manner. Rule-of-thumb methods are popular, e.g. setting "2 = 4.6851, gives approximately 95% asymptotic efficiency of L2 minimization on the standard normal distribution of residuals" (Belagiannis et al., 2015), setting 2 = 1.547 to obtain a breakdown of 1/2 (Rousseeuw and Yohai, 1984), or a balance of breakdown and efficiency (e.g. Riani et al., 2014). Other methods, rely on estimating quantiles of the data (e.g. Sinova and Van Aelst (2016)) minimising an estimate of parameter mean squared error (Li et al., 2021) or minimising the maximum change in parameter estimates from perturbing one observation (Li et al., 2021).

2.2.2 Non-parametric Kernel Density Estimation

Suppose that yi  g independently for i = 1, . . . , n and one wishes to estimate g. The kernel density estimate at a given value x is given by

g^h(x)

=

1 nh

n

K

x - yi h

(5)

i=1

where the kernel K(·) is a symmetric, finite variance probability density, and h is the bandwidth

parameter. For simplicity we focus on the Gaussian kernel K(x) = N (x; 0, 1).

The bandwidth h is an important parameter controlling the smoothness and accuracy of g^h. Popular strategies to set the bandwidth are rule-of-thumb and plug-in methods (e.g. Silverman, 1986), cross-

validation (Habbema et al., 1974; Robert, 1976) and minimising integrated square error (Rudemo,

1982; Bowman, 1984).

Unfortunately, standard Bayesian inference cannot be used to learn h from data. The reason is

that although (5) defines a proper probability distribution for a future observation x, a Bayesian

framework requires a model for the observed data given the parameter. In our notation, the model

likelihood f (y; , ) 

n i=1

g^h(yi)

with



=



and



= {h}

has

an

infinite

normalising

constant.

To

7

see this, note that

g^h(yi)

=

1 nh

n

K

j=1

yi - yj h

= 1 + 1

n
exp

nh 2 nh 2 j=i

-

(yi

- yj 2h2

)2

,

(6)

where the first term integrates to infinity with respect to yi. Hence, (5) illustrates a situation where one has an `algorithm' for producing a density estimate for future observations that defines an improper

probability model for the observed data.

2.3 The failure of standard technology

As we discussed a main challenge is that standard tools are not, in general, applicable to compare improper models. Another challenge occurs when one wishes to estimate a hyperparameter k of a given improper model k, e.g. Tukey's cut-off or the kernel bandwidth. For example, the general Bayesian might consider mimicking standard Bayes or marginal likelihood estimation by defining

^k := arg max k(k) exp{- k(y; k, k)}dk.

(7)

k K

Unfortunately, such procedure often produces degenerate estimates. For example, from (4), it is clear

that for fixed 2 = {, 2} Tukey's loss is increasing in 2 and therefore (7) selects ^2 = 0 independently

of the data. Similarly, in the kernel density estimation example (7) selects h = 0 (Habbema et al.,

1974; Robert, 1976).

2.4 Fisher's divergence and the Hyv¨arinen score

Hyv¨arinen (2005) proposed a score matching approach that is computationally convenient when a model's normalising constant is intractable. Score matching estimates parameters by minimising Fisher's divergence to the data generating density g(·) in (1), that is

k : = arg min DF (g(·)||fk(·; k)) = arg min Ezg [H(z; fk(·; k))] ,

(8)

k k

k k

where the right-hand side is obtained using integration by parts (Hyv¨arinen, 2005) and

H (z ;

fk(·;

k))

:=

2

2 z2

log

fk(z;

k )

+

 z

log

fk(z;

k )

2
,

(9)

is the Hyv¨arinen score (H-score). Given y  g one can estimate k by minimising

^k

:=

arg min
k k

1 n

n i=1

H(yi; fk(·; k))

(10)

8

A critical feature for our purposes is that the H-score depends only on the first and second derivatives of log fk and hence the normalising constant does not play a role, independently of whether it is finite or not. The H-score enjoys desirable properties. For example, Dawid et al. (2016) proved that ^k is consistent and asymptotically normal, and Dawid and Musio (2015); Shao et al. (2019) that its prequential application leads to consistent Bayesian model selection under improper priors.
Closest to our work, Matsuda et al. (2019) proposed the score matching information criteria to select between models with intractable, but finite, normalising constants. This criterion estimates Fisher's divergence by correcting the in-sample Hyv¨arinen score by an estimate of its asymptotic bias. We emphasise two main distinctions with our work. The first is the extension to improper models. Second, these authors used cross-validations and predictive criteria similar to the AIC, which do not lead to consistent model selection, whereas we focus on structural learning where one seeks guarantees on recovering the loss that best approximates the data-generating g.
3 Inference for improper models
We now present our framework. Section 3.1 interprets an improper model in terms relative probabilities and motivates Fisher's divergence as a criterion to fit such a model. Section 3.2 proposes using the Hscore to define a general Bayesian posterior to learn hyper-parameters and choose among a collection of models, some or all of which may be improper. Section 3.3 proposes a Laplace approximation to the H-Bayes factors, which we use both in our theoretical treatment and examples. Finally, Section 3.4 argues for using the H-score within a two-step procedure, first selecting a model and estimating hyper-parameters using the H-score, then reverting to standard general Bayes to learn the parameters of interest.
3.1 Inference through relative probabilities
Our goal is to select which model fk describes the data best, in terms of helping interpret the datagenerating g. The main difficulty is that, since fk may be improper, it is unclear how to define "best". Our strategy is to view fk as expressing relative probabilities, in contrast to the usual absolute probabilities. For example fk(y0, k)/fk(y1, k) describes how much more likely one is to observe y = y0 over y = y1. As an illustration, consider Tukey's loss in (4). For any pair (y0, y1) such that
9

|y0 - xT |, |y1 - xT | < 2 are small, Tukey's loss is approximately equal to the squared loss, hence

f2(y0; f2(y1;

2, 2,

2) 2)



N N

y0; xT (y1; xT

, ,

2 2)

,

(11)

In contrast, for any pair such that |y0 - xT |, |y1 - xT | > 2 we have f2(y0,2,2)/f2(y1,2,2) = 1.

That is, Tukey's loss induces relative beliefs that observations near the mode behave like Gaussian

variables, while all faraway observations are equally likely. This encodes the notion that one does

not know much about the tails beyond their being thick, which is difficult to express using a proper

probability distribution.

We argue that Fisher's divergence is well-suited to evaluate how closely the relative probabilities

of any such fk approximate those from g. Assuming that the gradients of g(y) and fk(y) are finite for

all y, Fisher's divergence in (1) can be expressed as

DF (g||f) =

lim

log

g(y+ ) g(y)

-

log

fk(y+ ;) fk (y;)

0

2
g(y)dy.
2

(12)

Therefore, minimising Fisher's divergence (equivalently, the Hyv¨arinen score) targets a fk that ap-

proximates the relative probabilities of g in an infinitesimal neighbourhood around y, in the quadratic

error sense, on the average with respect to g(y). This observation extends the usual motivation for the

Hyv¨arinen score as a replacement of likelihood inference when the normalising constant is intractable

to being a justifiable criteria to score improper models.

3.2 The H-score

We consider a general Bayesian framework where the loss is defined by applying the H-score to the

density fk(y; k, k) = exp{- k(y; k, k)}, which gives the general posterior

n

H (k, k|y)  k(k, k) exp - H(yi; fk(·; k, k)) .

(13)

i=1

We refer to (13) as the H-posterior. Note that (13) is different from the general Bayesian posterior

directly associated to k in (2). An important property of (13) is that it provides a consistent estimator for parameters k and hyper-parameters k. Specifically, Proposition 1 shows that, under regularity conditions, ~k = (~k, ~k) maximising (13) recovers the optimal k = (k, k) according to Fisher's divergence. This extends the consistency results of Hyv¨arinen (2005) (Corollary 3), who considered

the well-specified case where g(y) = fk(y; k, k), which in particular requires fk to be a proper model.

10

Proposition 1 also extends Dawid et al. (2016) (Theorem 2), who proved asymptotic normality for

H-score based estimators. Said asymptotic normality does in general not hold in interesting cases

where k lies on the boundary of the parameter space. For example, for Tukey's loss if the data are

truly

Gaussian

then

2

=

.

By

Proposition

1,

even

if

normality

does

not

hold,

one

still

attains

 n

consistency to estimate 2. Further, Proposition 1 extends previous results by explicitly considering

improper models and the learning of their hyperparameters.

Proposition 1 requires some mild regularity conditions A1-A3, stated and discussed in Section

A.1.2. Briefly, A1 requires continuous second derivatives of the Hyv¨arinen score, that it has a unique

minimiser, and that its first derivative has finite variance. A2 requires that the Hyv¨arinen score is

dominated by an integrable function, which can be easily seen to hold for Tukey's loss, for example.

Finally, A3 requires that the Hessian of the H-score is positive and finite around j.

Proposition 1. Let y = (y1, . . . , yn)  g, ~k = (~k, ~k) maximise (13), and k = (k, k) minimise Fisher's divergence from fk(k, k) to g. Assume Conditions A1-A2 in Section A.1.2. Then, as n  ,

||~k - k||2 = op(1),

where || · ||2 is the L2-norm. Further, if Condition A3 also holds, then

||~k - k||2 = Op(1/n).

A consequence of Proposition 1 is that one can use (13) to learn tempering hyper-parameters. Specifically, suppose that one considers a family of losses wk k(), where wk > 0 is a tempering parameter. While wk does not affect the point estimate of k given by (2), it plays an important role in driving the posterior uncertainty on k. Within our framework, one may define
fk(y; k, k) = exp{- k(y; k, k)} = exp{-wk k(y; k, k)}
where k = (k, wk) and k() = wk k(). By Proposition 1, one can consistently learn the Fisherdivergence optimal k, and in particular wk. In contrast, in the general Bayes posterior (2) it is challenging to estimate such wk. Current strategies to set wk are optimising an upper-bound on generalisation error in PAC-Bayes (Catoni, 2007), estimating wk via marginalisation similar to the "Safe Bayesian" fractional likelihood approach of Gru¨nwald (2012), or using information theoretic
11

arguments to calibrate wk to match certain limiting sampling distributions (Holmes and Walker, 2017; Lyddon et al., 2019). These strategies essentially view wk as a tuning parameter. In contrast, in our framework wk is viewed as a parameter of interest that controls the dispersion of the improper model and affects its interpretation. See Section 6 for an illustration in kernel density estimation.
Recall that our main goal is model comparison. To this end, in analogy to the marginal likelihood in standard Bayesian model selection, we define the integrated H-score

n

Hk(y) = k(k, k) exp - H(yi; fk(·; k, k)) dkdk.

(14)

i=1

Also, analogously to Bayes factors and posterior model probabilities, we define the H-Bayes factor as Bk(Hl ) := Hk(y)/Hl(y) and



-1

(k | y) =

Hk(y)(k) l Hl(y)(l)

=

1

+

l=k

Bl(kH)

(l) (k)



,

(15)

where (l) are given model prior probabilities. In our examples we use uniform (k), since we focus on the comparison of a few models, but in high-dimensional settings it may be desirable to set (k) to favour simpler models.
We note that an interesting alternative strategy for model comparison, also based on the H-score, is to extend the prequential framework of Dawid and Musio (2015) and Shao et al. (2019) designed for improper priors. Therein one could adopt a general Bayesian framework, replacing the likelihood by fk(xi; k, k) = exp{ k(xi; kk)}. Prequential approaches enjoy desirable properties, such as consistency and leading to joint coherent inference on the model and parameter values. Unfortunately, prequential inference is computationally hard, particularly when considering several models. First, inference needs to be updated n times to calculate the one-step-ahead predictive distribution. Second, said updates are not permutation invariant, so one should in principle consider the n! orderings of the data. Thus, while interesting, we leave such line of research for future work.

12

3.3 Laplace approximation and BIC-type criterion

While there are many strategies for computing integrals such has Hk(y) in (14) exactly (see Llorente

et al. (2020) for a review), it is convenient to have faster options. We consider a Laplace approximation

n

H~k(y)

:

=

(2)

dk 2

k

(~k)

exp

-

H (yi; fk (·; ~k))

|Ak

(~j

)

|-

1 2

,

(16)

i=1

n

with ~k : = arg min H(yi; fk(·; k)) - log k(k),

(17)

k i=1

being the mode of the log H-posterior, Ak (k) its hessian at k, and k = (k, k).

Computational tractability is important when one considers many models or the integrand is

expensive to evaluate, e.g. in our kernel density examples it requires O(n2) operations. Further, the

availability of a closed-form expression facilitates its theoretical study (Section 4). See Kass et al.

(1990) for results on the validity of Laplace approximations. We do not undertake such a study,

instead we prove our results directly for the approximation (16) that we actually use for inference. Although we motivate our methodology from a Bayesian standpoint, we note that H~k(y) can be

viewed as Bayesian-inspired information criteria analogous to the BIC (Schwarz, 1978). Specifically,

in regular settings where Ak is of order 1/n, one could take the leading terms in log H~k(y) to obtain

-

n

H

(yi;

fk(·;

~k))

-

dk 2

log(n)

+

log

k

(~j )

i=1

as a model selection criterion. This expression is analogous to the BIC, except for the log prior density

term, which converges to a constant for any k bounded away from 0 and infinity. The log prior term

can play a relevant role however when considering non-local priors where k(k) can be equal to 0 for

certain k, see Section 4.

3.4 Two-step inference and model averaging

As discussed the H-posterior (13) asymptotically recovers the parameters (k, k) minimising Fisher's divergence, whereas the general Bayesian posterior (2) recovers the parameters minimising the expected loss ~k = arg minkk k(y; k, k)g(y)dy, for a given k.
We adopt the pragmatic view that, while one may consider the H-posterior to choose a model k and learn the associated hyper-parameter k, after said choice one may want to obtain standard inference under the selected model. That is, one desires to learn

arg min
k k

k(y; k, k)g(y)dy.

(18)

13

For example, suppose that the H-score selects a proper probability model (e.g. the Gaussian model). One may then wish that inference collapses to standard Bayesian inference under that model. This is easily achieved with a two-step procedure. First, one uses (15) to select k^ and (13) to estimate ^k^. Second, given (k^, ^k^) one uses the general Bayesian posterior (2) for k^. A further alternative to selecting a single model is to mimic Bayesian model averaging (Hoeting et al., 1999), where the estimates under each model are weighted according to the posterior probabilities in (15).

4 Consistency of H-score model selection

We now state Theorem 1, our main result that (16) consistently selects the model closest in Fisher's divergence to the data-generating g. When several models attain the same minimum, as may happen when considering nested models, then (16) selects that of smallest dimension. The proof does not require that the Hyv¨arinen score is asymptotically normal, which holds under the conditions in Theorem 2 of Dawid et al. (2016), but simply the n-consistency proven in Proposition 1.
Theorem 1 mirrors standard results for Bayes factors (Theorem 1 in Dawid (1999)), the main difference being that it involves Fisher's rather than Kullback-Leibler divergence. As discussed after the theorem, when the optimal hyper-parameter occurs at the boundary the model selection consistency provided by Theorem 1 may not hold, unless one uses a suitable adjustment. Before stating the theorem, we interpret its implications. Part (i) considers a situation where one compares two models l and k such that the former is closer to g in Fisher's divergence. Then B~k(Hl ) converges to 0 at an exponential rate in n. Part (ii) considers that both models attain the same Fisher's divergence, for example for Tukey's loss and the Gaussian model when the data are truly Normal. Then, B~k(Hl ) favours the smaller model at a polynomial rate in n.

Theorem 1. Assume Conditions A1-A4, and let k = (k, k) be as in Proposition 1.

(i) Suppose that Eg[H(y; fl(·; l))] < Eg[H(y; fk(·; k))]. Then

1 n

log

B~k(Hl )

=

Eg [H (y;

fl(·;

l))]

-

Eg [H (y;

fk(·;

k))]

+

op(1).

(ii) Suppose that Eg[H(y; fl(·; l))] = Eg[H(y; fk(·; k))]. Then

log

B~k(Hl )

=

dl

- 2

dk

log(n)

+

Op(1).

14

The result requires Conditions A1-A4 given and discussed in Section A.1.2. Conditions A1-A3 are mild and also summarised before Proposition 1, whereas A4 imposes a Lipschitz condition on the H-score and its Hessian. In fact, under extended Conditions A1-A7 it is possible to prove that in the nested case the criterion of (Matsuda et al., 2019) does not guarantee consistent model selection, see Corollary A.1. This result is analogous to predictive criteria such as cross-validation or Akaike's information criterion not leading to consistent model selection, see Shao (1997).
As an important remark, the slower rate in Part (ii) is due to Condition A1 that the prior density l(l) > 0 at the optimal l, where l is the larger model. This defines a so-called local prior, in contrast to non-local priors (Johnson and Rossell, 2012; Rossell and Telesca, 2017) which place zero density at the value l\k where the more complicated model l recovers the simpler model k. Since non-local priors violate A1, Corollary A.1 extends Theorem 1 to show that non-local priors attain faster rates in Part (ii), while maintaining the exponential rates in Part (i). We will demonstrate that this improvement can have non-negligible practical implications in Section 5.2.
Another practically-relevant remark is that A3 requires a finite expected Hessian near the optimal (k, k), which can be problematic in certain settings. For example, in Tukey's loss if data are truly Gaussian then 2 = , which leads to an infinite Hessian. For Tukey's loss the problem can be avoided by reparameterising 2 = 1/22, for which the Hessian is finite, but more generally such a reparameterisation may not be obvious or not exist. These cases provide a further use for non-local priors. By Corollary A.1, one may set a non-local prior that vanishes sufficiently fast at the boundary (basically, a faster rate than that at which the Hessian diverges) to attain model selection consistency.
5 Robust Regression with Tukey's loss
We revisit the robust regression in Section 2.2, where one considers a Gaussian model and the improper model defined by Tukey's loss. Section 5.1 illustrates that when the data contain outliers, the Hscore chooses Tukey's model and learns its cut-off hyper-parameter in a manner that leads to robust estimation. Section 5.2 shows the opposite situation, where data are truly Gaussian, and the benefits of setting a non-local prior on Tukey's cut-off hyper-parameter to improve the model selection consistency rate. Finally, Section 5.3 shows two gene expression datasets, one exhibiting Gaussian behavior and the other thicker tails. We compare our results to the SMIC (Matsuda et al., 2019) which, despite not being designed to compare improper models, to our knowledge is the only existing criterion that can
15

be used for this task.

The H-scores for the squared loss ( 1) and Tukey's loss ( 2) (see Section A.2.1) are

H1(y; f (·; x, 1)) =

n

-

2 2

+

(yi

- xTi )2 4

(19)

i=1

H2(y; f (·; x, 2, 2)) =
|yi-xTi |2

(yi

- xTi ) 2

-

2(yi - xTi )3 224

+

(yi

- xTi )5 426

2
-

2

1 2

-

6(yi - xTi )2 224

+

5(yi - xTi )4 426

.

(20)

Note that minimising H2 in (20) has a trivial degenerate solution when (, , 2) are such that only one

observation satisfies yi - xTi   2, and yi = xTi  for that observation. To avoid such solutions, we

define a constraint on the parameters related to the notion of the breakdown point of an estimator, i.e.

the number of observations that can be perturbed without causing arbitrary changes to the estimator

(Rousseeuw and Yohai, 1984). The constraint is to focus attention on (, , 2) such that

1 (2, 2)

n i=1



yi

- xTi 



,

2



n 2

- p,

(21)

where 

yi -xTi 



,

2

=

2(yi; xi,

2,

2)

-

1 2

log(22),

2 is as in (4), and p = dim(xi). See Section

A.3.2 for the derivation and further discussion. Further, to satisfy the differentiability conditions of

Theorem 1 and enable the use of standard second order optimisation software, we implemented a

differentiable approximation to the indicator function in Tukey's loss (see Section A.3.3).

5.1 The marginal H-score in 

Our first example illustrates the properties of the H-score for calibrating the robustness-efficiency

trade-off, in a setting where the data contain outliers. We simulated n = 500 observations from the

data-generating g(y) = 0.9N (y; 0, 1)+0.1N (y; 5, 3). Related to our breakdown point discussion above,

in such a scenario one wishes to estimate the parameters of the larger component (uncontaminated

data), in a manner that is robust to the presence of data from the smaller component (outliers). We

compare the estimation from the Gaussian model f1, which is correctly specified for 90% of the data,

with that of the robust improper model arising from Tukey's loss f2. A first question of interest is studying the ability of the H-score to learn the cutoff hyper-parameter

2. To this end, we measured the evidence for different 2 provided by the marginal H-score

n

H2(y; 2) = 2(2) exp - H(yi; f2(·; 2, 2)) d2.

(22)

i=1

16

We set the priors 2  IG(0.1, 0.1) and µ|2  N (0, 52) and consider a grid 2  {1, 1.5, 2, . . . , 10}. The prior on (µ, 2) is further constrained by the breakdown condition (21).
(1 - )N (0, 1) + N (5, 32) Gaussian Tukey's-loss

~H-score 100 200 300 400 -0.50 -0.25 0.00 0.25 0.50
^µ Density 0.0 0.1 0.2 0.3 0.4

2

4

6

8

10

2

-2

0

2

4

6

8

Observations

Figure 2: Left: The integrated H-score H2(y; 2) for varying values of 2 (black) and box plots of µ^2(2) across B = 1000 samples from g(y). The grey line indicates the truly zero mean of the uncontaminated component. Right: Histogram of n = 500 observations from g(y) = 0.9N (y; 0, 1) + 0.1N (y; 5, 3) and fitted densities of Tukey's loss with ^2 = 5 and the Gaussian model (the height of Tukey's loss was set to match the mode of g(y)).

The left panel in Figure 2 shows H2(y; 2) (solid line), which is highest for the estimate ^2 = 5, along with box-plots for µ^(2), the posterior mean of µ for each fixed 2 (obtained from 1, 000 repeat samples from g(y)). For ^2 = 5 this estimator is on average close to the uncontaminated data mean, and exhibits small variance. For small 2 the average estimate is similar, i.e. is robust to the outliers, but there is larger variance due to a greater proportion of the data being above Tukey's loss threshold. For large 2, µ^(2) deviates from zero, indicating lack of robustness to outliers, and in the limit as 2 grows, µ^(2) recovers the sample mean.
The right panel of Figure 2 is a histogram of the observed data, the data-generating g(y) and the estimated Gaussian and improper Tukey-based model at ^2 = 5. The latter provides a better description of the central component of the data, in the sense of capturing the log-gradient of g(y) around the mode, and excludes the outliers.
5.2 Non-local priors and model selection consistency
We demonstrate the selection consistency (Theorem 1) when data are truly Gaussian, and that setting a non-local prior on the cutoff hyper-parameter 2 speeds up this selection (Corollary A.1). We
17

simulated 100 independent data sets of sizes n = 100, 1, 000, 10, 000 and 100, 000 from the data-

generating g(yi) = N yi; xTi , 2 , where the first entry in xi  R6 corresponds to the intercept and the remaining entries are Gaussian with unit variances and 0.5 pairwise covariances,  = (0, 0.5, 1, 1.5, 0, 0)

and 2 = 1. We set priors 2  IG(2.01, 0.5) and |2  N 0, 52I for both models.

Recall that Tukey's loss collapses to the Gaussian model for 2 = , and otherwise adds certain

flexibility by allowing one to consider an improper model. If this extra flexibility is not needed,

following Occam's razor one wants to choose the Gaussian model. While Theorem 1 guarantees this to

occur asymptotically, our experiments show that setting a local prior on 2 leads to poor performance,

even for n = 100, 000. Specifically, we compare a (local) half-Gaussian prior 2LP(2)  120N (2; 0, 1) where 2 = 1/22 and a (non-local) inverse-gamma prior 2NLP(2) = IG(2; a0, b0). We set default (a0, b0) = (4.35, 1.56) chosen to assign prior probability P (2  (1, 3)) = 0.95, i.e. the range between

1- and 3- that cuts offs 0.3% of Gaussian data outside the 3- region, and includes the 68.3% of

Gaussian data within 1-. See Section A.3.4 for further discussion of these priors. By Corollary A.2,

the

log-H-Bayes

factor

under

the

non-local

prior

should

favor

the

Gaussian

model

at

least

at

a

 n

rate, in contrast to the local prior's log(n) rate.

Figure 3 compares the SMIC with our integrated H-score under the local and non-local priors.

Score differences are plotted such that negative values indicate correctly selecting the Gaussian model.

Firstly, there is no evidence of SMIC being consistent as n grows, even for n = 100, 000 the wrong

model was selected 11% of the time. The H-score under the local prior has a decreasing median in n,

but exhibits heavy tails and even for n = 100, 000 it also failed to select the Gaussian model 11% of

the time. Under the non-local prior, already for n = 1, 000 the correct decision was made 99% of the

time. These experiments illustrate the benefits of non-local priors to penalise parameter values near

the boundary (1/22 = 0, in this example). Recall also that, as discussed in Section 4, in general in such situations a local prior need not even attain consistency.

5.3 Real datasets
We considered two gene expression data sets from Rossell and Rubio (2018). In the first, the data are well-approximated by a Gaussian distribution, whereas the second exhibits thicker tails. In both examples we used the H-score to compare the Gaussian model ( 1) and Tukey's loss ( 2), using the priors discussed in Section 5.2: 2  IG(2.01, 0.5) and |2  N 0, 52I for both models, and the non-local inverse-gamma prior 2NLP(2) = IG(2; 4.35, 1.56).

18

SMIC

H-score - LP

H-score - NLP

0

10 15 20

10 15 20

-100

Score Difference

5

Score Difference

5

Score Difference

0

0

-200

-300

-15 -10 -5

-15 -10 -5

n = 102 n = 103 n = 104 n = 105

n = 102 n = 103 n = 104 n = 105

n = 102 n = 103 n = 104 n = 105

Figure 3: Selecting between the Gaussian model and Tukey's loss across N = 100 data sets generated from a Gaussian model. SMIC (SMIC1(y) - SMIC2(y)) vs integrated H-score with local and non-local priors (log H~2(y) - log H~1(y)) with negative values correctly selecting the Gaussian model.

5.3.1 TGF- data
The dataset from Calon et al. (2012) concerns gene expression data for n = 262 colon cancer patients. Previous work (Rossell and Telesca, 2017; Rossell and Rubio, 2018) focused on selecting genes that have an effect on the expression levels of TGF-, a gene known to play an important role in colon cancer progression. Instead, we study the relation between TGF- and the 7 genes (listed in Section A.4.1) that appear in the `TGF- 1 pathway' according to the KEGGREST package in R (Tenenbaum, 2016), so that p = 8 after including the intercept.
The top panels in Figure 4 summarises the results. The integrated H-score for the Gaussian model was H~1(y) = 272.88 and that for Tukey's loss H~2(y) = 233.90, providing strong evidence for the Gaussian model. This is in agreement with the SMIC (SMIC1(y) = -283.93 and SMIC2(y) = -277.55, where minimisation is desired) and results in Rossell and Rubio (2018), who found evidence for Gaussian over (thicker) Laplace tails. The left panel shows the fitted densities which, in conjunction with the Q-Q Normal plots in Figure A.2, show that the residual distribution is well-approximated by a Gaussian. The right panel shows the squared difference between the H-posterior mean parameter estimates of each j under Tukey's model minus that under the Gaussian, and the differences between their sampling variances (estimated via bootstrap, dashed gray line). Both models returned very
19

(Pseudo) Density 0.0 0.1 0.2 0.3 0.4 0.5

Difference: Tukey's - Gaussian

TGF- data: ~ = 5.14, H~2(y1:262) = 233.9
Tukey's Loss N (0, 1)

0.04

Squared parameter difference Variance of Tukey's estimate minus variance of Gaussian estimate

0.02

0.00

-4

-2

0

2

(y - X^)/^

DLD data:  = 4.12, H~2(y1:192) = 783.94

0

1

2

3

4

5

6

7

Parameter index

1.0

0.5

Difference: Tukey's - Gaussian

(Pseudo) Density 0.0 0.1 0.2 0.3 0.4

0.0

-0.5

-15 -10

-5

0

5

10

(y - X^)/^

0

5

10

15

Parameter index

Figure 4: Top: TGF- data, where the H-score selected the Gaussian model. Bottom: DLD data, where the H-score selected Tukey's loss. Left: fitted Tukey-based density to the residuals. Right: Squared difference between the H-posterior mean estimates of each j under Tukey's model minus that under the Gaussian (solid black line), and difference between their variances (estimated with B = 500 bootstrap re-samples)

similar parameter estimates, but the Gaussian had smaller variance for all parameters. Altogether, these results strongly support that the Gaussian model should be selected over Tukey's.
5.3.2 DLD dataset
We consider an RNA-sequencing data set from Yuan et al. (2016) measuring gene expression for n = 192 patients with different types of cancer. Rossell and Rubio (2018) studied the impact of 57 predictors on the expression of DLD, a gene that can perform several functions such as metabolism regulation. To illustrate our methodology, we selected the 15 variables with the 5 highest loadings in the first 3 principal components, and used the integrated H-score to choose between the Gaussian and

20

Tukey's loss. Section A.4.1 lists the selected variables. The bottom panels of Figure 4 summarise the results. The H-score strongly supported Tukey's loss
(H~1(y) = 155.57 for the Gaussian model, H~2(y) = 783.94 for Tukey's), with H-posterior mean estimate ^2 = 4.12. Indeed, the bottom left panel indicates that the residuals have thicker-than-Gaussian tails, see also the Q-Q Normal residual plot in Figure A.2. The bottom right panel illustrates two things. Firstly, the estimated coefficients of 6 of the 16 predictors differ quite considerably between the Gaussian model and Tukey's loss (solid line). Second, the latter often have smaller variance (estimated via bootstrap). Both observations align with the presence of thicker-than-normal tails, which can cause parameter estimation biases and inflated variance. Notably, the H-score agrees with Rossell and Rubio (2018), who selected Laplace over Gaussian tails, but disagrees with the SMIC (SMIC1(y1:192) = -145.72 and SMIC2(y1:192) = -141.67, where minimisation is desired). We speculate that the SMIC results may have been affected by outliers, which could cause instability in the SMIC asymptotic bias estimation.

6 Kernel density estimation

We revisit the kernel density estimate g^h(·) from Section 2.2.2. The associated loss is

(yi; y, h, w) = -w log g^h(yi).

(23)

This loss has two hyper-parameters  = (h, w), where w > 0. As discussed in Section 3.2, standard Bayesian inference on the bandwidth h is not possible, since the loss does not define a proper probability model for the observed data y. We also included a tempering hyper-parameter w to illustrate how it can provide added flexibility and improve performance. A downside of including w however is that, when w = 1, the estimated density g^h,w(x) = exp{- (x; y, h, w)} for a future observation x is no longer a proper model, as it was when w = 1 (see Section 2.2.2), and its normalising constant is intractable. However, such one-dimensional integrals are easily approximated by numerical integration. We note that while kernel density estimation is a well-studied area, to the best of our knowledge the incorporation of a tempering parameter w has not been considered, and is hence a further innovation enabled by the Hyv¨arinen score.
The H-score associated to (23) and its derivation are in Section A.2.2. Analogously to Tukey's loss, we must take care to avoid certain cases with degenerate solutions where the in-sample Hyv¨arinen score attains negative infinity by setting h = 0. Such a mode can be avoided by lower-bounding h

21

away from 0, and for illustration in our examples we set a lower bound of n-1/5/100. Such a lower bound is justified as popular bandwidth plug-in methods are O(n-1/5) and dividing by a further 100 provides further flexibility. Our method therefore refines previous methods by learning h from the data, but at the same time ensures one does not drastically under-smooth. The estimated h^'s in the next section are all away from this lower bound.
6.1 Gaussian mixture implementations
We simulated data from four Gaussian mixture models considered by Marron and Wand (1992) (with two further examples provided in Section A.4.2). All scenarios consider a data-generating
J
g(y) = mjN (y; µj, j)
j=1
with parameters chosen as follows.
· Bimodal : J = 2 components, µ1 = -1.5, µ2 = 1.5, 1 = 2 = 1/2 and m1 = m2 = 0.5.
· Trimodal : J = 3 components, µ1 = -1.2, µ2 = 0, µ3 = 1.2, 1 = 3 = 3/5, 2 = 1/4, m1 = m3 = 0.45, and m2 = 0.1.
· Claw : J = 6 components, µ1 = 0, 1 = 1 and m1 = 0.5 then for j = 2, . . . , 6: µj = -2 + j/2, j = 0.1 and mj = 0.1.
· Skewed : J = 8 components, for j = 1, . . . , 8: j = (2/3)j-1, µj = 3(j - 1) and mj = 1/8.
We compared the H-score estimated density with fixed w = 1 to that when learning w from the data. We further compare against the default kernel estimate in R, which sets the bandwidth using Silverman's rule-of-thumb (Silverman, 1986), and a Bayesian non-parametric density estimate using a Dirichlet Process mixture of Gaussians (DPMM), implemented in R's dirichletprocess package (Ross and Markwick, 2018) using their default parameters. We sampled n = 1, 000 observations from each simulation setting above and standardised the data to zero mean and unit variance, as recommended for the dirichletprocess package. For the H-score kernel density estimate (KDE), we set the prior (h)  IG(1, 1)I(h  n-1/5/100) for the bandwidth parameter, reparameterised w = h + 1 +  to reduce the posterior dependence between (h, w), and set the prior ( | h)  N (; 0, 1)I(h + 1 +  > 0).
Figure 5 provides histograms of the standardised data and the estimated densities, while Table 1 shows Fisher's divergence of the fitted models to the data-generating g(y). The H-score density
22

bimodal

trimodal

Density 0.0 0.1 0.2 0.3 0.4

Density 0.0 0.2 0.4 0.6

Density 0.0 0.5 1.0 1.5

Density 0.0 0.2 0.4 0.6

-2

-1

0

1

2

x

claw

-2

-1

0

1

2

x

skewed

g(y) DPMM KDE (Silverman) KDE (H-posterior) KDE - w (H-posterior)

-3

-2

-1

0

1

2

3

x

-1

0

1

2

3

4

x

Figure 5: Density estimation for Gaussian mixture data. Histograms of observed data, standardised to 0 mean and unit variance, and estimated density by DPMM, R's density function with the bandwidth rule of Silverman (1986), the H-posterior estimate with no tempering (w = 1), and with tempering (w = 1).

estimate generally captured the modes better than the competing methods. This was particularly evident in the Claw data where the DPMM and R's kernel estimate missed the 5 modes, and to a lesser extent in the Trimodal and Skewed data sets. Lastly, in most examples the H-score estimate improved by learning the hyper-parameter w, relative to w = 1, suggesting that the tempering parameter can add useful flexibility.
7 Discussion
We considered a novel problem, that of selecting between probability models and possibly improper models defined via loss functions. Despite being non-standard, the latter can be naturally interpreted in terms of relative probabilities, and open an avenue to increase the flexibility at the disposal
23

Table 1: Fisher's divergence between the density estimates and the data-generating Gaussian mixtures in four simulation scenarios. The best performing method in each is highlighted in bold face

Bimodal Claw
Trimodal Skewed

KDE
1.03 13.77 0.26 21.34

DPMM
0.09 15.35 0.47 17.21

Hyv¨arinen KDE (w = 1) 0.26 3.39 0.19 10.55

Hyv¨arinen KDE (w = 1) 0.09 2.51 0.19 9.52

of the data analyst. These relative probabilities motivated our use of Fisher's divergence and the Hyv¨arinen score, which led to defining an associated H-posterior and H-Bayes factors to estimate hyper-parameters and model selection, respectively. We used Laplace approximations for computational convenience, and proved that one can recover the model that is closest to the data-generating truth in Fisher's divergence. If a proper model is chosen, one can use a two-step procedure to ensure that inference collapses to standard Bayesian methods. We illustrated how the method provides promising results in robust regression and kernel density estimation, providing a new strategy to address the robustness-efficiency trade-off for the former and enabling Bayesian inference in the latter.
Our asymptotic results mimic those for Bayesian model selection in proper models. There is however an important issue when considering hyper-parameters whose optimal value lies at the boundary, where consistency may fail for standard priors and it may be necessary to use non-local priors. As future work, one could attempt to improve the finite sample performance of the H-score using weighting procedures to calibrate its sampling distribution, e.g. as proposed by Dawid et al. (2016). Another interesting direction is to extend the robust regression examples to variable selection. Rossell and Rubio (2018) showed the importance of specifying a well-fitting error distribution to avoid non-negligible drops in statistical power, which is particularly important in high-dimensional cases where p > n. Considering robust improper models such as that stemming from Tukey's loss may improve the selection performance here.
We also provided a novel strategy to learn likelihood tempering hyper-parameters from the data, which could be combined with various tempered likelihood approaches (e.g. Gru¨nwald, 2012; Holmes and Walker, 2017; Miller and Dunson, 2018) to estimate this value for standard likelihoods. Yet another interesting future extension is to consider multivariate responses, for example in graphical or factor models where outliers can be hard to detect (e.g. Hall et al., 2005; Filzmoser et al., 2008)
24

and result in significantly worse inference. In summary, we believe there are many settings where the consideration of improper models can lead to fruitful research avenues.
Acknowledgements
JJ and DR were partially funded by the Ayudas Fundaci´on BBVA a Equipos de Investigaci´on Cientifica 2017 and Government of Spain's Plan Nacional PGC2018-101643-B-I00 grants. DR was also partially funded by the Europa Excelencia grant EUR2020-112096 and Ram´on y Cajal Fellowship RYC-201518544.
References
Basu, A., Harris, I. R., Hjort, N. L. and Jones, M. (1998), `Robust and efficient estimation by minimising a density power divergence', Biometrika 85(3), 549­559.
Beaton, A. E. and Tukey, J. W. (1974), `The fitting of power series, meaning polynomials, illustrated on band-spectroscopic data', Technometrics 16(2), 147­185.
Beaumont, M. A., Zhang, W. and Balding, D. J. (2002), `Approximate Bayesian computation in population genetics', Genetics 162(4), 2025­2035.
Belagiannis, V., Rupprecht, C., Carneiro, G. and Navab, N. (2015), Robust optimization for deep regression, in `Proceedings of the IEEE international conference on computer vision', pp. 2830­ 2838.
Bernardo, J. M. (1979), `Expected information as expected utility', the Annals of Statistics pp. 686­ 690.
Bissiri, P. G. and Walker, S. (2012), `On Bayesian learning via loss functions', Journal of statistical planning and inference 142(12), 3167­3173.
Bissiri, P., Holmes, C. and Walker, S. G. (2016), `A general framework for updating belief distributions', Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78(5), 1103­1130.
Black, M. J. and Rangarajan, A. (1996), `On the unification of line processes, outlier rejection, and robust statistics with applications in early vision', International journal of computer vision 19(1), 57­ 91.
25

Bowman, A. W. (1984), `An alternative method of cross-validation for the smoothing of density estimates', Biometrika 71(2), 353­360.
Box, G. E. (1953), `Non-normality and tests on variances', Biometrika 40(3/4), 318­335. Breiman, L. et al. (2001), `Statistical modeling: The two cultures (with comments and a rejoinder by
the author)', Statistical science 16(3), 199­231. Calon, A., Espinet, E., Palomo-Ponce, S., Tauriello, D. V., Iglesias, M., C´espedes, M. V., Sevillano,
M., Nadal, C., Jung, P., Zhang, X. H.-F. et al. (2012), `Dependency of colorectal cancer on a TGF--driven program in stromal cells for metastasis initiation', Cancer cell 22(5), 571­584. Carpenter, B., Gelman, A., Hoffman, M., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M. A., Guo, J., Li, P. and Riddell, A. (2016), `Stan: A probabilistic programming language', Journal of Statistical Software 20. Catoni, O. (2007), Pac-Bayesian supervised classification: the thermodynamics of statistical learning, IMS. Chang, L., Roberts, S. and Welsh, A. (2018), `Robust lasso regression using Tukey's biweight criterion', Technometrics 60(1), 36­47. Chernozhukov, V. and Hong, H. (2003), `An MCMC approach to classical estimation', Journal of Econometrics 115(2), 293­346. Dawid, A. (1999), The trouble with Bayes factors, Technical report, University College London. Dawid, A. P. and Musio, M. (2015), `Bayesian model selection based on proper scoring rules', Bayesian analysis 10(2), 479­499. Dawid, A. P., Musio, M. and Ventura, L. (2016), `Minimum scoring rule inference', Scandinavian Journal of Statistics 43(1), 123­138. Efron, B. (2020), `Prediction, estimation, and attribution', Journal of the American Statistical Association 115(530), 636­655. Ferentios, K. (1982), `On tcebycheff's type inequalities', Trabajos de Estadistica y de Investigacion Operativa 33(1), 125. Filzmoser, P., Maronna, R. and Werner, M. (2008), `Outlier identification in high dimensions', Computational Statistics & Data Analysis 52(3), 1694­1711. Good, I. J. (1952), `Rational decisions', Journal of the Royal Statistical Society, Series B 14, 107­114.
26

Gru¨nwald, P. (2012), The safe Bayesian, in `International Conference on Algorithmic Learning Theory', Springer, pp. 169­183.
Guedj, B. (2019), `A primer on PAC-Bayesian learning', arXiv preprint arXiv:1901.05353 . Habbema, J., JDF, H., Van den Broek, K. et al. (1974), `A stepwise discriminant analysis program
using density estimation.'. Hall, P., Marron, J. and Neeman, A. (2005), `Geometric representation of high dimension, low sample
size data', Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67(3), 427­ 444. Hoeting, J. A., Madigan, D., Raftery, A. E. and Volinsky, C. T. (1999), `Bayesian model averaging: a tutorial', Statistical science pp. 382­401. Hoffman, M. D. and Gelman, A. (2014), `The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.', Journal of Machine Learning Research 15(1), 1593­1623. Holmes, C. and Walker, S. (2017), `Assigning a value to a power likelihood in a general Bayesian model', Biometrika 104(2), 497­503. Huber, P. J. and Ronchetti, E. (1981), `Robust statistics, series in probability and mathematical statistics'. Hyv¨arinen, A. (2005), `Estimation of non-normalized statistical models by score matching', Journal of Machine Learning Research 6(Apr), 695­709. Johnson, V. E. and Rossell, D. (2012), `Bayesian model selection in high-dimensional settings', Journal of the American Statistical Association 107(498), 649­660. Kass, R. E., Tierney, L. and Kadane, J. B. (1990), `The validity of posterior expansions based on Laplace's method', Essays in Honor of George Barnard, eds. S. Geisser, J. S. Hodges, S. J. Press and A. Zellner, Amersterdam: North-Hollans pp. 473­488. Li, Q., Chen, H. and Zhu, F. (2021), `Robust estimation for Poisson integer-valued GARCH models using a new hybrid loss', Journal of Systems Science and Complexity pp. 1­19. Llorente, F., Martino, L., Delgado, D. and Lopez-Santiago, J. (2020), `Marginal likelihood computation for model selection and hypothesis testing: an extensive review', arXiv 2005.08334, 1­91. Lyddon, S., Holmes, C. and Walker, S. (2019), `General Bayesian updating and the loss-likelihood bootstrap', Biometrika 106(2), 465­478.
27

Marron, J. S. and Wand, M. P. (1992), `Exact mean integrated squared error', The Annals of Statistics pp. 712­736.
Matsuda, T., Uehara, M. and Hyvarinen, A. (2019), `Information criteria for non-normalized models', arXiv preprint arXiv:1905.05976 .
Miller, J. W. and Dunson, D. B. (2018), `Robust Bayesian inference via coarsening', Journal of the American Statistical Association pp. 1­13.
Molenberghs, G. and Verbeke, G. (2006), Models for discrete longitudinal data, Springer Science & Business Media.
Riani, M., Cerioli, A. and Torti, F. (2014), `On consistency factors and efficiency of robust Sestimators', Test 23(2), 356­387.
Robert, C. P. (2016), Approximate Bayesian computation: A survey on recent results, in `Monte Carlo and Quasi-Monte Carlo Methods', Springer, pp. 185­205.
Robert, P. (1976), `On the choice of smoothing parameters for Parzen estimators of probability density functions', IEEE Transactions on Computers 25(11), 1175­1179.
Ross, G. J. and Markwick, D. (2018), `dirichletprocess: An R package for fitting complex Bayesian nonparametric models'.
Rossell, D. (2021), `Concentration of posterior model probabilities and normalized L0 criteria', Bayesian Analysis (in press), 1­27.
Rossell, D. and Rubio, F. J. (2018), `Tractable Bayesian variable selection: beyond normality', Journal of the American Statistical Association 113(524), 1742­1758.
Rossell, D. and Telesca, D. (2017), `Nonlocal priors for high-dimensional estimation', Journal of the American Statistical Association 112(517), 254­265.
Rousseeuw, P. J. and Leroy, A. M. (2005), Robust regression and outlier detection, Vol. 589, John wiley & sons.
Rousseeuw, P. and Yohai, V. (1984), Robust regression by means of S-estimators, in `Robust and nonlinear time series analysis', Springer, pp. 256­272.
Rudemo, M. (1982), `Empirical choice of histograms and kernel density estimators', Scandinavian Journal of Statistics pp. 65­78.
Schwarz, G. (1978), `Estimating the dimension of a model', The Annals of Statistics 6(2), 461­464.
28

Shao, J. (1997), `An asymptotic theory for linear model selection', Statistica sinica pp. 221­242. Shao, S., Jacob, P. E., Ding, J. and Tarokh, V. (2019), `Bayesian model comparison with the Hyv¨arinen
score: computation and consistency', Journal of the American Statistical Association pp. 1­24. Silverman, B. W. (1986), Density estimation for statistics and data analysis, Vol. 26, CRC press. Sinova, B. and Van Aelst, S. (2016), Tukey's biweight loss function for fuzzy set-valued M-estimators
of location, in `International Conference on Soft Methods in Probability and Statistics', Springer, pp. 447­454. Tenenbaum, D. (2016), `Keggrest: Client-side rest access to kegg', R package version 1(1). Tukey, J. W. (1960), `A survey of sampling from contaminated distributions', Contributions to probability and statistics pp. 448­485. Van der Vaart, A. W. (2000), Asymptotic statistics, Vol. 3, Cambridge university press. Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020), `A tuning-free robust and efficient approach to high-dimensional regression', Journal of the American Statistical Association pp. 1­44. Yuan, T., Huang, X., Woodcock, M., Du, M., Dittmar, R., Wang, Y., Tsai, S., Kohli, M., Boardman, L., Patel, T. et al. (2016), `Plasma extracellular rna profiles in healthy and cancer patients', Scientific reports 6(1), 1­11.
29

A Supplementary Material
Section A.1 contains the proofs of the theoretical results associated with Sections 3 and 4 of the main paper. Section A.2 provides the H-score for the squared, Tukey's and the kernel density estimation losses. Section A.3 describes implementation details for Tukey's loss and kernel density examples, including smooth approximations that enable the use of second order optimisation and sampling methods, and how to set a non-local prior on Tukey's cut-off hyper-parameter 2. Finally, Section A.4 provides additional empirical results that complement those presented in the main paper.
A.1 Proofs
The proofs are structured as follows. First we establish our notation (Section A.1.1) in full, before stating and discussing the conditions for the theorems in Section A.1.2. Sections A.1.3 and A.1.4 provide the proof of Proposition 1, for ease of presentation we split the proof between two separate results. Section A.1.3 provides Lemma A.1 which proves consistency, op(1), of the maximum H-posterior estimates, while Section A.1.4 provides Proposition A.1 extending this to Op(1/n) convergence. This is then used in Section A.1.5 to prove our main result that the Laplace approximation to the H-Bayes factor provides consistent selection amongst models and losses to that which minimises Fisher's divergence to the data generating distribution g(y). Further, Section A.1.6 contains Corollary A.1 which proves that the SMIC of Matsuda et al. (2019) does not provide consistent model/loss selection between nested models, Corollary A.1 in Section A.1.7 extends the H-score consistency results to situations where non-local priors are placed on the additional parameters of nested models, and lastly Corollary A.2 extends this to the special case of the inverse-gamma non-local prior applied when selecting between a Gaussian model and Tukey's loss in Section 5.
A.1.1 Notation
Consider models k and l with possibly improper densities fj(y; j, j)  exp {- j(y; j, j)}, for j  {k, l}, where j already includes wj where applicable. To ease notation let j = (j, j) and call its dimension dj. Without loss of generality assume l is at least as complicated as k, i.e. that dl  dk. We obtain H-Bayes Factor rates under the assumption that y  g. In so doing
j := arg min Eg [H(y; fj(·; j)] .
j
are the parameter minimising Fisher's divergence from model fj to g(y).
30

Given sample y  g, define the in-sample H-score minimising estimate, ^j, and given and parameter

prior (j) the penalised maximum H-posterior estimate, ~j, as

n

n

^j := arg min H(yi; fj(·; j), ~j := arg min H(yi; fj(·; j) - log j(j),

j

i=1

j

i=1

where throughout we suppress the dependence on n to avoid cumbersome notation. The empirical

and expected Hessian matrix for model j are Aj(j) and Aj (j) respectively defined as

n

Aj (j ) : = 2j

H(yi; fj(·; j)) - log j(j) ,

i=1

Aj (j) : = 2j Eg [H(yi; fj(·; j))] ,

(24)

The Laplace approximation to the H-Bayes factor is then given by

B~k(Hl )

:=

H~k(y) H~l(y)

=

(2

)

dk

-dl 2

|Al |Ak

(~l) |1/2 (~k) |1/2

k l

(~k) (~l)

e

n i=1

H

(yi

;fl(·;~l

))-

n i=1

H

(yi

;fk

(·;~k

))

(25)

For some of the results below it is sometimes easier to consider the estimating equations associated

with minimising the Hyv¨arinen score. Define the empirical and expected vectors of first derivatives,

j(j) and j (j) respectively, as

n
j(j) : = j H(yi; fj(·; j), j(j) := j log j(j)
i=1
~ j(j) : = j(j) + j(j), j (j) := j Eg [H(y; fj(·; j)] .

A.1.2 Technical Conditions
Next, we list the technical conditions required to prove the consistency of the H-Bayes factor to the model minimising Fisher's divergence, Theorem 1.
A1. Standard regularity assumptions on the models j = l, k and their priors
­ The parameter and hyperparameter spaces j × j are compact. ­ The priors are continuous in j, twice continuously differentiable and j(j) > 0 j. ­ Model densities fj(y; j)  exp {- j(y; j)}, are twice continuously differentiable in y,
ensuring that their Hyv¨arinen score, H(y; fj(·; j), is continuous. ­ The variance of the first derivatives of the Hyv¨arinen score VG [j(j)] = VG j H(y; fj(·; j) <
.

31

­ The Hyv¨arinen scores, H(y; fj(·; j), are themselves twice continuously differentiable in parameter j.
­ These exists unique j and ~j for all n such that the first derivatives of the empirical and expected Hyv¨arinen score, ~ j(~j) = j (j) = 0.
A2. For model j = l, k, there exists a function bj(·; z) : Rdj  R such that |H(z; fj(·; j))| < bj(j; z) for all z where bj(j; z)dj < .

A3. For model j = l, k, the expected Hessian of the Hyv¨arinen scores (24) is such that

0 < |Aj (j)| < ,

and there exist constant, MA and A > 0

0 < MA



|

1 n

Aj (j )|,

for

all

j

such

that||j - j||2

< ,

with probability tending to 1 as n  .

A4. For model j = l, k there exist functions mH : R  R0 and mA(·) : R  R0 with E [mH (·)] <  and E [mA(·)] <  so that the following Lipschitz conditions hold

H(z; fj(·; j(a))) - H(z; fj(·; j(b)))  mH (z)

j(a) - j(b)

,
2

A(j1)(j(a)) - A(j1)(j(b))

 mA(z)
2

j(1) - j(b)

,
2

j(a), j(b), j(1), j(b),

for all z  R, where the notation A(j1)(·) emphasises that only one observation, z, is involved.

These conditions are weak and standard. A1 constitute standard assumptions required to ensure the Hyv¨arinen score applied to model/loss fj/ j, j = l, k is itself continuous and has continuous first and second derivatives (in parameters, e.g. condition S1 Matsuda et al. (2019)), as well as a ensuring the existence and uniqueness of a Hyv¨arinen score minimising parameters and that its first derivative has finite variance. A2 requires the existence of an function dominating the Hyv¨arinen score applied to loss j = k, l. Note that e.g. for Tukey's loss the improper-model itself is not dominated by an integrable function, but both the first and second log-derivatives vanish above the threshold and therefore the Hyv¨arinen score applied to Tukey's loss is dominated by a integrable function. A3 ensures the expected Hessian is positive and finite at the limiting parameter j (Molenberghs and Verbeke, 2006, e.g.), and that within a neighborhood of j the finite sample average Hessian is also

32

positive and finite with probability tending to 1. This allows us to deploy a reverse Lipschitz type inequality to bound differences between parameters using differences in the Hyv¨arinen score's gradient evaluated at those parameters. A4 then explicitly imposes Lipschitz bounds on the Hyv¨arinen score applied to j and its Hessian in order to use the parameter convergence of Proposition 1 to prove the required functions convergence to prove Theorem 1 (similarly to the assumption of (Van der Vaart, 2000, Theorem 5.21, 5.23)). In fact these bounds are loser than are required as we only need the Lipschitz to hold in the region of . One could further use first order Taylor expansions to provide such Lipschitz bounds which would require the bounding of the first and third derivatives of the Hyv¨arinen score applied to j which are standard assumptions (e.g. condition S3 Matsuda et al. (2019), A6 of Molenberghs and Verbeke (2006)). These however, result in slightly more specific conditions and therefore we considered the more general conditions for brevity.

A.1.3 Lemma A.1 (Hyv¨arinen score parameter consistency)

Before proving Theorem 1 it is useful to first prove Proposition 1, providing the consistency of the Hyv¨arinen score minimising parameters estimates to the Fisher's divergence minimising parameters. For clarity of presentation here we split Proposition 1 into Lemma A.1 proving op(1) and Proposition A.1 proving Op(1/n). Here we first prove Lemma A.1. We note Hyv¨arinen (2005) (Corollary 3) provided a similar result except this further required that g = fj(·; j) which we do not require.

Lemma A.1 (Hyv¨arinen score parameter consistency). Assume Conditions A1-A2. Given a sample

y  g, then as n   the maximum H-posterior parameters of model j have the following asymptotic

behaviour

~j - j 2 = op(1),

(26)

where || · ||2 is the L2-norm.

Proof. Firstly, ||~j - ^j||2 = op(1) as by A1 j(j) > 0 for all j. Then, Theorem 5.7 of Van der Vaart (2000) provides general conditions for ||^j - j|| = op(1) of
any M -estimator, of which the the in-sample Hyv¨arinen score is an example. As is discussed, (Van der

Vaart, 2000, p. 46) sufficient conditions for op(1) convergence are, compactness of the set j × j

(A1), uniqueness of j (A1), continuity of j  H(z; fj(·; j)) for all z (A1), and the dominance

of H(z; fj(·; j)) by an integrable function (A2). As a result, under our conditions

~j - j

=
2

op(1).

33

A.1.4 Proposition 1 (Hyv¨arinen score parameter consistency at rate Op(1/n))

To complete the proof of Proposition 1 we extend Lemma A.1 to establish that under Condition A3 also, the convergence of the Hyv¨arinen score minimising parameters happens at rate Op(1/n).

Proposition A.1 (Hyv¨arinen score parameter consistency at rate Op(1/n)). Assume Conditions A1A3. Given a sample y  g, then as n   the maximum H-posterior parameters of model j have the

following asymptotic behaviour

~j - j 2 = Op(1/n),

(27)

where || · ||2 is the L2-norm.

Proof. We achieve this by first proving that the gradient of the penalised empirical Hyv¨arinen loss

function

1 n

~ j

(j

)

converges

to

the

gradient

of

the

expected

Hyv¨arinen

loss

function

j (j)

at

rate

Op(1/n) (Step 1). We then use this and A3 on the Hessian matrix of the Hyv¨arinen score to extend

the Op(1/n) convergence of the gradients to that of the parameters (Step 2).

Step 1: Proof that

1 n

~ j

(j

)

-

j (j

)

= Op(1/n)
2

To do so firstly note that by the triangle inequality of norm || · ||2

P

 n

1 n

j

(j

)

-

1 n

j

(j

)

-

j (j)

>M
2

P

 n

1 n

j

(j

)

-

j (j

)

+
2

 n

1 n

j

(j

)

>M
2

for any M > 0 and under the assumption that j(j) > 0 (A1). Now for Op(1/n) convergence we are

interested in the behaviour of this probability for sufficiently large n > N and therefore,

P

 n

1 n

j

(j

)

-

j (j

)

+
2

 n

1 n

j

(j

)

>M
2

=P

 n

1 n

j

(j

)

-

j (j

)

>M-
2

1 n

j

(j

)

2

P

 n

1 n

j

(j

)

-

j (j

)

> M (j)
2

n  N,

where M (j) := M -

1 N

j

(j

)

.

Next

note

that

vector

valued,

 Eg n
 VG n

1 n

j

(j

)

-

j (j

)

1 n

j

(j

)

-

j (j

)

=0 = VG [j(j)] ,

34

where VG [j(j)] <  (A1). Then we can apply the L2-norm vector extension of Chebyshev's

 inequality (Ferentios, 1982) to vector valued Xn = n

1 n

j

(j

)

-

j

(j

)

which provides that

P

 n

1 n

j

(j

)

-

j (j

)

> M (j) <
2

VG [j(j)] (M (j))2

which proves that

P

 n

1 n

~ j

(j

)

-

j (j

)

>M <
2

VG (M

[j (j (j ))2

)]

,

n > N.

Lastly, we note that

VG[j (j )] (M (j ))2

can

be

made

arbitrarily

small

by

making

M

arbitrarily

big

and

under

the assumption that j(j) > 0 (A1) and therefore

1 n

~ j

(j

)

-

j (j

)

= Op(1/n) for all j.
2

Step 2: Proof that ||~j - j||2 = Op(1/n)

Next, we use A3 on the behaviour of the empirical Hessians around j to extend the fact that

1 n

~ j

(j

)

-

j (j)

2 = Op(1/n) to prove that ||~j - j||2 = Op(1/n).

Plugging in j for j and using ~ j(~j) = j (j) = 0 (by A1) gives,

1 n

~ j

(j

)

-

j (j)

=
2

1 n

~ j

(j)

-

1 n

~ j

(~j

)

= Op(1/n)
2

Next,

consider

a

first

order

Taylor

expansion

of

1 n

~ j

(·)

about

j

evaluated

at

~j .

Note

that

1 n

j

is

a

vector valued function and as a result we consider a Taylor expansion of each entry of the vector.

1 n

~ j

(~j

)

=

1 n

~ j

(j

)

+

j

1 n

~ j

(~j)(~j

-

j)



1 n

~ j

(~j

)

-

1 n

~ j

(j)

=
2

1 n

Aj

(~j)(~j

-

j)

2

for some j = ~j + (1 - )j for   [0, 1]dj . The fact that ~ j(·) is already the vector valued first

derivative

of

the

Hyv¨arinen

score

meant

that

j

1 n

~ j

(~j)

=

1 n

Aj

(~j

).

Now

the

right

hand

side

can

be

bounded as follows

1 n

Aj

(~j

)(~j

-

j)

=
2

(~j - j)T

1 n

Aj

(~j)

T

1 n

Aj

(~j)(~j

- j)

 min(~j)||~j - j||2, max(~j)||~j - j||2,

where

min(~j)

and

max(~j)

are

the

smallest

and

largest

Eigen-values

of

1 n

Aj

(~j)

respectively.

As

a

result we can bound

1 n

~ j

(~j

)

-

1 n

~ j

(j)

 min(~j)||~j - j||2.
2

35

Further, ||~j -j||2 = op(1) from Lemma A.1 and by A3ii) there exists A > 0 such that min(j) > MA with probability tending to 1 for ||j - j||2  A. Therefore, with probability tending to 1

1 n

~ j

(~j

)

-

1 n

~ j

(j

)

 MA||~j - j||2.
2

MA is a constant and as a result ||~j - j||2 = Op(1/n), as required.

Combining Lemma A.1 and Proposition A.1 proves Propisition 1.

A.1.5 Theorem 1 (H-Bayes Factor Consistency)

Given Proposition 1 and the conditions stated in Section A.1.2 we are now ready to prove the paper's main theorem. First we restate Theorem 1.

Theorem 1 (H-Bayes Factor Consistency). Assume Conditions A1-A4 given in Section A.1.2. Given a sample y  g the Laplace approximation to the H-Bayes factor (given by B~k(Hl ) := H~k(y)/H~l(y) for H~j(y) defined in (16)) of loss k over loss l has the following asymptotic behaviour as n  

(i) When Eg[H(z; fl(·; l))] - Eg[H(z; fk(·; k))] < 0 then

1 n

log

B~k(Hl )

=

Eg [H (z ;

fl(·;

l))]

-

Eg [H (z ;

fk(·;

k))]

+

op(1)

(28)

That is that when the more complex model l decreases Fisher's divergence relative to model k, the H-Bayes factor accrues evidence in favour of model l, B~k(Hl )  0, at an exponential rate.

(ii) When Eg[H(z; fl(·; l))] = Eg[H(z; fk(·; k))], with k being the simpler model

log B~k(Hl )

=

dl

- 2

dk

log(n)

+

Op(1)

(29)

That is that when the models are equally preferable according to Fisher's divergence then B~k(Hl )   at a polynomial rate in n.

Proof. The proof of Theorem 1 can be broken down into 3 stages. Step 1 deals with how the prior densities of models l and k contribute to B~k(Hl ). Step 2 does the same for the Hessians of the Hyv¨arinen score applied to models l and k. Lastly, Step 3 deals with the in-sample Hyv¨arinen score contribution,
considering the two cases of the theorem.

36

Step 1: Priors

From Proposition 1, given that ||~j - j||2 = Op(1/n) and under the assumption that (j) is continuous in j (A1), the Continuous Mapping Theorem provides j(~j) -P j(j), j  {l, k}.

Further, assuming that l(l) > 0 (A1), it follows that

k (~k ) l(~l)

-P

k (k ) l(l)

,

a strictly positive finite constant. Note: that for non-local priors either j(j) may be zero, in that

case the limit may be 0 or  . We address this point again later Theorem A.1.

Step 2: Hessians

Now for the Hessians firstly we rewrite |Aj(~j)| = ndj/2

1 n

Aj

(~j

)

.

Then we want to prove that

1 n

Aj

(~j

)

-P

Aj (j).

Firstly,

1 n

Aj

(~j

)

-

Aj (j)

=

1 n

Aj

(~j

)

-

1 n

Aj

(j)

+

1 n

Aj

(j)

-

Aj (j).

Then, we use the weak law of large numbers and the Lipschitz condition (A4)) to show that

1 n

Aj

(~j

)

-

Aj

(j

)

=
2

1 n

Aj

(~j

)

-

1 n

Aj

(j

)

+

1 n

Aj

(j)

-

Aj (j)

2



1 n

Aj

(~j

)

-

1 n

Aj

(j

)

+
2

1 n

Aj

(j)

-

Aj

(j

)

2

(tri. in)



1 n

n

Aj(~j) - Aj(j) 2 + op(1) (tri. in & WLLN.)

i=1



1 n

n
~j - j 2 mA(yi) + op(1) = op(1),

i=1

where

1 n

n i=1

mA(yi)

-P

E [mA(y)]

<



by

the

weak

law

of

large

numbers

and

~j - j

=
2

Op(1/n) as proved in Proposition 1. As a result we have that

|Al(~l)|1/2 |Ak (~k )|1/2

-P

n (dl-dk) 2

|Al (l)|1/2 |Ak (k )|1/2

.

Step 3: In-sample Hyv¨arinen score difference

Combining Steps 1-2, the right hand terms of (25) is asymptotically equivalent to

|Al (l)|1/2 k(k) |Ak(k)|1/2 l(l)

2 n

dk -dl

2

e

n i=1

H (yi ;fl (·;~l ))-

n i=1

H (yi ;fk (·;~k

)),

(30)

where the first two terms are constants (A1 and A3). The only step left is to characterize the last

term, the in-sample H-scoreratio. To do this it is necessary to distinguish two cases:

37

1. Model l decreases Fisher's divergence relative to model k, Eg [H(y; fl(·; l)] < Eg [H(y; fk(·; k)]. This encompasses the cases where the models are non-nested or nested models where the bigger of the two models is true.

2. Model l does not decrease Fisher's divergence relative to model k, Eg [H(y; fl(·; l)] = Eg [H(y; fk(·; k)]. This is the case when the models are nested and the simplest model k is sufficient for minimising Fisher's divergence.

Case 1) First note that the logarithm of (30) is equivalent to

1 n

c

-

dk - 2n

dl

log

2 n

+

n

1 n

H

(yi

;

fl

(·;

~l))

-

n

1 n

H

(yi

;

fk

(·;

~k

))

=

i=1

i=1

n

1 n

H

(yi

;

fl

(·;

~l))

-

n

1 n

H

(yi;

fk

(·;

~k

))

+

O(1/n),

i=1

i=1

for a constant c  R (A1 and A3) given by the first two terms in (30). Now

1 n

n

H (yi ;

fl(·;

~l))

-

1 n

n

H(yi; fk(·; ~k)) = Eg[H(y; fl(·; l))] - Eg[H(y; fk(·; k))]

i=1

i=1

+

1 n

n

H(yi; fl(·; ~l)) - Eg[H(y; fl(·; l))] -

1 n

n

H(yi; fk(·; ~k)) - Eg[H(y; fk(·; k))]

,

i=1

i=1

and for each of the terms on the bottom line (j  {k, l})

1 n

n

H(yi; fj(·; ~j)) - Eg[H(y; fj(·; j))]

i=1

=

1 n

n

H (yi ;

fj (·;

~j ))

-

1 n

n

H (yi ;

fj(·; j))

+

1 n

n

H(yi; fj(·; j)) - Eg[H(y; fj(·; j))].

i=1

i=1

i=1

38

We then use the weak law of large numbers and the Lipschitz condition (A4) to show that

1 n

n

H(yi; fj(·; ~j)) - Eg[H(y; fj(·; j))]

i=1

=

1 n

n

H (yi ;

fj (·;

~j ))

-

1 n

n

H (yi ;

fj (·;

j))

+

1 n

n

H(yi; fj(·; j)) - Eg[H(y; fj(·; j))]

i=1

i=1

i=1



1 n

n

H (yi ;

fj (·;

~j ))

-

1 n

n

H(yi; fj(·; j)) +

1 n

n

H(yi; fj(·; j)) - Eg[H(y; fj(·; j))]

i=1

i=1

i=1

1 n n

H(yi; fj(·; ~j)) - H(yi; fj(·; j)) + op(1) (tri. in & WLLN)

i=1



1 n

n

mH (yi)

~j - j

2 + op(1) = op(1)

i=1

(tri. in)

where

1 n

n i=1

mH

(yi

)

-P

E [mH (y)]

<



by

the

weak

law

of

large

numbers

and

~j - j 2 =

Op(1/n) by Proposition 1. As a result

1 n

log

Hk(y) Hl(y)

=

Eg[H(y; fl(·; l))]

-

Eg[H(y; fk(·; k))]

+

op(1),

where Eg[H(y; fl(·; l))] - Eg[H(y; fk(·; k))] < 0 by assumption. That is, the H-score ratio accrues evidence in favour of model l at an exponential rate.

Case 2)

Now Eg [H(y; fl(·; l)] = Eg [H(y; fk(·; k)] and we want to prove that the H-Bayes factor convergence towards the simpler model k. The key is to prove that

n

n

H(yi; fl(·; ~l)) - H(yi; fk(·; ~k)) = Op(1).

i=1

i=1

Note that as model k is nested in model l, there exists l\k such that

H(yi; fk(·; k)) = H(yi; fl(·; k, l\k )) k,

where l\k is the value of the hyperparameters of model l that recover model k. Henceforth, denote ~k = ~k, l\k . We can rewrite our objective of interest as

n

n

H(yi; fl(·; ~l)) - log l(~k) -

H(yi; fk(·; ~k)) - log l(~k) ,

i=1

i=1

39

provided l(l) > 0, l (A1). This facilitates the Taylor expansion of H~l(~k) :=

n i=1

H

(yi;

fl(·;

~k))-

log l(~k) about ~l, assuming that its first and second derivatives are finite.

H~ l (~k )

=

H~ l (~l )

+

(~k

-

~l)~ l(~l)

+

1 2

(~k

-

~l)T

2l Al(~l)(~k

-

~l),

for some l = ~k + (1 - )~l with   [0, 1]dl. Now by definition we have that ~ l(~l) = 0 for all n and therefore plugging this back into our target

equation we get that

n

n

H(yi; fl(·; ~l)) - log l(~k) -

H(yi; fk(·; ~k)) - log l(~l)

i=1

i=1

=

log

l(~l)

-

log

l (~k )

+

1 2

(~k

-

~l)T

Al(~l)(~k

-

~l)

=

log

l(~l)

-

log

l (~k )

+

1 2

 n(~k

-

~l)T

1 n

Al(~l)n(~k

-

~l).

Now,

||~j - j||2

=

 Op (1/ n),

j

=

{k, l}

from

Proposition

1,

where

in

this

nested

case

l

=

k = , which further provides that ||l - ||2 = Op(1/n) also. From Step 1 we know that

log j(~j) -P log j(j), j = k, l, and therefore log l(~l) - log l(~k) -P 0. Lastly, Step 2 proved

that

1 n

Aj

(~j

)

-

Aj (j)

= op(1) and therefore
2

n

H(yi; fl(·; ~l)) -

n

H(yi; fk(·; ~k))

=

1 2

n(Op(1/n)(Al (l)

+

 op(1)) n(Op

(1/n)

i=1

i=1

= Op(1) + Op(1)op(1) = Op(1)

As a result, that log-H-score accrues evidence in favour of simpler model k at a log(n) rate. That is, the H-score accrues evidence at polynomial rate.

A.1.6 Corollary A.1 (SMIC inconsistency for nested models)

Here we investigate whether the SMIC model selection criteria provided by Matsuda et al. (2019)

can provide the same model/loss selection consistency as Theorem 1, in particular focusing on nested

models. We do so under the same conditions as Theorem 1, where the Laplace approximation of the

H-score were shown to be consistent, but with minor further conditions. Firstly, define

n

Ij(j) : =

j H(yi; fj(·; j)) j H(yi; fj(·; j)) T

i=1

Ij(j) : = Eg j H(z; fj(·; j)) j H(z; fj(·; j)) T .

40

We then consider conditions

A5. For model j = l, k there exist functions mI (·) with E [mI (·)] <  so that the following Lipschitz condition holds

Ij(1)(j(a)) - Ij(1)(j(b))

 mI (z)
2

j(a) - j(b)

,
2

j(a), j(b),

where the notation Ij(1)(·) emphasises that only one observation, z, is involved.

A6

Models

k

and

l

are

such

that

||~j

-

j||2

>

 op(1/ n).

A5 provides a standard Lipschitz condition for the matrices Ij(j), while A6 says that the convergence of ~j to j is not faster than 1/n. While Proposition 1 proved this convergence of Op(1/n),
 it is rare to obtain parametric convergence of op(1/ n), for simplicity here we simply assume this
to be the case. Further, note that while we desired maximisation of our H-score, the SMIC is to be
minimised.

Corollary A.1 (SMIC inconsistency for nested models). Assume Conditions A1-A4, A5 and parametric convergence according to A6. Given a sample y  g, the difference in the SMIC proposed by Matsuda et al. (2019) of loss k over loss l has the following asymptotic behaviour as n  .

(i) When Eg[H(z; fl(·; l))] = Eg[H(z; fk(·; k))], with k being the simpler model

SMICk - SMICl > op(1)

(31)

That is that when the models are equally preferable according to Fisher's divergence the SMIC will not consistently select the simpler model.

Proof. To prove this we first establish that the trace terms, tr

1 n

Ij

(~j

)

1 n

Aj

(~j

)

-1

, tend to con-

stants. Then we show that under the conditions the difference {

n i=1

H

(yi;

fk

(·;

~k))

-

n i=1

H

(yi;

fl(·;

~l))}

is not op(1) and therefore for any n there is non-zero probability of SM ICk - SM ICl < 0 when the

model k was sufficient according to Fisher's divergence.

From Matsuda et al. (2019) (34)

n

n

SMICk - SMICl =

H(yi; fk(·; ~k)) - H(yi; fl(·; ~l))

i=1

i=1

+ tr

1 n

Ik (~k

)

1 n

Ak

(~k

)

-1

- tr

1 n

Il(~l)

1 n

Al(~l)

-1

.

41

Firstly, Step 2 of Theorem 1 proved that

||

1 n

Aj

(~j

)

-

Aj ()||2

=

op(1),

j = l, k.

Next,

we

use

similar

arguments,

and

A5

to

prove

that

||

1 n

Ij

(~j

)

-

Ij()||2

=

op(1).

1 n

Ij

(~j

)

-

Ij(j)

=
2

1 n

Ij

(~j

)

-

1 n

Ij

(j)

+

1 n

Ij

(j)

-

Ij(j)

2



1 n

Ij

(~j

)

-

1 n

Ij

(j)

+
2

1 n

Ij

(j)

-

Ij(j)

2

(tri. in)



1 n

n

Ij(~j) - Ij(j) 2 + op(1) (tri. in & WLLN.)

i=1



1 n

n
~j - j 2 mI (yi) + op(1) = op(1),

i=1

where

1 n

n i=1

mI

(yi)

-P

E [mI (y)]

<



by

the

weak

law

of

large

numbers

and

~j - j 2 =

Op(1/n) as proved in Proposition 1.

Therefore, for constant C = tr

1 n

Ik(k)

1 n

Ak

(k)

-1

-

tr

1 n

Il(l)

1 n

Al (l

)

-1

we have that

n

n

SMICk - SMICl =

H(yi; fk(·; ~k)) - H(yi; fl(·; ~l)) + C + op(1).

i=1

i=1

Lastly, Step 3, Case 2) of Theorem 1 proved that in the nested case

n

n

H(yi; fk(·; ~k)) - H(yi; fl(·; ~l))

=op(1)

+

1 2

 n(~k

-

~l)T

1 n

Al

(l)

+

op(1)

 n(~k - ~l).

i=1

i=1

As

a

result

unless

||~k

-

~l||2

=

 op(1/ n),

which

when

Eg [H (z ;

fl(·;

l))]

=

Eg [H (z ;

fk(·;

k))]

will

not

happen by assumption (A6), then

n

n

H(yi; fk(·; ~k)) - H(yi; fl(·; ~l)) > op(1)

i=1

i=1

as a result there exists  > 0 and M  > 0 such that for all n

n

n

P

H(yi; fk(·; ~k)) - H(yi; fl(·; ~l)) > M  > .

i=1

i=1

The consequences of this are as follows, in the nested modelling scenario {

n i=1

H

(yi;

fk

(·;

~k

))

-

n i=1

H (yi ;

fl(·;

~l))}



0,

therefore if C

> 0, then model k

will never be preferred to

model l.

For

nested models however it is often the case that C is less than 0. In this case is is possible that M 

42

is such that M  + C > 0 and therefore there is still non-zero probability that model l is chosen over model k even as n  . Knowing whether this is the case is impossible without knowledge of the data generating density g and therefore a practitioner can not be sure the SMIC will provide consistent model selection. Therefore, we describe it as inconsistent for this task.

Note Corollary A.1 considered the SMIC evaluated at the penalised Hyv¨arinen score minimiser ~k in order to simplify the extension from Theorem 1, however it is straightforward to see how this also holds for the unpenalised ^j also. A further consequence of this result is that cases where Aj (j) is not a finite matrix, as discussed in Section 4, will also affect the performance of the SMIC. For the SMIC there is no natural method to ameliorate this.

A.1.7 Theorem A.1 (H-Bayes Factor Consistency under non-local priors)
The conditions of Theorem 1 required that j(j) > 0 for all j and therefore these conditions will be violated if a non-local prior (Johnson and Rossell, 2012; Rossell and Telesca, 2017) is placed on any model parameters. Here, we extend Theorem 1 to allow for these, particularly focusing on nested models In order to do so we consider the following extended conditions defining the form of the nested models and the non-local prior.
A7 Models k and l are such that k = {k} with k  k =  (i.e. no hyperparameters k = ) and l = {l, l} with l =  and l  l = . Define l\k as the hyperparameters of model l that recovers model k,
H(yi; fk(·; k)) = H(yi; fl(·; k, l\k )), for all yi.

A8 Models l and k satisfying A7 have priors

i) The prior k(k) = k(k) for model k satisfies A1 and the prior for model l is l(l) = l(l, l) = lNLP(l)l(l|l) where l(l|l) satisfy A1 also.

ii) The non-local prior on l is lNLP(l) = dl( l - l\k 2)lLP(l), where lLP(l) satisfies A1,

dl(z) > 0 for all z = 0, and there is a monotonically decreasing function c(z) : R  R such

that

limz0 c(z) = ,

c(z/a)  c(z)/c(a),

and

c(z) - log dl(z)

= O(1)

as

z

 0.

43

A7 defines the form of the nested models we consider. Both models share the same parameter space , but the simpler model k has no hyperparameters. An example of A7 is where k is the Gaussian model and l Tukey's loss and l\k = . A8 specifies that a non-local prior is placed on the additional hyperparameter of model l which penalises their being too close to the value that recovered the simpler model k at a rate as least as fast as c(·). The parameters shared by both models are given local priors.
We define the non-local prior adjusted Laplace approximate Bayes factor as

B~kHl-NLP := H~k(y)/H~lLP(y) × 1/dl(||~l-l\k||2),

(32)

where H~k(y) and H~lLP(y) are Laplace approximations to the marginal likelihoods (16) whose respective priors k(k) = k(k) and l(l) = l(l|l)lLP(l) satisfy the conditions of A1 according to A8. This allows us to invoke Theorem 1 for the asymptotic behaviour of H~k(y)/H~lLP(y). Some justification for this objective is provided in Remark A.1 at the end of this section.
The following Theorem proves that (32) maintains consistent model selection of the H-Bayes factor as proved in Theorem 1, and can improve the rate at which the simpler of two nested models is selected when it is sufficient for minimising Fisher's divergence.

Theorem A.1 (H-Bayes Factor Consistency under Non-Local Priors). Assume Conditions A1-A4, models satisfying A7 and non-local priors satisfying A8.

(i) When Eg[H(z; fl(·; l))] - Eg[H(z; fk(·; k))] < 0 then

1 n

log

B~kHl -NLP

=

Eg [H (z ;

fl(·;

l))]

-

Eg [H (z ;

fk(·;

k))]

+

op(1).

(33)

That is that when the more complex model l decreases Fisher's divergence relative to model k, the non-local H-Bayes factor accrues evidence in favour of model l, B~kHl-NLP  0, at an exponential rate.

(ii) When Eg[H(z; fl(·; l))] = Eg[H(z; fk(·; k))], with k being the simpler model, there exists a constant Md such that with arbitrarily high probability

log

B~kHl -NLP

>

 c(1/ n)

dl

- 2

dk

c(lo1g/(nn) )

+

op(1)

+

Md

.

(34)

That is that when the models are equally preferable according to Fisher's divergence then B~kHl-NLP   at a rate depending on the NLP specification via the function c(·).

44

Proof. To prove this we decompose the log non-local H-Bayes factor in the local H-Bayes factor, which was dealt with by Theorem 1, and the non-local penalty term. Then, we consider the two cases of the theorem. When the more complicated model is true, we show that the non-local penalty term tends to a positive constant while when the simpler model is sufficient, we show that the properties of the non-local penalty function control the rate of convergence.
Firstly, from (32)

log B~kHl-NLP = log B~k(Hl ) - log dl(||~l - l\k||2),

(35)

and by Slutsky's Theorem we can investigate both terms separately. The term B~k(Hl ) was constructed so that it satisfied the conditions of Theorem 1 and therefore we can invoke these results here. Further, ~l = {~l, ~l} where defined as the Hyv¨arinen score minimisers penalised by the local prior, therefore satisfying the conditions of Proposition 1 and ensuring that ||~j - j||2 = Op(1/n). Next, we consider the two cases.

Case 1)

From Theorem 1 we have that

1 n

log

B~k(Hl )

=

Eg [H (z ;

fl(·;

l))]

-

Eg [H (z ;

fk(·;

k))]

+

op(1).

Further, for Eg[H(z; fl(·; l))] - Eg[H(z; fk(·; k))] < 0 it must be the case that l = l\k (by A7).

Therefore, ||~l - l\k||2 0 and by the continuous mapping theorem we have that |d(||~l - l\k||2) -

dl(||l - l\k||2)| = Op(1/n) with dl(||l - l\k||2) > 0 as a strictly positive constant (A8ii)). As a

result

1 n

d(||~l

- l\k||2)

=

O(1/n))

and

1 n

log

B~kHl -NLP

=

Eg [H (z ;

fl(·;

l))]

-

Eg [H (z ;

fk(·;

k))]

+

op(1).

Case 2)

In this case, Eg[H(y; fl(·; l))] = Eg[H(y; fk(·; k))] = 0 which by A7 requires that l = l\k. Therefore, ||~l - l\k||2 = Op(1/n), which by the continuous mapping theorem implies dl(||~l - l\k||2)  0. Here we establish the asymptotic rate of this convergence.
By (35), in order to establish (34) we require that

  , Md, Nd > 0 such that P - log dl(||~l - l\k||2) > Mdc(1/ n) > 1 - , n > Nd

From

A8ii),

c(z) - log dl(z)

=

O(1)

as

z



0

which

guarantees

the

existence

of

M

and



>

0

such

that

- log dl(z) c(z)

>

M

for

all

z

such

that

|z|

<

.

45

As a result, by the law of total probability

 P (- log dl(||~l - l\k||2) > Mdc(1/ n))
 =P (- log dl(||~l - l\k||2) > Mdc(1/ n) | ||~l - l\k||2  )P (||~l - l\k||2  )
 + P (- log dl(||~l - l\k||2) > Mdc(1/ n) | ||~l - l\k||2 > )P (||~l - l\k||2 > )

P (c(||~l

- l\k||2)

>

Md M

 c(1/ n)

|

||~l

- l\k||2



)P (||~l

- l\k||2



).

Given that ||~l - l\k||2 = Op(1/n) from Proposition 1 and  is a constant, the term P (||~l - l\k||2 

) is arbitrarily close to 1 as n  . Regarding the other terms, by the decreasing monotonicity of

c(z) (A8ii)) the conditioning events is such that

||~l - l\k||2   = c ||~l - l\k||2  c() ,

and

therefore

for

Md M

c(1/n)



c()

we

have

that

P

c(||~l

- l\k||2)

>

Md M

 c(1/ n)

|

c(||~l

- l\k||2)

>

c()

P

c(||~l

-

l\k ||2 )

>

Md M

 c(1/ n)

.

Since ||~l - l\k||2 = Op(1/n), by definition for every > 0 there exists M > 0 and N > 0 such

that

 P ( n||~l - l\k||2 < M ) > 1 - , n > N .

Further, since c(z) is strictly decreasing and c(z/a)  c(z)/c(a) by Assumption A8ii), we have that





P (c ||~l - l\k||2 /c(1/ n) > c(M ))  P (c ||~l - l\k||2/(1/ n) > c(M )) > 1 - n > N .

Consider the particular choice Md = c(M

)M

so that

Md M

= c(M

) giving that

P (c

||~l - l\k||2

>

Md M

 c(1/ n))

>

1

-

n > N

Therefore, taking (1 - ) > (1 - )

 P (- log dl(||~l - l\k||2) > Mdc(1/ n)) > (1 - )P (||~l - l\k||2) > (1 - )

for large enough n. Further, invoking Theorem 1 we have that with arbitrarily high probability there

46

exists constant Md such that for sufficiently large n

log B~kHl-NLP = log B~k(Hl ) - log dl(||~l - l\k||2)

>

log

B~k(Hl )

+

Md

×

 c(1/ n)

=

dl

- 2

dk

log(n)

+

Op(1)

+

Md

×

 c(1/ n)

 = c(1/ n)

dl

- 2

dk

c(lo1g/(nn) )

+

Op(1) c(1/ n)

+

Md

.

We follow Theorem A.1 with the special case corollary considering the inverse-gamma non-local

prior

applied

to

2

=

1 22

where

2

was

the

cut-off

parameter

of

Tukey's

loss

(Section

5.2).

First,

we

generalise that prior set-up from Section 5.2 as the following condition.

A9 Under the model setup of A7

i) Let l = {}  R be a univariate hyperparameter with l\k = 0 and   0. ii) lLP() = 2I0N ( - 0; 0, s20), a half-Gaussian distribution with scale s0. iii) lNLP() = IG( - 0; a0, b0), an inverse-gamma distribution with shape a0 and scale b0.
We note that the corresponding local prior to an inverse-gamma non-local prior would normally be a gamma distribution. However, this does not have finite second derivative as  - 0  0 and would therefore violate the conditions of Theorem 1.

Corollary A.2 (H-Bayes Factor Consistency under an Inverse-Gamma Non-Local Prior). Assume Conditions A1-A4, models satisfying A7 and non-local priors satisfying A8(i) and A9.

(i) When Eg[H(z; fl(·; l))] - Eg[H(z; fk(·; k))] < 0 then

1 n

log

B~kHl -NLP

=

Eg [H (z ;

fl(·;

l))]

-

Eg [H (z ;

fk(·;

k))]

+

op(1).

(36)

That is that when the more complex model l decreases Fisher's divergence relative to model k, the non-local H-Bayes factor accrues evidence in favour of model l, B~kHl-NLP  0, at an exponential rate.

(ii) When Eg[H(z; fl(·; l))] = Eg[H(z; fk(·; k))], with k being the simpler model, there exists a

constant Md such that with arbitrarily high probability

log

B~kHl -NLP

>

 n(op(1)

+

Md).

(37)

47

That is that when the models are equally preferable according to Fisher's divergence then

log B~kHl-NLP





at

a

rate

at

least

as

fast

as

 n

as

n



.

Proof. To prove this we derive the non-local penalty function associated with the prior setup of A9

and calculate its bounding function c(·) associated with A8ii). This allows us to invoke Theorem A.1

to provide the desired asymptotic behaviour.

Firstly, from A9,   0 and therefore dl(| - 0|) = dl( - 0). Then, the non-local penalty

function

dl(

- 0)

=

lNLP ( ) lLP ( )

given

by

the

prior

specification

of

A9

is

dl(

-

0)

= 

lNLP ( ) ba0lL0P(2)

s20

2(a0)

1  - 0

a0+1
exp

(

- 0)2 2s20

-



b0 - 0

,

and therefore

- log dl(

-

0)

=

- log

ba00

 2

2(a0)

s20

+

(a0

+

1) log (

-

0)

-

(

- 0)2 2s20

+

(

b0 - 0

)

.

Therefore, there exists  > 0 and M > 0 such that for all  s.t. | - 0| < 

-

log

dl(

-

0)

>

M



1 -

0

,

and

as

a

result,

c(z)

=

1 z

.

Now

we

consider

the

two

cases.

Case 1)

Follows directly from the proof of Theorem A.1, noting that dl(z) > 0 for all 0 < z < .

Case 2)

Using

Theorem

A.1

with

c(z)

=

1 z

provides

that

with

arbitrarily

high

probability

there

exists

Md

such that for sufficiently large n

log

B~kHl -NLP

>

 c(1/ n)

dl

- 2

dk

log(n) c(1/ n)

+

Op(1) c(1/ n)

+

Md

 =n

dl

- 2

dk

log(nn)

+

Op(1) n

+

Md

 = n(op(1) + Md),

as required.

48

Remark A.1 (Justification for (32)). Lastly, we provide some justification for the objective function

(32) to conduct model selection using non-local priors. Using the arguments of Rossell and Telesca

(2017), the integrated H-score (14) associated with the non-local prior specification A8 can be rewritten

as

n

HlNLP(y) = lNLP(l)l(l|l) exp - H(yi; fl(·; l, l)) dldl

= HlLP(y)

i=1

dl(||l

-

l\k ||2 )

lLP(l)l(l|l)

exp

{-

n i=1

HlLP(y)

H (yi ;

fl(·;

l,

l))}

dldl

= HlLP(y) dl(||l - l\k||2)lLP(l, l|y)dldl

where
n
lLP(l, l|y)  lLP(l)l(l|l) exp - H(yi; fl(·; j, j)) ,
i=1 n
HlLP(y) = lLP(l)l(l|l) exp - H(yi; fl(·; l, l)) dldl,
i=1
As a result we can write the H-Bayes factor of nested models satisfying A7 and non-local priors
according to A8 as

BkHl -NLP

=

Hk(y) HlNLP(y)

=

Hk(y) HlLP(y)

dl(||l

-

1 l\k ||2 )lLP (l ,

l|y)dldl

.

Now, the terms involved in the construction of Hk(y) and HlLP(y) satisfy the conditions of Theorem 1 (by A8i)), and therefore the behaviour of the Laplace approximations (16) to the H-Bayes factor,

H~k(y)/H~lLP(y), is detailed in Theorem 1. We further consider a somewhat looser, but convenient, approximation of dl(||l - l\k||2)lLP(l, l|y)dldl as dl(||~l - l\k||2) where ~l  ~l are the parameters maximising the H-posterior lLP(l, l|y). As a result we consider (32) to approximate the H-Bayes factor under non-local priors
We highlight that calculating the approximation H~lNLP(y) uses the maximum a posteriori estimates and evaluates the observed Hessian matrices according to the H-posterior under the local prior, before

evaluating the prior density as it appears in the Laplace approximation (16) at the non-local prior, as

shown in (32). This is convenient as it allows for a direct extension of Theorem 1.

A.2 Derivation of Hyv¨arinen scores
Section A.2.1 provides the Hyv¨arinen score for the Gaussian log-likelihood and for Tukey's loss. 49

A.2.1 The Hyv¨arinen score of the Gaussian Model and Tukey's Loss

Here we provide the derivations of the Hyv¨arinen score applied to the Gaussian model (19) and Tukey's

loss (20). Firstly, for the Gaussian model f1(yi; xi, 1, 1) = N (yi; xTi , 2) we have that

log

f1(yi;

xi,

1,

1)

=

-

1 2

log(2)

-

1 2

log

2

-

(yi

- xTi 22

)2

 yi

log

f1(yi;

xi,

1,

1)

=

- (yi

- xTi 2

)

2 yi2

log

f1(yi;

xi,

1,

1)

=

-

1 2

,

which results in Hyv¨arinen score

H1(yi;

f (·;

xi,

1))

=

-

2 2

+

(yi

- xTi 4

)2

.

For Tukey's loss, log f2(yi; xi, 2, 2) = - 2(yi; xi, 2, 2) given in (4) and



 yi

2(yi; xi, 2, 2)

=

 (yi-xTi )  2

-

2(yi-xTi )3 22  4

+

(yi-xTi )5 42  6

0

if |yi - xTi |  2 otherwise



2 yi2

2(yi; xi, 2, 2)

=

1  2

-

6(yi-xTi )2 22  4

+

5(yi-xTi )4 42  6

0

if |yi - xTi |  2 , otherwise

which results in Hyv¨arinen score

H2(yi; f (·; xi, 2, 2)) = I(|yi - xTi |  2)

(yi

- xTi ) 2

-

2(yi - xTi )3 224

+

(yi

- xTi )5 426

2

-2

1 2

-

6(yi - xTi )2 224

+

5(yi - xTi )4 426

.

A.2.2 The Hyv¨arinen score of kernel density estimation

We derive the Hyv¨arinen score for the kernel density estimation examples implemented in Section 6.

Combining the kernel density estimate improper density (6) with the power w (23) results in in-sample



w

g^w,h(yi)

=

 1 n 2h

n j=1

exp

-

(yi

- yj 2h

)2



,

50

or equivalently defining the kernel density loss function to be the log-density





log

g^w,h(yi)

=w

log

 1 n 2h

n j=1

exp

-

(yi

- yj 2h

)2







=w log  1 + 1

n
exp

n 2h n 2h j=i

-

(yi

- yj 2h

)2

.

The Hyv¨arinen score is then composed of the first an second derivatives of the log-density, given by

 yi

log

g^w,h(yi)

=w

-

n j=i

(yi-yj ) n 23

exp

-

(yi

-yj 2h

)2

1 + 1
n 2h n 2h

n j=i

exp

-

(yi

-yj 2h

)2

2 yi2

log

g^w,h(yi)

=w

n j=i

(yi-yj )2 n 2h5/2

exp

-

(yi

-yj 2h

)2

1 + 1
n 2h n 2h

-

n j=i

1 n 2h3/2

exp

n j=i

exp

-

(yi-yj 2h

)2

-w

n j=i

(yi-yj ) n 23

exp

-

(yi

-yj 2h

)2

2

1 + 1
n 2h n 2h

n j=i

exp

-

(yi

-yj 2h

)2

2.

-

(yi-yj 2h

)2

As a result, the Hyv¨arinen score of the kernel density estimate is

H(yi, log g^w,h(·)) = 2w

n j=i

(yi-yj )2 4

exp

-

(yi

-yj 2h

)2

-

n j=i

1 2

exp

1+

n j=i

exp

-

(yi

-yj 2h

)2

- (2w - w2)

n j=i

(yi-yj ) 2

exp

-

(yi-yj 2h

)2

1+

n j=i

exp

-

(yi

-yj 2h

)2

2
2.

-

(yi-yj 2h

)2

(38)

A.3 Implementation Details

This sections provides a thorough description of the experiments conducted in Section 5 and 6 of the main paper. Section A.3.1 outlines our implementation of the H-score in Stan. Section A.3.2 discusses a restriction placed on Tukey's loss parameters to prevent degenerate parameter estimates based on including a single observation in the fit. Given that Tukey's loss is not twice continuously differentiable, Section A.3.3 discusses a differentiable approximation to Tukey's loss that allows using second order sampling and optimisation methods, as well as satisfying the conditions of Theorem 1. Finally, Section A.3.4 discusses how to set a non-local prior on Tukey's loss cut-off hyper-parameter 2.

51

Code to reproduce the examples of Sections 5 and 6 can be found in the repository https:// github.com/jejewson/HyvarinenImproperModels.
A.3.1 Computation
To calculate the Laplace approximations of the H-score we used the second order LBFGS optimisation routine implemented in the probabilistic programming language Stan (Carpenter et al., 2016). Further, stan's implementation of the No U-Turn Sampler (Hoffman and Gelman, 2014) can further be used to sample from the General Bayesian posteriors. The code associated with all of our experiments is available as part of the supplementary material.
A.3.2 The breakdown of Tukey's loss
As discussed in Section 5, we define a restriction of the parameter space of Tukey's loss (, , 2) that prevents degenerate solutions where 2 is set to exclude all observations but one. Intuitively, the idea is to avoid 2 being too small. More specifically, we motive the restriction via a related notion of the breakdown point. The breakdown of an estimator is the number of observations that can be arbitrarily perturbed without causing arbitrary changes to the estimator. The squared loss can be seen to have breakdown 0, while clearly for Tukey's-loss any observation outside the threshold can be perturbed arbitrarily without changing the estimator. There exists abundant literature arguing that the breakdown can be no greater than 1/2, which intuitively means that an estimator should depend on more than half of the data. See Rousseeuw and Yohai (1984) and Rousseeuw and Leroy (2005) for discussion and thorough investigation of Tukey's loss breakdown in the context of S-estimation. The authors showed that for Tukey's breakdown to be less than 1/2 the condition in (21) must hold. Therefore, we restrict the parameter space to (, , 2) satisfying (21). We note that (21) applies to S-estimation rather than to our H-scores however, we found that (21) provides a simple rule that showed good performance in our examples, hence we recommend it as a default.
A.3.3 A differentiable approximation to the indicator
In order to satisfy the conditions of Theorem 1 as well as facilitate second order optimisation (or sampling) methods, it is necessary to have a log-density that is twice continuously differentiable. While Tukey's loss itself is defined to ensure it that this is the case, the Hyv¨arinen score applied to Tukey's loss, which is a function of its first and second derivatives is not. This is because the indicator
52

I(|yi - xTi |  2) is not differentiable at |yi - xTi | = 2. We ensured the required conditions by the approximating

|x| 

x2 +

1 k1

for

large

k1

I(x



0)



k2 (x)

=

1

1 + exp(-k2x)

for

large

k2

in (20). For our results we set k1 = k2 = 100.

A.3.4 Non-Local Prior Specification for Tukey's cut-off hyper-parameter

To

place

a

non-local

prior

on

2,

it

is

convenient

to

consider

the

reparametrisation

2

=

1 22

(as

discussed

in Section 4, this is also needed to ensure that the Hessian of Tukey's loss is finite as 2   (2  0)).

Given that the Gaussian model is recovered at 2 = 0, any prior setting positive density at 2 = 0

satisfies the definition of being a local prior. In our examples we considered a half-Gaussian prior

2LP(2)  N (2; 0, s20)I(2  0) where I() is the indicator function and s0 = 1. A possible alternative that also assigns non-zero density at 2 = 0 is to consider a gamma prior with shape parameter < 1,

but the associated log-prior does not have a finite Hessian as 2  0 and violates Condition A3 of

Theorem 1.

As a non-local prior we considered an inverse-gamma distribution 2NLP(2) = IG(2; a0, b0), which has zero density at 2 = 0. The values of the prior hyper-parameters (a0, b0) can have a considerable

affect on the finite sample performance of the H-score model selection, and therefore setting these in

a principled manner is important. Fortunately, 2 has a natural interpretation that allows restricting

attention to a range of values that would be reasonable in most applications. Recall from Section

2.2.1 that the role of 2 is to exclude observations that are more than 2 `standard deviations'  away

from the mean µ. First, if the data are Gaussian then there is > 0.99 probability of an observation

being within 3's of the mean. We thus set (a0, b0) such that there is high prior probability that

2  3. Second, given that the Gaussian has a substantial amount of central data within 1 from the

mean, we set high prior probability that 2 > 1, to ensure that this central component is captured.

Given these considerations, we set default parameter values (a0, b0) = (4.35, 1.56), which ensures a

prior probability P (2  (1, 3)) = 0.95. Figure A.1 plots these local and non-local priors for 2 and

the corresponding priors under the original parametrisation, 2.

53

Density 0.0 0.5 1.0 1.5 2.0 2.5
Density 0.0 0.2 0.4 0.6 0.8 1.0

Local Prior Non-Local Prior

0.0

0.5

1.0

1.5

2.0

2

0

1

2

3

4

5

6

7

2

Figure A.1: Left: Density of the local half-standard Gaussian and non-local IG(2; 4.35, 1.56) priors for 2. Right: The corresponding prior density for 2 = 1/2. The non-local prior sets P (2  (1, 3)) = 0.95.

A.4 Supplementary results
A.4.1 Variable selection for TGF- and DLD dataset
To illustrate our methodology for robust regression in Section 5, we selected between the Gaussian model and Tukey's loss when regressing the TGF- and DLD gene expressions on a subset of the variables available in the full data sets. The procedures for which variables were selected was outlined in Sections 5.3.1 and 5.3.2. To ensure that our results are reproducible, below we indicate the selected covariates and the supplementary material contains code for these variable pre-screening steps. We also provide supplementary figures providing a visual inspection of the Gaussian fit to the residuals of the TGF- and DLD examples.
TGF-: For the TGF- analysis we focused on 7 of the 10172 genes available in the data set that appear in the `TGF- 1 pathway' according to the KEGGREST package in R (Tenenbaum, 2016). These were the VIT, PDE4B, ATP8B1, MAGEA11, PDE6C, PDE9A and SEPTIN4 genes.

DLD: For the DLD analysis we selected the 15 genes with the 5 highest loadings in the first 3 principal components of the original 57 predictors. This procedure selected the following genes C15orf52, BRAT1, CYP26C1, SLC35B4, GRLF1, RXRA, RAB3GAP2, NOTCH2NL, SDC4, TTC22, PTCH2, ECH1, CSF2RA, TP53AIP1, and RRP1B.

54

Supplementary figures: We provide Figure A.2 to visually inspect the Gaussian approximation to the fitted standardised residuals of the TGF- and DLD datasets. The left panels overlay the fitted Gaussian models to the histogram of the residuals, while the right are Q-Q Normal residual plots. The top panels show that the TGF- data is well-approximated by a Gaussian model, whereas the bottom panels show that the DLD data set exhibits heavier tails. These graphical inspections support the H-score findings from Section 5 and Figure 4.

TGF- data, H~1(y1:262) = 272.88 Gaussian

Normal Q-Q Plot

Sample Quantiles -4 -3 -2 -1 0 1 2

Density 0.0 0.1 0.2 0.3 0.4

-4 -3 -2 -1

0

1

2

(y - X^)/^

DLD data, H~1(y1:192) = 155.57

-3

-2

-1

0

1

2

3

Theoretical Quantiles

Normal Q-Q Plot

Sample Quantiles -4 -2 0 2

Density 0.0 0.2 0.4 0.6

-4

-2

0

2

(y - X^)/^

-3

-2

-1

0

1

2

3

Theoretical Quantiles

Figure A.2: Top: TGF- data, where the integrated H-score selected the Gaussian model. Bottom: DLD data, where the integrated H-score selected Tukey's loss. Left: Gaussian approximations to the residuals. Right: Q-Q normal plot of the fitted residuals according to the Gaussian model.

A.4.2 Further Gaussian Mixture Examples Additionally to the experiments in Section 6.1, here we provide two further Gaussian mixture data sets from Marron and Wand (1992) · Asymmetric: 2-components, µ1 = 0, µ2 = 1.5, 1 = 1, 2 = 1/3, m1 = 0.75 and m2 = 0.25.
55

· Kurtotic: 2-components, µ1 = µ2 = 0, 1 = 1 2 = 0.1, m1 = 2/3 and m2 = 1/3.

Figure A.3 plots the approximation to the underlying g(y) of the four methods under consideration in Section 6.1, while Table A.1 estimates the Fisher's divergence of the estimates (posterior mean where appropriate) to g(y). For both examples the DPMM provides the best estimate according the Fisher's divergence. For the Kurtotic data set, the H-posterior KDE's perform comparably to the DPMM and better than the default KDE implementation. However, for the Asymmetric data set the H-posterior KDE's under-smooth the data and perform considerably worse than the other methods.

asymmetric
KDE (H-posterior) KDE - w (H-posterior) KDE (Silverman) DPMM g(y)

kurtotic

Density 0.0 0.2 0.4 0.6
Density 0.0 0.4 0.8 1.2

-3

-2

-1

0

1

2

x

-3

-2

-1

0

1

2

3

x

Figure A.3: Density estimation for Gaussian mixture data. Histograms of observed data, standardised to 0 mean and unit variance, and estimated density by DPMM, R's density function with the bandwidth rule of Silverman (1986), the H-posterior estimate with no tempering (w = 1), and with tempering (w = 1).

Table A.1: Fisher's divergence between the density estimates and the data-generating Gaussian mixtures in two simulation scenarios. The best performing method in each is highlighted in bold face.

KDE DPMM Hyv¨arinen KDE (w = 1) Hyv¨arinen KDE (w = 1)

Asymmetric 0.25 0.04

2.17

2.24

Kurtotic 3.14 2.05

2.55

2.44

56

