RAI-Net: Range-Adaptive LiDAR Point Cloud Frame Interpolation Network
Lili Zhao, Zezhi Zhu, Xuhu Lin, Xuezhou Guo, Qian Yin, Wenyi Wang, and Jianwen Chen* School of Information and Communication Engineering University of Electronic Science and Technology of China Chengdu, China chenjianwen@uestc.edu.cn

arXiv:2106.00496v1 [eess.IV] 1 Jun 2021

Abstract--LiDAR point cloud frame interpolation, which synthesizes the intermediate frame between the captured frames, has emerged as an important issue for many applications. Especially for reducing the amounts of point cloud transmission, it is by predicting the intermediate frame based on the reference frames to upsample data to high frame rate ones. However, due to high-dimensional and sparse characteristics of point clouds, it is more difficult to predict the intermediate frame for LiDAR point clouds than videos. In this paper, we propose a novel LiDAR point cloud frame interpolation method, which exploits range images (RIs) as an intermediate representation with CNNs to conduct the frame interpolation process. Considering the inherited characteristics of RIs differ from that of color images, we introduce spatially adaptive convolutions to extract range features adaptively, while a high-efficient flow estimation method is presented to generate optical flows. The proposed model then warps the input frames and range features, based on the optical flows to synthesize the interpolated frame. Extensive experiments on the KITTI dataset have clearly demonstrated that our method consistently achieves superior frame interpolation results with better perceptual quality to that of using state-of-the-art video frame interpolation methods. The proposed method could be integrated into any LiDAR point cloud compression systems for inter prediction.
Index Terms--Point cloud frame interpolation, range image
I. INTRODUCTION
Nowadays, light detection and ranging (LiDAR) sensors are gaining more and more popularity in practical applications, such as self-driving vehicles, mobile robots, drones, tablet PC or mobile phones (e.g., iPhone 12 Pro). LiDAR can acquire the precise 3D digital representation of the surrounding environment, i.e., LiDAR point clouds, which consist of 3D coordinates indicating the locations of points. It is wellknown that LiDAR point clouds have been widely utilized in many fields, e.g., augmented reality (AR), virtual reality (VR), simultaneous localization and mapping (SLAM) [1].
LiDAR point cloud frame interpolation (i.e., temporal interpolation), which aims to synthesize the intermediate frame between the captured frames, begins to attract more and more attention, since it is greatly demanded in various tasks
*Jianwen Chen is the corresponding author. © 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.

Ground Truth

Ours

DAIN

Super SloMo

Fig. 1. An example for LiDAR point cloud frame interpolation. We propose a range-adaptive point cloud interpolation approach. Our approach produces a high-quality result.

such as LiDAR point cloud comprssion [2], frame rate upconversion for data fusion [3]. LiDAR point cloud frame interpolation could address the above-mentioned issues by improving the frame rate. However, few works have eyes on it. Recently, PointINet [4] is proposed for LiDAR point cloud frame interpolation, which is by estimating bi-directional 3D scene flow between the two point clouds and generating the interpolated frame based on the 3D scene flow. Note that the whole processing is conducted in 3D space with high complexity. This paper will focus on frame interpolation of LiDAR point clouds with low complexity.
Recent years have witnessed great success in applying deep convolutional neural networks (CNNs) on video frame interpolation (VFI). The existing approaches for VFI can be divided into three categories: phase-based methods (e.g., [5]), kernel-based methods (e.g., [6]), and flow-based methods (e.g., [7]­[9]). It can be observed that flow-based approaches perform well in quantitative benchmarks [10]. Among them, the work DAIN [7] via the advanced flow estimation method PWC-Net [11], delivers the state-of-the-art performance. It is highly desirable to bring this technology to point cloud frame interpolation. However, it is not a direct extension. That is because point clouds have the characteristics: with the high dimensionality (i.e., 3D), huge amounts of data and the unstructured nature, imposing restrictions on adopting welldeveloped VFI methods (typically using 2D CNNs) directly.
It is worth noting that several works have explored lowcomplexity 3D segmentation via 2D representations of point clouds (e.g., [12], [13]). Instead of processing 3D points

directly, point clouds are firstly projected into a set of 2D range images (RIs) [one can see Fig. 2 (c)] and then segmented. Note that each pixel of RIs contains the range from each point to the center of the sensor's coordinate system. It is natural to consider this projection method for our work, where point clouds are transformed into RIs and then fed into VFI models to generate the interpolated frame. Nevertheless, naively applying these techniques usually leads to sub-optimal frame interpolation performance. It is likely due to the fact that the inherited characteristics of RIs differ from the normal color images. For example, the feature distribution of the color images is basically constant at their different spatial locations, while RIs are in the opposite case [13]. Meanwhile, the feature extractors adopted in the existing VFI approaches are typically standard convolutions, which have the spatial sharing nature, i.e., the parameters of convolutions are shared across the whole input. Therefore, the feature extractors may not be effective to handle the RIs, leading to relatively low frame interpolation performance.
In this paper, we address these challenges and propose a range-adaptive frame interpolation model, which can be applied to reduce the amounts of point cloud transmission (as shown in Fig. 3). The proposed approach follows the successful paths of flow-based VFI methods with novel adaptions. Specifically, first, 3D point clouds are projected to a set of 2D range images as input frames. Then, we estimate the bidirectional optical flows from two input frames via our flow estimator, which is modified from PWC-Lite, a lightweight version of PWC-Net [11]. Instead of the feature extractor adopted in the existing VFI methods for color images, we propose a range-adaptive feature extractor to obtain highefficient representations of range images. Then, we utilize softmax splatting [9] to warp the input frames and range features with the estimated flows. Finally, the in-between frame is generated. As shown in Fig. 1, our method can deliver a competitive performance for point cloud frame interpolation.
The main contributions of this paper are the following: · We propose a frame interpolation network for LiDAR
point cloud, which is conducted by exploring range images as an intermediate representation combined with CNNs. · To fully exploit spatial-temporal feature of point clouds as auxiliary information to synthesize the in-between frame, we introduce a range-adaptive feature extractor to produce spatial feature representations. On the other hand, we present a flow estimator for range images to generate high-efficient temporal feature. · We demonstrate that the proposed method can deliver a favorable performance against state-of-the-art video frame interpolation models (i.e., DAIN [7], Super SloMo [8]).
II. PROPOSED METHOD
A. Algorithm Overview
Given two input frames I0 and I1 of 3D point clouds, our approach aims to synthesize an intermediate frame It at

(a)

(b)

(c)
Fig. 2. Demonstrations of one frame from the KITTI dataset. (a) The 3D LiDAR point cloud; (b) The corresponding RGB front-view image for reference; (c) The 2D range image projected from (a).

LiDAR Sensors

N Hz Point Clouds

Encoder

Bitstream

Bitstream

Decoder

Decoded Point Clouds

Our Frame Interpolation
Network

2N Hz Point Clouds

Fig. 3. The illustration of the application into reducing the amount of the point cloud transmission.

time t  [0, 1]. Firstly, a 3D-to-2D projection is conducted to generate two 2D range images R0 and R1 as the input frames of our model. Then, the bi-directional optical flows, denoted by F01 and F10 respectively, are estimated. Next, based on the optical flows, we warp the input frames and range features, with a warping layer via softmax splatting. After that, we employ frame synthesis networks to produce the interpolated frame Rt in the range-image format. Finally, the interpolated point cloud frame It is reconstructed from Rt. Fig. 4 provides an overview of our framework. In what follows, the various stages of our framework will be respectively described in detail.
B. 3-D to 2-D Projection for Point Clouds
In our pipeline, LiDAR point clouds are firstly projected into a set of 2D range images [one can see Fig. 2 (c)], which is defined as [12]

u v

=

1 2

1 - arctan(y, x)-1

w

1 - arcsin zr-1 + up -1 h

(1)

where (x, y, z) are the 3D coordinates of a certain point in point clouds, and (u, v) are the 2D coordinates of the corresponding pixel after the projection. (h, w) are the height and width of the desired projected range image.  = up + down is the field-of-view of the LiDAR sensor along its vertical direction, and r = (x2 + y2 + z2) is the range from the point to the center of the sensor's coordinate system. In this way, each 3D point (x, y, z) is projected into a pixel (u, v) of the range image.

C. The Proposed Frame Interpolation Model
Our model is composed of the following sub-modules: the optical flow estimator, the range-adaptive feature extractor, and frame synthesis networks. We will subsequently discuss its individual components.

Input LiDAR Point Clouds

Frame !

Frame "

3D-2D Projection

! of Frame !
" of Frame " Range Images

Concatenate Concatenate Concatenate

!

!%

"

"%

Siamese Feature Pyramid Network

Conv 1×1

Warp 

Cost Volume

Correlation

!" and "!

Flow Estimation !$#

Upsampling with 2× Upscale

!$%##

Interpolated Frame $
Interpolated Frame $

Optical Flow Estimator

Frame ! and " (i.e., Coordinates and Range Values)

Warp

Warped Frames

Range Features

Warped Features

Frame Pre-Synthesis

Frame Refinement

2D-3D Reconstruction

Conv 3x3 SAC x 3 Conv 3x3 SAC x 3 Conv 3x3

Range-Adaptive Feature Extractor

Range Images

Frame Synthesis

Fig. 4. The flowchart of the proposed range-adaptive LiDAR point cloud frame interpolation method. It mainly consists of the following modules: 3D-to-2D projection, the optical flow estimator, range-adaptive feature extractor, the wraping layer via softmax, frame synthesis, and 2D-to-3D reconstruction. Note that the blue dotted box represents the specific structure of our optical flow estimator, which is modified from PWC-Lite [14], a lightweight architecture of PWC-Net [11].

Coordinate Map of Input Range Images Conv 7x7

Input Feature Map

Unfold (im2col)

Element-wise
Multiply Add

Conv 1x1

Conv 3x3

Fig. 5. Structure of the spatially adaptive convolution (SAC), which is extracted from the work [13]. It is incorporated into our feature extractor to generate range features adaptively for range images.

Optical flow estimator. It is well-known that PWC-Net [11] is one of the state-of-the-art optical flow methods. However, as a sub-module in the model of point cloud frame interpolation, its computational complexity is not desirable. Therefore, we modify PWC-Lite [14], a lightweight architecture of PWCNet, as our flow estimator. Our method follows the main pipeline of PWC-Lite, but with some adaptions in two aspects. (1) The original implementation aims to predict bi-directional optical flows for multiple frames. For low complexity and consistency with the input frame numbers of our system, we reduce the model size to estimate the optical flow only for two frames. The structure is depicted in Fig. 4 (top). (2) The loss function adopted in original PWC-Lite, is a measurement of pixel-wise similarities between color images. However, as mentioned previously, the inherited characteristics of range images are drastically different from that of color images. Therefore, Charbonnier penalty function [7] is utilized as the loss function due to its smoothness and robustness.
To further show the effectiveness of our modifications, we

additionally fine-tune PWC-Net for our frame interpolation method. The specific results can be seen in Table I. It can be obviously found that for range images, our flow estimator provides a better performance.
Range-adaptive feature extractor. In [13], a spatiallyadaptive convolutions (SAC) is adopted to achieve more efficient point cloud segmentation. Intuitively, it also could be a powerful tool for our feature extractor for range images. Therefore, we extract the SAC module (Fig. 5) to incorporate it into our feature extractor. As shown in Fig. 4 (bottom), we adopt the convolutional layer with the 3 × 3 kernel followed by three SAC modules, where Leaky ReLu ( = 0.1) is utilized after each convolutional layer. Note that there is not any normalization layer, e.g., batch normalization, since it has been proven that the batch normalization is not suited for image quality improvement [15]. We jointly train the feature extraction network with our entire model, which learns range features adaptively for point cloud frame interpolation.
Warping layer. It has been shown that forward warping via softmax splatting delivers new state-of-the-art results for video frame interpolation [9]. It can effectively address the issue incurred by the case that multiple source pixels are mapped to the same target location. Hence, in our work, with the estimated bi-directional optical flows, softmax splatting is employed to warp the input frames (i.e., containing x, y, and z coordinates, and range r), and the features of range images.
Frame synthesis. To generate the intermediate range image, following the coarse-to-fine architecture, we design the frame synthesis networks consisting of two cascaded U-Nets [16] with the same architecture, i.e., the frame pre-synthesis network and the frame refinement network respectively in Fig. 4. This is motivated by the fact that, benefited from a skip connection between the encoder and decoder, U-Net is well

suitable for the operation at pixel level [9]. To reduce the computation complexity, the cascade U-Nets are modified with less depths, which contain a six-layered encoder and fivelayered decoder. We use 3 × 3 kernels for all convolutional layers, followed with Leaky ReLu ( = 0.1). It should be noted that as illustrated previously, batch normalization is not adopted.
Note that the output of the frame pre-synthesis network is a `coarse' version of the range image. To yield one `fine' frame with better details, we concatenate it with two original range images as guidance information, and fed them into the frame refinement network to generate the interpolated range image Rt. Finally, the interpolated point cloud frame It can be reconstructed from Rt based on Equation (1). It is also worth noting that our model can generate the arbitrary intermediate frame It at time t  [0, 1].
D. Implementation Details
Loss function. In this work, we train the flow estimator and the frame interpolation model, both by optimizing the Charbonnier penalty function [17], i.e., f (x) = x2 + 2 ( = 1e - 6), but with different terms respectively.
For the flow estimator, suppose in each layer i of the feature pyramid network, Ri is the ground-truth frame, and Ri is the target frame, which is obtained by backwarping input frame with predicted optical flow F01. Then the loss function L1 can be computed by

1L

L1(R, R) = L

f (Ri(x) - Ri(x)),

(2)

i=1 x

where L is the total number of pyramid levels. For our interpolation model, the loss function contains

two loss terms, where the first term is for the pre-synthesis network, and the second term is for the frame refinement network. Assume I is the ground-truth frame, and I is the final synthesized frame, while Ip is the corresponding frame from the frame pre-synthesis network. The loss function L2 can be formulated as

L2(I, I) = 1 f (Ip(x) - I(x)) + 2 f (I(x) - I(x)),

x

x

(3)

where we empirically set 1 = 0.1 and 2 = 1. The rationale

behind this setting is that `fine' frame interpolation, as the

final objective, is always dominant, while the effect of `coarse'

version generation is limited.

Training dataset. We conduct our training process on the KITTI dataset [18]. It provides the odometry benchmark containing 22 sequences of point cloud data, which are collected by LiDAR sensors Velodyne HDL-64E, covering various road scenes. For each sequence, we partition every 3 consecutive frames into a triplet. With regard to each triplet, the first and third frames are as the input frames of our model, while the second frame is the ground-truth frame. In this way, 16 sequences (10327 triplets) are chosen as the training

dataset. Among the others, 3 sequences (2400 triplets) are for validation dataset, while 3 sequences (1784 triplets) are for test dataset. It is worthwhile to noting that each dataset covers all kinds of scenes.
Training strategy. Note that PWC-Lite is designed for videos (i.e., a sequence of color images), while the range images have diverse characteristics from color images. Therefore, instead of using the parameters of a pre-trained PWC-Lite, we train our flow estimator from scratch for 40 epochs to converge. The ADAM optimizer is adopted with 1 = 0.9, 2 = 0.999, and  = 1e-7. The initial learning rate is set to 1e-4 and reduced by 0.1 when the validation loss has stopped decreasing for 4 epochs.
After obtaining the well-trained flow estimation model, its parameters will be freezed. Then, we train the interpolation model for 40 epochs, with the same initial learning rate and learning rate schedule as mentioned above. Next, we reduce the learning rate to 1e - 5 and fine-tune the entire model for another 20 epochs. Similarly, the learning rate during finetuning process will be decayed by a factor of 0.2, when the validation loss has stopped declining for 5 epochs. The ADAM optimizer is with the default setting and batch size is set to 1. We implement all the training process with the NVIDIA 1080Ti GPU, which takes about 4 days to converge.
III. EXPERIMENTAL RESULTS
We evaluate our method on the test dataset derived from the KITTI dataset, and compare it to state-of-the-art video frame interpolation methods quantitatively. The same approaches for 3D-2D projection and 2D-3D reconstruction using Equation (1), are applied into the video frame interpolation methods under comparison, where we set H = 64 and W = 2048, as used in [12], [13]. All experiments are implemented using PyTorch and run on the same machine using an Intel Core i7-7700K and 11G RAM with GTX 1080Ti.
A. Evaluation Dataset and Metric
Test dataset. As described in Section II-D, three sequences of the KITTI dataset are chosen as the test dataset, containing totally 1784 triplets. We randomly select a quarter of the test dataset for this evaluation.
Metric. It should be noted that metrics adopted in video frame interpolation, i.e., PSNR and SSIM, are for 2D normal images, which are obviously not proper for 3D LiDAR point clouds. It is well-known that Symmetric Nearest Neighbor Root Mean Square Error (SNNRMSE) [19], is typically used to evaluate the quality of point cloud reconstruction. Therefore, in this paper, it is utilized for the performance evaluations of point cloud frame interpolation.
Assume P and Q are the original frames and interpolated frames of point clouds, where each point p in P can be find its closest point q in Q by the k-d tree search, denoted as q = f (p, Q). Then, the mean squared error (MSE) is obtained by measuring the euclidean distance of each point p in P to its nearest neighbor q in Q. That is,

Ground Truth

DAIN

Super SloMo

Ours

Fig. 6. Visual comparisons for two frames on KITTI dataset with zoom-out and zoom-in views. For all the shown examples, especially the frames with edges or points lie close to straight lines, our method perceptually outperforms other state-of-the-arts by a large margin.

MSE(P, Q) = (p - q)2/|P |,

(4)

pP

where |P | is the point number of P . Similarly, MSE(Q, P ) can be computed as well. Lastly, SNNRMSE is obtained by

M SE(I, Q) + M SE(Q, I)

RMSESNN (P, Q) =

. (5) 2

B. Performance Evaluations

Evaluation for flow estimation. As illustrated previously, our flow estimator is modified from PWC-Lite [14], which is a lightweight model of the state-of-the-art method PWCNet [11]. To show the effectiveness of our adaptations, we compare our flow estimator with these two approaches. Since our work aims at frame interpolation issue, we just make comparisons by the inference loss via Equation (2) for reference, during the designing stage of our flow-estimator. It is worth to mention that we also provide the result of the finetuned PWC-Net, which outperforms the original PWC-Net. It turns out that the original models for flow estimation are trained for color images. Therefore, for the final evaluations of our entire model, instead of using the weights of pre-trained models under comparison, we fine-tune all the methods using our training dataset.
As shown in Table I, our proposed method delivers the best results on flow estimation in all key aspects, i.e., runtime and inference loss. Another illustration for the effectiveness of our

TABLE I QUANTITATIVE COMPARISONS FOR FLOW ESTIMATION METHODS ON THE KITTI DATASET. `-FT' MEANS FINE-TUNING ON THE TRAINING SET.
THE PROPOSED FLOW ESTIMATION METHOD OUTPERFORMS OTHER APPROACHES IN TERMS OF LOSS AND RUNTIME.

Method PWC-Net [11]
PWC-Net-ft PWC-Lite [14]
Ours

Runtime (s) 0.0177 0.0183 0.0143 0.0142

Loss 3.1319 0.7226 3.1709 0.5242

TABLE II QUANTITATIVE COMPARISONS FOR FRAME INTERPOLATION METHODS ON THE KITTI DATASET. `-FT' MEANS FINE-TUNING ON THE TRAINING
SET. THE PROPOSED METHOD OUTPERFORMS OTHER APPROACHES IN RMSE WITH REAL-TIME PERFORMANCE.

Method Super SloMo-ft [8]
DAIN-ft [7] Ours + PWC-Net-ft [11]
Ours

Runtime (s) 0.0269 0.2380 0.0976 0.0961

RMSE (m) 0.5393 0.6224 0.3684 0.3107

flow estimator can be found in Table II, where the fine-tuned PWC-Net is adopted as the flow estimator in our model for comparison, denoted as `Ours + PWC-Net-ft '.
Evaluation for frame interpolation. To justify the effectiveness of the proposed frame interpolation method, we compare our method with the other two state-of-the-art methods for video frame interpolation, i.e., DAIN [7] and Super SloMo [8].

Table II reports the results on the test dataset. It can be clearly observed that our method outperforms other approaches in term of RMSE. Though our approach is computationally much more expensive than Super SloMo, we can still deliver the real-time performance (running faster than the frequency of LiDAR sensors, i.e., 10Hz).
In addition, Fig. 6 shows two representative frames of interpolated samples. It can be seen that our method is able to constantly provide the most desirable interpolation results from the viewpoint of perceptual quality, especially on the regions with edges or points lie close to straight lines (referred to red and blue-framed regions cropped from the frames respectively).
IV. CONCLUSION
In this paper, we propose a novel framework for LiDAR point cloud frame interpolation, which can be applied to reduce the amounts of point cloud transmission. The main contribution lies in the newly developed model, which exploits spatial-temporal features, extracted from 2D representations (range images) of 3D point clouds, as the auxiliary information to synthesize the intermediate frame. Considering that inherited characteristics of range images are diverse from that of normal color images, we introduce spatially adaptive convolutions to generate range features adaptively, while a high-efficient flow estimator is presented for optical flow extraction. Lastly, we warp input frames and range features with estimated flow to synthesize the intermediate frame. Extensive experiments have demonstrated that the proposed method can consistently deliver superior results and the most visuallypleasant quality, compared with that by using other state-ofthe-art video frame interpolation methods. The directions for out future work are to develop a faster and more efficient model, and integrate the model into multiple point cloud compression structures.

[8] H. Jiang, D. Sun, V. Jampani, M. Yang, E. Learned-Miller, and J. Kautz, "Super SloMo: High quality estimation of multiple intermediate frames for video interpolation," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), Jun. 2018, pp. 9000­9008.
[9] S. Niklaus and F. Liu, "Softmax splatting for video frame interpolation," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 5436­5445.
[10] S. Baker, D. Scharstein, J.P. Lewis, S. Roth, M.J. Black, and R. Szeliski, "A database and evaluation methodology for optical flow," Int. J. Comput. Vis., vol. 92, no. 1, pp. 1­31, 2011.
[11] D. Sun, X. Yang, M. Liu, and J. Kautz, "PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), Jun. 2018, pp. 8934­8943.
[12] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss, "RangeNet ++: Fast and accurate LiDAR semantic segmentation," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), Nov. 2019, pp. 4213­4220.
[13] C. Xu, B. Wu, Z. Wang, W. Zhan, P. Vajda, K. Keutzer, and M. Tomizuka, "Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation," arXiv e-prints, arXiv:2004.01803, Apr. 2020.
[14] L. Liu, J. Zhang, R. He, Y. Liu, Y. Wang, Y. Tai, D. Luo, C. Wang, J. Li, and F. Huang, "Learning by analogy: Reliable supervision from transformations for unsupervised optical flow estimation," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), Jun. 2020, pp. 6489­6498.
[15] S. Nah, T. H. Kim, and K. M. Lee, "Deep multi-scale convolutional neural network for dynamic scene deblurring," in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Jul. 2017, pp. 257­265.
[16] P. Fischer O. Ronneberger and T. Brox, "U-net: Convolutional networks for biomedical image segmentation," in Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent., Oct. 2015, pp. 234­241.
[17] P. Charbonnier, L. Blanc-Feraud, G. Aubert, and M. Barlaud, "Two deterministic half-quadratic regularization algorithms for computed imaging," in Proc. IEEE Int. Conf. Inf. Process.(ICIP), Nov. 1994, vol. 2, pp. 168­172.
[18] A. Geiger, P. Lenz, and R. Urtasun, "Are we ready for autonomous driving? the kitti vision benchmark suite," in Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), Jun. 2012, pp. 3354­3361.
[19] M. Ruhnke, L. Bo, D. Fox, and W. Burgard, "Hierarchical sparse coded surface models," in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), May 2014, pp. 6238­6243.

REFERENCES
[1] X. Chen, A. Milioto, E. Palazzolo, P. Gigue`re, J. Behley, and C. Stachniss, "SuMa++: Efficient LiDAR-based semantic SLAM," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), Nov. 2019, pp. 4530­ 4537.
[2] C. Tu, E. Takeuchi, A. Carballo, and K. Takeda, "Real-time streaming point cloud compression for 3D LiDAR sensor using U-Net," IEEE Access, vol. 7, pp. 113616­113625, Aug. 2019.
[3] T. N. Mundhenk, K. Kim, and Y. Owechko, "Frame rate fusion and upsampling of EO/LIDAR data for multiple platforms," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2014, pp. 762­769.
[4] Fan Lu, Guang Chen, Sanqing Qu, Zhijun Li, Yinlong Liu, and Alois Knoll, "PointINet: Point Cloud Frame Interpolation Network," arXiv e-prints, arXiv:2012.10066, Dec. 2020.
[5] S. Meyer, A. Djelouah, B. McWilliams, A. Sorkine-Hornung, M. Gross, and C. Schroers, "Phasenet for video frame interpolation," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2018, pp. 498­507.
[6] S. Niklaus, L. Mai, and F. Liu, "Video frame interpolation via adaptive separable convolution," in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 261­270.
[7] W. Bao, W. Lai, C. Ma, X. Zhang, Z. Gao, and M. Yang, "Depthaware video frame interpolation," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), Jun. 2019, pp. 3703­3712.

