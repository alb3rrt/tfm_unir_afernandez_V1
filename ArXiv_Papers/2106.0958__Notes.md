
# A Generalizable Approach to Learning Optimizers

[arXiv](https://arxiv.org/abs/2106.0958), [PDF](https://arxiv.org/pdf/2106.0958.pdf)

## Authors

- Diogo Almeida
- Clemens Winter
- Jie Tang
- Wojciech Zaremba

## Abstract

A core issue with learning to optimize neural networks has been the lack of generalization to real world problems. To address this, we describe a system designed from a generalization-first perspective, learning to update optimizer hyperparameters instead of model parameters directly using novel features, actions, and a reward function. This system outperforms Adam at all neural network tasks including on modalities not seen during training. We achieve 2x speedups on ImageNet, and a 2.5x speedup on a language modeling task using over 5 orders of magnitude more compute than the training tasks.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{almeida2021generalizable,
      title={A Generalizable Approach to Learning Optimizers}, 
      author={Diogo Almeida and Clemens Winter and Jie Tang and Wojciech Zaremba},
      year={2021},
      eprint={2106.00958},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

