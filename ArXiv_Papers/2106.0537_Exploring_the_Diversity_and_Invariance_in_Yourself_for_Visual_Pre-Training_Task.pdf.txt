arXiv:2106.00537v1 [cs.CV] 1 Jun 2021

Exploring the Diversity and Invariance in Yourself for Visual Pre-Training Task
Longhui Wei1,2, Lingxi Xie2, Wengang Zhou1, Houqiang Li1, Qi Tian2 1University of Science and Technology of China, 2Huawei Inc. weilh2568@gmail.com, 198808xc@gmail.com
zhwg@ustc.edu.cn, lihq@ustc.edu.cn, tian.qi1@huawei.com
Abstract
Recently, self-supervised learning methods have achieved remarkable success in visual pre-training task. By simply pulling the different augmented views of each image together or other novel mechanisms, they can learn much unsupervised knowledge and significantly improve the transfer performance of pre-training models. However, these works still cannot avoid the representation collapse problem, i.e., they only focus on limited regions or the extracted features on totally different regions inside each image are nearly the same. Generally, this problem makes the pre-training models cannot sufficiently describe the multi-grained information inside images, which further limits the upper bound of their transfer performance. To alleviate this issue, this paper introduces a simple but effective mechanism, called Exploring the Diversity and Invariance in Yourself (E-DIY). By simply pushing the most different regions inside each augmented view away, E-DIY can preserve the diversity of extracted region-level features. By pulling the most similar regions from different augmented views of the same image together, E-DIY can ensure the robustness of region-level features. Benefited from the above diversity and invariance exploring mechanism, E-DIY maximally extracts the multi-grained visual information inside each image. Extensive experiments on downstream tasks demonstrate the superiority of our proposed approach, e.g., there are 2.1% improvements compared with the strong baseline BYOL on COCO while fine-tuning Mask R-CNN with the R50-C4 backbone and 1× learning schedule.
1 Introduction
Nowadays, contrastive learning based methods [17, 3, 5, 29, 37] have achieved remarkable success in computer vision tasks. With simply pulling the different augmented views of each image together and pushing the different images away, they can learn much unsupervised knowledge from largescale unlabeled images. Extensive experiments have shown these works can achieve competitive or sometimes even better results on downstream tasks compared with the fully-supervised learning methods [7, 8, 43, 21]. For the above appealing points taken by these self-supervised learning methods, more and more researchers pay attention on this to learn good visual pre-training models.
Recently, there are mainly two new directions for self-supervised learning methods. One is to weaken the necessity of negative pairs for avoiding careful treatment of these pairs [14, 40]. For example, BYOL [14], the first work with only pulling the different augmented views of each image together, can achieve much competitive transfer performances on downstream tasks. The other is to focus on region-level (or pixel-level) information but not imagelevel as usual [33, 40], e.g., Xie et al. [40] proposed Pixel-to-Propagation (PixPro) module to explore the region-level consistency and extract discriminative region-level features.

BYOL

Though extensive experiments show the above works can significantly push forward the transfer performance of pretraining models, they still meet the representation collapse challenge and thus cannot describe the whole information inside each image.

E-DIY

For example, as shown in Fig. 1, because BYOL only targets to pull each different augmented

Figure 1: Illustration of the saliency maps generated by different pretraining methods. Generally, BYOL tends to attend on limited regions, but our proposed E-DIY tends to learn pre-training knowledge from more regions.

views together, the

learned knowledge is

biased inevitably and focuses on limited regions (e.g., only attending on the bird body), which is

somewhat similar with the character of the supervised learning methods [21, 36]. Moreover, the

region-level self-supervised learning methods, taking PixPro as an example, can force the pre-training

models to focus on the region-level information. However, it only preserves the consistency of the

features extracted on neighborhood regions, but cannot ensure the diversity of features extracted on

totally different regions inside each image. Therefore, the useful learned knowledge can also be

restricted to limited regions (e.g., only the overlapped regions of two augmented views). We call

the above phenomenon as the representation collapse problem. Generally, a perfect pre-training

model should contain multi-grained information (e.g., the bird, the stump and the background) as

much as possible. Therefore, though the above works have achieved good transfer performances on

downstream tasks, there is still much room for further improvement.

Target to address this issue, we propose a simple but effective mechanism, called Exploring the Diversity and Invariance in Yourself (E-DIY). Based on BYOL, E-DIY additionally includes two modules, the region-level diversity exploring module (R-DEM) for enhancing the diversity of extracted features on totally different regions inside each image and the region-level invariance exploring module (R-IEM) for ensuring the robustness of extracted region-level features. As shown in Fig. 3, the most similar regions in two augmented views will be found and pulled together in R-IEM, which can enhance the region-level invariance compared with only pulling the global features in BYOL. Meantime, the most dissimilar regions inside each augmented view will be searched and pushed away in R-DEM, which can guide the model to learn on more regions other than limited biased discriminative regions (e.g., the regions only benefitting to the classification task). Therefore, compared with BYOL only ensuring the image-level invariance and PixPro aiming for the region-level consistency, E-DIY targets to explore both the diversity and invariance inside each image. Generally, E-DIY can maximally learn fine-grained information on each region and meantime ensure the robustness of extracted features, which benefits a lot to visual pre-training task.

We evaluate E-DIY on various detection and segmentation downstream tasks. Extensive experimental results show E-DIY heavily improves the transfer performance of pre-training models. For example, compared with the strong baseline BYOL, there are 2.1%/1.7% improvements on COCO detection/instance segmentation task while fine-tuning Mask R-CNN with R50-C4 backbone and 1× learning schedule. Moreover, compared with the recent region-level pre-training works [33, 40], our proposed E-DIY still shows good advantages on downstream tasks, which further reflects the necessity of exploring both the region-level invariance and diversity in visual pre-training task. More details of E-DIY and discussions with previous works are shown in Sec. 3.

2 Related Work
The key for self-supervised learning [17, 44, 15, 10, 34, 24, 32, 12, 1, 26, 16, 23, 20, 31] is how to design the pretext task. Previously, the commonly used pretext tasks in self-supervised learning field are jigsaw puzzle [35, 10], image restoration [22, 28], image rotation prediction[13, 4] and etc. Though these works indeed have the ability to extract useful visual information from massive

2

unlabeled images, there is still much room to achieve competitive performance as the supervisedlearning methods [7, 8, 21, 42].
Recently, contrastive learning sheds a new way to effectively learn unsupervised representation. Compared with the previous pretext tasks, contrastive learning heavily pushes forward the upper-bound performance of self-supervised learning methods. Inspired by the above, lots of contrastive learning based works [37, 17, 5, 3, 14, 39] are designed. For example, Momentum Contrast (MoCo) [17] proposed to build a dynamic memory bank for enlarging the contrastive negative samples, which can break the limitation of GPU memory and heavily improve the final performance. Compared with the traditional supervised learning methods, MoCo firstly revealed the advantages of self-supervised learning methods in the visual pre-training task. Moreover, SimCLR [3] evaluated the importance of data augmentation for contrastive learning based self-supervised methods, and significantly improved the transfer performance. Additionally, InfoMin [30] further analyzed the necessity of view selection. Other than the contrastive learning based methods, some other novel approaches have been also proposed. For example, BYOL [14] evaluated that simply pulling the different augmented views of each image together can also achieve competitive performance. SwAV [2] proposed one online-clustering based mechanism to further improve the performance of pre-training models.
However, most of the above works only consider the instance-level discrimination, but ignore the details inside each image. Target to achieve better results on dense prediction downstream tasks, e.g., detection and segmentation task. Dense Contrast Learning (DenseCL) [33] firstly utilized the regionlevel (or pixel-level) contrast loss to enhance the region-level discrimination of pre-training models. Besides, Pixel-to-Propagation (PixPro) [40] proposed to explore the region-level consistency and utilized the pixel propagation module to achieve spatial smoothness of region-level representation. By simply adding the region-level contrast or consistency loss, the above two works significantly improve the transfer performance of pre-training models, e.g., compared with the baseline (MoCo), there are 0.5% improvements for DenseCL on COCO detection task.
Our work is also a region-level based self-supervised method. Different from DenseCL and PixPro, the core of E-DIY is to explore the naturally existing diversity inside each image (its importance is ignored previously). Meanwhile, target to ensure the robustness of extracted region-level features, E-DIY also utilizes the region-level invariance exploring mechanism. More details about E-DIY are described in the next section.

3 Explore the Region-level Diversity and Invariance in Yourself

3.1 Preliminaris: Self-supervised Learning

The core of self-supervised learning is to design a suitable pretext task for maximally guiding the network to learn useful knowledge on unlabeled data. Suppose the unlabeled data as D and pretext task as P , the optimization goal of self-supervised learning can be represented as:

Knowledge = arg max Optim(P, D),

(1)

P

where Optim denotes the optimizer for unsupervised knowledge learning.

Bootstrap Your Own Latent (BYOL) [14] is a current state-of-the-art self-supervised learning method. Through a simple pretext task with only keeping the instance-level representation consistency of two different augmented views of each image, BYOL can effectively learn discriminative unsupervised representation. Suppose one image xi, its two augmented views can be represented as xi and xi , respectively. The online and target networks for different views are represented as f and g, thus the loss of BYOL can be formulated as:

LBxiYOL

=

4

-

2

||f

f (xi ) (xi )||2

· g(xi ) · ||g(xi )||2

-

2

g(xi ) ||g(xi )||2

·

· f (xi ) ||f (xi )||2

,

(2)

The core of BYOL is to prevent the model collapse by designing a prediction task with an iteratively enhanced target network and online network. However, though BYOL is relatively simple compared with contrastive learning based methods [17, 5], it still meets the overfitting challenge.

As shown in Eq. 2, suppose the augmentation policies for online network f and target network g are the same, it is hard to optimize for f because that any random weights but same with g can

3

Online Network : !

Aug View1 #$

Image #

Target Network : "

Aug View2 #

Global Projector

Backbone

Local Projector

Teacher

(

Backbone

Momentum Backbone

!&(# $) LR-DEM

!&(# $)

) "& (# )

LR-IEM

Momentum Local Projector

Momentum Global Projector

Predictor
!(#$) LBYOL
"(#)

Figure 2: The framework of E-DIY. Teacher Backbone represents a pre-trained teacher model for computing the region similarity and then guiding the online network to learn the region-level relationship (diversity and invariance character inside each image). Global Projector is to output the instance-level feature and Local Projector is to output the region-level feature. The architectures of these two modules are the same as the projector in BYOL [14]. More details about E-DIY can be seen in Sec. 3.

make the loss minimum. Therefore, the learned knowledge is random and meaningless. Target to address this problem, researchers mainly carefully design the augmentation policy. By regularizing the input with different augmentation policies for online network f and target network g, it actually can prevent the model collapse but further cause the representation collapse problem. For example, while two augmented views are cropped from totally different regions in one image, the optimization goal of Eq. 2 is to pull their generated features together, which is unreasonable (e.g., the feature generated on the sky region (the background) should not be the same with the feature generated on the bird region (the foreground)). However, while there are overlapped regions for these two inputs, the optimization goal will guide the model to focus on the overlapped regions but ignore other information. As shown in Fig. 1, BYOL only attends on limited discriminative regions (i.e., part of the foreground), which restricts its upper-bound performance on downstream tasks. The above phenomenon reveals the representation collapse problem, for that the pre-training models learned by the above scheme cannot fully describe the multi-grained information inside each image. Given the suppose that, more knowledge learned in the pre-training stage, better performance on downstream tasks, the naturally existing multi-grained information inside every single image should be further exploited for better pre-training models.
3.2 Region-level Diversity and Invariance Exploring Mechanism
Target to enhance the transfer performance of BYOL, we propose a region-level diversity and invariance exploring mechanism (called E-DIY). The overall framework of this mechanism is illustrated in Fig. 3. E-DIY is motivated by the information inside each image is multi-grained. As shown in Fig. 1, even for the simple image with one bird, it still contains two regions, i.e., the background and foreground. Moreover, in the foreground, its head region in the bird is different from its body region. Therefore, it is unreasonable to describe the image with limited discriminative region features for the visual pre-training task. Target to address this issue, we firstly propose the region-level diversity exploring module (R-DEM) to enhance the richness of extracted features. As shown in Fig. 3, R-DEM seeks the most dissimilar regions in one image, and tries to push them away in feature space. Obviously, R-DEM aims for exploring the naturally existing diversity character inside each image. However, only pushing the dissimilar regions tends to make the learned features sensitive to spatial relation, which is harmful to the pre-training task. To further ensure the robustness of pre-training model, we propose the region-level invariance exploring module (R-IEM). Similar to R-DEM, R-IEM seeks the most similar regions in different augmented views, and tries to pull them
4

Inner-level Feature Map of View 1 Pull
Pull

Inner-level Feature Map of View 2

Pull

Pull

Push

Push

R-IEM

Push

Push

R-DEM

R-DEM

Figure 3: Illustration of the proposed region-level diversity exploring module (R-DEM) and regionlevel invariance exploring module (R-IEM). Generally, R-DEM tries to push the most different regions inside the same augmented view away, and R-IEM tries to pull the most similar regions among the different augmented views of the same image together. The matched pairs (different or similar regions) are searched by ranking the similarity of their region features. More details can be seen in Sec. 3.

together in the latent feature space. Thus, guided by R-IEM, the pre-training model can be ensured to learn robust and discriminative region-level features.
Based on the above discussion, the loss function of our E-DIY for each image xi can be formulated as:

LExi-DIY = 1LRxi-DEM + 2LRxi-IEM + 3LBxiYOL,

(3)

where  represents the loss weight for each module. The details of R-DEM and R-IEM are described

as follows.

3.2.1 Region-level Diversity Exploring Module (R-DEM)

The motivation of our Region-level Diversity Exploring Module (R-DEM) is to prevent the overfitting problem and extract the multi-grained information as much as possible. Therefore, the core of R-DEM is how to prevent the representation collapse ( i.e., avoid focusing on limited regions). Target to achieve this, R-DEM extends the representation space of images by enlarging the distance of different regions inside each image. However, it is impossible to require the region labels for every image because of the costly annotations. To relieve this burden, and naturally combine with the self-supervised learning framework, R-DEM utilizes one extra self-supervised learning model ( e.g., the model trained by BYOL) as the teacher to seek the different regions by the feature similarity degree. Therefore, given the region xi:rj in one augmented view of image xi, its most different region inside the same augmented view can be found by the following:

RD
xi:rj

= arg min sim(T(xi )rz , T(xi )rj ),
z

(4)

where T represents the feature encoder of teacher model, T(xi )rz represents the feature of one region xi:rz in the augmented view xi . After finding the most different region for each region in one image, the loss function of D-REM can be formulated as:

LRxi-DEM

=

1 n

n j=1

f l(xi )rj ||f l(xi )rj ||2

·

f

(x ) l 
i RD xi:rj

·

||f

(x ) l

 i RD xi:rj

||2

+

1 n

n j=1

f l(xi )rj ||f l(xi )rj ||2

· ·

f l(xi )RD xi:rj ||f l(xi )RD xi:rj

||2

,

(5)

5

where n denotes the sum of region number in each augmented view, f l represents the region-level feature encoder (as shown in Fig. 2), thus f l(xi )rj denotes the feature of the region xi:rj in the feature map f l(xi ).

3.2.2 Region-level Invariance Exploring Module (R-IEM)

Contrary to R-DEM, Region-level Invariance Exploring Module (R-IEM) targets to exploit the naturally existing region-level consistency in different augmented views of the same image. Thus, the core of R-IEM is to look for consistent regions. Similar to R-DEM, R-IEM also utilizes the teacher guided feature similarity ranking scheme to find the most matched region of each query region:

RS
xi:rj

= arg max sim(T(xi )rz , T(xi )rj ),
z

(6)

Therefore, the loss function of R-IEM can be represented as:

LxRi-IEM

=

1 2-
n

n j=1

f l(xi )rj ||f l(xi )rj ||2

· ·

gl(xi )RSxi:rj ||gl(xi )RSxi:rj

||2

1 -
n

n j=1

f l(xi )rj ||f l(xi )rj ||2

· ·

gl(xi )RSxi:rj ||gl(xi )RSxi:rj

, ||2

(7)

where gl represents the region-level feature encoder of the target network.

3.3 Difference from Previous Works
The major difference with previous region-level based works [40, 33] is that our E-DIY is designed to explore the multi-grained information inside every single image. Unlike the PixPro only keeping the region-level consistency of two augmented views, or the DenseCL utilizing other global images in the batch as the negative samples, E-DIY looks for the most different regions inside each image and enlarges the similarity of their features for maximally guiding the pre-training model to attend on multiple regions. Moreover, to ensure that the extracted region-level features are discriminative and robust, E-DIY further utilizes the region-level consistency principle, which is similar to PixPro. Though the modification of E-DIY is simple compared with these previous works, extensive experiments on downstream tasks show E-DIY heavily pushes forward the transfer performances on various downstream tasks.
Moreover, to our best knowledge, E-DIY is the first to reveal that the representation collapse problem does harm to pre-training models. No matter for the supervised learning or self-supervised learning based pre-training methods, they generally overfit on their pretext task. As a result, the learned knowledge is coarse and contains only a small share of the overall knowledge provided by original data. E-DIY struggles with this challenge by enlarging the diversity of learned knowledge. Therefore, other than self-supervised learning methods (e.g., BYOL), E-DIY can also be regarded as a supplement to other supervised pre-training methods, which is not the core of our paper and will be left for future research.

4 Experiments
4.1 Datasets
ImageNet-1K [9] is the most commonly used pre-training dataset in computer vision community. It contains 1.28 million images of 1, 000 classes. In this paper, we also pre-train E-DIY on ImageNet-1K and evaluate its effectiveness on various downstream datasets.
Following DenseCL [33], we mainly conduct evaluations on PASCAL VOC [11] and COCO [25]. PASCAL VOC is a commonly used detection dataset, and the size of this dataset is relatively small. In the following, we denote PASCAL VOC as VOC for short. In this paper, all of the experiments on VOC will be trained on VOCtrainval07+12, and tested on VOCtest07. COCO is a more large-scale dataset, and it contains varies of annotations, e.g., bounding box, instance segmentation, keypoint and etc. In this paper, we mainly conduct evaluations on detection and instance segmentation tasks of COCO. All of the models will be trained on COCOtrain2017 and tested on COCOval2017.

6

Table 1: Results of different pre-training approaches while fine-tuning Mask-RCNN with the R50FPN backbone and 1× learning schedule on COCO. All of the models are pre-trained with 300 epochs on ImageNet-1K. TG represents our proposed teacher guided region sampling scheme, R denotes the random region sampling scheme, and R-INS represents that randomly sampling the global feature of another image as DenseCL [33]. COCODet denotes the detection task on COCO, and COCOInsSeg represents the instance segmentation task on COCO, respectively.

BYOL
-

R-DEM
TG
TG TG TG R R-INS

R-IEM
TG TG TG TG R TG

COCODet APbb APb50b APb75b 40.1 61.6 43.7 35.3 54.7 38.4 3.8 7.1 3.7 39.8 59.9 43.5 40.5 61.3 44.2 40.3 61.5 44.1 41.1 62.2 45.0 40.4 61.6 44.2 40.3 61.5 43.8

COCOInsSeg APmk APm50k APm75k 36.7 58.4 39.4 32.3 52.0 34.5
3.7 6.8 3.6 36.2 57.0 38.9 36.8 58.3 39.5 36.8 58.4 39.2 37.3 58.9 40.1 36.9 58.7 39.4 36.7 58.4 39.1

4.2 Implementation Details
In the pre-training stage while training E-DIY, we strictly follow the image augmentation strategy in BYOL [14], i.e., random resized crop operation followed by color jitter, grayscale conversion, Gaussian blur, solarization, and random flip. For the network architecture, we also follow the same design of BYOL. Differently, there are two projectors in E-DIY, where one is for the instance-level feature and the other is for the region-level feature. The architecture of these two projectors is the same as the projector in BYOL. Moreover, target to compute the region-level similarity, there is one additional teacher model (ResNet-50 [19], trained by BYOL). We calculate the similarity of regions with the last feature map in layer-4 of the teacher model. For the mini-batch of each GPU, we randomly sample 128 images. Finally, we utilize 16 GPUs, and thus a total of 2048 images are sampled in each iteration. Following BYOL, we adopt LARS optimizer [41] and the initial learning rate is set as 3.2. To address the limited batch size and reproduce the results of BYOL, we utilize the gradient accumulation strategy. Finally, we train E-DIY with 1, 000 epochs on Imagenet-1K.
In the downstream evaluation stage, we utilize Detectron2 [38] framework to fine-tune the Faster R-CNN [27] with R50-C4 backbone on VOC. For the training and testing on the detection task of VOC, we strictly follow the settings in MoCo [17]. Additionally, for the detection and instance segmentation task on COCO, we fine-tune the Mask R-CNN [18] with different backbones and learning schedules as MoCo. More details can be seen in this paper [17].
4.3 Ablation Study
Importance of region-level diversity and invariance exploring module. In this section, we firstly evaluate the importance of our proposed region-level diversity and invariance exploring mechanism. As shown in Tab. 1, only utilizing the R-DEM or R-IEM, the pre-training models fail to transfer into downstream tasks. Especially, while only adopting R-IEM to guide the model training, it will meet the model collapse problem. However, while combining the R-DEM or R-IEM with BYOL, the transfer performance can be slightly improved. For example, there are 0.4% improvements on the Average Precision (AP) metric of COCO detection task while combing BYOL with R-DEM module. Moreover, it is interesting that, the improvements of E-DIY (consisting of R-DEM, R-IEM and BYOL) are more significant than each of the combinations of every two modules, which further reveals these three modules are complementary in enhancing the performance of pre-training models.
Additionally, compared with the method that pulling the randomly sampled regions of the same augmented views together and pushing the randomly sampled regions of different augmented views away, E-DIY can learn much more transferable knowledge. This result demonstrates the effectiveness of our teacher guided region sampling strategy. Moreover, compared with the negative pairs sampling mechanism in DenseCL [33] (which randomly sampled other different images as the negatives pairs for the anchor patch in current image), E-DIY heavily surpasses its performance. The significant
7

Table 2: Ablation study on the loss weights of each module in E-DIY. All of the models are trained with 100 epochs on ImageNet-1K.

BYOL
0.1 1.0 1.0 1.0

R-DEM
1.0 0.1 1.0 1.0

R-IEM
1.0 1.0 0.1 1.0

COCODet APbb APb50b APb75b 38.6 58.5 41.9
40.1 61.2 43.6
38.8 59.3 42.4
40.3 60.8 44.3

COCOInsSeg APmk APm50k APm75k 35.1 55.7 37.7
36.6 58.1 39.2
35.4 56.1 37.9
36.6 57.6 39.1

improvements can mainly own to two reasons: fixing the feature misalignment problem and avoiding the representation collapse challenge. The negative pairs sampling mechanism in DenseCL tries to push the region-level features and global-level features, which exists the misalignment problem. Differently, E-DIY tries to handle the relations of different regions. Therefore, the feature misalignment problem does not exist in E-DIY. Moreover, E-DIY explores the diversity inside each image at least, which can guide the pre-training model to attend on more regions as much as possible. However, previous works [33, 40] cannot make sure this and easily meet the overfitting problem.
The impact of hyper-parameters. We also conduct evaluations on the hyper-parameters of E-DIY. As shown in Tab. 2, while reducing the loss weights of each module, the transfer performance on downstream tasks will drop with different degrees. For example, while reducing the loss weight of R-IEM module from 1.0 to 0.1, the transfer performance on COCO detection task drops from 40.3% to 38.8% on AP metric. The above results further evaluate the importance of each module in E-DIY for the visual pre-training task.

4.4 Comparisons with Previous Works on Downstream Tasks

Evaluations on VOC. We firstly conduct com-

parisons with previous works on VOC detection task. As shown in Tab. 3, E-DIY achieves Table 3: Results of different pre-training apcompetitive performance compared with recent proaches on VOC detection task. Following BYOL, state-of-the-art methods. Though the baseline E-DIY is trained with 1000 epochs on ImageNet(i.e., BYOL) achieves much worse performance 1K. The reported numbers of E-DIY are the averon VOC, E-DIY can heavily push forward its age of 3 trials.

performance (e.g, there are 7.1% improvements on AP75 metric compared with BYOL). Moreover, compared with other recent state-of-the-art

Approaches SimCLR [3]

VOC AP AP50 AP75 51.5 79.4 55.6

methods, E-DIY still shows competitive trans-

MoCo-v2 [5] 57.4 82.5 64.0

fer performances. Generally, the above results demonstrate the advantages of our proposed

Infomin [30] 57.5 82.5 64.0 SwAV [2] 56.1 82.6 62.7

method in visual pre-training task.

SimSiam [6] 57.0 82.4 63.7

Evaluations on COCO. To further evaluate the effectiveness of E-DIY, we conduct evaluations

BYOL [14] 53.1 81.9 58.6

E-DIY

58.4 83.8 65.7

on COCO detection and instance segmentation

tasks. Following the experimental settings in

MoCo [17], we fine-tune the Mask R-CNN with different backbones and learning schedules on

COCO. All of the results are shown in Tab. 4 and Tab. 5. As shown in these tables, E-DIY shows

clear advantages on transferring the pre-trained knowledge. For example, while fine-tuning the

Mask R-CNN with R50-C4 backbone and 2× learning schedule on COCO, E-DIY achieves 42.4%

in AP metric on detection task, which surpasses BYOL with 1.2% improvements. In the instance

segmentation task, E-DIY also achieves 0.9% improvements compared with BYOL. Moreover,

compared with recent state-of-the-art methods, E-DIY also shows consistent advantages on different

downstream tasks, which clearly demonstrates the effectiveness of E-DIY for visual pre-training task.

4.5 Impact on Linear Classification Task
Though E-DIY achieves consistent improvements on various downstream tasks, one additional question is whether E-DIY could do harm to image classification task for it only requiring limited

8

Table 4: Results of adopting different pre-training models to fine-tune Mask R-CNN with the R50-

FPN backbone and different learning schedules on COCO. Following BYOL, E-DIY is trained with

1000 epochs on ImageNet-1K. Most of the reported methods follow the fine-tuning protocol in MoCo,

but represents that the method follows the fine-tuning protocol in InfoMin.

Methods

COCODet

1× schedule

2× schedule

APbb APb50b APb75b APbb APb50b APb75b

COCOInsSeg

1× schedule

2× schedule

APmk APm50k APm75k APmk APm50k APm75k

FSup-Softmax [17] 38.9 59.6 42.7 40.6 61.3 44.4 35.4 56.5 38.1 36.8 58.1 39.5

MoCo-v2 [5]

39.2 59.9 42.7 41.6 62.1 45.6 35.7 56.8 38.1 37.7 59.3 40.6

BYOL [14]

41.2 62.6 45.1 42.0 63.3 46.0 37.5 59.4 40.2 38.1 60.1 41.2

InfoMin Aug. [30] 40.6 60.6 44.6 42.5 62.7 46.8 36.7 57.7 39.4 38.4 59.7 41.4

SCAN [37]

41.8 62.1 45.7 43.2 63.3 47.1 37.8 59.2 40.9 38.8 60.6 41.6

DenseCL [33]

40.3 59.9 44.3 -

-

-

36.4 57.0 39.2

-

-

-

PixPro [40]

41.4 61.6 45.4 -

-

-

-

-

-

-

-

-

E-DIY

41.5 62.4 45.3 42.5 63.3 46.4 37.5 59.2 40.2 38.3 60.3 41.1

E-DIY

42.1 62.1 46.0 43.4 63.5 47.3 37.9 59.1 40.9 39.1 60.7 42.2

Table 5: Results of adopting different pre-training models to fine-tune Mask R-CNN with the R50-C4

backbone and different learning schedules on COCO. Following BYOL, E-DIY is trained with 1000

epochs on ImageNet-1K.

Methods

COCODet

1× schedule

2× schedule

APbb APb50b APb75b APbb APb50b APb75b

COCOInsSeg

1× schedule

2× schedule

APmk APm50k APm75k APmk APm50k APm75k

FSup-Softmax [17] 38.2 58.2 41.2 40.0 59.9 43.1 33.3 54.7 35.2 34.7 56.5 36.9

MoCo-v2 [5]

39.5 59.1 42.7 41.2 61.0 44.8 34.5 55.8 36.7 35.8 57.6 38.3

BYOL [14]

38.6 59.2 41.6 41.2 61.4 44.7 33.6 55.3 35.4 35.7 57.8 38.1

InfoMin Aug. [30] 39.0 58.5 42.0 41.3 61.2 45.0 34.1 55.2 36.4 36.0 57.9 38.3

SCAN [37]

40.1 60.2 43.2 41.7 61.7 45.4 34.9 56.6 37.1 36.2 58.3 38.6

E-DIY

40.7 60.8 43.8 42.4 62.4 45.7 35.3 57.3 37.7 36.6 58.9 39.1

discriminative information, e.g., the foreground region, which is similarly contradictory with the

motivation of E-DIY. Target to answer this question, we further conduct evaluations on linear

evaluation task of ImageNet-1K. All of the experimental details are strictly followed with BYOL.

As shown in Tab. 6, the performance of

E-DIY is slightly decreased compared with the baseline BYOL, e.g., dropping Table 6: Accuracy of adopting different pre-training from 74.3% to 73.8% on Top-1 accu- methods to fine-tune ResNet-50 under linear evaluation racy. Accordingly, we can conclude task on ImageNet-1K.

that though E-DIY targets to extract the Methods

Top-1 Acc.(%) Top-5 Acc.(%)

naturally existing multi-grained infor- CMC [29]

64.1

-

mation from as many regions as possi- SimCLR [3]

69.3

89.0

ble inside each image, it also preserves MoCo-v2 [5]

71.1

-

the most discriminative information well. InfoMin Aug. [30]

73.0

91.1

Therefore, E-DIY is a more reliable pre- BYOL [14]

74.3

91.6

training method because of its stable per- SwAV [2]

75.3

-

formance on downstream tasks.

E-DIY

73.8

91.5

5 Conclusion

This paper mainly reveals the necessity of avoiding the representation collapse problem in visual pre-training task. Target to maximally explore the multi-grained information inside each image, we present a simple but effective mechanism, called Exploring the Diversity and Invariance in Yourself (E-DIY). By simply pushing the different regions in the same augmented view away and pulling the consistent regions in the different augmented views from the same image together, E-DIY can preserve the diversity of extracted region-level features well and meantime ensure their semantics discrimination ability. Extensive experiments on various downstream tasks show E-DIY heavily pushes forward the transfer performance of pretraining models. However, this paper exploits the region-level relationship only on the last single feature map. In the future, we will extend our region-level relationship exploring mechanism to multiple layers in the same network for guiding the model to learn more pre-training knowledge.

9

References
[1] Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Yongxin Yang, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song. Vectorization and rasterization: Self-supervised learning for sketch and handwriting. arXiv preprint arXiv:2103.13716, 2021.
[2] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.
[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.
[4] Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via auxiliary rotation loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12154­12163, 2019.
[5] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.
[6] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint arXiv:2011.10566, 2020.
[7] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.
[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702­703, 2020.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.
[10] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE international conference on computer vision, pages 1422­1430, 2015.
[11] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303­338, 2010.
[12] Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, and Patrick Pérez. Online bag-of-visual-words generation for unsupervised representation learning. arXiv preprint arXiv:2012.11552, 2020.
[13] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.
[14] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.
[15] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297­304, 2010.
[16] Tengda Han, Weidi Xie, and Andrew Zisserman. Self-supervised co-training for video representation learning. arXiv preprint arXiv:2010.09709, 2020.
[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729­9738, 2020.
10

[18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961­2969, 2017.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630­645. Springer, 2016.
[20] Allan Jabri, Andrew Owens, and Alexei A Efros. Space-time correspondence as a contrastive random walk. arXiv preprint arXiv:2006.14613, 2020.
[21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint arXiv:2004.11362, 2020.
[22] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[23] Quan Kong, Wenpeng Wei, Ziwei Deng, Tomoaki Yoshinaga, and Tomokazu Murakami. Cyclecontrast for self-supervised video representation learning. arXiv preprint arXiv:2010.14810, 2020.
[24] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Procotypical contrastive learning of unsupervised representation. In International Conference on Learning Representation, 2021.
[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740­755. Springer, 2014.
[26] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6707­6717, 2020.
[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91­99, 2015.
[28] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and variational inference in deep latent gaussian models. In International Conference on Machine Learning, volume 2, 2014.
[29] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.
[30] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020.
[31] Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu. Selfsupervised spatio-temporal representation learning for videos by predicting motion and appearance statistics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4006­4015, 2019.
[32] Xiao Wang and Guo-Jun Qi. Contrastive learning with stronger augmentations. arXiv preprint arXiv:2104.07713, 2021.
[33] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. arXiv preprint arXiv:2011.09157, 2020.
[34] Chen Wei, Huiyu Wang, Wei Shen, and Alan Yuille. Co2: Consistent contrast for unsupervised visual representation learning. arXiv preprint arXiv:2010.02217, 2020.
[35] Chen Wei, Lingxi Xie, Xutong Ren, Yingda Xia, Chi Su, Jiaying Liu, Qi Tian, and Alan L Yuille. Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1910­1919, 2019.
11

[36] Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, and Qi Tian. Circumventing outliers of autoaugment with knowledge distillation. arXiv preprint arXiv:2003.11342, 2020.
[37] Longhui Wei, Lingxi Xie, Jianzhong He, Jianlong Chang, Xiaopeng Zhang, Wengang Zhou, Houqiang Li, and Qi Tian. Can semantic labels assist self-supervised visual representation learning? arXiv preprint arXiv:2011.08621, 2020.
[38] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.
[39] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3733­3742, 2018.
[40] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. arXiv preprint arXiv:2011.10043, 2020.
[41] Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training. arXiv preprint arXiv:1708.03888, 6:12, 2017.
[42] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE International Conference on Computer Vision, pages 6023­6032, 2019.
[43] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.
[44] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of visual embeddings. In Proceedings of the IEEE International Conference on Computer Vision, pages 6002­6012, 2019.
12

