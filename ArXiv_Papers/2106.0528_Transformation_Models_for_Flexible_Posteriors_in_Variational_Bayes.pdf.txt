Transformation Models for Flexible Posteriors in Variational Bayes

arXiv:2106.00528v1 [stat.ML] 1 Jun 2021

Sefan Ho¨rtling 1 Daniel Dold 1 Oliver Du¨ rr 1 Beate Sick 2

Abstract
The main challenge in Bayesian models is to determine the posterior for the model parameters. Already, in models with only one or few parameters, the analytical posterior can only be determined in special settings. In Bayesian neural networks, variational inference is widely used to approximate difficult-to-compute posteriors by variational distributions. Usually, Gaussians are used as variational distributions (Gaussian-VI) which limits the quality of the approximation due to their limited flexibility. Transformation models on the other hand are flexible enough to fit any distribution. Here we present transformation model-based variational inference (TM-VI) and demonstrate that it allows to accurately approximate complex posteriors in models with one parameter and also works in a mean-field fashion for multi-parameter models like neural networks.

ally, this is done by first choosing a family of parametric distributions, usually Gaussians, and then tuning the parameters of the variational distribution until its distance to the posterior is minimized. Obviously, the quality of the VI approximation depends on the similarity of the true posterior with the optimized member of the chosen distribution family. In cases where the posterior takes a complex shape, a simple variational distribution, such as a Gaussian, can never yield a good approximation.
We propose to use transformation models (TMs) (Hothorn et al., 2014) to approximate complex posteriors via VI. The main advantage of TMs is their guarantee that any distribution shape can be achieved without predefining the family of the distribution. In this paper, we show how to combine TMs and VI to accurately approximate flexible posteriors for all parameters in variational Bayes models via a computational efficient optimization process. Moreover, we benchmark our approach against exact Bayesian models, MCMC-Simulations, and Gaussian-VI approximations.

1. Introduction
Uncertainty quantification is important, especially if model predictions are used to support high stake decision-making. Quantifying uncertainty in statistical or machine learning models is often achieved by Bayesian approaches, where the uncertainty of the estimated model parameters are represented by posterior distributions. Determining these posterior distributions analytically is usually impossible if the posterior takes a complex shape or if the model has many parameters, such as a neural network (NN). Variational inference (VI) is a well established method to approximate difficult-to-compute distributions through optimization (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017). In VI the posterior is approximated by a variational distribution by minimizing the Kullback-Leibler divergence between the variational distribution and the posterior. Usu-
1IOS, Konstanz University of Applied Sciences, Germany 2IDP, Zurich University of Applied Sciences, Switzerland, and EBPI, University of Zurich, Switzerland. Correspondence to: Stefan Ho¨rtling <stefan.hoertling@htwg-konstanz.de>, Oliver Du¨rr <oliver.duerr@htwg-konstanz.de>, Beate Sick <sick@zhaw.ch>.

2. Related Work
TMs were developed in the statistics community and have the focus on modeling a potentially complex conditional outcome distributions in regression models (Hothorn et al., 2014). The main idea of TMs is to learn a potentially complex transformation function that transforms a simple base distribution, such as the Standard Gaussian N (0, 1), to a potentially complex outcome distribution under which the likelihood of the observed outcomes is maximized. The choice of the simple base distribution is unimportant for prediction purposes but gets crucial for inference (Hothorn et al., 2018). Up to now, TMs were used to model unconditional distributions or conditional outcome distributions in statistical or deep learning regression models (Kook et al., 2020; Buri et al., 2020; Sick et al., 2021; Baumann et al., 2020). Recently, a first Bayesian version of TMs were proposed (Carlan et al., 2020), which yields exact posteriors by Hamilton Monte Carlo sampling, but is restricted to relatively small models and requires experience with designing priors.
Independently to TMs, normalizing flows were developed in the deep learning community (Dinh et al., 2014) and are based on the same idea as TMs. Normalizing flows

Transformation Models for Flexible Posteriors in Variational Bayes

learn a chain of many simple, bijective transformations and are mainly used to model unconditional high-dimensional distributions for generative models (Kobyzev et al., 2020). In generative deep learning models, VI was used to approximate the posterior of the latent variables (Rezende & Mohamed, 2015) and, indirectly, by constructing a flexible mixing density, to build the variational distribution of the weights from multivariate Gaussians (Louizos & Welling, 2017).
To the best of our knowledge, TM based or normalizing flow based VI that directly approximates the posteriors of all model parameters, such as the weights in a NN model, were not yet developed.
When using VI for models with many parameters, such as NNs, it is usually not possible to optimize a joint variational distribution over all parameters that accounts for all potential dependencies between the parameters. In such cases, mean-field VI is used where the variational distributions of the parameters are optimized independently from each other. For deeper NNs it has been demonstrated that Gaussian-VI achieves the same quality in terms of the modeled outcome distribution regardless if the posteriors were optimized independently from each other or if correlations between the parameters were taken into account (Farquhar et al., 2020).
3. Methods
In the following, we describe our proposed TM-VI approach, where we use transformation models in variational inference to achieve accurate approximations to potentially complex posteriors for parameters in Bayesian models. The code is publicly available on github1. The main idea is to enable the VI procedure to approximate the posterior of the model parameters by a flexible variational distribution. In TMs a complex target distribution of interest is fitted by learning a bijective transformation function h that transforms between latent variable z following a fixed simple distribution, e.g. z  N (0, 1), and a variable of interest following a potentially complex target distribution (see Figure 1). Our target distribution of interest is the variational distribution that approximates the posterior of model parameters (e.g. the weights w of a NN).
3.1. Transformation model
The complete transformation function h(z) = w consisting of a chain of transformations h = f3  f2    f1 as shown in Figure 1. To achieve a bijective overall transformation h, it is sufficient that each transformation fi is a strictly monotone increasing transformation. A first scale and shift transformation f1(z) = a · z + b followed by a sigmoid function transforms the standard Normal distributed z into
1https://github.com/stefan1893/TM-VI

[0, 1]. To ensure a strictly monotonic increase of f1, we constrain the slope a to be positive.
The core of the transformation is the flexible Bernstein polynomial of degree M :

f2(z ) =

M

Bei(z

) i M+

1

(1)

i=0

With Bei(z ) being densities of Beta-functions which are defined on z  [0, 1]. It is known that the Bernstein polynomials can uniformly approximate every function in z  [0, 1] (Bernstein, 1912), see (Farouki, 2012) for a further discussion. An additional benefit of the Bernstein polynomials is that a strict a monotonic increase of f2(z ) w.r.t. z can be achieved by simply enforcing that the Bernstein coefficients are increasing, i.e. 0 < 1 < . . . < M . The last transformation is again a scale and shift transformation f3(w ) =  · w +  for which we constrain  to be positive to ensure a monotone increasing transformation. Altogether the complete transformation h(z) is described by M + 5 variational parameters  = (a, b, 0, . . . M , , ).

3

2

´

2 ´
 ~

1

Figure 1. Overview of the transformation h for modeling a potentially complex variational distribution q(w) of a unconstrained weight w in a Bayesian NN (lower left). The transformation h is a chain of transformations, h = f3  f2    f1, that starts from a simple distribution, here N (0, 1) (depicted in red), and ends with the complex distribution q(w) (depicted in green). The first part of the flow,   f1, transforms N (0, 1) into a distribution with support [0, 1], the flexible Bernstein polynomial used in f2 allow for the creation of a complex shaped distribution, and f3 yields the variational distribution q(w).
We apply the following manipulations to the unrestricted parameters  = (a , b, 0, . . . M ,  , ) to ensure the above

Transformation Models for Flexible Posteriors in Variational Bayes

described constrains of the parameters that guarantee a bijective transformation: a = softplus(a ),  = softplus( ), 0 = 0, and i = i-1 + softplus(i) for i = 1, . . . , M .

q(wt) = p(zt) ·

h(zt) -1 z

(5)

To facilitate the fitting of distributions with potentially complex shapes or long tails, we use the fact that f2(0) = 0 and f2(1) = M (see (Ramasinghe et al., 2021)) to initialize our weights such that the Bernstein transformation f2 yields a distribution which assigns substantial probability mass over the support of w before optimization. This can be achieved by defining a range on w with wmin and wmax and initializing  with 0 = wmin and i = softplus-1((wmax - wmin)/M ) for i = 1, . . . , M . Thus, they have the same initial support regardless of the degree of the Bernstein polynomial.

4. Results and discussion
We performed a couple of experiments to benchmark our TM-VI approach versus exact Bayesian solutions and Gaussian-VI.
4.1. Models with a single parameter
We first discuss two experiments with models containing only one parameter. This has the advantage that we can rule out the mean field assumption as a potential reason for observed deficits of the achieved variational distribution.

3.2. Transformation model based variational inference

In VI the variational parameters  = (a, b, 0, . . . M , , ) are tuned such that the resulting variational distribution q(w) is as close to the posterior p(w|D) as possible. This is done by minimizing the KL divergence between the variational distribution and the (unknown) posterior:

Bernoulli example We first look at an unconditional Bayesian model for a random variable y following a Bernoulli distribution y  Ber() which we fit based on only two samples (y1 = 1, y2 = 1). In this simple Bernoulli model, it is possible to determine the Bayesian solution analytically.

KL(q(w) p(w|D)) =

q(w) log

q(w) p(w|D)

dw

Since the parameter  can only take values between zero and one we choose a Beta-distribution p() = Beta( = 1.1,  = 1.1) as prior which leads to the conjugated pos-

= log(D)-(Ewq (log(p(D|w))) - KL(q(w) p(w))) terior p(|data) = Beta( + yi,  + n - yi) (see

ELBO()

analytical posterior in Figure 2).

(2)

Instead of minimizing equation (2) usually only the evidence lower bound ELBO (see e.g. (Blundell et al., 2015)) is maximized which consists of the expected value of the

3

Analytical posterior

TM-VI M=1; KL=7.30e-03

2

TM-VI M=10; KL=9.87e-04 TM-VI M=30; KL=8.13e-04

Gauss-VI; KL=2.22e-02

p( |y)

log-likelihood, Ewq (log(p(D|w))), minus the KL di-

1

vergence between the variational distribution q(w) and the known prior p(w). In practice, the negative ELBO is

0

0.0

0.2

0.4

0.6

0.8

1.0

minimized by gradient descent. We approximate the ex-

pected log-likelihood by averaging over T weight samples

wt  q(w). To get these weight samples we first draw T samples zt  N (0, 1) and then compute the corresponding weight samples via wt = h(zt). We can approximate the expected log-likelihood for the training data by:

Figure 2. Comparison of the analytical posterior for the parameter  in the Bernoulli model y  Ber() with variational distributions achieved via TM-VI and Gaussian-VI. The blue lines show the results from our TM-VI model with different degrees M of the

Bernstein polynomial shown in different line styles. In addition

1

Ewq (log(p(D|w)))  T

log (p(Di|wt))

t,i

(3)

the KL divergence between the variational distributions and the analytical posterior KL(q(w) p(w|D)) is shown in the legend.

For an animated version, showing the training process for TM-VI

We use the same weight samples wt to approximate the with M = 10 see: https://youtu.be/_RA7QirjXMM

Kullback-Leibler divergence between the variational distri-

bution and the prior via:

We now use our TM-VI method to approximate the pos-

terior. To investigate how the flexibility of the Bernstein

K L(q (w)

p(w))  1 T

log
t

q(wt) p(wt)

polynomial impacts the quality of the achieved variational (4) distribution, we have used Bernstein polynomials with dif-
ferent degree M . To enforce the modeled variational dis-

where the probability density q(wt) can be calculated, from tribution to be restricted on [0, 1] we pipe the result of f3

the samples zt using the change of variable function as:

(see Figure 1) through a sigmoid transformation to get the

Transformation Models for Flexible Posteriors in Variational Bayes

variational distribution of the parameter . Figure 2 shows the achieved variational distributions after minimizing the negative ELBO via gradient descent as described in section 3.2. With increasing degree M of the Bernstein polynomial, the variational distributions gets closer to the posterior (see KL(q(w) p(w|D)) in the legend of Figure 2). Using the TM-VI with M = 30 yields a variation distribution that approximates the posterior very accurately. As expected, the Gaussian based VI has not enough flexibility to lead to a variational distribution that approximates the analytical posterior nicely (see Figure 2).

Cauchy example For the next experiment we follow an example from (Yao et al., 2020) and we fit an unconditional Cauchy model y  Cauchy(, ) to six samples which we have drawn from mixture-Cauchy distribution y  Cauchy(1 = -2.5, ) + Cauchy(2 = 2.5, ). Because of the miss-specification of the model, the true posterior of the parameter  has a bi-modal shape which we have determined via MCMC (see Figure 3). We have used TM-VI and Gaussian-VI to approximate the posterior of the Cauchy parameter  by a variational distribution. Because the possible range of  is not restricted we can use the result of f3 (see Figure 1) as variational distribution for . While TM-VI has enough flexibility to capture the bi-modal shape of the posterior, Gauss-VI fails as expected.

p( |D)

0.6

MCMC

TM-VI, M=1

0.4

TM-VI, M=10 TM-VI, M=30

Gauss-VI

0.2

0.0 4 3 2 10 1 2 3 4

3 neurons per layer model, and one with 2 hidden layers and 10 neurons per layer. The variational distributions for the weights propagate to the distribution of the conditional mean µ(x). Figure 4 demonstrates, that in the smaller 1hidden-layer NN, mean-field TM-VI slightly outperforms mean-field Gaussian-VI. In the larger 2-hidden-layer NN, both mean-field VI approaches perform comparably. Both, mean-field Gaussian-VI and TM-VI, fail to capture the uncertainty in-between the two clusters of data points. We attribute both observations to the used mean-field approach, which is known to underestimate the uncertainty (Blei et al., 2017) and probably also masks the benefit of the flexible TM-VI.

1-hidden-layer network

2-hidden-layer network

MCMC

TM-VI

Gauss-VI
Figure 4. Posterior distribution for the conditional mean µ(x) of the conditional outcome distribution N (µ(x), ) modeled by a Bayesian NN with 1 hidden layer (left panel) or 2 hidden layers (right panel). The Bayesian solution was determined via MCMC (first row) or via mean-field TM-VI (second row) or via mean-field Gaussian-VI (third row).

Figure 3. Posterior distribution of the parameter  in the missspecified Cauchy model y  Cauchy(, ). Comparing the bimodal true posterior resulting from MCMC with variational distributions estimated via Gaussian-VI or TM-VI shows that the flexibility of TM-VI is needed to capture the bi-modal shape of the posterior.
4.2. Multi-weight neural networks
We now investigate how our TM-VI approach performs in conditional multi-parameter models like NNs. For this experiment, we sample 16 data points clustered in two regions of a noisy sinus wave (see points in Figure 4). We then use MCMC, TM-VI, and Gaussian-VI to determine the solution of a Bayesian NN which controls the conditional mean µ(x) of the conditional outcome distribution N (µ(x), ). We use two different NNs: one with 1 hidden layer and

5. Conclusion and outlook
We have introduced TM-VI to achieve flexible variational distributions that accurately approximate potential complex parameter posteriors. In single-parameter models with a complex posterior, we have shown that TM-VI perfectly approximates the posterior while Gaussian-VI fails. In multiparameter NN models, we have demonstrated that TM-VI can be used in a mean-field fashion. For small NNs, our TMVI approach produces slightly superior results compared to the Gaussian-VI, but the limitations of the mean-field approach are clearly visible. In the future, we plan to extend TM-VI for Bayesian inference in models with few interpretable parameters by dropping the mean-field assumption to get accurate posterior approximations.

Transformation Models for Flexible Posteriors in Variational Bayes

6. Acknowledgements
Part of the work has been founded by the Federal Ministry of Education and Research of Germany (BMBF) in the project DeepDoubt (grant no. 01IS19083A).
References
Baumann, P. F., Hothorn, T., and Ru¨gamer, D. Deep conditional transformation models. arXiv preprint arXiv:2010.07860, 2020.
Bernstein, S. De´monstration du the´oreme de weierstrass fonde´e sur le calcul des probabilities. Comm. Soc. Math. Kharkov, 13:1­2, 1912.
Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859­877, 2017.
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network. In International Conference on Machine Learning, pp. 1613­1622. PMLR, 2015.
Buri, M., Curt, A., Steeves, J., and Hothorn, T. Baselineadjusted proportional odds models for the quantification of treatment effects in trials with ordinal sum score outcomes. BMC medical research methodology, 20:1­14, 2020.
Carlan, M., Kneib, T., and Klein, N. Bayesian conditional transformation models. arXiv preprint arXiv:2012.11016, 2020.
Dinh, L., Krueger, D., and Bengio, Y. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. An introduction to variational methods for graphical models. Machine learning, 37(2):183­233, 1999.
Kobyzev, I., Prince, S., and Brubaker, M. Normalizing flows: An introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Kook, L., Herzog, L., Hothorn, T., Du¨rr, O., and Sick, B. Deep and interpretable regression models for ordinal outcomes. arXiv preprint arXiv:2010.08376, 2020.
Louizos, C. and Welling, M. Multiplicative normalizing flows for variational bayesian neural networks. In International Conference on Machine Learning, pp. 2218­2227. PMLR, 2017.
Ramasinghe, S., Fernando, K., Khan, S., and Barnes, N. Robust normalizing flows using bernstein-type polynomials. arXiv preprint arXiv:2102.03509, 2021.
Rezende, D. and Mohamed, S. Variational inference with normalizing flows. In International Conference on Machine Learning, pp. 1530­1538. PMLR, 2015.
Sick, B., Hathorn, T., and Du¨rr, O. Deep transformation models: Tackling complex regression problems with neural network based transformation models. In 2020 25th International Conference on Pattern Recognition (ICPR), pp. 2476­2481. IEEE, 2021.
Wainwright, M. J. and Jordan, M. I. Graphical models, exponential families, and variational inference. Now Publishers Inc, 2008.
Yao, Y., Vehtari, A., and Gelman, A. Stacking for nonmixing bayesian computations: The curse and blessing of multimodal posteriors. arXiv preprint arXiv:2006.12335, 2020.

Farouki, R. T. The bernstein polynomial basis: A centennial retrospective. Computer Aided Geometric Design, 29(6): 379­419, 2012.

Farquhar, S., Smith, L., and Gal, Y. Liberty or depth: Deep bayesian neural nets do not need complex weight posterior approximations. arXiv e-prints, pp. arXiv­2002, 2020.

Hothorn, T., Kneib, T., and Bu¨hlmann, P. Conditional transformation models. Journal of the Royal Statistical Society: Series B: Statistical Methodology, pp. 3­27, 2014.

Hothorn, T., Moest, L., and Buehlmann, P. Most likely transformations. Scandinavian Journal of Statistics, 45 (1):110­134, 2018.

