arXiv:2106.01254v1 [cs.LG] 2 Jun 2021

Survey Equivalence: A Procedure for Measuring
Classifier Accuracy Against Human Labels
Paul Resnick
School of Information, University of Michigan, Ann Arbor, MI 48109, presnick@umich.edu
Yuqing Kong
Center on Frontiers of Computing Studies, Peking University, Beijing, China 100871, yuqing.kong@pku.edu.cn
Grant Schoenebeck
School of Information, University of Michigan, Ann Arbor, MI 48109, schoeneb@umich.edu
Tim Weninger
Department of Computer Science and Engineering, University of Notre Dame, Nore Dame, IN 46656, tweninger@nd.edu
In many classification tasks, the ground truth is either noisy or subjective. Examples include: which of two alternative paper titles is better? is this comment toxic? what is the political leaning of this news article? We refer to such tasks as survey settings because the ground truth is defined through a survey of one or more human raters. In survey settings, conventional measurements of classifier accuracy such as precision, recall, and cross-entropy confound the quality of the classifier with the level of agreement among human raters. Thus, they have no meaningful interpretation on their own. We describe a procedure that, given a dataset with predictions from a classifier and K ratings per item, rescales any accuracy measure into one that has an intuitive interpretation. The key insight is to score the classifier not against the best proxy for the ground truth, such as a majority vote of the raters, but against a single human rater at a time. That score can be compared to other predictors' scores, in particular predictors created by combining labels from several other human raters. The survey equivalence of any classifier is the minimum number of raters needed to produce the same expected score as that found for the classifier.
Key words : survey equivalence, machine learning, classification, measurement, human raters, ground truth
1. Introduction
For many classification tasks, what we will call survey settings, classifiers have to be evaluated against human labels even though not all humans provide the same label for each item. Consider some common classification tasks:
и Which of two search engine results is "better" for a particular query and thus should appear higher in the search results?
и Is this image so offensive as to violate "community standards" on a social media site? и Is there a person in this image? и Does this tweet convey positive affect? и Does this news article contain misinformation? и What is the political bias of this news article? In survey settings, there are two reasons why raters may not agree on an item. One we can think of as random noise. The person may not be paying close attention, may be tired or hungry, or may accidentally click on a different label than they intended to. If some items are harder than others for people to label correctly, we can think of the harder items as having more random noise. The other reason is subjectivity. Some people may focus on different attributes of the item or have a different interpretation of the labeling criteria. All standard accuracy scoring functions for hard and soft classifiers (e.g., precision, recall, correlation coefficients, AUC, cross-entropy) penalize a classifier for disagreement with a human labeler, without taking into account how often human labelers agree with each other. To see why this is problematic, suppose that we simulate making the underlying classification problem harder for people by adding some noise to the observed human labels, without changing the classifier in any way. Since this is now a harder classification problem, we should intuitively think of the (unchanged)
1

classifier as better: it produces outputs that are just as close to the real underlying truth, but it's a truth that any individual person finds harder to discern. While the classifier is intuitively better, it will have a lower score on the noisy labels than on less noisy labels.
Alternatively, suppose that there are some "easy" items, where 95% of human raters assign the same labels, and some "hard" items, where there is a 60 - 40 split among human raters. Moreover, suppose we have a classifier that agrees with the majority of human raters 95% on the easy items but only 60% on the hard items. In other words, the classifier is exactly as good as a single human rater at producing the most common human label. If we take a sample of mostly easy items it will look like the classifier is very accurate but if we take a sample of mostly hard items it will appear very inaccurate. Neither is a true picture of how good the classifier is. The problem is not with any particular accuracy metric. The problem is that any metric that compares classifier outputs to a set of human labels that are treated as if they are the ground truth is insufficient for understanding how good a classifier is.
Research practice in some sense recognizes this problem with reporting raw accuracy scores. There is a general expectation that researchers will provide baseline classifiers against which to compare the scores of the new classifier they are trying to evaluate. If a classifier has a high raw score but a simple baseline gets a higher score, the classifier is not considered very good.
Our approach is to use classifiers created from simulated surveys of human raters as baselines for comparison. Suppose that we have a dataset where, for each item, we have K human labels. We simulate a survey of k < K raters by taking a sample of the labels for each item. We then compute how well those k ratings per item do at predicting the labels of a randomly selected held-out reference rater. For whatever accuracy scoring function we use (precision, AUC, etc.), we can think of the average accuracy score for many simulation runs as the expected score of a k person survey. As k increases, the accuracy at predicting a held-out labeler should go up; we call a graph of the expected score of k person surveys the survey power curve. The classifier's survey equivalence is the smallest survey size that predicts a held-out human rater as well as the classifier does.
If this yields a fractional survey equivalence value, there is still a natural interpretation in terms of a process involving human surveys. Suppose, for example, that the survey equivalence value is 4.77. That corresponds to a process that begins with a randomization step. Note that 4.77 = .77  5 + .23  4. Thus, for 77% of the items five raters are surveyed and for the remainder only four. Either way, the survey outcome is used to predict what some other reference rater will say. We can even interpret a survey equivalence of less than 1: for some items we get a label and use that to predict what the reference rater will say; for other items we merely predict the base rate of labels for all items.
Note that we never need to assess a classifier's accuracy against the majority vote of a survey. We simulate surveys of size k to generate baseline classifiers for comparison. All classifiers, both the one to be evaluated and the ones based on surveys of size k, are evaluated for accuracy against a single randomly selected reference rater's label for each item. This permits efficient use of the available ratings, as we can score the surveys of up to size K - 1 given K ratings per item. It also allows for an information theoretic interpretation of classifier scores in terms of the mutual information between a classifier and a randomly selected human rater.
Rescaling raw classifier scores to corresponding survey equivalence values produces a metric that more appropriately reflects the difficulty of the underlying task. In the scenarios above that increase rater disagreement, either by adding more rater noise on all items or sampling more of harder items, a classifier's raw accuracy score will go down, but its survey equivalence will go up.
Survey equivalence values are also practically meaningful. There is often a practical choice to be made of whether to use a classifier or to instead rely on a more expensive process of human labeling. The survey equivalence value states how large a survey of human raters would be required to do better than relying on the classifier.
To summarize our contributions, we:

и describe a general procedure for computing survey power curves, which plot the expected accuracy scores from surveys of increasing size.
и define survey equivalence as a general metric for a classifier: the size of a human survey that would provide the same accuracy as the classifier;
и provide a formal model based on i.i.d. label generation from distributions that define the item states and use it to prove that:
-- a computationally tractable procedure for aggregating k raters' labels, called the Anonymous Bayesian Combiner, produces a soft classifier that converges to the Bayesian posterior distribution as the number of items grows,;
-- the Anonymous Bayesian Combiner and the cross-entropy scoring function can be used to quantify the information gained from a survey of k raters in information theoretic terms;
-- when using the cross-entropy scoring function, the Anonymous Bayesian Combiner yields the minimum possible survey equivalence score for a classifier;
-- in a restricted setting, the survey equivalence will be the same whether scoring against a single reference rater or the majority vote of several reference raters.
и provide a software library that implements the procedures1.
1.1. Related Work Much previous work has addressed settings where ground truth labels are not accessible and only human labels are provided. Some (e.g. Manwani and Sastry (2013), Natarajan et al. (2013), Ghosh et al. (2017)) consider a model where the human labels represent an objective ground truth with the addition of random noise and focus on learning despite the noise. They propose learning methods which yield trained classifiers that produce the same outputs as classifiers trained on noiseless labels.
We offer a more general model of human labeling processes where the state of an item is not a single ground truth label but the distribution of labels that raters would assign to the item. Our model can apply to settings where there is an objective truth perceived with some noise but the noise level is different for different items: some items are harder than others. For example, Dumitrache et al. (2018) have argued that some sentences in the FrameNet computational linguistics corpus are inherently more ambiguous than others, and thus it is necessary to capture the degree of agreement among raters rather than just the consensus rating. Our model also can apply to settings where the rating process is inherently subjective and there is no underlying objective truth that summarizes the distribution of subjective responses.
The recommender systems literature models settings with subjective labels and personal preferences for items. One class of techniques use matrix factorization to predict users' labels for items they have not yet rated; items predicted to have positive labels are then recommended to the user (Koren et al. 2009). The use of matrix factorization is justified by a model of the labeling process where there is some latent attribute space; both item states and user preferences are expressed in terms of those latent attributes. Our model can be thought of as a special case, where individual differences among users are not modeled. This model is suitable to a setting where we are interested in characterizing the distribution of responses that randomly selected people would have to the items, not in predicting the response of any particular person.
Our focus is on evaluating classifiers rather than training them. With a model of ground truth plus random noise, the majority vote of many raters for each item will converge to the ground truth (Condorcet 1785). Thus, the majority vote of a large number of raters can be treated as an inferred ground truth against which to evaluate a classifier using standard metrics. Of course, in practice, we don't usually have a large number of raters, and it is not safe to just treat a single rater or the majority vote of a few raters as if it were the ground truth.
1 The software can be downloaded from https://pypi.org/project/surveyequivalence/

In the social sciences, and more recently computational social science, there is a long history of using human labels to "code" texts and images according to concepts articulated by the investigator, such as emotions (Lucey et al. 2010), political leaning (Zhou et al. 2011), or outrage (Sobieraj and Berry 2011). The typical practice is to have multiple raters independently label a set of items and then resolve disagreements by consensus after discussion (Neuendorf 2002). The level of agreement is measured according to some metric such as Krippendorf's alpha (Hayes and Krippendorff 2007). If the score is acceptably high, then the consensus labels are used. If not, the training materials for raters are revised and the process is repeated on another sample of items. In some cases, with sufficiently high agreement, additional items will be labeled by just a single rater. While this process is designed to yield higher agreement among raters and to set a floor below which the labels will not be used, it does not eliminate inter-rater disagreement. Thus, some additional technique is needed for evaluating classifiers against human labels that takes into account the frequency of inter-rater disagreement.
Two previous papers in other domains contain results that are based on the same premise as our survey equivalence method. Rothschild and Wolfers (2011) proposed a method for assessing how informative alternative political polls are. These polls ask respondents to predict an election outcome rather than or in addition to who they intend to vote for. Using the actual election results as ground truth, the authors suggest a "structural interpretation" which models the informativeness of the prediction question in terms of the number of friends each respondent would have to report for on the standard vote intention question. In experimental economics, Erev et al. (2007) proposed a method to quantify the accuracy of a theory or model that predicts the performance of human subjects on some previously unplayed game. The theory is compared against actual play of people and does not match perfectly. They assess how good the theory is by determining how many observations of human players are needed to achieve similar prediction accuracy for the mean behavior of the full sample, the "Equivalent Number of Observations." They determine that two predictions have similar accuracy when their coefficients from a linear regression model are equal. That is analogous to our survey equivalence value, the equivalent number of raters needed to produce the same accuracy score as a classifier.
We propose a more general framework that can work with any functions that combine several humans inputs to produce a prediction about what another human will say. This flexibility is important because the favored choice of accuracy metrics is context specific. In particular, our results move beyond the statistical methods of these papers, to a more data science/machine learning approach. Moreover, the full power curve produced by our method provides more information than a single survey equivalence number.
In the domain of assessing classifiers, Wulczyn et al. (2017) evaluate a classifier by comparing its performance to that of groups of raters. Both the human "predictor groups" and the classifier are scored against the consensus rating of a "truth group", another set of human raters. A key insight of our method is that only one rater is needed in the "truth group", and thus, we only need one set of K raters in order to generate estimates of the survey power of any predictor group up to size K-1. We also provide a way to calculate fractional survey equivalence values and a meaningful interpretation of fractional values in terms of randomizing the size of the survey.
Gordon et al. (2021) also argue that it is more appropriate to compare classifier outputs to a single rater at a time rather than to a consensus rating of a group of raters. They argue that satisfaction with a classifier will be based on how frequently individuals disagree with the outputs, rather than how frequently the classifier matches the consensus. Unlike our approach, however, they treat the expected agreement with a single rater as the final outcome. This may be useful if a decision is being made about whether there is too much disagreement among human raters to permit the use of any classifier, regardless of its quality. We rescale this score into the equivalent survey size that would yield the same score, which is more useful for assessing whether a classifier is a good substitute for human ratings.

Which do you prefer?

Cat

Dog

hc

hs

Items

Item Set I

Label Set L ={C, D}

Figure 1 A stylized running example.

Rating Matrix W

Classifier Output

2. Model
Figure 1 illustrates a stylized rating process. Each item to be classified is a pair of images, a cat on the left and a dog on the right. The ground truth is provided by nine people who report whether they prefer the cat or dog image. An automated classifier processes the same set of images. With a hard classifier, the output is a single label of cat or dog. With a soft classifier, the output is a probability, which we can interpret as the probability that the cat is the preferred image within the pair. In the figure, fictitious rater labels and classifier outputs are provided for ten fictitious items; these will be used as a running example throughout the paper.
Our model of the rating process corresponds to a setting where there is a queue of items and a pool of raters. When we collect labels for an item, we repeatedly assign the item to the next available rater who hasn't rated the item, until enough labels have been collected. We can think of this as drawing a rater from a distribution of rater states (e.g., a cat lover or a dog lover; expert or novice) and a draw from rating contexts (e.g., before or after lunch). Our goal is to evaluate a classifier's predictions for whomever may be selected as the next rater, not for any particular individual rater. Moreover, we assume that when we conduct a survey of k raters, we will not use any information about the identity of the particular k raters, only their anonymized labels. This allows us to greatly simplify our modeling and notation, because the random draws of the next rater state, rating context, and label can be collapsed into a single random draw. That is, the state of an item is defined by a single distribution of labels. The sequence of human labels for an item is a set of realized i.i.d. draws from that distribution.
Figure 2 illustrates this model for the running example. Each small urn is a possible item state. The urn containing other urns represents the distribution of item states. The running example includes only three distinct item states: either 80% of raters label the item as C (for cat), 10% do, or 50% do. Seven of the ten urns are of the first type, indicating that 70% of items will have that state, where 80% of raters pick the cat image.
Formally, I denotes the space of all possible items to be rated. L is the setting of possible labels that a rater can pick for an item (in the running example L = {C, D}). Each item i  I has an unknown state s(i)  S = L where L is the set of all possible probability distributions over L. That is, a state s(i) is a distribution over labels: raters' labels for item i are i.i.d and the probability a random rater chooses label  L is s(i) . We assume there is a space of items from which we can select items. This induces a distribution DS  L over the item states2. When we acquire K
2 To avoid degenerate conditions, we assume both that states have full support over the set of labels, and that there is more than one possible state.

s(i)
item i
Figure 2 Model for the running example.
labels for each of a collection of items I, we use an |I| О K matrix W to denote the labels; the ith row Wi denotes the raters' labels for item i. While the item states are unknown, we model the labels as if they were generated as i.i.d. draws from the item states, and the item states as i.i.d. draws from DS.
To illustrate computations in this paper, we will refer to the rating matrix W in Figure 2, with ten items and nine ratings per item. We also compute a survey power curve and survey equivalence values for a larger synthetic dataset generated based on the same model, with 1,000 items and ten ratings per item. The krippendorff alpha score, measuring inter-rater agreement, for this larger synthetic dataset is 0.33.
The rating matrix W will be used to evaluate two kinds of classifiers. A (hard) classifier hc maps each item i to a label hc(i) in L. A soft classifier hs maps each item i to a probability distribution hs(i)  L for the likelihood that a random rater for that item will output each possible label.
For our running example, our fictitious classifiers' outputs were generated based on the true underlying state of each item, which would not be available to a classifier in practice. A real classifier would use other features to make its decisions, perhaps drawn from analysis of the images themselves. For an item in the 80 - 20 state, our fictitious hard classifier outputs C 90% of the time. For an item in the 10 - 90 state, our fictitious hard classifier outputs C 5% of the time. And for the split items, it picks C half the time. Thus, the fictitious hard classifier is somewhat better than any one human rater at agreeing with the majority of human raters. As illustrated in the figure, the soft classifier outputs 77% whenever the hard classifier would output C and 32% whenever the hard classifier outputs D.3
3. Survey Equivalence: The Procedure
This section introduces the general procedure for calculating the survey equivalence of a classifier. We first introduce the concepts of scoring functions and combiner functions, then proceed to computing power curves and survey equivalence.
3.1. Scoring a Classifier A scoring function for a hard classifier Score : (L О L)  R takes n pairs of classifiers' outputs and labels {(h(i), yi)}i as input and outputs an evaluation score. Popular scoring functions for hard classifiers include percent agreement (accuracy) and F1 score. In the running example in Figure 1, the agreement score of the hard classifier with the last rater is 90% because they agree on all items except the first one.
3 This soft classifier is the "calibrated" classifier given the way the hard classifier outputs are generated and the distribution of states DS represented by the urn of urns.

ALGORITHM 1: Scoring a Classifier's Predictions HScore(h(I), W, Score(и))

Result: Hypothesis's score hscore 1 for jr  raters 2 do

/* Raters are the columns of W */

3 scoresjr  Score({(h(i), Wi,jr )}i) /* Score against each reference rater */ 4 end

5 return mean(scores)

A scoring function for a soft classifier Score : (L О L)  R takes n pairs of predictions and

labels {(pi = h(i), yi)}i as input and outputs an evaluation score for the predictions. One popular

scoring function is area under the receiver operator curve (AUC). Another is cross-entropy, which

is defined as

CrossEntropy({(pi, yi)}ni=1)

=

1 n

log(pi(yi)).

i

The cross-entropy scoring function has nice information-theoretic properties, as we shall see in

Section 4. In the running example, the cross-entropy score of the soft classifier's outputs against the last column of ratings is 1/10  [log(.32) + 7  log(.77) + 2  log(.68)] = -.54. As a baseline, consider an alternative classifier that always predicted .63, the prior probability of a C label for a randomly

selected item according to the model in Figure 2. That baseline's cross-entropy score against the

last column of ratings would be 1/10  [8  log(.63) + 2  log(.37)] = -.82. Thus, the soft classifier offers an improvement of .28 bits over the baseline, when scored against the last rater.
Scoring a classifier against any single rater provides an unbiased estimate of the expected score

computed against a randomly selected reference rater. To reduce the variance of this estimate, we compute the score for the classifier's outputs against each of the reference raters and take the mean, as described in Algorithm 1.

3.2. Human Classifiers: Combiner Functions A combiner Comb takes as input a set of labels for an item and, optionally a rating matrix W and the item i from W that is being labeled; it outputs a prediction for some other rater's label for the item. A myopic combiner makes use of only the observed labels for the particular item, ignoring W. All of the example combiners described in this section are myopic. In Section 4 we will introduce a non-myopic combiner that is learned from a dataset; the prediction that will be made for a given set of observed labels for one item will depend on the pattern of labels observed for the rest of the items in W.
A hard combiner outputs a single label. For example, with binary labels, the majority vote combiner outputs the label assigned by the majority of the raters, picking at random to break ties. The plurality combiner generalizes to a setting with more than two labels; it outputs the most common label even if less than half of the raters assign it.
A soft combiner produces a probability distribution instead of a single label. For example, the frequency combiner outputs, for every possible label, the frequency of that label in the k observed labels. If a soft combiner is to be scored against the cross-entropy scoring function, the logarithm of 0 is undefined, so we adjust outputs of 0 or 1 by a small epsilon, .02 in our implementation.
As a special case, the combiner should provide a "default" output, when no labels are provided. For the majority vote combiner, when there are no raters' labels, it picks a label at random. For the frequency combiner, the default output is equal probabilities for all labels.
In the running example, if we take the first k columns of rater labels, the majority vote and frequency combiner outputs would be as shown in Figure 3. Note that in computing a power curve, as described below, we would randomly select a subset of k columns rather than always taking the first k.

k=1 k=2 k=3 k=4 k=5 k=6 k=7 k=8 D DDDDC C C C CCCCCCC DCCCCCCC DDCCCCCC D DDC C C C C C CCCCCCC C DCCCCCC D CDCCCCC D DDDDDDD D DDDDDDD

k=1 k=2 k=3 k=4 k=5 k=6 k=7 k=8 0.00 0.00 0.33 0.50 0.40 0.50 0.57 0.62 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.50 0.67 0.75 0.80 0.83 0.86 0.88 0.00 0.50 0.67 0.75 0.80 0.83 0.86 0.88 0.00 0.50 0.33 0.50 0.60 0.67 0.71 0.75 1.00 1.00 1.00 1.00 1.00 1.00 0.86 0.88 1.00 0.50 0.67 0.75 0.80 0.83 0.86 0.88 0.00 0.50 0.33 0.50 0.60 0.67 0.57 0.62 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.25 0.20 0.17 0.14 0.25

Figure 3

Rating Matrix W

Majority Vote Predictions

Frequency Combiner Predictions

The majority vote and frequency combiners for the running example.

ALGORITHM 2: RaterSubsets(raters, k)

Result: Collection of subsets of k raters

1 if

|raters| k

 200 then

2 return all size k subsets of raters

3 else

4 return 200 size k subsets of raters selected randomly without replacement

5 end

3.3. Computing a Power Curve Algorithm 3 describes a process for computing a survey power curve, the expected scores for different numbers of raters. For each item, we simulate randomly selecting k of the K raters who rated the item, and combine their labels to output a prediction, also passing in the rating matrix W and the item we are predicting for if the combiner is non-myopic (line 5 of Algorithm 3). We then score the predictions against the labels from any of the other K - k raters in the dataset. This process gives an unbiased estimator of the expected score from a randomly selected subset of k raters scored against a single randomly selected reference rater. To reduce variance, we score against all of the remaining reference raters and take the mean (line 5 of Algorithm 1); we also repeat the entire process for many subsets of k raters, and take the mean across subsets (line 10 of Algorithm 3). We refer to this as ck, the power of k raters to predict the label of a randomly selected reference rater. By doing this for different values of k, we generate a survey power curve.
Because the predictions have to be scored against at least one reference rater, the score ck can be calculated for k up to K - 1 raters. We will call c0 the baseline score: it is the score for the default predictions that are output from the combiner when no rater labels are provided.
The software package we have developed offers the option of bootstrap sampling of sets of items, in order to provide error bars around the ck values in the power curve. That is, we can imagine repeating many times the whole procedure of generating an |I|ОK rating matrix W and getting the best estimate of survey power at each k. We can simulate that with bootstrap sampling, generating new matrices with |I| rows, sampling with replacement from the rows of W. From many samples, we can plot an error bar covering 95% of the samples. Note that we use the actual rows from W rather than sampling the rater columns with replacement. Typically, there will be too few raters for bootstrap sampling of columns to make sense.
The power curve in Figure 4 was generated from a synthetic dataset of 1000 items and ten raters, following the model for the running example described in Section 2, and a hard classifier whose

ALGORITHM 3: Calculating Survey Power Curve SPC(W, Comb(и), Score(и))

Result: Power curve c  RK-1 1 K  |raters|

/* raters are the columns of W */

2 for 0  k < K do

3 for rs = {j1, j2, и и и , jk}  RaterSubsets(raters, k) do

4

for i  I do

5

pi  Comb({Wi,j1, Wi,j2 и и и , Wi,jk }, W, i) /* predict for each item */

6

end

7

unused  raters - rs

8

scoresrs  HScore(p, Wunused, Score(и)) /* score against unused raters

*/

9 end

10 ck  mean(scores) 11 end

12 return c

percent agreement with reference rater

Running Example: Majority Vote + Agreement Score
1 k raters mock hard classifier
0.8

0.6

0.4

0.2

Figure 4

0

0

2

4.31

6

8

Number of raters

A survey power curve using the majority vote combiner and the percent agreement scorer.

predictions were generated as described in that section. Subsets of k raters' labels were combined using the majority vote combiner and scored based on the percentage of predictions that matched the reference rater's labels. Because of the majority vote combiner, performance improves only at odd numbers of raters. Error bars are based on 500 bootstrap samples of items.
3.4. Computing Survey Equivalence The integer survey equivalence of a classifier h is the smallest integer k such that the expected score of k raters against a random reference rater, ck, is greater than or equal to the expected score of h against a random reference rater. The fractional survey equivalence is a linear interpolation between the integer survey equivalence k and k - 1. Algorithm 4 provides a more formal description of the procedure.
Graphically, the survey equivalence is just the x-value for the point where the horizontal line representing the score of the classifier intersects the power curve c. If there is no point of intersection, the survey equivalence is either "less than 0" or "more than K" where K is the maximum number

ALGORITHM

4:

Calculating

SEq(h, I, W; Comb(и), Score(и)))

Result: Human Survey Equivalence

1 hscore  HScore(h(I), W; Score(и))

2 c  SPC(W; Comb(и), Score(и))

3 if hscore  c0 then

4 return "less than 0"

5 end

6 for 1  k  K do

7 if ck > hscore then

8

v



k

-

1

+

hscore -ck-1 ck -ck-1

9

return v

10 end

11 end

12 return "more than K"

Human

Survey

Equivalence

/* Algorithm 1 */ /* Algorithm 3 */

of raters per item in the dataset W. In the perverse case where the power curve is non-monotonic and there are multiple points of intersection, the survey equivalence is defined as the leftmost intersection point. For the dataset in our running example, with the majority vote combiner and agreement score, the survey equivalence was 4.31, as shown in Figure 4.
The software package we have developed also provides a way to compute error bars for the survey equivalence. As described above, many power curves are calculated, each from one bootstrap sample of rows from the original matrix W. Each power curve yields one estimate of the survey equivalence value. Across 500 bootstrap samples, the mean survey equivalence was 4.17 raters and the range [2.88, 4.81] covered the equivalence value found for 95% of the samples.
4. Learning a Combiner from the Dataset: the Anonymous Bayesian Combiner
One combiner, which we call the Anonymous Bayesian combiner ABC, has nice informationtheoretic properties when combined with the cross-entropy scoring rule. ABC produces, for any combination of observed labels so far, a predicted probability distribution for the next label. Unlike the frequency combiner previously described, which takes into account only the frequency of labels assigned to the current item, ABC also takes into account how frequent various combinations of labels have been on other items in the dataset. If, on other items, two positive labels and a negative have been followed by a positive label 90% of the time, ABC will predict .9. Thus, the particular predictions are "learned" from the dataset.
We can think of the ABC in two parts, a learner and an executor. The learner, described in Algorithm 5, estimates the probabilities of label sequences based on the labeled dataset W.4 The executor, described in Algorithm 6, uses the estimated probability to predict the next rater's label, given observed labels from k other raters.
The ABC is anonymous in the sense that it does not try to make any inferences based on the identities of the raters. It does not customize its predictions to the particular rater who will provide the next label, and it does not use any information about idiosyncratic rating patterns of the previous labelers; it treats all labels as if they were realized independent draws from an item's "state", its unknown distribution of labels. No assumptions are made about the distribution
4 While the ABC algorithm takes in the entire data matrix W, the learner used for item i depends only on W-i, excluding the labels of item i. We describe the algorithm as taking the entire matrix as input because this permits an implementation with faster running time (see Section 5 and Appendix C).

ALGORITHM 5: LabelSeqProb(y1, y2, и и и , yk, W, excluded item)

Result: An unbiased estimator of Pr[y1, y2, и и и , yk] 1   L, y( )  number of raters giving label among y1, y2, и и и , yk /* label counts */ 2 v, |I|  SumOfProbabilities({y(l)}, W)

3 v  v - ProbabilityOneItem({y(l)}, Wexcluded item) /* exclude item's effect */

4

return

|I

1 |-1

v

Procedure SumOfProbabilities({y(l)}, W)

Result: Sum, over all items, of probability of observed label counts

1 k  y( )

2 Collect all items I that have  k raters

3 v0

4 for i  I do

5

v  v + ProbabilityOneItem({y(l)}, Wi)

6 end

7 return v, |I|

Procedure ProbabilityOneItem({y( )}, Wi) 1 k  y( )

2   L, Wi( )  the number of raters giving label for item i

3 |Wi|  the number of raters for item i

4 if   L, Wi( )  y( ) then

5

return

( ) Wi( ) y( )

y(

)!

( ) |Wi| k

k!

6 end

7 else

8

return 0

9 end

ALGORITHM 6: AnonymousBayesianCombiner ABC(y1, y2, и и и , yk, W, excluded item)

Result: Prediction p

  L, p = 1

LabelSeqProb(y1,y2,иии ,yk, ,W,excluded item) LabelSeqProb(y1,y2,иии ,yk,W,excluded item)

2 return p

/* Algorithm 5 */

of states for items, but it is assumed that all items were realized independent draws from that distribution of states, following the model described in Section 2.
The ABC has several desirable properties, which we will state formally below and prove in an appendix. The key reason these properties hold is that the ABC produces predictions that approximate the Bayesian posterior for the next label, given the observed labels for the item and Bayesian updating from a prior that is estimated from the overall frequencies of each of the labels.
To provide intuitions about how the algorithm works, and why it produces an approximation of the Bayesian posterior, consider the estimation of the probability of a specific label sequence (D, C, D, C, C). The subroutine ProbabilityOneItem computes, for each item i, the probability of that specified label sequence occurring if five raters were randomly selected from among the actual raters for item i. The average of this probability over all items (Algorithm 5, line 4) is the probability of getting the specified label sequence for a randomly selected item. One subtle point, however, is that if predictions are made for the same dataset W from which the distribution is learned, those predictions should never benefit from information about ratings of the item for

which predictions are made. Thus, on line 3 we subtract out the impact of the excluded item, the item for which predictions will be made.
Let's follow the subroutine ProbabilityOneItem for one specific item, the highlighted item from the running example rating matrix W in Figure 1. The item overall has 2 D labels and 7 C labels. What is the probability of getting the specified sequence (D, C, D, C, C) from a randomly chosen sequence of five labels?
The number of all possible sequences of k = 5 raters is

9  5!
5

because we can first chose the set of five raters and then choose their order. Of these, how many

sequences match the specified label sequence? We can choose the set of two D raters and then their

order in positions 1 and 3 of the label sequence, for a total of

2 2

 2! possibilities. These choices

of D raters can be combined with any valid choice of the three C raters for positions 2, 4 and 5,

7 3

 3!.

Thus, the fraction of sequences that match is

(

2 2

 2!)(

7 3

 3!)

9 5

 5!

This explains the key formula on line 5 in ProbabilityOneItem:

Wi( ) y( )

 y( )!

|Wi | k

 k!

In the special case where the ABC is asked to predict the first rating for an item (i.e., k = 0), the denominator in Algorithm 6 will be 1 and the numerator will be the fraction of labels for all items (excluding the current item) that are labeled . Thus, it estimates the probability of each label by that label's overall frequency on the other items in the dataset.
Since ABC approximates the Bayesian prior when k = 0 and the Bayesian posterior for other k, we have an information-theoretic interpretation of the cross-entropy scores. With enough items, the difference in cross-entropy scores for k and 0 approaches the theoretical information gain in predicting a held out rater from first acquiring labels from k other raters, i.e., the mutual information between a held out rater and k other raters. This also implies that (with enough items) the power curve will be monotonically increasing because information increases with more raters and thus residual entropy decreases. Formally, we use random variables Y1, . . . , Yk, Yk+1 to denote the joint distribution of the first k + 1 labels of a randomly drawn item. Note that, by assumption, conditional on the item the Yi's are i.i.d.

Theorem 1 (ABC+CE estimates mutual information). For any fixed set of raters and any k < |raters|, almost surely

lim SPC(W, ABC(и), CE(и))k - SPC(W, ABC(и), CE(и))0 = MI(Yk+1; Y1, Y2, и и и , Yk)5.
|I |

5 For random variables W, Z, the Shannon mutual information (1948) between W, Z is defined as

Pr[W = w|Z = z]

MI(W ; Z) := Pr[W = w, Z = z] log

.

Pr[W = w]

w,z

Just as there is an information-theoretic interpretation for the power of k raters, so too there is an information-theoretic interpretation of the cross-entropy score of any soft classifier, so long as it is "calibrated."6 With enough items, the difference between the cross-entropy score for a calibrated soft classifier and the baseline score approaches the information gain from using the classifier's output to predict a random rater, rather than using a default prediction of the prior.7 Formally, we use random variables X, Y to denote a random item and one of the item's labels, and random variable h(X) to denote classifier h's output for a random item X.
Theorem 2 (CE+CALIBRATED CLASSIFIER estimates mutual information). For any calibrated soft classifier h, and for any fixed set of raters, almost surely

lim HScore(h(I), W, CE(и)) - SPC(W, ABC(и), CE(и))0 = MI(h(X); Y )
|I |

Since the ABC produces the Bayesian posterior in the limit, no other combiner can, in the limit, produce predictions that have higher mutual information with a reference rater. Thus, no other combiner can have a higher expected cross-entropy score. By extension, if we are using the cross-entropy scoring function, the ABC yields the lowest possible survey equivalence value. This is formalized in Theorem 3 and Corollary 1.

Theorem 3 (ABC Maximizes CE score). For any fixed set of raters, any k < |raters|, and any combiner function Comb(и), assuming for all y1, y2, и и и , yk  Lk, lim|I| Comb(y1, y2, и и и , yk, W-i) exists almost surely, the following inequality holds almost surely.

lim SPC(W, ABC(и), CE(и))k  lim sup SPC(W, Comb(и), CE(и))k

|I |

|I |

Corollary 1 (ABC Minimizes Survey Equivalence). For any predictor h, any fixed set of raters and any combiner function Comb(и),

lim SEq(h, I, W; ABC(и), CE(и)))  lim inf SEq(h, I, W; Comb(и), CE(и)))

|I |

|I |

The power curve in Figure 5 was again generated from the synthetic dataset of 1000 items and ten raters generated from the model for the running example described in Section 2. Here, subsets of k raters' labels were combined using the Anonymous Bayesian Combiner and scored with the cross-entropy scoring function. The y-axis values plot information gain values, by subtracting the score c0 for predictions based on the Bayesian prior.
Figure 5 also shows the expected score for the soft classifier described in Section 2 and its survey equivalence, 1.96, with errors bars covering the range [1.63, 2.54]. Because of the information theoretic properties of the Anonymous Bayesian Combiner and the cross-entropy scoring function articulated in Theorem 1, it makes sense to say that the soft classifier provides the same amount of information as 1.96 raters. As we would expect given Corollary 1, using the frequency combiner instead of the Anonymous Bayesian Combiner yields an inflated value for the survey equivalence: 6.32 in this case. Note also that using the majority vote combiner and the agreement scoring

6 Intuitively, a classifier is not calibrated if it is systematically over-optimistic or over-pessimistic in its predictions. Formally, a soft classifier h is calibrated if
h(X) = Pr[Y = |h(X)].

7 It is standard practice to calibrate soft classifiers by post-processing their predictions. If the classifier were not calibrated, the measured score hscore - c0 would be lower and not fully reflect the information contained in the classifier output. Thus, we only have an information-theoretic interpretation of the classifier's score if the classifier is calibrated.

Running Example: ABC + Cross Entropy
0.4 k raters omniscient classifier calibrated hard classifier
0.3

information gain (ck - c0)

0.2

0.1

Figure 5

0

0

1.96

4

6

8

Number of raters

The power curve for the running example using the Anonymous Bayesian Combiner and the CrossEntropy Scoring Function

function as in Figure 4 also yielded a higher value for the survey equivalence, 4.31. The survey equivalence computed using the Anonymous Bayesian Combiner and the cross-entropy scoring function gives the best assessment of how informative the classifier is; other combiners and scoring functions would make it appear that the classifier is better than it really is at predicting a random rater's label.
How high would the power curve go if it continued to more than nine raters? Since we know the model from which the data were generated, we can compute what the score would be for infinite size surveys, which would yield an omniscient classifier that somehow magically knows the true state of each item and outputs the associated probability. Its expected cross-entropy score would be .7[.8 log(.8) + .2 log(.2)] + .1[.5 log(.5) + .5 log(.5)] + .2  [.1 log(.1) + .9 log(.9)  -.699. E(c0) is the expected cross-entropy of the baseline classifier that always outputs .63, the prior probability of a C label, which is .63 log(.63) + .37 log(.37)  -.951. Thus, the information gain (mutual information) would be .252. This tells us what the asymptote of the power curve would be if we had a very large number of ratings per item and thus could approximate the omniscient classifier. The asymptote tells us how much room there is for improvement, about .029 bits additional beyond the 0.223 information gain with nine raters. In practice, with real rather than simulated datasets, such an omniscient classifier would not be available and we would have to guess at the asymptote by projecting from the available points on the power curve.
5. Running Time of Power Curve Computation
To compute a power curve with natural scoring functions-- such as accuracy, F1, Pearson correlation, or cross-entropy-- and simple combiners such as majority vote and frequency, there exists an implementation such that the total running time is O(K2|I|). The Anonymous Bayesian Combiner's implementation requires more time. A naive implementation would require O(K2|I|2|L|). However, memoization can reduce the time complexity.
Theorem 4 (Running time of power curve computation). To compute the survey power curve, when we use the frequency combiner, there exists an implementation such that the total running time is O(K2|I|); when we use Anonymous Bayesian Combiner, there exists an implementation such that the total running time is min{O(K|L||I||L|), O(K|I|2|L|)}.
The proof can be found in appendix C.

6. One Reference Rater or Consensus of Several
Survey equivalence characterizes a classifier in terms of the survey size that would yield the same predictive power. It is not obvious, however, what should be predicted. In our procedure, both the classifier and the survey are evaluated as if they are trying to predict the rating of a single, randomly selected reference rater. Another option would be to score them against a combination of several reference raters, perhaps the majority vote of three or five reference raters. The intuitive appeal of scoring against a combination of several reference raters is that noise in individual reference ratings may be canceled out by combining several of them. One practical drawback of scoring against a combination of multiple reference raters is that more reference raters must be held out. If we gather a dataset with ratings from K raters for each item, and we score against groups of five held out reference raters, we can only compute the predictive power of surveys of size up to K - 5. More generally, if we score against groups of r reference raters, the survey power curve can only extend to surveys of size K - r. Choosing r = 1 lets us compute the expected score of larger surveys than is possible if we choose a larger r.
More fundamentally, scoring against a single reference rater provides a better indicator of how informative a classifier is in survey settings. As a reminder, in survey settings the amount of agreement among raters can vary between items. This may be due to the ground truth being subjective, as in preferences for cute cat versus dog images. Alternately, it may be due to the objectively correct label being more difficult to perceive for some items than others. Theorem 1 shows that, with a particular combiner and scoring function, scoring against a single reference rater yields an information-theoretic measure of the information gained about an item's state. It measures how well a classifier predicts the fraction of people who will assign different labels to items; it rewards classifiers that can provide information about whether an item will get 51% or 98% C labels. By contrast, scoring against the majority of several reference raters gives a better measure of how well a classifier predicts the single most popular answer. It rewards classifiers that provide information about whether an item will get 51% or 49% C labels more than it rewards those that distinguish 51% items from 98% items.
There is one special case, the uniform noise setting, where scoring against a single reference rater or scoring against a combination of them yields the same survey equivalence value. In this setting, there is exactly one state corresponding to each label. For example, if there are two possible labels, C and D, there are just two possible item states: perhaps some items will receive 88% C labels and others will receive 35%, but no other percentages are possible. Intuitively, this would arise if the true states are C and D and raters perceive those states with some noise, but the level of noise is uniform across items.
In this uniform noise setting, there is a scoring function under which survey equivalence values are invariant to the number of reference raters used. The scoring function is designed based on a recently defined information measure, Determinant Mutual Information (DMI) (Kong 2020).
Definition 1 (DMI Scorer). For a hard classifier h,
DMIScore({(h(i), yi)}i) = | det(M)|
where M  R|L|О|L| is the empirical joint frequency matrix.8 Let HScore(h(I), s(I), Score(и)) denote a hypothetical scoring algorithm which scores against
the ground truth state s(i) of each item, in contrast to the original HScore which scores against a
8 That is, each cell Mc,c is the count of items where the label is c and the classifier outputs c; then all cells are normalized by the total number of items so that the sum of all cells adds up to 1. Similarly, for a soft classifier that outputs a vector of probabilities pi for each item, each unnormalized cell Mc,c is the sum, over items where the label is c , of the probability assigned to c, and cells are again normalized by the total number of items so that they add up to 1.

single reference rater.9 Let HScorer(h(I), W, Score(и)) denote a scoring algorithm which scores against the plurality vote of r raters.10
Survey equivalence can be defined based on either HScore or HScorer.11 Note that in computing SEqr(h, I, W; Comb(и), Score(и))), since we need the number of unused raters to be greater than r for the calculation of the plurality vote, the power curve will be computed only up to K - r.
Lemma 1. For all r > 0, there exists 0 < r  1 such that for all classifiers h,

lim HScorer(h(I), W, DMIScore(и)) = r lim HScore(h(I), s(I), DMIScore(и))

|I |

|I |

Intuitively, Lemma 1 says that when using the DMIScore scorer, in the limit, changing the number of reference raters merely scales HScorer.
Theorem 5. For all r > 0, for all combiner functions Comb, when lim|I| SEq(h, I, s(I), W; Comb(и), DMIScore(и)))  K - r,

lim SEqr(h, I, W; Comb(и), DMIScore(и))) = lim SEq(h, I, s(I), W; Comb(и), DMIScore(и)))

|I |

|I |

Proofs appear in the appendix. Intuitively, the theorem implies that a classifier's score and the power curve values for k raters will all be scaled by the same (unknown) fraction r from what they would be if they were scored against the (unknown) ground truth states. Since the scaling is the same for all k and for the classifier, the survey equivalence point will be the same as it would be if they were all scored against the ground truth. With a different number of reference raters r, the hypothesis might have a different expected observed score, but the survey equivalence would be the same as what would be generated from scoring against the ground truth, and the same as would be generated by scoring against just one reference rater.

7. Discussion
In survey settings, where people don't always agree about the correct label for each item, conventional classifier scoring functions are not meaningful on their own. Intuitively, the score for a classifier should reveal how good the classifier is and not be dependent on how hard the classification problem is for people. Among scoring functions, cross-entropy has the benefit of an informationtheoretic interpretationГ it quantifies the number of bits of information about an item's state that are provided by the classifier. Even that, however, is not fully meaningful on its own. Depending on the distribution from which item states are drawn, there will be a different maximum information gain that is possible from an optimal classifier that perfectly predicts every item's state. Thus, an expected information gain of 0.2 bits might indicate a nearly optimal classifier, or one that has much room for improvement. Some other information is required besides a cross-entropy, precision, recall, or other raw score.
An interesting avenue for future work would be to directly characterize how much room for improvement there is for a classifier. As k increases, the power curve scores ck should approach an asymptote, the score of an omniscient classifier. Indeed, if we had, say, a million ratings for each item, we could determine with high confidence what fraction of the full population would assign each of the potential labels to that item; that is, the item's true underlying state. An ideal soft classifier would always output that fraction and no classifier could achieve a higher score than such
9 More formally, to compute HScore(h(I), Score(и)) change line 3 in Algorithm 1 to use Score({h(i), s(i)}i). Note that typically s(i) would not be available, but we can still define this hypothetically.
10 More specifically, to compute HScorer(h(I), W, Score(и)) change line 1 in Algorithm 1 to `for every size r subset of raters, rs' and change line 3 to `score against the plurality vote of the r reference raters in rs'.
11 To compute SEq(h, I, W; Comb(и), Score(и))) or SEqr(h, I, W; Comb(и), Score(и))), substitute calls to compute HScore(h(I), W, Score(и)) or HScorer(h(I), W, Score(и)) into line 1 in Algorithm 4 and line 8 in Algorithm 3

an ideal classifier. We could then describe the performance ratio of any classifier as the fraction of the information it provides as compared to the information that would be provided by the ideal classifier with a million ratings for each item. With our synthetic running example, where we know those underlying states, it was possible to simulate the ideal classifier. It may also be possible to use a dataset with only a few ratings per item to make inferences about the ideal classifier's performance and thus the performance ratio of actual classifiers, if we make some suitable assumptions about the family of distributions from which item states are drawn.
Absent a way to estimate the performance ratio, the survey equivalence procedure yields an interpretable number, by comparing the raw score of a classifier to other classifiers that can be simulated. In particular, a family of other classifiers serve as baselines, each simulating a survey of k raters whose labels are combined to generate a single output. If a classifier has equivalent performance to a survey of many raters, and moreover the slope of the power curve is approaching 0, then it is reasonable to conclude that the classifier is nearly as good as possible.
The survey equivalence approach is especially appropriate in settings where the practical alternative to using a classifier is to employ one or a few people to judge items. For example, online platforms like Facebook and Twitter employ content moderators to determine whether content has violated the platform's stated policies, typically relying on the judgment of a single moderator for each item. Other examples might include assessing images from medical scans or airport security devices or sports instant replays. While even experts may not always agree in their assessments of these items, judgments have to be made and society accepts some level of variability.
In these cases, the survey equivalence value has a very practical interpretation. It says how many people would be needed in order to make better judgments on average than the classifier could provide. The number of independent human judgments that are normally collected about each item becomes a threshold for survey equivalence beyond which the human judgments should be replaced by or at least combined with the classifier judgments in some way. In particular, if a single human judgment is used, once an automated classifier can predict a single human judgment better than another human judgment can predict it, it becomes hard to defend a practice of using only the human judgment.
Of course, in order to make this interpretation, we should be sure that we are using the human judgments as well as possible. If we combine labels from k raters very poorly, yielding artificially bad predictions of what a reference rater will say, then the survey equivalence will be inflated. Fortunately, we can guard against that possibility by using the Anonymous Bayesian Combiner. As claimed in Theorem 3 and Corollary 1, as the number of items grows, the ABC will produce optimal predictions, with the maximum possible cross-entropy score among all possible combiners, and the lowest possible survey equivalence.
Another interesting application of the survey equivalence process is in comparing the performance of a survey of less-expert human raters against more expert but still fallible human raters. Suppose that for each item in a dataset W we have A ratings from "amateur" human raters and E ratings from "expert" human raters. Applying Algorithm 3, we can compute the power curve for up to E - 1 expert raters. Analogously, we can take subsets of k  A amateur raters, combine their labels to make a prediction for each item, and score the predictions against the labels of one of the expert raters. This procedure will yield an amateur power curve and an expert power curve.
We can then apply Algorithm 4 from one point on either power curve to the other power curve. For example, we can compute the "expert equivalence" of ten amateurs by treating the predictions produced by combinations of ten amateurs as the classifier h and the expert power as the ck values. If the equivalence is 2, that means that surveys of ten amateurs have the same predictive power for a held-out expert as surveys of two other experts have. An expert rater is treated as the reference point because the ratings of an expert are presumed to be good, even though imperfect.
A key insight of the survey equivalence process is that in comparing a classifier to a simulated survey of k people, we only need to see how well each of them predicts the rating of a single held-out reference rater. Thus, with a dataset of K ratings per item, we can simulate a power curve for

surveys of up to size K - 1. As argued in Section 6, scoring against just a single reference rater at a time provides at least as good and usually a better measure of the information gain from a classifier than would come from scoring against a combination of several raters.
8. Conclusion
The survey equivalence process takes a dataset of items with many ratings per item and classifier outputs for those items. A scoring function compares predictions to a single rater's labels for the items; a variety of scoring functions may be used. A combiner function combines several ratings to predict what a single other rater will say. As output, the survey equivalence process produces an equivalence value that says how many labels would be needed to get the same expected score that the classifier got.
The survey equivalence value is more meaningful than the raw score of the classifier, because it takes into account the level of agreement among human raters. If the dataset includes more difficult items, where humans agree less, the raw score of the classifier will typically go down but so will the expected score for k raters, and thus the survey equivalence will not necessarily go down. Moreover, in the common setting where the practical alternative to an automated classifier is human raters, the number of human raters who would be used becomes a target performance level. If a classifier can achieve a survey equivalence larger than that number, it is ready to go into production.
Appendix A: Case Studies
We illustrate the survey equivalence calculation through three case studies of previously published research where the ground truth was subjective and the labels were provided by human raters. In the first, the items were comments made on Wikipedia and the classifier was the initial version of the Jigsaw personal attacks classifier described in (Wulczyn et al. 2017). In the second, the items were news articles and the classifier was the one described in (Mitra and Gilbert 2015). In the last, the items were pairs of images, and the classifier selected whichever image had accumulated a higher net upvote score on Reddit. In all three cases, multiple human ratings were available for each item, making it possible to compute a survey power curve and use held out raters to score subsets of the raters as well as the classifiers.
A.1. Personal Attacks
Online platforms often employ machine learning tools to automatically flag or remove questionable content. In their development of ML-based content moderation systems, Jigsaw in collaboration with the Wikimedia foundation solicited thousands of human labels indicating whether a comment contained a "personal attack or harassment" or not (Wulczyn et al. 2017). A set of 23,179 Wikipedia comments was annotated by at least 10 and at most 20 human raters. For each, an ML model also produced a predicted probability that a rater would label the comment as a personal attack.
For our analysis, we score predictions against a single rater's binary label for each comment (attack or not). Thus, we only consider scoring functions that operate on discrete labels. Since the paper's classifier produces a predicted attack probability, we only consider scoring functions that operate on soft classifiers and we only consider combiner functions that produce continuous probability values. Figure 6 shows power curves and survey equivalence scores using pairings of two scoring functions (cross-entropy and AUC) and two combiners (ABC and the frequency combiner).
A classifier's cross-entropy score is artificially deflated if the classifier is not calibrated. Thus, we include scores both for the raw classifier predictions and a classifier calibrated using isotonic regression.12 As expected, this increased the cross-entropy score but had almost no effect on the AUC score. As argued in the paper, we consider the survey equivalence value using ABC and cross-entropy to be the best description of how much information the classifier provides. Thus, our best estimate of the survey equivalence of the classifier is 7.63 human raters. Unlike in Figure 5, here there is a wide error bar needed to cover 95% of the bootstrap confidence intervals. Even though the error bars around the classifier score and the power curve values are relatively small, the classifier's score falls in a relatively flat part of the power curve, leading to wide error bars around the survey equivalence value.
The original paper conducted a "Human Baseline Comparison", which is a form of survey equivalence. They scored against a set of 10 held-out reference raters, rather than the single reference rater that we have
12 Calibration was performed using the CalibratedClassifierCV class in the python package sklearn.

Info Gain (ck - c0)

ABC + Cross Entropy

k raters

0.4

Jigsaw Personal Attack Classifier

Calibrated Personal Attack Classifier

0.2

0

-0.2 0

4.24 6 7.63

12 15 18

Number of raters

ABC + AUC
1 k raters Jigsaw Personal Attack Classifier Calibrated Personal Attack Classifier
0.8

Info Gain (ck - c0)

Frequency Combiner + Cross Entropy

k raters

0.4

Jigsaw Personal Attack Classifier

Calibrated Personal Attack Classifier

0.2

0

-0.2 1 0.8

0 3 6 9.49 12

16.43 19

Number of raters

Frequency Combiner + AUC

k raters Jigsaw Peresonal Attack Classifier Calibrated Personal Attack Classifier

score

score

0.6

0.6

Figure 6

0.4 0

3.77 6 9 12 15 18
Number of raters

0.4

0 3 6 9 12 15

19

Number of raters

Survey Equivalence between human labels and Jigsaw's Wikipedia comment personal attack classifier under different combiner and scoring function pairings. Survey equivalence score is indicated on the x axis. Error bars cover 95% of 500 bootstrap item samples.

argued is more desirable. They conducted two versions of the analysis. The first used AUC as the scoring function, what we call the frequency combiner to average the labels of surveyed raters, and what we call the majority vote combiner for combining the held-out reference raters. The second used Spearman rank correlation as the scoring function and the frequency combiner for both the surveyed raters and the held-out reference raters. They did not calculate a fractional survey-equivalence value, but reported that with either approach, their model performed better than groups of three reference raters but worse than groups of five, lower than our estimate of 7.63.
Their result using AUC and the frequency combiner differs from our analysis using those same functions, for two reasons. First, they scored against a majority vote of 10 randomly selected raters while we score against a single randomly selected rater, which we have argued is a better approach. Second, for their evaluation they used a stratified sample to get more comments that were personal attacks; half the comments came from people who were blocked for violating Wikipedia's policy on personal attacks. We evaluated using a random sample of 2,000 comments from the dataset that they published.
A.2. News Credibility
The spread of misinformation and disinformation in online social media has led to numerous efforts to identify and remove misinformation news stories and sources. The CredBank dataset is a large corpus of 1,377 news events, each annotated by 30 crowd workers who viewed a collection of tweets related to those events (Mitra and Gilbert 2015). The annotation was on a five-point scale from "certainly inaccurate" to "certainly accurate", but for the analysis was reduced to a binary label of "certainly accurate" or anything else. An automated classifier was created that processed linguistic features of the collection of tweets related to an event (Mitra et al. 2017). The classifier output was a predicted bucket for the fraction of raters who would label the event as "certainly accurate": > 90%, 80 - 90%, 60 - 80%, or < 60%. The paper reporting on the linguistic classifier scored its performance in terms of precision and recall at predicting the correct bucket. Performance was much better than that of a baseline classifier that guessed without examining the tweet contents. It is not clear, however, from precision and recall scores of 50 - 75%, whether we should think of the classifier as doing pretty well or not so well.

ABC + Cross Entropy

0.08

k raters

CredWeb Classifier

0.06

0.04

score

0.02

0

Figure 7

-0.02

0.48 3

6

9

12

15

18

Number of raters

Survey Equivalence between human labels of news credibility and CredBank's heuristic classifier for ABC and Cross-entropy scorer. Survey equivalence score is indicated on the x axis. Error bars cover 95% of 500 bootstrap samples.

We compute the survey equivalence for their classifier using the Anonymous Bayesian Combiner and crossentropy. We calibrated the classifier by using the empirical dataset to determine the optimal prediction corresponding to each of the four discrete labels it output. Among items where it output > 90%, the empirical frequency of "certainly accurate" labels was 83.1%. Similarly 80 - 90% mapped to 80.0%,60 - 80% to 77.6% and < 60% to 77.7%. The calibrated classifier has an information gain of 0.0023 bits as compared to a baseline prediction based on the overall frequency of "certainly accurate" labels, which was 79.7%.
Figure 7 graphs the power curve for different numbers of raters whose labels are combined using the Anonymous Bayesian Combiner. It produced as output a predicted probability for a held-out reference rater to label the event as "certainly accurate". We find a survey equivalence of 0.48.
Recall that we can interpret a survey equivalence of less than one in terms of a randomized process. For 48% of items, we get a single rater's label and output a prediction based on the Bayesian posterior given that label, 81.4% if the label is "certainly accurate" and 73.1% if it is not. For the other items, we do not get a label at all and just make the baseline prediction of 79.7%. We suspect that this may be a fairly typical result for many automated classifiers, where performance is not as good as a single human rater.
A.3. Social Rating Systems
How informative are the upvoting systems on social rating sites like reddit? Each time a user likes, retweets, or upvotes a post or comment they are indicating their preference for one kind of content. Social media systems take those ratings into account and typically increase the visibility of highly-rated content. An important consequence of this feedback loop is path-dependency, wherein user-preferences for content are not perfectly reflected in the content's popularity (Salganik et al. 2006). Thus, even though an item may have been seen by and potentially voted on by hundreds or thousands of people, the net upvotes may not fully reflect the information inherent in the opinions of all those viewers.
One paper tried to assess whether items that receive more net upvotes are reliably preferred by users to those that receive fewer (Glenski et al. 2018). It presented pairs of images that had previously appeared in a subreddit and asked people which they preferred. The pairs were presented in a game-like interface that attracted a lot of voluntary use, yielding a large dataset that we are able to reanalyze. In the original paper, the authors concluded that the net upvotes were a somewhat reliable way to distinguish better liked images only when there was a very large difference in the number of upvotes.
With a survey equivalence analysis, we are able to quantify how many people's independent information is captured in the reddit net upvotes. Here, conceptually we can think of the "classifier" as the entire sociotechnical process of reddit presenting posts in some ranked order and collecting upvotes and downvotes. We defined a hard classifier whose output for each pair of images is the image that received the highest final score

Info Gain (ck - c0)

Info Gain (ck - c0)

ABC + Cross Entropy

Frequency + Cross Entropy

0.1

0.1

k raters

k raters

Reddit Scores

Reddit Scores

0.05

0.05

0

0

-0.05 0.8

0 2.39 6 9 12 15 18
Number of raters
Majority Vote Combiner + F1
k raters Reddit Scores

0.6

-0.05 0 3 6 9 11.37 15 18
Number of raters Majority Vote Combiner + Agreement
0.8 k raters Reddit Scores
0.6

Agreement Score

F1 Score

0.4

0.4

0.2 0

3.87 6 9 12 15 18
Number of raters

0.2 0

3 4.57

9 12 15 18

Number of raters

Figure 8

Survey Equivalence between GuessTheKarma survey responses and Reddit scores under different combiner and scoring function pairings. Survey equivalence score is indicated on the x axis. Error bars cover 95% of 500 bootstrap item samples.

(i.e., # of upvotes - # of downvotes) on Reddit. From that, we created a calibrated classifier that outputs 57.8% for the left item if the left item had more net votes, and 42.8% otherwise. We score the calibrated classifier using the cross-entropy scoring rule and the hard classifier using F1 score and agreement. Figure 8 shows the power curves and survey equivalences for several combiner functions and scoring functions.
The survey equivalence value was 2.39 computed using the Anonymous Bayesian Classifier and crossentropy. This says the number of independent raters that would be needed to produce the same amount of information as is provided by the ordering of the reddit net upvote scores. Here, the information is about what a random held-out rater will say about which image of the pair is better. The result is disconcerting. Knowing which of two images has a higher net upvote score, after hundreds or thousands of people have been exposed to the images on reddit, tells us less than we could learn by conducting an independent poll of three people!
Few images pairs in the dataset had net upvote scores that were very close to each other, so the lack of information in the ordering of their upvote scores is not due entirely to that. It is possible, however, that a more informative classifier could be constructed, by taking into account the absolute net vote scores of the two images and not just which one was larger.
The survey power curve when using the frequency combiner is instructive. The dip below zero indicates that the frequency combiner performs worse than randomly guessing until about 9 raters are sampled and is eventually equivalent to the Reddit-classifier at 11.41 raters. The reason is that with few raters the frequency combiner produces predictions that are too extreme. For example, one rater preferring the "left" image from a pair leads to a prediction that everyone will prefer that image. To prevent taking the log of 0 in the cross-entropy scoring function, we treat a prediction of 100% as 98% and 0% as 2%, but even predictions of 98% and 2% after one rating are overconfident and the cross-entropy scoring function strongly penalizes overconfident predictions.
Perhaps a more intuitive result is shown in the bottom row of Fig. 8. The Majority Combiner results in the same power curve shape using either F1 or agreement as the scoring function, but at different scales. Both scoring functions result in similar survey equivalence values.

Appendix B: Proofs of Properties of the Anonymous Bayesian Combiner This section presents formal proofs for ABC's two information-theoretic properties, that both ABC+CE and CE+calibrated classifier estimates mutual information values between pairs of random variables. We will also formally prove a corollary of the first property, that ABC+CE has a monotone power curve. The proofs show that these properties hold "almost surely"; there could be pathological sequences of increasingly large item sets for which the properties don't hold, but such sequences happen with probability 0 if the item sets are drawn at random.
Theorem 1 (ABC+CE estimates mutual information). For any fixed set of raters and any k < |raters|, almost surely
lim SPC(W, ABC(и), CE(и))k - SPC(W, ABC(и), CE(и))0 = MI(Yk+1; Y1, Y2, и и и , Yk)13.
|I |
Proof of Theorem 1 We will show that as the number of items goes to infinity, when k = 0, ABC outputs the Bayesian prior over the label of a randomly selected rater for a randomly selected item; and for other k, ABC outputs the Bayesian posterior conditioning on the input label sequence. Formally, we will show
Lemma 2. For any sequence of labels y = (y1, . . . , yk) and any item i and any label  L, almost surely,
lim ABC(y1, y2, и и и , yk; W, i) = Pr[Yk+1 = |Y1 = y1, Y2 = y2, и и и , Yk = yk].
|I |
We first use Lemma 2 to prove the theorem, and then prove Lemma 2. Intuitively, since the ABC predictions will converge to their expectation almost surely and because the cross-entropy score is continuous, almost surely the limit of SPC(W, ABC(и), CE(и))0 will be the entropy of a randomly selected rater's label for a randomly selected item. Moreover, almost surely, the limit of SPC(W, ABC(и), CE(и))k is the entropy of a randomly selected rater's label for a randomly selected item conditioning on k other raters' labels. More formally, almost surely:

lim SPC(W, ABC(и), CE(и))k
|I |

1 = lim
|I| |I |

1 |RS|

1 |raters| - k

log ABC(yji1 , . . . , yjik , W, i)yjir

iI

(j1 ,...,jk )RS

jr raters-{j1,...,jk}

1 = lim
|I| |I |

1 |RS|

1 |raters| - k

log Pr[Yk+1 = yjir |Y1 = yji1 , . . . , Yk = yjik ]

iI

(j1 ,...,jk )RS

jr raters-{j1,...,jk}

(Lemma 2)

=

Pr[Y1 = y1, Y2 = y2, и и и , Yk+1 = yk+1]

y1,y2,иии ,yk+1

 log(Pr[Yk+1 = yk+1|Y1 = y1, Y2 = y2, и и и , Yk = yk])

where yji is the jth label of item i, RS is the set of RaterSubsets from Algorithm 2, and jr is the reference rater in Algorithm 1. The first line follows from definitions; the second line follows, almost surely, because of Lemma 2 and because the log function is continuous, so we can bring the limit inside of both the summations and the log; the third line follows, almost surely, from the strong law of large numbers.
Using this, we can see that the difference between the survey power curve at k and the survey power curve at 0 is the difference between the entropy and the conditional entropy, which is just the information gain or mutual information. Almost surely:
lim SPC(W, ABC(и), CE(и))k - SPC(W, ABC(и), CE(и))0
|I |

13 For random variables W, Z, the Shannon mutual information (1948) between W, Z is defined as

Pr[W = w|Z = z]

MI(W ; Z) := Pr[W = w, Z = z] log

.

Pr[W = w]

w,z

=

Pr[Y1 = y1, Y2 = y2, и и и , Yk+1 = yk+1]

y1,y2,иии ,yk+1

 log(Pr[Yk+1 = yk+1|Y1 = y1, Y2 = y2, и и и , Yk = yk]) - log(Pr[Yk+1 = yk+1])

=(-H(Yk+1|Y1, Y2, и и и , Yk)) - (-H(Yk+1))

=MI(Yk+1; Y1, Y2, и и и , Yk)

It is left to show Lemma 2. The key step is to show that LabelSeqProb(y1, y2, и и и , yk; W) from Algorithm 5 converges to the probability Pr[Y1 = y1, Y2 = y2, и и и , Yk = yk] as the number of items increases. With the above result, almost surely

LabelSeqProb(y1, y2, и и и , yk, ; W, i)  Pr[Yk+1 = , Y1 = y1, Y2 = y2, и и и , Yk = yk]

LabelSeqProb(y1, y2, и и и , yk; W, i)

Pr[Y1 = y1, Y2 = y2, и и и , Yk = yk]

= Pr[Yk+1 = |Y1 = y1, Y2 = y2, и и и , Yk = yk]

(1)

Step 1 follows from the fact that the denominator and numerator have non-zero limits in the above formula since we assume that the distribution has full support. Step 2 follows from the definition of conditional probability.
We now show the key step. To get an unbiased estimator of Pr[Y1 = y1, и и и , Yk = yk], we could select a random sequence of raters for a random item and see if they happen to produce the specified label sequence. To reduce variance, we could run this process many times and take the mean. However, we can efficiently calculate what those means converge to in closed form.
First, note that the process chooses items uniformly at random. Thus, the overall probability is just the mean of the probability of the label sequence for each item. For each item, we enumerate all the possible sequences of raters and compute the fraction that match the specified label sequence. That gives the probability conditional on choosing that item.
To illustrate the enumeration process, we consider a simple running example computing the probability, for the highlighted item from the running example rating matrix W in Figure 1, that a random selection of five labels for that item will yield the sequence (D, C, D, C, C). The item overall has 2 D labels and 7 C labels. The number of all possible sequences of k = 5 raters is

9  5!
5

because we can first chose the set of five raters and then choose their order.

Of these, how many sequences match the specified label sequence (D, C, D, C, C)? We can choose the

set of two D raters and then their order in positions 1 and 3 of the label sequence, for a total of

2 2

 2!

possibilities. These choices of positive raters can be combined with any valid choice of the three negative

raters for positions 2, 4 and 5,

7 3

 3!.

Thus, the fraction of sequences that match is

(

2 2

 2!)(

7 3

 3!)

9 5

 5!

For the general case, using the notations defined previously, the probability of the label sequence for item

i is:

Wi( ) y( )

 y( )!

|Wi|  k! k

ProbabilityOneItem outputs exactly this quantity for one item. Algorithm 6 takes the mean across

items.

Since the items are i.i.d., when i is set to be the first item, for all y1, y2, . . . , yk the processes converges almost surely to Pr[Y1 = y1, и и и , Yk = yk] due to the strong law of large numbers. Note that which item is excluded in line 3 in Algorithm 5 does not affect the convergence because it can only change the answer by

1 |I |-1

,

which

goes

to

0

uniformly

as

|I |

grows.

This concludes the proof of the Lemma, and thus the proof of the Theorem as well.

Corollary 2 (ABC+CE has a strictly monotone power curve).

lim SPC(W, ABC(и), CE(и))k < lim SPC(W, ABC(и), CE(и))k+1,  k  1

|I |

|I |

A key ingredient to our proof of this corollary is that more observations add strictly more information. This will not be true in some degenerate cases. However, this is true under our assumption that there is more than one state and each state has support over all the labels.
Proof of Corollary 2 According to Theorem 1, to show the corollary, it's equivalent to show that
MI(Yk+1; Y1, Y2, и и и , Yk) > MI(Yk+2; Y1, Y2, и и и , Yk+1)
Since raters are homogeneous, we can rewrite the above formula as
MI(Y ; Y1, Y2, и и и , Yk) > MI(Y ; Y1, Y2, и и и , Yk+1)
where Y is label of a randomly selected held out rater (i.e., not one of the first k + 1 raters) for a random item. Intuitively, we just need to show that Yk+1 adds strictly more information.

MI(Y ; Y1, Y2, и и и , Yk+1) - MI(Y ; Y1, Y2, и и и , Yk)

=

Pr[Y = y, Y1 = y1, и и и , Yk = yk, Yk+1 = yk+1]

y,y1,y2,...,yk ,yk+1

 (log(Pr[Y = y|Y1 = y1, и и и , Yk = yk, Yk+1 = yk+1] - log(Pr[Y = y|Y1 = y1, и и и , Yk = yk]))

=

Pr[Y1 = y1, и и и , Yk = yk, Yk+1 = yk+1]

y,y1,y2,...,yk ,yk+1

 Pr[Y = y|Y1 = y1, и и и , Yk = yk, Yk+1 = yk+1]

 (log(Pr[Y = y|Y1 = y1, и и и , Yk = yk, Yk+1 = yk+1] - log(Pr[Y = y|Y1 = y1, и и и , Yk = yk]))

>0

Importantly, we must argue that the last inequality is strict. This follows from the fact that the log scoring rule is strictly proper and the following claim:

Claim 1. When each state has support over all the labels, and there is more than one state, there exists  L, such that Pr[Y = |Y1 = , и и и , Yk = ] < Pr[Y = |Y1 = , и и и , Yk = , Yk+1 = ].

To prove this, we pick a label such that there exists two states s = s with s( ) = s ( ). We have

Pr[Y = |Y1 = , и и и , Yk = ] =

s Pr[S = s](s( ))k+1 s Pr[S = s](s( ))k

< s Pr[S = s](s( ))k+2 s Pr[S = s](s( ))k+1

(Cauchy inequality (

= Pr[Y = |Y1 = , и и и , Yk+1 = ]

s asbs)2  (

s a2s )(

s b2s ))

We apply the Cauchy inequality by picking as = Pr[S = s](s( ))k and bs = Pr[S = s](s( ))k+2. The

inequality is strict since there exists s = s

such that

bs as

= s(

)=s (

)

=

bs as

.

Theorem 2 (CE+CALIBRATED CLASSIFIER estimates mutual information). For any cali-

brated soft classifier h, and for any fixed set of raters, almost surely

lim HScore(h(I), W, CE(и)) - SPC(W, ABC(и), CE(и))0 = MI(h(X); Y )
|I |
Proof of Theorem 2 Let h be a calibrated soft classifier with range R(h). We first show that lim|I| HScore(h(I), W, CE(и)) = xR(h),yL Pr[X = x, Y = y]  log(Pr[Y = y|h(X) = h(x)]).

1

lim
|I |

HScore(h(I ),

W,

CE(и))

=

lim
|I |

|I |

log(hY

(i)(i))

1

= lim log( Pr [Y = y|h(X) = h(i)])

|I| |I |

X,Y

1

= lim

log( Pr [Y = y|h(X) = h(i)])

|I |

|I |

X,Y

xR(h),yL i:Y (i)=y,h(i)=x

=

Pr[X = x, Y = y]  log(Pr[Y = y|h(X) = h(x)])

xR(h),yL

where the first line follows by definition of HScore;14 the second line follows because h is calibrated; the third line rearranges; in the fourth line, the limit converges almost surely from the strong law of large numbers. 15
Similarly, we show that lim|I| SPC(W, ABC(и), CE(и))0 = yL Pr[Y = y]  log(Pr[Y = y).

1

lim
|I |

SPC(W,

ABC(и),

CE(и))0

=

lim
|I |

|I

|

log(AB C (,

W,

i)Y

(i))

1

= lim |I| |I |

log(ABC(, W, i)y)

yL

i:Y (i)=y

= Pr[Y = y]  log(Pr[Y = y)
yL

where the first line follows by definition of SPC;16 the second line rearranges; in the third line, both the limit of the fraction for each label and the limit of ABC converges for all excluded items i simultaneously almost surely from the strong law of large numbers because the deviation of ABC evaluated excluding different items goes to zero.
Finally, putting this together:

lim HScore(h(I), W, CE(и)) - SPC(W, ABC(и), CE(и))0
|I |
= Pr[X = x, Y = y]  log(Pr[Y = y|h(X) = h(x)]) - log(Pr[Y = y])
x,y
=MI(h(X); Y )

Theorem 3 (ABC Maximizes CE score). For any fixed set of raters, any k < |raters|, and any combiner function Comb(и), assuming for all y1, y2, и и и , yk  Lk, lim|I| Comb(y1, y2, и и и , yk, W-i) exists almost surely, the following inequality holds almost surely.

lim SPC(W, ABC(и), CE(и))k  lim sup SPC(W, Comb(и), CE(и))k

|I |

|I |

Proof of Theorem 3 We first show that, assuming that, lim|I| Comb(y1, y2, и и и , yk, W-i) exists, we have, almost surely,

lim SPC(W, Comb(и), CE(и))k
|I |

= lim log(Comb(y1, y2, . . . , yk, W)(yk+1)) |I | iI

=

Pr[Y1 = y1, Y2 = y2, и и и , Yk = yk]

y1,y2,иии ,yk+1

 Pr[Yk+1 = yk+1|Y1 = y1, Y2 = y2, и и и , Yk = yk] log(Comb(y1, y2, и и и , yk)(yk+1)).

(1)

14 Strictly speaking, it should be averaged over all the possible reference raters. Here we are assuming that HScore is just selecting one reference rater, which does not affect the analysis.
15 This can be extended to soft classifiers with continuous outputs using a generalization of the Glivenko-Cantelli Theorem (Gaenssler and Wellner 2006).
16 Again, strictly speaking it should be averaged over all the possible reference raters. Here we are assuming that SPC is just selecting one reference rater, which does not affect the analysis.

The first line follows by definition, and the second line follows from the strong law of large numbers. Applying this twice we get:

lim SPC(W, ABC(и), CE(и))k
|I |

=

Pr[Y1 = y1, Y2 = y2, и и и , Yk = yk]

y1,y2,иии ,yk+1

 Pr[Yk+1 = yk+1|Y1 = y1, Y2 = y2, и и и , Yk = yk] log(ABC(y1, y2, и и и , yk)(yk+1))

=

Pr[Y1 = y1, Y2 = y2, и и и , Yk = yk]

y1,y2,иии ,yk+1

 Pr[Yk+1 = yk+1|Y1 = y1, Y2 = y2, и и и , Yk = yk] log(Pr[Yk+1 = yk+1|Y1 = y1, Y2 = y2, и и и , Yk = yk])



Pr[Y1 = y1, Y2 = y2, и и и , Yk = yk]

(log scoring rule is proper)

y1,y2,иии ,yk+1

 Pr[Yk+1 = yk+1|Y1 = y1, Y2 = y2, и и и , Yk = yk] log(Comb(y1, y2, и и и , yk)(yk+1))

= lim SPC(W, Comb(и), CE(и))k |I |

Where the first line follows from Equation 1, the second from Lemma 2, the third from the properties of the log scoring rule, and the fourth is from applying Equation 1 in the reverse direction.

Corollary 1 (ABC Minimizes Survey Equivalence). For any predictor h, any fixed set of raters and any combiner function Comb(и),

lim SEq(h, I, W; ABC(и), CE(и)))  lim inf SEq(h, I, W; Comb(и), CE(и)))

|I |

|I |

Proof of Corollary 1 From Corollary 2 we have that lim|I| SEq(h, I, W; ABC(и), CE(и))) is strictly increasing. The Survey Equivalence is then the first time that this strictly increasing curve is greater than HScore(h(I), W, CE(и)), which happens at a particular point, c = lim|I| SEq(h, I, W; ABC(и), CE(и))). If c is exactly an integer, then it follows directly from Theorem 3. If c is not an integer, then by Theorem 3 both at the integers before and after c ( c and c respectively), we have that the survey power curve of the ABC combiner is at least that of the other combiner, Comb. Therefore, the linearly interpolated survey power curve value of the ABC combiner is at least that of the other combiner at all points in between c and c . It follows that the first point at which the survey power curve of the ABC combiner reaches HScore(h(I), W, CE(и)) comes before the first point at which the survey power curve of the Comb does.

Appendix C: Running Time Analysis
Theorem 4 (Running time of power curve computation). To compute the survey power curve, when we use the frequency combiner, there exists an implementation such that the total running time is O(K2|I|); when we use Anonymous Bayesian Combiner, there exists an implementation such that the total running time is min{O(K|L||I||L|), O(K|I|2|L|)}.
Proof of Theorem 4 To compute a power curve with confidence intervals around the score for each number of raters k, Algorithm 3 will be run a constant number of times, once for each bootstrap sample of items. For each run and each rater subset, on line 8 we have to compute a score, a total of at most 200  K invocations. All the natural scoring functions such as agreement, F1, Pearson correlation, and cross-entropy, require O(|I|) running time. Thus, the total running time for scoring is O(K|I|). We also have to compute a prediction for each item for each run and rater subset, on line 5 of Algorithm 3. In the implementation, we cache the results of invoking the combiner, so that we never invoke it more than once for each rater subset for each item. Thus, there are at most 200K|I| invocations of the combiner. The total running time, then, will depend on the running time of the combiner function. For example, the running time of the frequency combiner, which just averages k ratings, is O(k). Since we always have k < K, the total running time for computing the power curve with the frequency combiner is O(K 2 |I |). The implementation of the Anonymous Bayesian Combiner is more complicated. In executing Algorithm 6, each invocation of ProbabilityOneItem requires looping through the item's labels in order to compute

counts on line 2, plus O(K) time to compute line 5 using approximations, so the running time is O(K). Each invocation of SumOfProbabilities loops through all the items, invoking ProbabilityOneItem each time for a total running time of O(|I|K). Thus, with an unoptimized implementation of the Anonymous Bayesian Combiner, the total running time for computing a power curve is O(K2|I|2).
In practice, |I| may be large (e.g. 1000 or more). Therefore, the above procedure will be slow. To reduce total running time, we optimize the implementation of the Anonymous Bayesian Combiner through memoization, looking up previously computed values rather than recomputing when an algorithm is invoked with the same inputs.
First, we memoize computed counts wi(l) for items (line 2 of ProbabilityOneItem). The total number of items that need such counts is |I| and thus the total cost of all executions of line 2 is O(|I|K). This is of lower order than the cost of some other operations, so we need not account for it separately. The remaining cost of each invocation of ProbabilityOneItem is only O(|L|). Thus, for each invocation of SumOfProfabilities, the time spent outside of computing or looking up label counts is O(|I||L|).
Second, we also memoize the results of computing ProbabilityOneItem. The reason Algorithm 5 invokes SumOfProbabilities on the entire matrix W and then subtracts out the probability for the excluded item is so that all invocations of SumOfProbabilities will have the same matrix W, rather than a reduced one. The only thing that varies between invocations is the label counts y(l). The maximum number of distinct invocations of SumOfProbabilities, then, is K|L|. |L| may be as small as two (binary labels), and thus the number of distinct invocations may be considerably less than the total number of invocations, which is O(K|I|). With these optimizations, the total running time is O(K|L||I||L|) or O(K|I|2|L|), whichever is smaller.

Appendix D: Analysis of DMI Scorer Lemma 1. For all r > 0, there exists 0 < r  1 such that for all classifiers h,

lim HScorer(h(I), W, DMIScore(и)) = r lim HScore(h(I), s(I), DMIScore(и))

|I |

|I |

We will prove the above lemma by showing that as the number of items goes to infinity, DMIScore of h is a special version of mutual information, Determinant Mutual Information (DMI), between h's output and the reference label. DMI's special multiplicative property induces the above lemma's result.
Definition 2 (DMI (Kong 2020)). Given two random variables X, Y which have the same support C, we define the determinant mutual information between X and Y as

DMI(X; Y ) = | det(UX,Y )|.
where UX,Y is the joint distribution over X and Y , i.e., x, y, UX,Y (x, y) = Pr[X = x, Y = y]. When both X and Y are binary (0 or 1), by simple calculations |DMI(X; Y )| is proportional to the classic
correlation formula |EXY - EXEY |. Proof of Lemma 1 We use yir to denote the plurality vote of r reference raters for item i and yi to denote
the ground truth state of item i. Random variable Y r denotes the plurality vote of r reference raters for a random item. Y  denotes a random item's ground truth state. As the number of items goes to infinity, the frequency matrix approaches to the joint distribution matrix. Thus,

lim
|I |

DMIScore({h(i),

yir

}i

)

=

DMI(h(X

);

Y

r

),

lim
|I |

DMIScore({h(i),

yi

}i)

=

DMI(h(X

);

Y



)

In the uniform noise setting, since conditioning on the ground truth state Y  classifier h's output is independent of the reference raters, we have:

lim HScorer(h(I), W, DMIScore(и)) =DMI(h(X); Y r)
|I |
= det(TY ,Y r )DMI(h(X); Y ) =r lim HScore(h(I), s(I), DMIScore(и))
|I |
where TY ,Y is the transition matrix from the reference label Y to the ground truth state Y . This concludes the proof of the lemma.
The above lemma directly implies the following theorem.

Theorem 5. For all r > 0, for all combiner lim|I| SEq(h, I, s(I), W; Comb(и), DMIScore(и)))  K - r,

functions

Comb,

when

lim SEqr(h, I, W; Comb(и), DMIScore(и))) = lim SEq(h, I, s(I), W; Comb(и), DMIScore(и)))

|I |

|I |

Proof of Theorem 5 The survey power curve can be defined based on either HScore or HScorer.17 Lemma 1 implies that

lim SPCr(W, Comb(и), DMIScore(и)) = r lim SPC(W, Comb(и), DMIScore(и)).

|I |

|I |

The survey equivalence only depends on the ratio of the survey power curve score at each k and the
classifier's score. This ratio remains the same no matter whether we score against the plurality vote of any r
reference raters or against the ground truth state. Therefore, for all r > 0, for all combiner functions Comb, when lim|I| SEq(h, I, s(I), W; Comb(и), DMIScore(и)))  K - r,

lim SEqr(h, I, W; Comb(и), DMIScore(и))) = lim SEq(h, I, s(I), W; Comb(и), DMIScore(и))).

|I |

|I |

Acknowledgments
This material is based upon work supported by the National Science Foundation under Grant Nos. IIS1717688, CCF-1618187, CCF-2007256, and IIS-1652492. Additionally, this material is based upon work supported by the National Natural Science Foundation of China under Grant No. 62002001. The authors would like to thank Maria Glenski and Greg Stoddard for their input during the development of these concepts.
References
Condorcet MJC (1785) Essai sur l'application de l'analyse `a la probabilit┤e des d┤ecisions rendues `a la pluralit┤e des voix (De l'Imprimerie royale).
Dumitrache A, Aroyo L, Welty C (2018) Capturing ambiguity in crowdsourcing frame disambiguation. the Sixth AAAI Conference on Human Computation and Crowdsourcing (HCOMP-18), 12Г20 (Association for the Advancement of Artificial Intelligence).
Erev I, Roth AE, Slonim RL (2007) Learning and equilibrium as useful approximations: Accuracy of prediction on randomly selected constant sum games. Economic Theory 33:29Г51.
Gaenssler P, Wellner JA (2006) Glivenko-cantelli theorems. Encyclopedia of statistical sciences .
Ghosh A, Kumar H, Sastry P (2017) Robust loss functions under label noise for deep neural networks. Thirty-First AAAI Conference on Artificial Intelligence.
Glenski M, Stoddard G, Resnick P, Weninger T (2018) Guessthekarma: A game to assess social rating systems. Proceedings of the ACM on Human-Computer Interaction 2(CSCW):59.
Gordon ML, Zhou K, Patel K, Hashimoto T, Bernstein MS (2021) The disagreement deconvolution: Bringing machine learning performance metrics in line with reality. Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1Г14.
Hayes AF, Krippendorff K (2007) Answering the call for a standard reliability measure for coding data. Communication methods and measures 1(1):77Г89.
Kong Y (2020) Dominantly truthful multi-task peer prediction with a constant number of tasks. Chawla S, ed., Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 2020, 2398Г2411 (SIAM), URL http://dx.doi.org/10.1137/1. 9781611975994.147.
Koren Y, Bell R, Volinsky C (2009) Matrix factorization techniques for recommender systems. Computer 42(8):30Г37.
17 To compute SPC(W, Comb(и), DMIScore(и)) or SPCr(W, Comb(и), DMIScore(и)), substitute calls to compute HScore(h(I), W, Score(и)) or HScorer(h(I), W, Score(и)) into line 8 in Algorithm 3.

Lucey P, Cohn JF, Kanade T, Saragih J, Ambadar Z, Matthews I (2010) The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. 2010 ieee computer society conference on computer vision and pattern recognition-workshops, 94Г101 (IEEE).
Manwani N, Sastry P (2013) Noise tolerance under risk minimization. IEEE transactions on cybernetics 43(3):1146Г1151.
Mitra T, Gilbert E (2015) Credbank: A large-scale social media corpus with associated credibility annotations. ICWSM, 258Г267.
Mitra T, Wright GP, Gilbert E (2017) A parsimonious language model of social media credibility across disparate events. Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing, 126Г145 (ACM).
Natarajan N, Dhillon IS, Ravikumar PK, Tewari A (2013) Learning with noisy labels. Advances in neural information processing systems, 1196Г1204.
Neuendorf KA (2002) The Content Analysis Guidebook (Thousand Oaks, CA: Sage).
Rothschild DM, Wolfers J (2011) Forecasting elections: Voter intentions versus expectations. Available at SSRN 1884644 URL https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1884644.
Salganik MJ, Dodds PS, Watts DJ (2006) Experimental study of inequality and unpredictability in an artificial cultural market. science 311(5762):854Г856.
Shannon CE (1948) A mathematical theory of communication. The Bell system technical journal 27(3):379Г 423.
Sobieraj S, Berry JM (2011) From incivility to outrage: Political discourse in blogs, talk radio, and cable news. Political Communication 28(1):19Г41.
Wulczyn E, Thain N, Dixon L (2017) Ex machina: Personal attacks seen at scale. Proceedings of the 26th International Conference on World Wide Web, 1391Г1399, WWW '17 (Republic and Canton of Geneva, Switzerland: International World Wide Web Conferences Steering Committee), ISBN 978-1-4503-49130, URL http://dx.doi.org/10.1145/3038912.3052591.
Zhou DX, Resnick P, Mei Q (2011) Classifying the political leaning of news articles and users from user votes. Fifth International AAAI Conference on Weblogs and Social Media.

