
# Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions

[arXiv](https://arxiv.org/abs/2106.01098), [PDF](https://arxiv.org/pdf/2106.01098.pdf)

## Authors

- Leslie O'Bray
- Max Horn
- Bastian Rieck
- Karsten Borgwardt

## Abstract

Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to evaluate and compare them. In this paper, we enumerate the desirable criteria for comparison metrics, discuss the development of such metrics, and provide a comparison of their respective expressive power. We perform a systematic evaluation of the main metrics in use today, highlighting some of the challenges and pitfalls researchers inadvertently can run into. We then describe a collection of suitable metrics, give recommendations as to their practical suitability, and analyse their behaviour on synthetically generated perturbed graphs as well as on recently proposed graph generative models.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/evaluation-metrics-for-graph-generative](https://paperswithcode.com/paper/evaluation-metrics-for-graph-generative)

## Bibtex

```tex
@misc{obray2021evaluation,
      title={Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions}, 
      author={Leslie O'Bray and Max Horn and Bastian Rieck and Karsten Borgwardt},
      year={2021},
      eprint={2106.01098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

