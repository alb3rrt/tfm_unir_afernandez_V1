Risk Minimization from Adaptively Collected Data: Guarantees for Supervised and Policy Learning

arXiv:2106.01723v1 [stat.ML] 3 Jun 2021

Aurélien Bibaut Netflix

Antoine Chambaz Université Paris Descartes

Maria Dimakopoulou Netflix

Nathan Kallus Cornell University and Netflix

Mark van der Laan University of California, Berkeley

Abstract
Empirical risk minimization (ERM) is the workhorse of machine learning, whether for classification and regression or for off-policy policy learning, but its modelagnostic guarantees can fail when we use adaptively collected data, such as the result of running a contextual bandit algorithm. We study a generic importance sampling weighted ERM algorithm for using adaptively collected data to minimize the average of a loss function over a hypothesis class and provide first-of-their-kind generalization guarantees and fast convergence rates. Our results are based on a new maximal inequality that carefully leverages the importance sampling structure to obtain rates with the right dependence on the exploration rate in the data. For regression, we provide fast rates that leverage the strong convexity of squared-error loss. For policy learning, we provide rate-optimal regret guarantees that close an open gap in the existing literature whenever exploration decays to zero, as is the case for bandit-collected data. An empirical investigation validates our theory.
1 Introduction
Adaptive experiments, wherein intervention policies are continually updated as in the case of contextual bandit algorithms, offer benefits in learning efficiency and better outcomes for participants in the experiment. They also make the collected data dependent and complicate standard machine learning approaches for model-agnostic risk minimization, such as empirical risk minimization (ERM). Given a loss function and a hypothesis class, ERM seeks the hypothesis that minimizes the sample average loss. This can be used for regression, classification, and even off-policy policy optimization. An extensive literature has shown that, for independent data, ERM enjoys model-agnostic, best-in-class risk guarantees and even fast rates under certain convexity and/or margin assumptions [e.g. 5, 6, 35, among others]. However, these guarantees fail under contextual-bandit-collected data, both because of covariate shift due to using a context-dependent logging policy and because of the policy's dataadaptive evolution as more data are collected. A straightforward and popular approach to deal with the covariate shift is importance sampling (IS) weighting, whereby we weight samples by the inverse of the policy's probability of choosing the observed action. Unfortunately, applying standard maximal inequalities for sequentially dependent data to study guarantees of this leads to poor dependence on these weights, and therefore incorrect rates whenever exploration is decaying and the weights diverge to infinity, as happens when collecting data using a contextual bandit algorithm.
In this paper, we provide a thorough theoretical analysis of IS weighted ERM (ISWERM; pronounced "ice worm") that yields the correct rates on the convergence of excess risk under decaying exploration. To achieve this, we present a novel localized maximal inequality for IS weighted sequential empirical
Alphabetical order
Preprint. Under review.

processes (Section 2) that carefully leverages their IS structure to avoid a bad dependence on the size of IS weights, as compared to applying standard results to an IS weighted process (Remark 2). We then apply this result to obtain generic slow rates for ISWERM for both Donsker-like and nonDonsker-like entropy conditions, as well as fast rates when a variance bound applies (Section 3). We instantiate these results for regression (Section 4) and for policy learning (Section 5), where we can express entropy conditions in terms of the hypothesis class and obtain variance bounds from convexity and margin assumptions. In particular, our results for policy learning close an open gap between existing lower and upper bounds in the literature (Remark 3). We end with an empirical investigation of ISWERM that sheds light on our theory (Section 6).

1.1 Setting
We consider data consisting of T observations, O¯T = (O1, . . . , OT ), where each observation consists of a state, action, and outcome, Ot = (Xt, At, Yt)  O = X × A × Y. The spaces X , A, Y are general measurable spaces, each endowed with a base measure X , A, Y ; in particular, actions can be finite or continuous (e.g., A can be counting or Lebesgue). We assume the data were generated sequentially in a stochastic-contextual-bandit fashion. Specifically, we assume that the distribution of O¯T has a density p(T ) with respect to (wrt) TO = (X × A × Y )T , which can be decomposed as
T
p(T )(o¯T ) = pX (xt)g~t(at | xt, o¯t-1)pY (yt | xt, at),
t=1
where we write o¯t = (x1, a1, . . . , yt), using lower case for dummy values and upper case for random variables. We define gt(a | x) = g~t(a | x, O¯t-1) so that gt represents the random O¯t-1-measurable context-dependent policy that the agent has devised at the beginning of round t, which they then proceed to employ when observing Xt. Remark 1 (Counterfactual interpretation). We can also interpret this data collection from a counterfactual perspective. At the beginning of each round, (Xt, {Yt(a) : a  A}) is drawn from some stationary (i.e., time-independent) distribution P , Xt is revealed, and after acting with a non-anticipatory action At we observe Yt = Yt(At). This corresponds to the above with pX being the marginal of Xt under P  and pY (· | x, a) the conditional distribution of Yt(a) given Xt = x.
1.2 Importance Sampling Weighted Empirical Risk Minimization
Consider a class of hypotheses F, a loss function : F × O  R, and some fixed reference g(a | x), any function, for example, a conditional density. As we will see in Examples 1 to 3 below we will often simply use g(x | a) = 1. Define the population reference risk as
R(f ) = EpX×g×pY [ (f, O)] = (f, (x, a, y))pY (y | x, a)g(a | x)pX (x)dO(x, a, y).

We are interested in finding f with low risk R(f ). We consider doing so using ISWERM, which is ERM where we weight each term by the density ratio between the reference and the policy at time t:

f^T  argmin
f F

R^T (f ) =

1 T

T g(At | Xt) t=1 gt(At | Xt)

(f, Ot)

.

Example 1 (Regression). Consider Y = R, F  [X × A  R], and (f, o) = (y - f (x, a))2. Then f with small R(f ) is good at predicting outcomes from context and action. In particular, for any g, we have that µ(x, a) = ypY (y | x, a)dY (y) solves µ  argminf:X ×AY R(f ). And, we can write R(f ) - R(µ) = EpX×g [(f - µ)2(X, A)].
Consider the counterfactual interpretation in Remark 1. Then R(f ) = EP  [(Y (a) - f (X, a))2]g(x | a)dA(a). For example, if |A| < , A is the counting measure, and g(x | a) = 1, then R(f ) = aA EP  [(Y (a) - f (X, a))2] is the total counterfactual prediction error. Alternatively, if g(a | x) = 1(a = a) and given some H  [X  Y] we let F = {fh(x, a) = h(x) : h  H}, then we have R(fh) = EP  [(Y (a) - h(X))2], that is, the regression risk for predicting the counterfactual outcome Y (a) from X.

2

Example 2 (Classification). In the same setting as Example 1, suppose Y = {±1}. Then µ(x, a) =

2pY (1 R(f )

| x, a)-1. And, if we restrict F  [X ×A  {±1}], being misclassification rate, an unrestricted minimizer

letting

(f, o)

=

1 2

-

1 2

of which is sign(µ(x,

yf (x, a) leads to a)). Focusing on

misclassification of sign(f (x, a)) for F  [X × A  R], we can also use a classification-calibrated

loss [6], such as logistic (f, o) = log(1 + exp(-yf (x, a))), hinge (f, x) = (1 - yf (x, a))+, etc..

Example 3 (Policy learning). Consider Y = R, F  [X × A  R], g(a | x) = 1 and (f, o) = yf (x, a). Then R(f ) = ypY (y | x, a)dY (y)f (a | x)dA(a)pX (x)dX (x) = EpX×f×pY [y] is
the average outcome under a policy f . If we interpret outcomes as costs (or, negative rewards), then seeking to minimize R(f ) means to seek a policy with least risk (or, highest value).

Consider in particular the counterfactual interpretation in Remark 1 with |A| < . Consider deterministic policies: given H  [X  A], let F = {fh(x, a) = 1(h(x) = a) : h  H}. Then we have R(fh) = EP  [Y (h(X))], that is, the average counterfactual outcome.

1.3 Related Literature
Contextual bandits. A rich literature studies how to design adaptive experiments to optimize regret, simple regret, or the chance of identifying best interventions [see 12, 36, and biblioraphies therein]. Such adaptive experiments can significantly improve upon randomized trials (aka A/B tests), which is why they are seeing increased use in practice in a variety of settings, from e-commerce to policymaking [3, 4, 29, 32, 37, 43, 44, 53]. However, while randomized trials produce iid data, adaptive experiments do not, complicating post-experiment analysis, which motivates our current study. Many stochastic contextual bandit algorithms (stochastic meaning the context and response models are stationary, as in our setting) need to tackle learning from adaptively collected data to fit regression estimates of mean reward functions, but for the most part this is based on models such as linear [7, 14, 23, 37] or Hölder class [26, 47, 47], rather than on doing model-agnostic risk minimization and nonparametric learning with general function classes as we do here. Foster and Rakhlin [19] use generic regression models but require online oracles with guarantees for adversarial sequences of data. Simchi-Levi and Xu [50] use offline least-squares ERM but bypass the issue of adaptivity by using epochs of geometrically growing size in each of which data are collected iid. Other stochastic contextual bandit algorithms are based on direct policy learning using ERM [1, 8, 16, 20]; by carefully designing exploration strategies, they obtain good regret rates that are even better than the minimax-optimal guarantees given only the exploration rates, as we obtain (Remark 3).
Inference with adaptive data. A stream of recent literature tackles how to construct confidence intervals after an adaptive experiment. While standard estimators like inverse-propensity weighting (IPW) and doubly robust estimation remain unbiased under adaptive data collection, they may no longer be asymptotically normal making inference difficult. To fix this, Hadad et al. [24] use and generalize a stabilization trick originally developed by Luedtke and van der Laan [38] for a nonadaptive setting with different inferential challenges. Their stabilized estimator, however, only works for data collected by non-contextual bandits. Bibaut et al. [9] extend this to a contextual-bandit setting. Our focus is different from these: risk minimization and guarantees rather than inference.
Policy learning with adaptive data. Zhan et al. [58] study policy learning from contextual-bandit data by optimizing a doubly robust policy value estimator stabilized by a deterministic lower bound on IS weights. They provide regret guarantees for this algorithm based on invoking the results of Rakhlin et al. [45]. However, these guarantees do not match the algorithm-agnostic lower bound they provide whenever the lower bounds on IS weights decay to zero, as they do when data are generated by a bandit algorithm. For example, for an epsilon-greedy bandit algorithm with an exploration rate of t = t-, their lower bound on expected regret is (T -(1-)/2) while their upper bound is O(T -(1/2-)). We close this gap by providing an upper bound of O(T -(1-)/2) for our simpler IS weighted algorithm. See Remark 3. Our results for policy learning also extend to fast rates under margin conditions, non-Donsker-like policy classes, and learning via convex surrogate losses.
IS weighted ERM. The use of IS weighting to deal with covariate shift, including when induced by a covariate-dependent policy, is standard. For estimation of causal effects from observational data this usually takes the form of inverse propensity weighting [28]. The same is often used for ERM for regression [15, 22, 48] and for policy learning [34, 52, 59]. When regressions are plugged into causal effect estimators, weighted regression with weights that depend on IS weights minimize the resulting estimation variance over a hypothesis class [13, 18, 30, 31, 49]. All of these approaches however

3

have been studied in the independent-data setting where historical logging policies do not depend on the same observed data available for training, guarantees under which is precisely our focus herein.
Sequential maximal inequalities. There are essentially two strands in the literature on maximal inequalities for sequential empirical processes. One expresses bounds in terms of sequential bracketing numbers as introduced by van de Geer [55], generalizing of standard bracketing numbers. Another uses sequential covering numbers, introduced by Rakhlin et al. [46]. These are in general not comparable. Foster and Krishnamurthy [20], Zhan et al. [58] use sequential L and Lp covering numbers, respectively, to obtain maximal inequalities. van de Geer [55, Chapter 8] gives guarantees for ERM over nonparametric classes of controlled sequential bracketing entropy. However, applying her generic result as-is to IS weighted processes provides bad dependence on the exploration rate in the case of larger-than-Donsker hypothesis classes (see Remark 2). We also use sequential bracketing numbers, but we develop a new maximal inequality specially for IS weighted sequential empirical processes, where we use the special structure when truncating the chaining to avoid a bad dependence on the size of the IS weights. Equipped with our new maximal inequality, we obtain first-of-their kind guarantees for ISWERM, including fast rates that have not been before derived in adaptive settings.

2 A Maximal Inequality for IS Weighted Sequential Empirical Processes

A key building block for our results is a novel maximal inequality for IS weighted sequential empirical processes. For any sequence of objects (xt)t1, we introduce the shorthand x1:T to denote tehveersyeqtuen[Tce],(xtt)isTt=O¯1t.-W1-emseaaysuthraabt lae,sie.qe.u,eisncseomoferfaunndcotimonvoafriOa¯btl-e1s.1:T is O¯1:T -predictable if, for

IS weighted sequential empirical processes. Let Pg denote the distribution on O with density w.r.t. X ×A ×Y given by pX ×g ×pY and let us use the notation Pgh(o) := h(o)dPg(o). Consider a sequence of F -indexed random processes of the form T := (t(f ))Tt=1 : f  F where, for every f  F , 1:T (f ) is an O¯1:T -predictable sequence of O  R functions. The IS-weighted sequential empirical process induced by T is the F-indexed random process

1 MT (f ) := T

T t=1

gt(At | Xt) gt(At | Xt)

t(f )(Ot) - E

t(f )(Ot) | O¯t-1

1T

g

= T

(Ot - Pgt )

t=1

gt t(f ) .

Sequential bracketing entropy. For any O¯1:T -predictable sequence sequence 1:T of functions

O  R, we introduce the pseudonorm T,g (1:T ) := (T -1

T t=1

t

2 2,g

)1/2

.

We say that a collection of 2N -many O¯1:T -predictable sequences of O  R functions {(k1:T , 1k:T ) : k  [N ]} is an ( , T,g )-sequential bracketing of T , if (a) for every f  F , there exists k  [N ] =
{1, . . . , N } such that kt  t(f )  tk t  [T ] and (b) for every k  [N ], T,g (1k:T - k1:T )  . We denote by N[ ]( , T , T,g ) the minimal cardinality of an ( , T,g )-sequential bracketing of T .

The special case of classes of classes of deterministic functions. Consider the special case t(f ) := (f ), where  := {(f ) : f  F} is a class of functions where for every f  F, (f ) is a deterministic O  R function. Observe that for a fixed function  : O  R, letting t := , we have that T,g (1:T ) =  2,g . Therefore, N[ ]( , T , T,g ), the ( , T,g )-sequential bracketing
number of T , reduces to N[ ]( , , · 2,g ), the usual -bracketing number  in the · 2,g norm.

The maximal inequality. Our maximal inequality will crucially depend on the decay rate of the the IS weights, that is, the exploration rate of the adaptive data collection.

Assumption 1. There exists a deterministic sequence of positive numbers (t) such that, for any

t  1, g/gt   t, almost surely. Define Tavg := T -1

T t=1

t

and

Tmax

:=

maxt[T ]

t.

For example, if the data were collected under an t-greedy contextual bandit algorithm then we have t = -t 1. If we have t = t- for   (0, 1) then Tmax = O(Tavg) = O(T ).
Theorem 1. Consider T := {1:t(f ) : f  F } as defined above. Suppose that Assumption 1 holds, and that there exists B > 0 such that maxt[T ] supfF t(f )   B. In the special case

4

where t(f ) = (f ), 1:T  T , are deterministic functions, we let T := Tavg. Otherwise, in the general case, we let T := Tmax. Let r > 0. Let FT (r) := {f  F : T,g (1:T (f ))  r}. For any r-  [0, r/2], and any x > 0, it holds with probability at least 1 - 2e-x that

sup MT (f )
f FT (r)

r- +

T r T r-

log(1 + N[ ]( , T , T,g ))d

+

TmaxB T

log(1

+

N[ ](

, T , T,g ))

+

T x + BTmaxx .

T

T

Remark 2 (Leveraging IS structure). Theorem 1 is based on a finite-depth adaptive chaining device,

in which we leverage the IS-weighted structure to carefully bound the size of the tip of the chains. In

contrast, applying Theorem 8.13 of van de Geer [55] to the IS weighted sequential empirical process

would lead to suboptimal dependence on t. The crucial point is to work with IS-weighted chains of

the form (g/gt)(f ) = (g/gt){((f ) - uJ,f ) +

J j=0

(uj,f

- uj-1,f ) + u0,f },

where

the

uj,f

are

upper brackets of the unweighted class , at scales 1 > . . . > J (we simplify here a bit the chaining

decomposition for ease of presentation compared with the proof). In adaptive chaining, the tip is bounded by the L1 norm of the corresponding bracket. In our case, denoting lJ,f the lower bracket corresponding to uJ,f , the tip is bounded by Pgt (g/gt)|uJ,f - lJ,f | = Pg |uJ,f - lJ,f |, in which we integrate out the IS ratio, thereby paying no price for it. Applying directly Theorem 8.13 of van de Geer [55], we would be working with a bracketing of the IS weighted class {(g/gt)(f ) : f  F}.
When working with generic L2 brackets of the weighted class, the IS-weighting structure is lost, and
we cannot do better than bounding the L1 of the tip by its L2 norm, which depends on t. Since in
sequential settings, t generally diverges to , an optimal dependence is paramount to obtaining

tight, informative results. Our proof technique otherwise follows the same general outlines as those

of van de Geer [55, Theorem 8.13] and van Handel [57, Theorem A.4] (or, 40, Theorem 6.8 in the iid

setting). Like these, we too leverage an adaptive chaining device, as pioneered by Ossiander [41].

3 Applications to Guarantees for ISWERM

We now return to ISWERM and use Theorem 1 to obtain generic guarantees for ISWERM. We will start with so-called slow rates that give generic generalization results and then present so-called fast rates that will apply in certain settings, where a so-called variance bound is available. Let f1 be a minimizer of the population risk R over F , that is f1  argminfF R(f ).
Assumption 2 (Entropy on (F)). Define (F) := { (f, ·) : f  F }. There exist an envelope function  : O  R of (F), and p > 0 such that, for any > 0,
log N[ ](  2,g , (F ), · 2,g ) -p.

The case p < 2 corresponds to the Donsker case, and p  2 to the (possibly) non-Donsker case. Assumption 3 (Diameters on (F )). There exist b0 > 0 and 0 > 0 such that

supfF (f, ·) - (f1, ·)   b0  2,g ,

supfF (f, ·) - (f1, ·) 2,g  0  2,g .

Theorem 2 (Slow Rates for ISWERM). Suppose Assumptions 1 to 3 hold. Then for any   (0, 1/2), we have that, with probability at least 1 - ,

R(f^T ) - inf R(f )
f F




  2,g ×
 

0

Tavg T

-0 p/2 +

+  Tavg

1 p

T

0

Tavg T

log(1/)

+ b0Tmax
T

-0 p + log(1/)

log(1/)

+

b0 Tmax T

-0 p + log(1/)

p < 2, p > 2.

For p = 2 the bound is similar to the second case but with polylog terms; for brevity we omit the

p = 2 case in this paper. Theorem 2 suggests that the excess risk of ISWERM converges at the rate

of

(Tavg

/T

)

1 p



1 2

.

For

example,

if

Tavg

=

O(T )

and

p

<

2,

we

obtain

O(T

-

1 2

(1-)

).

For



=

0

this

matches the familiar slow rate of iid settings. However, in many cases we can obtain faster rates.

5

Assumption 4 (Variance Bound). For some  > 0, we have

(f, ·) - (f1, ·) 2,g

 2,g

R(f ) - R(f1)  2,g

 2

f  F.

Assumption 5 (Convexity). F is convex and (·, O) is almost surely convex.
Theorem 3 (Fast Rates for ISWERM). Suppose Assumptions 1 to 5 hold with p < 2. Then for any   (0, 1/2), we have that, with probability at least 1 - ,

R(f^T ) - R(f1)

 2,g ×

Tavg T

1 2-+p/2
+

b0Tmax T

1 1+p/2

+

Tavg log(1/)

1 2-

+

b0Tmax log(1/)

T

T

The entropy condition. Assumption 2 assumes an entropy bound on the loss class (F). For many loss functions, we can easily satisfy this condition by assuming an entropy condition on F itself.
Assumption 6 (Entropy on F). There exists p > 0 and an envelope function F of F such that
log N[ ]( F 2,g , F , · 2,g ) -p.
Lemma 1 (Lemma 4 in Bibaut and van der Laan [10]). Suppose that { (·, o) : o  O} is a set of R  R unimodal functions that are equi-Lipschitz. Then Assumption 6 implies Assumption 2.
There are many examples of F for which bracketing entropy conditions are known. The class of -Hölder smooth functions (meaning having derivatives of orders up to b = sup{i  Z : i < } and the b-order derivatives are ( - b)-Hölder continuous) on a compact domain in Rd has p = d/ [56, Corollary 2.7.2]. The class of convex Lipschitz functions on a compact domain in Rd has p = d/2 [56, Corollary 2.7.10]. The class of monotone functions on R has p = 1 [56, Theorem 2.7.5]. If F = {f (o; ) :   }, f (o; ) is Lipschitz in , and   Rd is compact, then any p > 0 holds [56, Theorem 2.7.11]. The class of càdlàg functions [0, 1]d  R with sectional variation norm (aka Hardy-Krause variation) no larger than M > 0 has envelope-scaled bracketing entropy O( -1 |log(1/ )|2(d-1)) [10], so Assumption 6 holds with any p > 1 (or, we can track the log terms). Since trees with bounded output range and finite depth fall in the class of càdlàg functions with bounded sectional variation norm, decision tree classes also satisfy Assumption 6 with any p > 1.

4 Least squares regression using ISWERM

 We now instantiate ISWERM for least squares regression. Consider Y = [- M , M ], for some M > 0, F  [X × A  Y], and (f, o) = (y - f (x, a))2. If F is convex, strongly convex losses such as always yield a variance bound with respect to any population risk minimizer over F (see e.g. lemma 15 in [6]). Let f1  argminfF R(f ) be such a population risk minimizer. We present in the lemma below properties relevant for application of theorems 2 and 3
Lemma 2 (Properties of the square loss.). Consider the setting of the current section. The square loss over F × O satisfies the following variance bound:
 (f, ·) - (f1, ·) 2,g  4 M (R(f ) - R(f1))1/2 f  F ,
and the following Lispchitz property: 
| (f, o) - (f , o)|  M |f (a, x) - f (a, x)| f, f  F, o  O.

Theorem 4 (Least squares regression). Suppose Assumption 1 holds. Suppose Assumption 6 holds for the envelope taken to be constant equal to M , the range of the regression functions. Then for any   (0, 1/2), we have that, with probability at least 1 - ,

R(fT ) - R(f1)


  M×
 

Tmax T

1

+ 1+p/2

Tmax log(1/) T

Tavg T

1

+ + p

Tmax

T

+ Tavg log(1/)
T

Tmax log(1/) T

if p < 2, if p > 2.

6

5 Policy Learning using ISWERM

We next instantiate ISWERM for policy learning. Consider Y = [-M, M ], F  [X × A  R] as in Example 3. Let (f, o) = yf (x, a) and g(a | x) = 1 so that Pg (f, ·) = EpX×f×pY [y] = EpX [ aA f (X, a)µ(X, a)] is exactly the average outcome under a policy f (or, its negative value).
We first give specification-agnostic slow rates, which also close an open gap in the literature.
Theorem 5 (ISWERM Policy Learning: slow rates). Suppose Assumption 1 holds and suppose that Assumption 6 holds withe envelope constant equal to 1 (which is the maximal range of policies). Then for any   (0, 1/2), we have that, with probability at least 1 - ,

R(f^T ) - inf R(f )
f F


 M×


Tavg T

log(1/)

+

Tmax T

log(1/)

Tavg T

1 p
+

Tavg T

log(1/)

+

Tmax T

log(1/)

p < 2, p > 2.

Remark 3 (Comparison to Zhan et al. [58]). Zhan et al. [58] consider a deterministic policy class

(fh(x, a) = 1(h(x) = a)), H  [X  A]) and assume that

1 0

log NH ( 2, H)d <  where

NH is the Hamming covering. This roughly corresponds to our case p  1 if we heuristically

equate bracketing numbers with Hamming covering numbers. While this equality does not hold

and the two complexity measures are simply different, nonetheless, many classes that have finite

Hamming entropy integral also have bracketing entropy with p < 2. This includes, for example,

policy classes given by finite-depth trees and policy classes parametrized by finite-dimensional

parameter. In their setting of a finite Hamming entropy integral, Zhan et al. [58] show a lower bound

of ((TavgT )-1/2) on the expected regret over all policy-learning algorithms and all logging policies satisfying Assumption 1 (see their Theorem 1), that is, (T -(1-)/2) when Tavg = (T ), which matches the upper bound one would obtain under epsilon-greedy exploration with t = t-. At

the same time, the upper bound they provide on the expected regret incurred by their algorithm

is O(TavgT -1/2) (see their Corollary 2.1), that is, O(T -1/2+) when Tavg = O(T ). This is a potentially significant gap in the regret rate when exploration is diminishing,  > 0, as is often the

case with bandit-collected data. In contrast, in the almost-analogous case of p < 2, our Theorem 5

gives the rate O((TavgT )-1/2) on the expected regret of policy learning with ISWERM (given by integrating the tail inequality in Theorem 5), that is, O(T -(1-)/2) when Tavg = O(T ). This matches the rate in the lower bound of Zhan et al. [58] and closes this open gap.

The gap arose from the specific technical route Zhan et al. [58] followed (not their algorithm). For the sake of exposition, we give an explanation of the phenomenon in a non-sequential i.i.d. setting, under stationary logging policy g1, and under our own notation. The same phenomenon translates to the sequential setting. Since they use a symmetrization and covering-based approach, they need to work with uniform covering-type entropies2 of the form supQ log N ( , H, L2(Q)) for a certain class H, where the supremum is over all finitely supported distributions. Their approach amounts to taking H to be the weighted loss class {(g/gt) ()}. While for Q = Pg1 , it holds that (g/g1)( () - ( )) 2,Pg1  11/2 () - ( )| 2,Pg 11/2dH (,  ), for a general Q, the best bound is (g/g1)( () - ( )) 2,Q 1dH (,  ), where dH is the Hamming distance. By contrast, when working with bracketing entropy, one only needs to control the size of brackets in terms of L2(Pg1 ), that is the L2-norm under the distribution of the data Pg1 . This allows to save a
1 factor. Our results also show that a simple IS weighting algorithm suffices to obtain optimal rates, and the stabilization by t employed by Zhan et al. [58], which is inspired by the stabilization employed by Hadad et al. [24], Luedtke and van der Laan [38] for inference purposes, may not be necessary for policy learning purposes. The doubly-robust-style centering may still be beneficial in practice for reducing variance but it does not affect the rate.
Remark 4 (Comparison to [20]). Foster and Krishnamurthy [20], albeit in a slightly different setting, derive a maximal inequality under sequential covering entropy that also exhibits the correct dependence on the exploration rate as ours. This shows in particular that the suboptimal dependence on the exploration rate of Zhan et al. [58] is not a necessary consequence of using sequential covering entropy. Analogously to us, Foster and Krishnamurthy [20] exploit the specific IS-weigthed structure of the loss process, and work with covers of the unweighted policy class directly. Using an L

2See van der Vaart and Wellner [56, Chapter 2.3] for an explanation of why uniform covering entropy is natural for bounding symmetrized Rademacher processes.

7

sequential cover of the unweighted class and using Holder's inequality, they are able to factor out the L1 norm of the IS ratios. This allows them to circumvent the type of sequential cover of the weighted class that Zhan et al. [58] need, and yields optimal  scaling. One caveat of this approach is that the entropy integral in the corresponding bound is expressed in terms of L sequential covering entropy, which makes it hard to obtain fast rates via localization. Indeed, while variance bounds that allow for localization in L2 norm are common, it is in general much harder to obtain localization in L norm.

In well-specified cases, much faster rates of regret convergence are possible. We focus on finitelymany actions, |A| < . Define µ(X) = minaA µ(X, a) and fix a(X) with µ(X, a(X)) = µ (X ). Assumption 7 (Margin). For a constant   [0, ], we have for all u  0,
PrpX minaA\{a(X)} µ(X, a) - µ(X)  M u 1/ u,
where we define 01/ = 0 and x1/ = 1 for x  (0, 1].

This type of margin condition was originally considered in the case of binary classification [39, 54]. The condition we use is more similar to that used in multi-arm contextual bandits [25, 26, 42]. The condition controls the density of the arm gap near zero. It generally holds with  = 1 for sufficiently well-behaved µ and continuous X and with  =  for discrete X [see, e.g., 27, Lemmas 4 and 5].

Lemma 3. Suppose Assumption 7 holds and minfF R(f ) = EpX µ(X). Then Assumption 4 holds for  = /( + 1) and  : o  M .

Theorem 6 (ISWERM Policy Learning: fast rates). Suppose Assumptions 1, 6 and 7 hold with p < 2 and minfF R(f ) = EpX µ(X). Then for any   (0, 1/2), with probability at least 1 - ,

R(f^T ) - EpX µ(X) M

Tavg T

1+ 2+ (1+p/2)
+

Tmax T

1+ 1+ (1+p/2)

+

Tavg log(1/)

1+
2+ + Tmax log(1/)

.

T

T

Remark 5 (Classification using ISWERM). The above results can easily be rephrased for the classification analogue to the regression problem in Section 4, where Y = {±1} and we want a classifier based on features x, a to minimize misclassification error. Because the policy learning problem is both of greater interest and greater generality, we focus our presentation on policy learning.

6 Empirical Study

Next, we empirically investigate various risk minimization approaches using data collected by a contextual bandit algorithm, including both ISWERM and unweighted ERM among others. We take 51 different mutli-calss classification dataset from OpenML-CC18 [11] and transform each into a multi-arm contextual bandit problem [15, 17, 51, following]. We then run an epsilon greedy algorithm for T = 100000, where we explore uniformly with probability t = t-1/3 and otherwise pull the arm that maximizes an estimate of µ(x, a) based on data so far. Details are given in Appendix F.1.

We then consider using this data to regress Yt on Xt, At using different methods where each

observation is weighted by wt using different schemes: (1) Unweighted ERM: wt = 1; (2) ISWERM:

wt = gt-1(At | Xt); (3) ISFloorWERM: wt = t-1, where­inspired by [58]­we weight by the

inverse (nonrandom) floor t = t/|A| of the propensity scores; (4) SqrtISWERM: wt = gt-1/2(At |

Xt), which applies the stabilization of [24, 38] to ISWERM; (5) SqrtISFloorWERM: wt = t-1/2;

(6)

MRDRWERM:

w(t)

=

, 1-gt (At |Xt )
gt2 (At |Xt )

which

are

the weights

used by

Farajtabar et al.

[18];

(7)

MRDRFloorWERM: w(t)

=

, 1-t
t2

which

is

like

MRDRWERM

but

uses

the

propensity

score

floors t. With these sample weights, we run either Ridge regression, LASSO, or CART using

sklearn's RidgeCV(cv=4), LassoCV(cv=4), or DecisionTreeRegressor, each with default

parameters. For Ridge and LASSO we pass as features the intercept-augmented contexts {(1, Xt)}Tt=1 concatenated by the product of the one-hot encoding of arms {At}Tt=1 with the intercept-augmented contexts {(1, Xt)}Tt=1. For CART, we use the concatenation of the contexts {Xt}Tt=1 with the one-hot

8

(a) LASSO outcome model with cross-validated regularization parameter.
(b) Ridge outcome model with cross-validated regularization parameter.
(c) CART outcome model with unrestricted tree depth. Figure 1: Comparison of weighted regression run on contextual-bandit-collected data. Each dot is one of 51 OpenML-CC18 datasets. Lines denote ±1 standard error. Dots are blue when ISWERM is clearly better, red when clearly worse, and black when indistinguishable within one standard error.
encoding of arms {At}Tt=1. To evaluate, we play our bandit anew for T test = 1000 rounds using a uniform exploration policy, g(a | x) = 1/K, and record the mean-squared error (MSE) of the regression fits on this data. We repeat the whole process 64 times and report estimated average MSE and standard error in Fig. 1. Results. Figures 1a and 1b show that ISWERM clearly outperforms unweighted ERM and all other weighted-ERM schemes for linear regression, with ISWERM's advantage being even more pronounced for LASSO. Intuitively, since a linear model is misspecified, this can be attributed to ISWERM's ability to provide agnostic best-in-class risk guarantees. In contrast, for a better specified model such as CART, all ERM methods perform similarly, as seen in Fig. 1c. We highlight that our focus is not necessarily methodological improvements, and the aim of our experiments is to explore the implications of our theory, not provide state-of-the-art results. We provide additional empirical results in Appendix F.2, the conclusions from which are qualitatively the same.
7 Conclusions and Future Work
We provided first-of-their-kind guarantees for risk minimization from adaptively collected data using ISWERM. Most crucially, our guarantees provided good dependence on the size of IS weights leading to correct convergence rates when exploration diminishes with time, as happens when we collect data using a contextual bandit algorithm. This was made possible by a new maximal inequality specifically for IS weighted sequential empirical processes. There are several important avenues for future work. We focused on a fixed hypothesis class. One important next question is how to do effective model selection in adaptive settings. We also focused on IS weighted regression and policy learning, but recent work in the iid setting highlights the benefits of using doubly-robust-style centering [2, 21, 33]. These benefits are most important to avoid rate deterioration when IS weights are estimated, while our IS weights are known, but there are still benefits in reducing the loss variance in the leading constant. Therefore, exploring such methods in adaptive settings is another important next question.
9

8 Societal Impact
Our work provides guarantees for learning from adaptively collected data. While the methods (IS weighting) are standard, our novel guarantees lend credibility to the use of adaptive experiments. Adaptive experiments hold great promise for better, more efficient, and even more ethical experiments. At the same time, adaptive experiments, especially when all arms are always being explored (t < ) even if at vanishing rates (t = (1)), must still be subject to the same ethical guidelines as classic randomized experiments regarding favorable risk-benefit ratio of any arm, informed consent, and other protections of participants. There are also several potential dangers to be aware of in supervised and policy learning generally, such as the training data possibly being unrepresentative of the population to which predictions and policies will be applied leading to potential disparities as well as the focus on average welfare compared to prediction error or policy value on each individual or group. These remain concerns in the adaptive setting, and while ways to tackle these challenges in non-adaptive settings might be applicable in adaptive ones, a rigorous study of applicability requires future work.
References
[1] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, pages 1638­1646. PMLR, 2014.
[2] Susan Athey and Stefan Wager. Policy learning with observational data. Econometrica, 89(1): 133­161, 2021.
[3] Susan Athey, Sarah Baird, Julian Jamison, Craig McIntosh, Berk Özler, and Dohbit Sama. A sequential and adaptive experiment to increase the uptake of long-acting reversible contraceptives in cameroon, 2018. URL http://pubdocs.worldbank.org/en/606341582906195532/ Study-Protocol-Adaptive-experiment-on-FP-counseling-and-uptake-of-MCs. pdf. Study protocol.
[4] Eytan Bakshy, Lili Dworkin, Brian Karrer, Konstantin Kashin, Benjamin Letham, Ashwin Murthy, and Shaun Singh. Ae: A domain-agnostic platform for adaptive experimentation. In Workshop on System for ML, 2018.
[5] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The Annals of Statistics, 33(4):1497­1537, 2005.
[6] Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138­156, 2006.
[7] Hamsa Bastani and Mohsen Bayati. Online decision making with high-dimensional covariates. Operations Research, 68(1):276­294, 2020.
[8] A. F. Bibaut, A. Chambaz, and M. J. van der Laan. Generalized policy elimination: an efficient algorithm for nonparametric contextual bandits, 2020.
[9] Aurelien Bibaut, Maria Dimakopoulou, Nathan Kallus, Antoine Chambaz, and Mark van der Laan. Post-contextual-bandit inference for policy evaluation. 2021.
[10] Aurélien F Bibaut and Mark J van der Laan. Fast rates for empirical risk minimization over càdlàg functions with bounded sectional variation norm. arXiv preprint arXiv:1907.09244, 2019.
[11] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael G Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. arXiv preprint arXiv:1708.03731, 2017.
[12] Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1­122, 2012. ISSN 1935-8237. doi: 10.1561/2200000024.
10

[13] Weihua Cao, Anastasios A Tsiatis, and Marie Davidian. Improving efficiency and robustness of the doubly robust estimator for a population mean with incomplete data. Biometrika, 96(3): 723­734, 2009.
[14] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208­214. JMLR Workshop and Conference Proceedings, 2011.
[15] Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, and Guido Imbens. Estimation considerations in contextual bandits. arXiv preprint arXiv:1711.07077, 2017.
[16] Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. arXiv preprint arXiv:1106.2369, 2011.
[17] Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. Statistical Science, 29(4):485­511, 2014.
[18] Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-policy evaluation. In International Conference on Machine Learning, pages 1447­1456. PMLR, 2018.
[19] Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In International Conference on Machine Learning, pages 3199­3210. PMLR, 2020.
[20] Dylan J Foster and Akshay Krishnamurthy. Contextual bandits with surrogate losses: Margin bounds and efficient algorithms. arXiv preprint arXiv:1806.10745, 2018.
[21] Dylan J Foster and Vasilis Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036, 2019.
[22] David A Freedman and Richard A Berk. Weighting regressions by propensity scores. Evaluation Review, 32(4):392­409, 2008.
[23] Alexander Goldenshluger and Assaf Zeevi. A linear response bandit problem. Stochastic Systems, 3(1):230­261, 2013.
[24] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. arXiv preprint arXiv:1911.02768, 2019.
[25] Yichun Hu, Nathan Kallus, and Xiaojie Mao. Fast rates for contextual linear optimization. arXiv preprint arXiv:2011.03030, 2020.
[26] Yichun Hu, Nathan Kallus, and Xiaojie Mao. Smooth contextual bandits: Bridging the parametric and non-differentiable regret regimes. In Conference on Learning Theory, pages 2007­2010, 2020.
[27] Yichun Hu, Nathan Kallus, and Masatoshi Uehara. Fast rates for the regret of offline reinforcement learning. In Conference on Learning Theory, 2021.
[28] Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015.
[29] Nathan Kallus and Madeleine Udell. Dynamic assortment personalization in high dimensions. Operations Research, 68(4):1020­1037, 2020.
[30] Nathan Kallus and Masatoshi Uehara. Intrinsically efficient, stable, and bounded off-policy evaluation for reinforcement learning. In Advances in neural information processing systems, 2019.
[31] Nathan Kallus, Yuta Saito, and Masatoshi Uehara. Optimal off-policy evaluation from multiple logging policies. In International Conference on Machine Learning, 2021.
11

[32] Maximilian Kasy and Anja Sautmann. Adaptive treatment assignment in experiments for policy choice. Econometrica, 89(1):113­132, 2021.
[33] Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. arXiv preprint arXiv:2004.14497, 2020.
[34] Toru Kitagawa and Aleksey Tetenov. Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86(2):591­616, 2018.
[35] Vladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization. Annals of Statistics, 34(6):2593­2656, 2006.
[36] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
[37] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661­670, 2010.
[38] Alexander R. Luedtke and Mark J. van der Laan. Statistical inference for the mean outcome under a possibly non-unique optimal treatment strategy. The Annals of Statistics, 44(2):713 ­ 742, 2016. doi: 10.1214/15-AOS1384.
[39] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics, 27(6):1808­1829, 1999.
[40] Pascal Massart. Concentration inequalities and model selection. 2007.
[41] Mina Ossiander. A central limit theorem under metric entropy with l_2 bracketing. Annals of probability, 15(3):897­919, 1987.
[42] Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The Annals of Statistics, 41(2):693­721, 2013.
[43] Sheng Qiang and Mohsen Bayati. Dynamic pricing with demand covariates. arXiv preprint arXiv:1604.07463, 2016.
[44] Simon Quinn, Alex Teytelboym, Maximilian Kasy, Grant Gordon, and Stefano Caria. A sequential and adaptive experiment to increase the uptake of long-acting reversible contraceptives in cameroon, 2019. URL https://www.socialscienceregistry.org/trials/3870. Study registration.
[45] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform martingale laws of large numbers. 2013.
[46] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform martingale laws of large numbers. Probability Theory and Related Fields, 161(1-2):111­153, 2015.
[47] Philippe Rigollet and Assaf Zeevi. Nonparametric bandits with covariates. arXiv preprint arXiv:1003.1630, 2010.
[48] James M Robins, Miguel Angel Hernan, and Babette Brumback. Marginal structural models and causal inference in epidemiology, 2000.
[49] Daniel B Rubin and Mark J van der Laan. Empirical efficiency maximization: Improved locally efficient covariate adjustment in randomized experiments and survival analysis. The International Journal of Biostatistics, 4(1), 2008.
[50] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. Available at SSRN, 2020.
[51] Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims. Cab: Continuous adaptive blending for policy evaluation and learning. In International Conference on Machine Learning, pages 6005­6014. PMLR, 2019.
12

[52] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback through counterfactual risk minimization. The Journal of Machine Learning Research, 16(1): 1731­1755, 2015.
[53] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In Mobile Health, pages 495­517. Springer, 2017.
[54] Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics, 32(1):135­166, 2004.
[55] Sara van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.
[56] Aad W van der Vaart and Jon A Wellner. Weak convergence and empirical processes. Springer, 1996.
[57] Ramon van Handel. On the minimal penalty for markov order estimation. Probability theory and related fields, 150(3-4):709­738, 2011.
[58] Ruohan Zhan, Zhimei Ren, Susan Athey, and Zhengyuan Zhou. Policy learning with adaptively collected data. arXiv preprint arXiv:2105.02344, 2021.
[59] Yingqi Zhao, Donglin Zeng, A John Rush, and Michael R Kosorok. Estimating individualized treatment rules using outcome weighted learning. Journal of the American Statistical Association, 107(499):1106­1118, 2012.
13

Supplementary Material for:
Risk Minimization from Adaptively Collected Data: Guarantees for Supervised and Policy Learning

A Proof of the maximal inequality for IS weighted sequential empirical processes

A.1 Preliminary lemmas

For any sequence g1, . . . , gT of conditional densities and any finite sequence 1:T := (t)Tt=1 of O  R functions, let

T,g1:T (1:T ) :=

1T T

t

2 2,gt

t=1

1/2
.

For any conditional density g : (a, x)  A × X  g(a | x), let

T,g(1:T ) := T,g1:T (1:T ),
where we set gt := g for every t  [T ]. Lemma 4. Any T,g1:T as defined above is a pseudonorm over the vector space (O  R)T .

Proof of Lemma 4. It is immediate that for any real number , and finite sequence 1:T of O  R functions T,g1:T (1:T ) = ||T,g1:T (1:T ).
We now check that T,g1:T satisfies the triangle inequality. Let 1(1:T) and 1(2:T) be two sequences of O  R functions. We have that

T,g1:T (1(1:T) + 1(2:T) ) =

1T T
t=1

t(1) + t(2) 2
2,gt

1/2

1T 
T
t=1

2 1/2

t(1)

+ t(2)

2,gt

2,gt



1T T
t=1

t(1) 2
2,gt

1/2
+

1T T
t=1

t(2)
2,gt

1/2

=T,g1:T (1(1:T) ) + T,g1:T (1(2:T) ),

where the second line above follows from the triangle inequality applied to the pseudonorms · 2,gt , t = 1, . . . , T , and where the third line follows from the triangle inequality applied to the Euclidean

norm x  RT  (

T t=1

x2t )1/2.

Lemma 5. Consider g and g1, . . . , gT as defined in the main text. Suppose that assumption 1 holds. Then, for any finite sequence of functions (t)Tt=1  (O  R)T ,

T ,g1:T

g g1:T 1:T



TmaxT,g (1:T ).

If all elements of the sequence t are the same, that is, if there exists  : O  R such that t =  for every t  [T ], then

T ,g1:T

g g1:T 1:T



Tavg  2,g .

14

Proof of Lemma 5. We have that

g

1T

g

2 1/2

T ,g1:T

g1:T 1:T

=

T

Pgt

t=1

gt t

=

1T

T

Pg

t=1

g gt

t2

1/2



1 T

T

tPg t2

1/2
,

t=1

where the inequality follows from Assumption 1. If there exists  : O  R such that t =  for every t = 1, . . . , T , then,

T,g1:T 

1 T

T

tPg t2

1/2

t=1

= Tavg  2,g .

Otherwise, we have

T,g1:T 

1 T

T

tPg t2

1/2

t=1



1

max
t[T ]

t

T

T
Pg t2

1/2

t=1

= TmaxT,g1:T (1:T ).

The following lemma is a restatement under our notation of Corollary A.8 in van Handel [57].
Lemma 6. Let 11:T , . . . , 1N:T be N O¯1:T -predictable sequences of O  R functions, and let A be an O¯T -measurable event. Then, for any r > 0 and any b > 0 such that maxi[N],t[T ] ti   b, it holds that

E

1 max i[N ] T

N
(Ot - Pgt )ti1(T,g1:T (1i:T )  r) | A

t=1

log(1 + N/P [A]) B

r

+ log(1 + N/P [A]).

t

t

A.2 Proof of Theorem 1
Proof of Theorem 1. We treat together both the general case where, for each f , 1:T (f ) is an O¯1:T predictable sequence, and the case where, for every f , there exists a deterministic (f ) : O  R such that t(f ) = (f ) for every t  [T ]. We refer to the former as case 1 and to the latter as case 2 in the rest of the proof. In case 1, we let T := mT ax, and in case 2, we let T := ¯T .
From a conditional expectation bound to a high probability bound. Let x > 0. We introduce the following event:

A := sup MT (f )  (x) ,
f F

15

where

(x) :=C

r- +

T r T r-

log(1 + N[ ]( , T , T,g ))d

+

BTmax T

log(1

+

N[

](r,

T

,

T,g ))

r x + Tmaxx , TT

where C is a universal constant to be discussed further down. Suppose we can show that

1

E

sup MT (f ) | A
f F



log

1+ P [A]

.

Then, we will have that (x)  (log(2/P [A])), that is P [A]  2e-x, which is the wished claim.

Setting up the chaining decomposition. Let 0 := r, and, for every j  0, let j := 02-j. For any j  0, let

Bj := (js,k, tj,k)Tt=1 : k  [Nj ]

be a minimal ( j, T,g )-sequential bracketing of T . For any f  F , let k(j, f )  [Nj] be such that

sj,k(j,f)  t(f )  j,k(j,f) for every t  [T ],

and let jt,f := sj,k(j,f) - js,k(j,f) and uj,f := sj,k(j,f). For any j  0, let N¯j :=

j i=0

Ni.

For

any j  0, and t  [T ] let

aj,t := j

T log(1 + N¯j/P [A])

T . t

Let J  0 such that J+1 < r-  J . The integer J will be the maximal depth of the chains in our chaining decomposition. For any t  [T ], f  F, let

t(f ) := inf j  0 : jt,f > aj,t  J,

be the depth at which we truncate the chains, adaptively depending on the value of jt,f , so that jt,f 1(t(f ) > j) is no larger than aj,t in supremum norm at any depth j. For any f  F and any t  [T ], the following chaining decomposition holds:

J
t(f ) = (t(f ) - uj,f  uj-1,f )1(t(f ) = j)
j=0

tip of the chain

J

+

(uj,f  uj-1,f - uj-1,f )1(t(f ) = j) + (uj,f - uj-1,f )1(t(f ) > j)

j=1

+

u0t ,f

.

root of then chain

links of the chain

Control of the tips.

16

· Case j = J. We have that

1 T

T
(Ot
t=1

-

Pgt

)

g gt

(t(f

)

-

uJt ,f

 utJ-1,f )1(t(f ) = J )

1 
T

T t=1

Pgt

g gt

Jt ,f

1T =
T

Jt ,f 1,g

t=1



1T T

1/2

Jt ,f

2 2,g

t=1

 J.

Therefore

E

sup
f F

1 T

T
(Ot -Pgt
t=1

g gt

)(t(f

)

-

uJt ,f

 utJ-1,f )1(t(f ) = J ) | A

 J.

· Case j < J.

1 T

T
(Ot
t=1

-

Pgt

)

g gt

(t(f

)

-

ujt,f

 ujt-1,f )1(t(f ) = j)

1 
T

T t=1

Pgt

g gt

jt,f 1(t(f )

=

j)

1 
T

T t=1

Pg

(jt,f )2 aj,t



21 jT

T t=1

1 aj,t

=j

log(1 + N¯j/P [A]) T

11

T

T

t
t=1

j

T log(1 + N¯j/P [A]) . T

(The last inequality is an equality in case 2).

Control of the links. We start with bounding the T,g1:T pseudo-norm of the IS weighted links. We have that

T ,g1:T

g gt

(ujt,f



ujt -1,f

-

ujt -1,f

T t=1

T ,g1:T

g gt

(ujt,f

-

ujt -1,f

T t=1

 T T,g uj1,:fT - uj1-:T1,f

 T T,g uj1,:fT - 1:T (f ) + T,g 1:T (f ) - u1j-:T1,f

T j ,
where we have used lemma 5 is the third line and where the fourth line above follows from the triangle inequality.

17

We now bound the supremum norm of the links. For every t  [T ],

(ujt,f  utj-1,f - ujt,f )1(t(f ) = j) =(ujt,f  utj-1,f - t(f ))1(t(f ) = j)
- (utj-1,f - (f ))1(t(f ) = j). Using the definition of t(f ), we obtain

0  (ujt,f  ujt-1,f - t(f ))1(t(f ) = j)  (utj-1,f - t(f ))1(t(f ) = j)  aj-1,t aj,t,

and 0  (ujt-1,f - t(f ))1(t(f ) = j)  aj-1,t aj-1,t.

Therefore,

where

max
t[T ]

g gt

ujt,f  ujt-1,f - utj-1,f

1(t(f ) = j)


taj,t = bj

bj := j

T T log(1 + N¯j/P [A])

Similarly, we have

0  (ujt,f - t(f ))1(t(f ) > j)  aj,t and 0  (utj-1,f - t(f ))1(t(f ) > j)  aj-1,t,

and therefore, for every t  [T ]

g gt

ujt-1,f - utj-1,f

1(t(f ) > j)


taj,t = bj

Denote

vtj,f

:=

g gt

(ujt,f  ujt-1,f - ujt,f )1(t(f ) = j) + (ujt,f - utj-1,f )1(t(f ) > j)

.

Observe that as f varies over F , v1j,:fT varies over a collection of at most Nj × Nj-1  N¯j elements. Therefore, lemma 6 yields

E

sup
f F

1 T

T
(Ot
t=1

-

Pgt

)

g gt

vtj,f

j

T

log(1

+ N¯j/P [A]) T

+

bj T

log(1

+

N¯j/P [A])

j

T

log(1

+

N¯j /P

[A]) .

T

Control of the root. For any f such that T,g ((t(f ))Tt=1)  r, we have that
T,g1:T (((g/gt)u0t ,f )Tt=1)  T T,g (u01,:Tf )  T (T,g (u01:,Tf - 1:T (f )) + T,g (1:T (f )).
Without loss of generality, we can assume that maxt[T ] u0t ,f   B, since thresholding to B preserves the bracketing property. Therefore, maxt[T ] (g/gt)u0t ,f   TmaxB .

Then, from lemma 6,

E sup

1 T

T
(Ot - Pgt )t(f ) : f  F , T,g ((t(f ))Tt=1)  r

t=1

 T log (1 + N¯0 + BTmax log 1 + N¯0

T

P [A]

T

P [A]

18

Adding up the bounds. We obtain

E sup MT (f ) | A
f F

T log (1 + N¯0

B + log

1+

N¯0

T

P [A] T

P [A]

root contribution

+

T J T

j log

1 + N¯j P [A]

j=1

links contribution

+

T J-1 T

j log

1 + N¯j P [A]

+J

j=0

tip contribution

J+

T J T

j log

1 + N¯j P [A]

+ BTmax log 1 + N¯0

T

P [A]

.

j=0

We use the classical technique from finite adaptive chaining proofs to bound the sum in the second term with an integral [see e.g. 8, 57]. We obtain

J

j log

1 + N¯j P [A]

j=0

r

1

r-

log(1 + N[ ]( , T , T,g ))d + log

1+ P [A]

.

Therefore,

E sup MT (f ) | A
f F

r- +

T r T r-

log(1 + N[ ]( , T , T,g ))d

+

BTmax T

log(1

+

N[ ](r,

T

,

T,g ))

+ T log 1 + 1 + BTmax log 1 + 1 .

T

P [A]

T

P [A]

Therefore, for an appropriate choice of the universal constant C in the definition of , we have that

1

E

sup MT (f ) | A
f F



log

1+ P [A]

,

which, from the first paragraph of the proof, implies the wished claim.

B Proof of the excess risk bounds for ISWERM
B.1 Proof of Theorem 2
Proof of Theorem 2. Let
1T MT (f ) := T (Pgt - Ot )( (f ) - (f1)).
t=1
Since RT (fT ) - RT (f1)  0, and from the diameter assumption 3, we have that R(fT ) - R(f1)  sup{MT (f ) : f  F , (f ) - (f1) 2,g  0  2,g }. Therefore, from the diameter assumption (Assumption 3), Theorem 1 yields, via the change of variable r =   2,g , for any x > 0,

19

-  [0, 0/2], that it holds with probability at least 1 - 2e-x that

R(fT ) - R(f1) 

 2,g - +

Tavg 0 T -

log(1 + N[ ](  2,g , (F ),

+

b0Tmax T

log(1

+

N[ ](0



2,g ,

(F ),

· 2,g ))

+ Tavgx + b0Tmaxx .

T

T

· 2,g d

In the case p  (0, 2), setting - = 0 and x = log(1/), and plugging in the entropy assumption
(Assumption 2) immediately yield the claim. In the case p > 2, setting x = log(1/), plugging in the entropy assumption and optimizing the value of - yields the claim.

B.2 Proof of Theorem 3

Proof. From convexity of f  (f, ·) and of F, the following implication holds, for any r > 0: f  F , R(f ) - R(f1)  r2 andRT (f ) - RT (f1)  0
= f  F , R(f ) - R(f1) = r2 andRT (f ) - RT (f1)  0. Let
1T MT (f ) := T (Pgt - Ot )( (f ) - (f1)).
t=1
Let  > 0. Since RT (fT ) - RT (f1)  0, we have that P R(fT ) - R(f1)  2  2,g 2,g

P f  F , R(f ) - R(f1) = 2  2,g and RT (f ) - RT (f1)  0

P [f  F , sup {MT (f ) : f  F , (f ) - (f1) 2,g }  2,g ] ,

where we have used the variance bound in the last line (Assumption 4). From theorem 1 and the loss diameters assumption 3, we have, for any x,  > 0, that it holds with probability at least 1 - 2e-x
that

sup {MT (f ) : f  F , (f ) - (f1) 2,g } T (),

with

T () :=

 2,g

Tavg  T0

log(1 + N[ ](  2,g , (F ), · 2,g ))d

+

b0Tmax T

log(1

+

N[ ](



2,g ,

(F ),

· 2,g )

+  Tavgx + b0Tmaxx .

T

T

Therefore, if  is such that 2  2,g  T (), then with probability at least 1 - 2e-x,

R(fT ) - R(f1) 2  2,g .

We therefore compute an upper bound on T (). From the entropy assumption (2), we have that

T ()  2,g

Tavg (1-p/2) + b0Tmax -p +  Tavgx + b0Tmax x .

T

T

T

T

Therefore, a sufficient condition for 2  2,g  T () is that

2  max

Tavg (1-p/2), b0Tmax -p, 

T

T

Tavgx , b0Tmax x

T

T

20

that is

2  max

Tavg

1 2-+p/2
,

Tavg

1 2-
,

b0Tmax

1 1+p/2

,

b0Tmaxx

,

T

T

T

T

which immediately implies the wished claim.

C Proof of the results on least squares regression using ISWERM

Proof of lemma 2. For any o = (x, a, y)  O, f, f : O  R, we have

| (f )(o) -

(f

)(o)|

=|2y 

-

f (a, x)

-

f

(a, x)||f (a, x)

-

f1(a, x)|

4 M |f (a, x) - f1(a, x)|,

which is the second claim. This inequality further gives that, for any f  F
 (f ) - (f1) 2,g  4 M f - f1 2,g .

We now show that f - f1 2,g  R(f ) - R(f1). Recall the definition of µ: for any (a, x)  A, X ,

µ(a, x) := EpY [Y

| A = a, X = x]. From Pythagoras, R(f ) = E[(Y - µ(A, X))2] +

µ-f

2 2,g

.

For any h1, h2 : A × X  R, denote h1, h2 := EpX,g [h1(A, X)h2(A, X)]. We have that

R(f ) - R(f1) - f - f1 2,g

=

f -µ

2 2,g

-

f1 - µ

2 2,g

-

f - f1

2 2,g

= f - f1, f1 - µ

0,

since f1 is the projection for ·, · of µ onto the convex set F. This yields the first claim.

Proofof Theorem 4. From the definition of the range of the outcome and of the regression functions, o  Mis an envelope for F and o  4M is an envelope for (F). From Lemma 7, and the fact that is 4 M -equiLipschitz w.r.t. its first argument,

 N[ ](4M , (F ), · 2,g ) N[ ]( M , F , · 2,g ) -p,

 where the last inequality follows from the fact that Assumption 6 holds for F with envelope o  M .

Therefore, Assumption 2 holds for envelope  : o  4M . In addition, for this envelop definition,

Assumption 3 holds with 0 = b0 = 1. Finally, from lemma Lemma 2, 
(f ) - (f1) 2,g  4 M (R(f ) - R(f1))1/2

= 2(4M )

R(f ) - R(f1)

1 2
,

4M

that is Assumption 4 holds. Theorem 4 then follows directly by instantiating Theorem 2 and Theorem 3, respectively in the case p > 2 and in the case p  (0, 2), with  : o  4M ,  = 1, b0 = 0 = 1.

D Proof of the results on policy learning using ISWERM

Proof of Theorem 5 and Theorem 6. Note that since the outcome has range [-M, M ], is M equiLipschitz w.r.t. its first argument. Therefore, from Lemma 7 and the fact that F satisfies Assumption 6 with envelope constant equal to 1, Assumption 2 holds with envelope  : o  M .
Furthermore, Assumption 3 holds for b0 = 0 = 1. Therefore, instantiating Theorem 2 with  = M , 0 = b0 = 1 yields Theorem 5.
Under realizability and Assumption 7, Lemma 3, gives us that Assumption 4 holds for  = /( + 1). Theorem 6 follows by instantiating Theorem 6 with  = /( + 1), b0 = 0 = 1.

21

E Technical lemmas

E.1 Long version of lemma 4 in [10]
We restate here under our notation the full version of lemma 4 in [10], of which we gave a short version under the form of lemma 1. Lemma 7 (Lemma 4 in [10], long version). Let : F × O  R. Suppose that there exists
: R × O  R such that
· it holds that f : O  R, o  O, (f, o) = (f (o), o),
· is L-equiLipschitz w.r.t. its first argument, that is,
| (z2, o) - (z1, o)|  L|z1 - z2|, o  O, z1, z2  R

· for every o  O, z  (z, o) is unimodal. Then, for any measure µ on O, any p  1, and > 0, it holds that
N[ ](L , (F ), · µ,p)  N[ ]( , F , · µ,p).

E.2 Proof of the variance bound under margin condition

Proof of Lemma 3. By assumption there exists f1  F such that R(f1) = EpX µ(X). Applying Assumption 7 with u = 0 shows that we necessarily have |argminaA µ(X, a)| = 1 almost surely. Therefore, almost surely, f1(X, a(X)) = 1 and f1(X, a) = 0 for a = a(X).

Now fix any f  F. Given X, let A  A be random variable draw from f (X, ·). We will henceforth

denote expectations and probabilities as wrt (X, A)  pX × f . For brevity we will also denote A = a(X). Note that

(f, ·) -

(f1, ·)

2 2,g



M 2P (A

=

A)

and that



2 2,g

R(f ) - R(f1)  2,g


= M 2(E [µ(X, A) - µ(X, A)] /M )/(+1).

Denoting  = minaA\{a(X)} µ(X, a) - µ(X), Assumption 7 says that for some  > 0 we have P (  u)  (u/M ), where 1 = 1 and x = 0 for x  [0, 1).

Fix u > 0. Then

E [µ(X, A) - µ(X, A)] = E [(µ(X, A) - µ(X, A))1(A = A)]  E [(µ(X, A) - µ(X, A))1(A = A,  > u)]  uP (A = A,  > u) = u (P (A = A) - P (A = A,   u))  u (P (A = A) - P (  u))  u (P (A = A) - (u/M )) .

Set u = (( + 1)/M )-1/ P (A = A)1/ and obtain

E [µ(X, A) - µ(X, A)]  ( + 1)-(+1)/ (/M )-1P (A = A)(+1)/ ,

whence

P (A = A)  -/(+1)( + 1) ((/M )E [µ(X, A) - µ(X, A)])/(+1) .

We conclude that

(f, ·) -

(f1, ·)

2 2,g

as desired.

M 2 (E [µ(X, A) - µ(X, A)] /M )/(+1)

22

F Additional Details and Results for the Empirical Investigation

Here we provide additional details and results for Section 6.

F.1 Contextual Bandit Data from Multi-Class Classification Datasets
To construct our data, we turn K-class classification tasks into a K-armed contextual bandit problems [15, 17, 51], which has the benefits of reproducibility using public datasets and being able to make uncontroversial comparisons using actual ground truth data with counterfactuals. We use the public OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18; BSD 3-Clause license) [11], which has datasets that vary in domain, number of observations, number of classes and number of features. Among these, we select the classification datasets which have less than 60 features. This results in 51 classification datasets from OpenML-CC18 used for evaluation. Table 1 summarizes the characteristics of the 51 OpenML datasets used.

Samples < 1000  1000 and < 10000  10000

Count 16 25 10

Classes =2
> 2 and < 10  10

Count 30 15 6

Features  2 and < 10  10 and < 30  30 and  60

Count 14 22 14

Table 1: Characteristics of the 51 OpenML-CC18 datasets used for evaluation.

Each dataset is a collection of pairs of covariates X and labels L  {1, . . . , K}. We transform each
dataset to the contextual bandit problem as follows. At each round, we draw Xt, Lt uniformly at random with replacement from the dataset. We reveal the context Xt to the agent, and given an arm pull At, we draw and return the reward Yt  N (1{At = Lt}, 1). To generate our data, we set T = 100000 and use the following -greedy procedure. We pull arms uniformly at random until each
arm has been pulled at least once. Then at each subsequent round t, we fit µt-1 using the data up to that time. Specifically, for each a, we take the data {(Xs, Ys) : 1  s  t - 1, As = a} and pass it to a regression algorithm in order to construct µt-1(·, a). In Section 6, we presented results where we use sklearn's LinearRegression to fit µt-1(·, a) (using sklearn defaults). In Appendix F.2, we repeat the experiments where we instead use sklearn's DecisionTreeRegressor (using sklearn defaults). We set A~t(x) = argmaxa=1,...,K µt-1(a, x) and t = t-1/3. We then let gt(a | x) = t/K for a = A~t(x) and gt(A~t(x) | x) = 1 - t + t/K. That is, with probability t we pull a random arm, and otherwise we pull A~t(Xt).

F.2 Additional Results
In Section 6, we presented results where we use a linear-contextual -greedy bandit algorithm to collect the data. Here, we repeat our experiments when the data are instead collected by a treecontextual -greedy bandit algorithm, as described in Appendix F.1 above. The results are shown in Fig. 2. The conclusions are generally the same: ISWERM compares favorably for fitting linear models, while all methods perform similarly for fitting tree models.

F.3 Code and Execution Details
The IPython notebook to reproduce the experimental results of the main paper and the appendix is included as an attachment in the Supplemental Material. One needs to obtain an OpenML API key to run this code (instructions can be found at https://docs.openml.org/Python-guide/) and replace the string 'YOURKEY' in summarize_openmlcc18() and in download_openmlcc18() functions with it. After that, if the notebook is executed as is, it reproduces Figure 1 (38h 26min on a single Intel Xeon machine with 32 physical cores/64 CPUs). Changing variable bandit_model from 'linear' to 'tree' reproduces Figure 2 (56h 45min on a single Intel Xeon machine with 32 physical cores/64 CPUs).

23

(a) LASSO outcome model with cross-validated regularization parameter.
(b) Ridge outcome model with cross-validated regularization parameter.
(c) CART outcome model with unrestricted tree depth. Figure 2: Comparison of weighted regression run on contextual-bandit-collected data. Each dot is one of 51 OpenML-CC18 datasets. Lines denote ±1 standard error. Dots are blue when ISWERM is clearly better, red when clearly worse, and black when indistinguishable within one standard error.
24

