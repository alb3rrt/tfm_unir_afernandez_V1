JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Deep Reinforcement Learning-based UAV Navigation and Control: A Soft Actor-Critic with
Hindsight Experience Replay Approach
Myoung Hoon Lee and Jun Moon

arXiv:2106.01016v2 [eess.SY] 5 Jun 2021

Abstract--In this paper, we propose SACHER (soft actor-critic (SAC) with hindsight experience replay (HER)), which constitutes a class of deep reinforcement learning (DRL) algorithms. SAC is known as an off-policy model-free DRL algorithm based on the maximum entropy framework, which outperforms earlier DRL algorithms in terms of exploration, robustness and learning performance. However, in SAC, maximizing the entropy-augmented objective may degrade the optimality of learning outcomes. HER is known as a sample-efficient replay method that enhances the performance of off-policy DRL algorithms by allowing the agent to learn from both failures and successes. We apply HER to SAC and propose SACHER to improve the learning performance of SAC. More precisely, SACHER achieves the desired optimal outcomes faster and more accurately than SAC, since HER improves the sample efficiency of SAC. We apply SACHER to the navigation and control problem of unmanned aerial vehicles (UAVs), where SACHER generates the optimal navigation path of the UAV under various obstacles in operation. Specifically, we show the effectiveness of SACHER in terms of the tracking error and cumulative reward in UAV operation by comparing them with those of state-of-the-art DRL algorithms, SAC and DDPG. Note that SACHER in UAV navigation and control problems can be applied to arbitrary models of UAVs.
Index Terms--Deep reinforcement learning, soft actor-critic, hindsight experience replay, UAV navigation and control
I. INTRODUCTION
In recent years, problems of navigation and control for unmanned aerial vehicles (UAVs) have been utilized in various applications, such as wildfire monitoring, target tracking and surveillance, and formation and collision avoidance [1]­[3]. Most recent studies on navigation and control of UAVs depend on model accuracy and/or prior knowledge of operation environment. However, identifying accurate model and operation environment information is challenging due to the lack of complete environment information. Deep reinforcement learning (DRL) would be an alternative approach to overcoming such limitations, since DRL does not need the UAV model information, which can be applied to various operation environments [4]­[6]. In addition, the model-based RL algorithms in [7]­[9]
Myoung Hoon Lee is with the Research Institute of Electrical and Computer Engineering, Hanyang University, Seoul 04763, South Korea; email: leemh91@hanyang.ac.kr.
Jun Moon is with the Department of Electrical Engineering, Hanyang University, Seoul 04763, South Korea; email: junmoon@hanyang.ac.kr.
This research was supported by an Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No.2020-0-01373, Artificial Intelligence Graduate School Program (Hanyang University)).

can be applied to control UAVs, provided that the UAV model satisfies the assumptions in [7]­[9].
DRL adopts both deep learning (DL) and reinforcement learning (RL) principles, where deep neural networks in DL are used to approximate Q-functions (or value functions) in RL and the RL agent learns the optimal strategy that maximizes the long-term cumulative rewards. Most of the DRL algorithms can be classified into two categories: on-policy and off-policy methods. On-policy algorithms, such as trust region policy optimization (TRPO) [10] and proximal policy optimization (PPO) [11], attempt to evaluate and improve the policy used to select actions. Off-policy algorithms, such as the deep Qnetwork (DQN) [12] and deep deterministic policy gradient (DDPG) [13], evaluate and then improve policies different from those used to generate the data. It is known that off-policy methods are more sample-efficient compared to on-policy methods, since in off-policy methods, the past experiences performed by any policy can be used for learning [14], [15].
Soft actor-critic (SAC) is a class of off-policy DRL algorithms, which optimizes the stochastic policy based on the maximum entropy framework [16]. SAC can be applied to various environments to achieve the state-of-the-art performance with respect to large continuous state and action spaces. SAC has advantages in terms of exploration and robustness compared with other DRL algorithms. That is, SAC outperforms earlier off-policy and on-policy DRL methods such as DQN and DDPG in terms of learning speed and cumulative reward [16]. Note that SAC and other off-policy DRL algorithms include a technique called experience replay to take advantage of past accumulated experiences [17]. Although SAC with the experience replay can learn various environments with the advantages of exploration and robustness, the maximum entropy framework in SAC may degrade the optimality of learning outcomes after reaching the steady-state phase.
Recently, hindsight experience replay (HER) was proposed in [18] to improve the learning performance of DDPG. Specifically, HER is a sample-efficient replay method that enhances the performance of off-policy DRL algorithms by allowing the agent to learn from both failures and successes, similar to humans. Using the concept of goal, HER provides the supplementary reward to the agent, which improves the optimality of the learning outcomes even if the goal is not achieved. For HER in DDPG, the unshaped (binary or sparse) reward was used. However, this kind of reward is less informative for DRL agents to learn in environment with large continuous states and action spaces, which may decrease efficiency and speed

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

of the learning process. Moreover, although DDPG is able to deal with environments with continuous state and action spaces, it suffers from instability, i.e., it may converge to unstable solutions or diverge, due to the high sensitivity to hyperparameters in DDPG [19].
In this paper, we propose a class of deep reinforcement learning (DRL) algorithms, SACHER, i.e., soft actor-critic (SAC) with hindsight experience replay (HER). As mentioned above, SAC outperforms earlier DRL algorithms including DQN and DDPG in terms of exploration, robustness, and learning performance. However, in SAC, maximizing the entropy-augmented objective function may degrade the optimality of the learning outcomes. We resolve this limitation by proposing SACHER, which improves the learning performance of SAC via HER. More precisely, SACHER achieves the desired optimal outcomes faster and more accurately than SAC, since HER improves the sample efficiency of SAC. Also, SACHER is able to avoid instability in DDPG with HER.
The main distinction between SACHER and SAC is that unlike SAC, SACHER stores a transition tuple of current and next states, action, reward, and initial goal. Next, by HER, SACHER samples additional goals from the states visited in the current episode. Although the transition tuple with the initial goal has a poor reward, SACHER obtains a supplementary reward for each additional goal and then stores the transition tuple of current and next states, the action, the supplementary reward, and the additional goal. By iterating this process, SACHER is able to generate learning outcomes more accurately and faster than SAC.
We apply SACHER to the navigation and control problem of unmanned aerial vehicles (UAVs), where SACHER generates the optimal navigation path for the UAV under various obstacles. In simulation benchmark results, we demonstrate the effectiveness of SACHER in terms of the collision avoidance performance and the cumulative reward in UAV operation by comparing them with state-of-the-art DRL algorithms, SAC and DDPG. Note that SACHER in UAV navigation and control problems can be applied to arbitrary models of UAVs, since SACHER does not require specific information of UAV models and types of controllers.
In summary, the main contributions of the paper can be stated as follows: (a) We apply HER to SAC and propose SACHER to improve
the learning performance of SAC; (b) We apply SACHER to the navigation and control problem
of UAVs under various obstacles. We mention that SACHER cannot be viewed as a trivial application of HER to SAC. Specifically, the main technical challenges of SACHER and its application to the UAV navigation and control problems are as follows:
(i) The goal of HER causes intricacies throughout the entire SAC structure;
(ii) The unshaped (binary or sparse) reward of HER in [18] is not directly applicable to environments with large continuous states and action spaces;
(iii) Various model-based approaches in UAV navigation and control problems may not be able to deal with complex obstacles and operation constraints.

Regarding (i), the overall structure of SAC should be modified to consider the goal of HER. Unlike SAC, the SACHER agent has to select an action by considering not only the state but also the goal. This leads to intricacies while evaluating from policy to Q-function for implementing of SACHER. As for (ii), HER only returns binary or sparse rewards to the SACHER agent, which may not be applicable in environment with large continuous states and action spaces. We address (i) by merging the goal into the state space of SAC. In addition, (ii) is addressed by combining the unshaped reward with auxiliary reward, where the auxiliary reward is shaped as a quadratic function of state and goal. In (iii), the model-based approaches in UAV control and navigation problems are dependent on specific types of obstacles, UAVs, and objective functions. Moreover, it is necessary to compute associated gradients and/or complex Hamilton-Jacobi PDEs to obtain the optimal navigation path under obstacles [20]­[23]. In this paper, we apply SACHER to address (iii). Specifically, by designing an appropriate UAV environment, SACHER is able to generate the optimal navigation path for the UAV to avoid collisions and obstacles regardless of UAV models and/or controllers.
The paper is organized as follows. SACHER is proposed in Section II. The simulation setup and environment design of the SACHER-based UAV navigation and control problem is discussed in Section III. The simulation results are provided in Section IV. We conclude this paper in Section V.
II. SOFT ACTOR-CRITIC ALGORITHM WITH HINDSIGHT EXPERIENCE REPLAY
In this section, we first describe SAC and HER studied in [16] and [18], respectively. Then we propose SACHER and explain its detailed algorithm and implementation.

A. Soft Actor-Critic (SAC) Algorithm
As shown in [16], SAC is a class of the maximum entropy DRL algorithms, which optimizes the following objective function:
T
J () = E(st,at) [r(st, at) + H((·|st))] , (1)
t=0
where rt = r(st, at) is the reward obtained when the SACHER agent executes the action at  A in the state st  S,  is the policy,  is the joint distribution over states and actions induced by the policy ,  is the temperature weight of the entropy term H, and H((·|st)) = -E[log (·|st)] = - A (a|st) log (a|st)da is the entropy. Here, A and S denote action and state spaces, respectively.
The main objective of SAC is to find the optimal policy  that maximizes the entropy-augmented reward function J() in (1), which requires the soft policy iteration of soft Q- and value functions. The soft Q-function satisfies the following soft Bellman equation:

Q(st, at) := r(st, at) + Est+1p [V (st+1)] ,

(2)

where

V (st) := Eat [Q(st, at) -  log (at|st)]

(3)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

is the soft value function, and p = p(st+1|at, st) is the state transition probability, which represents the probability density of the next state st+1  S given the current state st  S and the action at  A. We can evaluate the soft Q value of a fixed policy  by applying the Bellman equation in (2) to each time step, which is the so-called soft policy evaluation in the soft policy iteration. The objective function for the soft policy iteration can be written as follows:

1

J() = Es0p DKL (·|s0) exp  Q(s0, ·)

,

(4)

where DKL denotes the Kullback-Leibler (KL) divergence. We can easily show that the original maximization problem of (1) is equivalent to the minimization of (4) due to the definition of DKL [16]. The minimization of the objective J in (4) with respect to the policy  is called as soft policy improvement in the soft policy iteration. Note that applying directly the above soft policy iteration to large continuous and action spaces requires a certain type of practical approximations [16].
Instead of executing the policy iteration until convergence, parameterized neural networks for the Q-function and the policy are used as function approximators. The soft Q-network is parameterized by , where the parameter for the soft policy network is denoted by . Then the soft Q-function parameters  can be optimized by minimizing the squared soft Bellman residual given by

1

JQ() = E(st,at)D 2 (Q(st, at) - (r(st, at)

(5)

+ Est+1 V¯(st+1)))2 ,

where D denotes the replay buffer, and ¯ is the target Qfunction parameter. It should be noted that the soft value function is also parameterized by the soft Q-function parameter , due to the relation with the Q-function in (3). The policy network, parameterized by , can be learned by minimizing the expected KL divergence in (4):

J() = EstD Eat [ log (at|st) - Q(st, at)] . (6)
Finally, one can minimize the loss functions in (5) and (6) by using the stochastic gradient descent (SGD) method. For SGD, we use two soft Q-networks parameterized by i, i = 1, 2, which are trained independently to optimize the soft bellman residual in (5). The minimum of the two soft Qfunctions is used for SGD to minimize the loss functions in (5) and (6). As mentioned in Section I, since SAC is an off-policy maximum entropy-based algorithm, it has advantages in terms of exploration and robustness. The outstanding performance of SAC is demonstrated in [16]. The results of SAC outperform those of the earlier off-policy and on-policy DRL methods (including both DDPG and PPO) in terms of learning speed and cumulative reward.

B. Hindsight Experience Replay (HER)
The main idea of hindsight experience replay (HER) in [18] is to allow the DRL agents to learn from both failures and

successes, similar to humans. To achieve this, HER employs the concept of a goal g  G used in [24], where g represents the goal (or objective) that the DRL agent has to achieve in the environment and G represents the corresponding goal space. Then the modified reward function rt = r(st, at, g) is defined as a function of not only the state and action, but also the goal. The closer the state st is to the goal g, the greater the reward the DRL agent receives.
The detailed process of HER is as follows. After executing the environment steps in each episode, HER has the knowledge of the visited states  = {s0, s1, . . . , sT }. Based on the knowledge, HER first stores in the replay buffer D every transition tuple (st, at, rt, st+1) together with the original goal g. Then, HER stores extra transition tuples (st, at, rt, st+1) together with g  , where  = {g1, g2, . . . , gm} is a set of the additional goal uniformly sampled from the visited states  in the current episode. From this process, HER gives some supplementary rewards rt = r(st, at, g ) to the DRL agent, although the original goal g is not achieved in that episode and the SACHER agent may get the poor reward. This process enhances the original DRL algorithm in terms of the learning speed and success rate of reaching the goal.
C. SACHER: Soft Actor-Critic Algorithm with Hindsight Experience Replay
In this subsection, we propose SACHER in Algorithm 1 and address (i) in Section I. In detail, SACHER first empties the replay buffer D, and then randomly initializes two soft Q-networks parameterized by 1 and 2 and the soft policy network parameterized by . Moreover, the goal g is initialized from the goal space G. Then SACHER updates two target Q-networks parameterized by ¯1 and ¯2. For each episode, SACHER randomly chooses an initial state s0. Then SACHER interacts with the environment during the environment steps by executing the actions based on the policy  and observing the states  = {s0, s1, . . . , sT }. For each environment step, SACHER obtains the reward rt = r(st, at, g), and stores the transition tuple (st, at, rt, st+1, g) in the replay buffer D, which corresponds to standard experience replay. Next, the additional goals  = {g1, g2, . . . , gm} are uniformly sampled from the set of visited states  = {s0, s1, . . . , sT } in the current episode. For each additional goal g  , SACHER obtains the supplementary reward rt = r(st, at, g ), and then stores the transition tuple (st, at, rt, st+1, g ) in the replay buffer D, which corresponds to HER. Finally, the soft Qnetworks, the soft policy network, and the temperature weight  are optimized based on the SGD method. For each gradient step of SGD, the Q-function parameters 1 and 2, and the policy parameter  are optimized to minimize the loss functions in (5) and (6), respectively. The adjusted temperature weight  is optimized in each gradient step of SGD to minimize the following objective:
J () = Eat - log (at|st) - H¯ ,
where H¯ is desired target entropy. The target Q-function parameters ¯1 and ¯2 are updated by the exponentially moving average method with the smoothing constant  . After the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Algorithm 1 Soft actor-critic with hindsight experience replay (SACHER)

1: Initialize Q-function parameters 1, 2, policy parameters , empty replay buffer D   2: Initialize target Q-function parameters ¯1  1,¯2  2

3: Initialize goal g from the goal space G

4: for each episode do

5: Sample initial state s0

6: for each environment step do

7:

Select action based on the current policy at  (at|st, g)

8:

Execute action at and observe new state st+1  p(st+1|at, st, g)

9: end for

10: for each environment step do

11:

Obtain reward rt = r(st, at, g) from the environment

12:

Store transition tuple (st, at, rt, st+1, g) in the replay buffer D

13:

Sample additional goals  = {g1, g2, . . . , gm} from the states  = {s0, s1, . . . , sT }

14:

for each additional goal g   do

15:

Obtain supplementary reward rt = r(st, at, g ) from the environment

16:

Store transition tuple (st, at, rt, st+1, g ) in the replay buffer D

17:

end for

18: end for

19: Sample a minibatch from the replay buffer D

20: for each gradient step do

21:

Update the Q-function parameters i  i - Q^ i JQ(i) for i  {1, 2}

22:

Update the soft policy parameters    - ^ J()

23:

Adjust temperature weight    - ^ J()

24:

Update the target Q-function parameters ¯i   ¯i + (1 -  )¯i for i  {1, 2}

25: end for

26: end for

Standard experience replay Hindsight experience replay

iteration of the above learning process, SACHER gives the optimized soft Q-function parameters, the soft policy parameter , and the corresponding transition tuples.
The preceding analysis indicates that in SACHER, the concept of the goal space G and the additional storage of the transition tuple (st, at, rt, st+1, g ) are introduced by HER. Therefore, as stated in (i) of Section I, the entire framework of SACHER becomes more complicated than the structure of SAC due to the implementation of goals in HER. Specifically, let S be the state space, A the action space, and G the goal space. Then the corresponding reward r : S × A × G  R is defined as a function of state, action, and goal. In every environment step, the SACHER agent selects the action based on the policy  : S × G  A and then observes the next state st+1 based on the state transition probability p(st+1|at, st, g). The Q-function now depends on the state-action pair together and the goal, i.e., Q(st, at, g) = E[Rt|st, at, g]. These complex procedures cause intricacies while evaluating the policy iteration, the Q-function, as well as the stochastic gradient descent. We resolve this difficulty by merging the goal g into the state st, since the goal is fixed and not changed during the entire learning process.
We now discuss the convergence of SACHER. Note that HER in SACHER does not affect the network structure of SAC. Moreover, HER is related to the concept of the goal space G and the additional storage of the transition tuple (st, at, rt, st+1, g ). One notable modification of SACHER from SAC is the structure of the Markov decision process

(MDP) in SACHER. Specifically, by HER, the MDP of SACHER is represented by the 6-tuple (S, A, R, p, , G), where the goal space G is augmented into the MDP of SAC represented by the 5-tuple (S, A, R, p, ). Then given the goal space, the MDP of SACHER can be modified by the 5-tuple (S¯, A, R, p, ) with S¯ = S × G. This implies that the MDP of SACHER represented by the modified 5-tuple (S¯, A, R, p, ) can be viewed as the MDP of SAC represented by the 5tuple. We now apply the convergence analysis of SAC given in [16, Theorem 1] to SACHER. That is, we can easily modify [16, Theorem 1] to show the convergence of SACHER by representing the MDP of SACHER as the modified 5-tuple (S¯, A, R, p, ). This implies the convergence of SACHER.
III. SIMULATION SETUP AND ENVIRONMENT DESIGN
In this section, we first state the detailed simulation setup for the SACHER-based navigation and control problem of UAVs. Then we design two environments; Environment I is for the case without obstacles and Environment II is for the case with obstacles. Note that these two environment design analysis addresses (ii) in Section I.
A. SACHER-based UAV Navigation and Control
The entire framework of the SACHER-based UAV navigation and control system is described in Fig. 1. In the learning phase, based on the UAV environment design, SACHER learns the corresponding environment and then generates the optimal

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

Figure 1. Framework of SACHER-based UAV navigation and control. In the learning phase, SACHER learns the UAV environment, and then generates the optimal navigation path. After completing the learning phase, the output of SACHER is considered as the reference input for the UAV control system.

navigation path for UAVs via Algorithm 1. After completing the learning phase, the output of SACHER is considered as the reference input for the UAV control system. In the UAV control system, the tracking controller controls the UAV to follow the optimal navigation path generated by SACHER (see Fig. 1).
In our simulations, the tilted-hexarotor UAV model in [25], [26] is used. Moreover, we design the standard backstepping controller (see [27]) for the hexarotor UAV to track the optimal navigation path generated by SACHER. Note that in the learning phase, since SACHER does not require any information of UAV models (e.g., quadrotor, aircraft, or ground vehicles) and types of controllers, any (optimal/nonoptimal) nonlinear (or linear) controllers (with appropriate design modifications) can be used in Fig. 1 instead of the hexarotor UAV and the backstepping controller. We use the hexarotor UAV and the backstepping controller not to show their control performances but to demonstrate the performance of SACHER.

B. Environment I: UAV without Obstacles

We first consider a simple UAV environment, where the UAV lands in a landing area with the shortest path without any obstacles. To design this environment, we use a simple position and angle update equation of the UAV. Specifically, let h = [x, y, z] be the position,  and  be the yaw angle and yaw angular speed, respectively, and  be the yaw torque of the UAV. Then the position and angle update equation of the UAV can be written as follows:

xt+1 = xt + v1 cos(t)t, yt+1 = yt + v1 sin(t)t

zt+1 = zt - v2t

(7)

t+1 = t + tt, t+1 = t + tt,

where v1 and v2 are the positive constant velocities, and t is the sampling time. We note that (7) does not depend on specific types of UAVs and environments.
While interacting with environment, the SACHER agent observes the state s = [x, y, z, , ] , and uses the yaw torque  as an action (a =  ). Note that the main objective of the UAV navigation and control problem is to reach the landing

area on the xy-plane. We define g = [gx, gy] by the center of the landing area, which is the goal for Environments I and II. For simplicity, we assume that the landing area is the origin, i.e., g = [0, 0] (note that g does not need to be the origin). Since the goal g is defined on the xy-plane, whether the goal is achieved or not depends on states x and y updated by (7). The landing area for the UAV environment is defined by a square located at the goal, i.e.,
L = {h  R3 | |x - gx|  lx, |y - gy|  ly, z = 0}, (8)

where lz and ly are the boundary constants. Then the reward function for each time step is defined by

r(st, g) = r1(st, g) + r2(st, g),

(9)

where

r1 = -k1 (xt - gx)2 + (yt - gy)2 - k2(zt)2

(10)

r2 =

c1, 0,

if |xt - gx|  lx, |yt - gy|  ly, otherwise

(11)

with positive constant weights k1 and k2, and constant reward c1. Here, (10) is the auxiliary reward, whereas (11) is the

binary reward, which are used to address (ii) in Section I.

The termination conditions of the UAV environment are set

when (i) z in (7) becomes zero, or (ii) x and y in (7) go into

the landing area L of (8). The update model in (7) continues

to descend with the constant velocity v2 along with z-axis,

regardless of the action of the SACHER agent. Therefore,

under the environment of (7), (9), and the above termination

condition, the SACHER agent seeks to find the optimal path

of the UAV reaching the landing area by maximizing the

cumulative reward R =

T t=0

rt.

Note

that

the

cumulative

reward R is maximized when the second termination condition

is satisfied with the fastest terminal time T , which provides

the optimal navigation path for the UAV landing operation.

As stated in (ii) of Section I, the approach used in HER

only uses the binary reward as r2 in (11), which is not

appropriate in environment with large continuous state and

action spaces. With (11), SACHER could not obtain any

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

information before reaching the goal. In this case, even if the supplementary reward in HER exists, it is difficult for the agent to know the proper learning direction needed to achieve the goal. Therefore, we use the composite reward r in (9), which includes the auxiliary reward in (10), where (10) is shaped as a quadratic function of state and goal. In (11), although the binary reward returns the zero reward when the goal is not achieved, the auxiliary reward provides another reward as a guideline of SACHER, indicating the degree of goal achievement. By using the composite reward in (9), we can prevent SACHER from learning through the wrong direction and reduce the number of episodes required for learning.
Remark 1: In the learning phase of Environment I, to generate the optimal navigation path by SACHER, we apply the following learning processes based on Algorithm 1:

(s.1) With g = [gx, gy] , the SACHER agent observes the state s = [x, y, z, , ] from Environment I;

(s.2) Given the action a =  of SACHER, the next state st+1 is updated based on (7);

(s.3) The SACHER agent receives the reward rt in (9) from Environment I ((s.1)-(s.3) show the collection of the state

transition tuple (st, at, rt, st+1, g));

(s.4) After the storage of the state transition tuple

(st, at, rt, st+1, g) in the replay buffer D, SACHER
uniformly samples m additional goals  = {gi = (gx,i, gy,i), i = 1, . . . , m} from the visited x and y states {(xt, yt), t = 0, . . . , T }1,2;

(s.5) SACHER obtains the supplementary regard rt for each additional goal g   from Environment I (using (9)),

and stores the transition tuple (st, at, rt, st+1, g ) in the replay buffer D;

(s.6) Under terminal conditions (i) and (ii), the SACHER

agent obtains the optimal policy that maximizes the

cumulative reward R =

T t=0

rt

based

on

experiences

stored in the replay buffer D.

Note that (s.1)-(s.6) are repeated for sufficient large learning (training) episodes.3 After completing the learning phase, the output of SACHER is the reference input of the SACHERbased UAV navigation and control system (see Fig. 1).

C. Environment II: UAV with Obstacles
We consider a more complex UAV operation, where the UAV tries to avoid obstacles while tracking the optimal navigation path reaching the landing area generated by SACHER.
A cylindrical obstacle Oi, 1  i  N, is represented by the following zero-sublevel set:
Oi := {h  R3 | (x - xo,i)2 + (y - yo,i)2 - ro2,i  0}, (12)

Table I ENVIRONMENT PARAMETERS

Parameter
v1: velocity on xy-plane v2: velocity for z-axis t: sampling time t: yaw angle torque lx, ly: boundary constants k1, k2: weight constants c1: reward constant c2, c3: penalty constants

Value 2 0.5 0.1 [-0.5, 0.5] 0.2, 0.2 10-3, 10-4 10 10, 0.2

Table II SACHER HYPERPARAMETERS

Hyperparameter
optimizer for SGD learning rate for optimizer Q, ,  discount factor  number of hidden layers number of hidden units minibatch size target smoothing coefficient  activation function
replay buffer capacity number of additional goals (HER) m target entropy H¯

Value
Adam [28] 3 × 10-4 0.99 2 256 256 0.005 ReLU 106 4 -dim(A) = -1

where ro,i is the radius of the obstacle, and (xo,i, yo,i) is its location on the xy-plane. The reward for the combined UAV operation of landing and obstacle avoidance is defined by

N

r¯(st, g) = r(st, g) + pi(st),

(13)

i=1

where

pi =

-c2, 0,

if (xt - xo,i)2 + (yt - yo,i)2 - ro2,i  c3, otherwise

with the penalty constants c2 and c3. Note that in (13), r

is given in (9), which can address (ii) stated in Section I as

discussed in Section III-B. The penalty constant c2 decreases

the cumulative reward R¯ =

T t=0

r¯t

when

the

UAV

collides

with the obstacles in (12), and c3 acts as a margin that

prevents the UAV from colliding with the obstacles. Under the

environment of (7), (12), (13), and the termination conditions

of Environment I, the SACHER agent seeks to find the

optimal path reaching the landing area without collisions by maximizing the cumulative reward R¯.

We note that the implementation of SACHER for Environ-

ment II is identical with that of Environment I in Remark 1.

That is, the SACHER agent in Environment II follows the

same learning phase as Environment I to generate the optimal

navigation path under obstacles, where the reward in (13) and

the cumulative reward R¯ =

T t=0

r¯t

have

to

be

used

instead

of (9) and R =

T t=0

rt

in

Remark

1.

1In the implementation of HER in (s.4) and (s.5), we set m = 4 for the number of the sampled additional goals (see Table II).
2As mentioned above (8), since the goal g is defined on the xy-plane, the goal achievement depends on states x and y.
3The number of learning episodes is dependent on the problem setup.

IV. SIMULATION RESULTS
We provide the simulation results for navigation and control of the hexarotor UAV based on SACHER, where the detailed simulation setup is described in Section III and Fig. 1. For

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

SAC DDPG SACHER

Table III LEARNING TIME (H) FOR SIMULATIONS

Ant-v2
6.22h 7.54h 6.11h

Walker2d-v2
4.81h 5.61h 4.75h

Hopper-v2
5.08h 6.12h 5.05h

HalfCheetah-v2
4.63h 5.59h 4.53h

Env. I
1.85h 2.59h 1.71h

Env. II
1.93h 2.80h 1.91h

Figure 2. Benchmark results of SAC, DDPG and SACHER for the MuJoCo physics engine environment. The blue-colored line is the reward curve of SACHER, the yellow-colored line is the reward curve of SAC, and the purplecolored line is the reward curve of DDPG. In each plot, the solid curve represents the average reward, and the shaded area represents the minimum and maximum rewards in the four learning trials.
the comparison and validation of the learning performance of SACHER, we provide the simulation results of SAC [16] and DDPG [13] under the same simulation environments.
The simulations are performed via Python 3.6 on the Windows 10 operating system with Intel Core i9-10900X CPU and 64 GB RAM. For both Environments I and II, the initial state of the hexarotor UAV in the environment is fixed as s0 = [x0, y0, z0, 0, 0] = [20, 20, 10, 5/4, 0] and the goal is fixed as g = [gx, gy] = [0, 0] (origin on xy-plane).
The values of the parameters in (7)-(11) and (13) are given in Table I. The values of SACHER hyperparameters are provided in Table II. In Environments I and II, HER selects 4 additional goals (m = 4) from the visited states in the current episode, and then stores the transition tuples with the corresponding supplementary rewards (see Remark 1). For Environment II, we consider the case when there exist N = 9 obstacles, Oi, i = 1, 2, . . . , 9, where their locations (xo,i, yo,i) in (12) are {(5, 5), (5, 10), (5, 15), (10, 5), . . . , (15, 10), (15, 15)} with the same radius of ro,i = 1.
Table III provides the learning time of SAC [16], DDPG [13] and SACHER for MuJoCo physics engine (see Fig. 2) and Environments I-II (see Figs. 3 and 5). From Table III, we can see that SAC and SACHER show faster learning time than that of DDPG. In Table III, although SACHER and SAC show the similar learning time, SACHER provides the better

learning performance than SAC as seen from Figs. 2-7.
Fig. 2 shows the cumulative reward of evaluation episodes for SAC, DDPG and SACHER during the learning (training) phase in several robotics environments implemented in OpenAI Gym [29], which use the MuJoCo [30] physics engine. Each algorithm is trained for 10, 000 episodes with four different instances, and each episode carries out 1, 000 environmental steps. The solid curve corresponds to the mean, and the shaded area corresponds to the minimum and maximum rewards in four trials. Fig. 2 shows that in overall, SACHER outperforms SAC and DDPG in terms of the learning speed and performance. As for the learning speed, SACHER reaches steady-state faster than SAC and DDPG in Ant-v2, Hopperv2 and HalfCheetah-v2. Regrading the learning performance, SACHER achieves the higher average rewards than those of SAC and DDPG for every episode in all environments.
The simulation results of Environment I are shown in Figs. 3 and 4. The learning curves of Environment I with SAC(left), DDPG(middle) and SACHER(right) are shown in Fig. 3. The brown-colored solid line represents the cumulative reward R in each episode, and the yellow-colored dotted line shows the average of the last 20 rewards in each episode. From Fig. 3(left and middle), although the learning process reaches steadystate by SAC and DDPG, the final position of the navigation paths generated by SAC and DDPG are rarely included in the landing area. During 2, 000 episodes, SAC provides only 32 successful paths, and DDPG provides only 15 successful paths reaching the landing area. On the other hand, in Fig. 3(right), the navigation path generated by SACHER frequently reaches the landing area after the steady-state learning process. During 2, 000 episodes, SACHER provides 667 successful paths reaching the landing area.
The simulation results of navigation and tracking control of the hexarotor UAV with SACHER for Environment I are shown in Fig. 4. The (red-colored) dotted line is the ideal path without obstacles (obtained from the straightforward computation), for which the straightforward computation yields the cumulative reward R = -28.7854. The (bluecolored) solid line indicates the path of the hexarotor UAV that follows the optimal path generated by SACHER via the backstepping controller (see Fig. 1). From Fig. 4, we can see that SACHER generates the almost ideal path reaching the landing area, where the corresponding computed cumulative reward is R  -29 that coincides with the ideal cumulative reward above (R = -28.7854).
The simulation results of Environment II are shown in Figs. 5 and 7. The learning curves of Environment II with SAC(left), DDPG(middle) and SACHER(right) are shown in Fig. 5. The brown-colored solid line represents the cumulative reward R¯ for each episode, and the yellow-colored dotted line shows the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

reward reward reward

-20 -40 -60 -80 -100 -120 -140 -160 -180
0

Environment I with SAC

-20 -25 -30 -35 -40 -45
500

reward average reward

1000

1500

2000

500

1000

1500

Number of episodes

2000

-20 -40 -60 -80 -100 -120 -140 -160 -180

Environment I with DDPG

reward average reward

-20

-25

-30

-35

-40

-45

1000

1500

2000

500

1000

1500

Number of episodes

2000

0 -50 -100 -150 -200 -250 -300
0

Environment I with SACHER

-20 -25 -30 -35 -40 -45
500

reward average reward

1000

1500

2000

500

1000

1500

Number of episodes

2000

Figure 3. Learning curves of SAC (left), DDPG (middle) and SACHER (right) in Environment I: The (brown-colored) solid line is the cumulative reward and the (yellow-colored) dotted line is the average of the last 20 rewards. The subfigure illustrates the cumulative reward for each episode after steady-state.

Figure 4. The path of the hexarotor UAV with SACHER for Environment I. The path on the xy-plane is shown in the subfigure. The hexarotor UAV follows the optimal path generated by SACHER, which coincides with the ideal path without obstacles (obtained from the straightforward computation).

are 9 cylindrical obstacles, the turquoise-colored dashed line is the path of the hexarotor UAV that follows the navigation path generated by SAC, the green-colored dotted line is the path of the hexarotor UAV that follows the navigation path generated by DDPG, and the blue-colored solid line is the path of the hexarotor UAV that follows the optimal navigation path generated by SACHER. The navigation paths of SAC, DDPG and SACHER are the outcomes generated after the steady-state of the learning process. We observe that although the path of the hexarotor UAV with SAC and DDPG avoids the obstacles, the hexarotor UAV could not reach the landing area. Note that the path of the hexarotor UAV with SACHER successfully reaches the landing area while avoiding the obstacles. From our simulation results, we conclude that SACHER provides a more reliable path than SAC and DDPG with the cumulative reward of R¯  -31 after completing the learning phase.
The simulation results of navigation and tracking control for the hexarotor UAV with SACHER for Environments II-A and II-B are shown in Figs. 8 and 9, respectively. Note that Environments II-A and II-B are variations of Environment II, where

average of last 20 rewards in each episode. From Fig. 5(left and middle), although the learning process reaches steadystate by SAC and DDPG, the final position of the navigation paths generated by SAC and DDPG are rarely included in the landing area. During 2, 000 episodes, SAC provides only 33 successful paths, and DDPG provides only 13 successful paths reaching the landing area. On the other hand, in Fig. 5(right), the navigation path generated by SACHER frequently reaches the landing area after the steady-state learning process. During 2, 000 episodes, SACHER provides 568 successful paths reaching the landing area.
The simulation results of navigation and tracking control of the hexarotor UAV with SACHER for Environment II are shown in Fig. 7. The paths of position and yaw angle, and their tracking errors are demonstrated in Fig. 6. The (redcolored) dotted line is the optimal navigation path generated by SACHER. The (blue-colored) solid line is the path of the hexarotor UAV controlled by the backstepping controller. From Fig. 6, it can be seen that the haxarotor UAV follows the optimal navigation path generated by SACHER with negligible tracking errors. In Fig. 7, the red-colored shapes

· Environment II-A modifies the velocity v1 = 8 instead of v1 = 2 in Environment II;
· Environment II-B considers N = 16 obstacles instead of N = 9 in Environment II.
From Figs. 8 and 9, we can see that under the different settings in Environments II-A and II-B, the hexarotor UAV with SACHER successfully reaches the landing area while avoiding the obstacles.
Based on the above variations of Environment II, we may conclude that for any different environment settings, SACHER is able to generate the optimal navigation path for UAVs via Algorithm 1. Hence, SACHER can be applied to any operations of UAVs with an appropriate environment design.
Remark 2: It should be mentioned that the learning curves in Figs. 3 and 5 show the reward, which do not affect the stability of the hexarotor UAV. Specifically, even if the reward is low in the learning curves in Figs. 3 and 5, the trajectory of the hexarotor UAV generated by SACHER is smooth. However, in this case, the hexarotor UAV cannot reach the landing area.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

reward reward reward

0 -50 -100 -150 -200 -250 -300
0

Environment II with SAC

-30
-35
-40
-45 500

reward average reward

1000

1500

2000

500

1000

1500

Number of episodes

2000

0 -50 -100 -150 -200 -250 -300

Environment II with DDPG

-30
-40
-50
-60 1000

reward average reward

1500

2000

500

1000

1500

Number of episodes

2000

-20 -40 -60 -80 -100 -120 -140 -160 -180 -200
0

Environment II with SACHER

-30
-35
-40
-45 500

reward average reward

1000

1500

2000

500

1000

1500

Number of episodes

2000

Figure 5. Learning curves of SAC (left), DDPG (middle) and SACHER (right) in Environment II. The (brown-colored) solid line is the cumulative reward and the (yellow-colored) dotted line is the average of the last 20 rewards. The subfigure illustrates the cumulative reward for each episode after steady-state.

x-axis [m]

20

20

15

15

y-axis [m]

10

10

5

5

0

0

0

50

100 150 200

0

50

100 150 200

time [sec]

time [sec]

10

0

SACHER

hexarotor UAV -1

yaw [rad]

5

-2

z-axis [m]

0

-3

0

50

100 150 200

0

50

100 150 200

time [sec]

time [sec]

error [m]

0.1 0
-0.1 -0.2
0
0 -0.02 -0.04 -0.06
0

x-axis position

50

100 150 200

time [sec]

z-axis position

50

100 150 200

time [sec]

error [rad]

error [m]

0.2 0.1
0 -0.1 -0.2
0
0.04 0.02
0 -0.02 -0.04
0

y-axis position

50

100 150 200

time [sec]

yaw angle

50

100 150 200

time [sec]

y-axis

20

obstacle

15 10 landing area

5

0

-5

-10

-5

0

5

10

x-axis

SAC DDPG SACHER

15

20

error [m]

Figure 6. The paths and tracking errors of the hexarotor UAV using the backstepping controller for Environment II. The (red-colored) dotted line is the optimal navigation path provided by SACHER, and the (blue-colored) solid line is the path of the hexarotor UAV controlled by the backstepping controller.

Figure 7. The paths of the hexarotor UAV with SAC, DDPG and SACHER for Environment II. The turquoise-colored dashed line is the path of the hexarotor UAV with SAC, the green-colored dotted line is the path of the hexarotor UAV with DDPG, and the blue-colored solid line is the path of the hexarotor UAV with SACHER. The result shows that the hexarotor UAV with SACHER successfully reaches the landing area, while the others do not.

V. CONCLUSIONS

In this paper, we have proposed a class of deep reinforcement learning algorithms, SACHER. In SACHER, HER improves the sample efficiency by allowing SAC to learn from both failures and successes when trying to achieve the goal. We

have shown that SACHER achieves the desired optimal outcomes faster and more accurately than SAC. Our SACHER has been applied to the navigation and control problem of UAVs to generate the optimal navigation path for the UAV. Note

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

y-axis
y-axis

20 18 16 14 12 10
8 6 4 2 0 -2
-2 0 2 4 6 8 10 12 14 16 18 20 x-axis

20 15 10
5 0
0

5

10

15

20

25

x-axis

Figure 8. Environment II-A: The paths of the hexarotor UAV with SACHER Figure 9. Environment II-B: The paths of the hexarotor UAV with SACHER

under 9 obstacles when v2 = 8.

under 16 obstacles.

that unlike the existing model-based approaches, SACHER in UAV navigation and control problems can be applied arbitrary models of UAVs, i.e., SACHER does not require specific information of UAVs. The effectiveness of SACHER has been validated through simulations, which include comparisons with state-of-the-art DRL algorithms, SAC and DDPG. One possible future work of this paper is to extend SACHER to the partially-observed case and apply it to the navigation and control problem of UAVs.
REFERENCES
[1] R. Wang and J. Liu, "Trajectory tracking control of a 6-DOF quadrotor UAV with input saturation via backstepping," Journal of the Franklin Institute, vol. 355, pp. 3288­3309, 2018.
[2] N. H. Motlagh, M. Bagaa, and T. Taleb, "UAV-based IoT platform: A crowd surveillance use case," IEEE Communications Magazine, vol. 55, no. 2, pp. 128­134, 2017.
[3] C. K. Verginis, Z. Xu, and D. V. Dimarogonas, "Decentralized motion planning with collision avoidance for a team of UAVs under high level goals," in 2017 IEEE International Conference on Robotics and Automation, 2017, pp. 781­787.
[4] L. Salt, D. Howard, G. Indiveri, and Y. Sandamirskaya, "Parameter optimization and learning in a spiking neural network for UAV obstacle avoidance targeting neuromorphic processors," IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 9, pp. 3305­3318, 2020.

[5] S. Shao, M. Chen, and Y. Zhang, "Adaptive discrete-time flight control using disturbance observer and neural networks," IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 12, pp. 3708­3721, 2019.
[6] H. Li, Q. Zhang, and D. Zhao, "Deep reinforcement learning-based automatic exploration for navigation in unknown environment," IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 6, pp. 2064­2076, 2020.
[7] A. Perrusqu´ia and W. Yu, "Discrete-time H2 neural control using reinforcement learning," IEEE Transactions on Neural Networks and Learning Systems, 2020, accepted (10.1109/TNNLS.2020.3026010).
[8] M. Radac and T. Lala, "Robust control of unknown observable nonlinear systems solved as a zero-sum game," IEEE Access, vol. 8, pp. 214 153­ 214 165, 2020.
[9] W. He, H. Gao, C. Zhou, C. Yang, and Z. Li, "Reinforcement learning control of a flexible two-link manipulator: An experimental investigation," IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2020, accepted (10.1109/TSMC.2020.2975232).
[10] J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel, "Trust region policy optimization," in Proceedings of the 32nd International Conference on Machine Learning, vol. 37, 2015, p. 1889­1897, https://arxiv.org/abs/1502.05477.
[11] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," 2017, https://arxiv.org/abs/1707.06347.
[12] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, "Playing Atari with deep reinforcement learning," 2013, https://arxiv.org/abs/1312.5602.
[13] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

learning," in 4th International Conference on Learning Representations, 2016, https://arxiv.org/abs/1509.02971. [14] H. Seijen, H. Hasselt, S. Whiten, and W. Marco, "A theoretical and empirical analysis of expected Sarsa," in IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, 2009. [15] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018. [16] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, and S. Levine, "Soft actor-critic algorithms and applications," 2018, https://arxiv.org/abs/1812.05905. [17] L.-J. Lin, "Reinforcement learning for robots using neural networks," Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, Tech. Rep., 1993. [18] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba, "Hindsight experience replay," in Advances in Neural Information Processing Systems, vol. 30, 2017, https://arxiv.org/abs/1707.01495. [19] G. Matheron, N. Perrin, and O. Sigaud, "The problem with DDPG: understanding failures in deterministic environments with sparse rewards," 2019, https://arxiv.org/abs/1911.11679. [20] S. Sundar and Z. Shiller, "Optimal obstacle avoidance based on the Hamilton-Jacobi-Bellman equation," IEEE Transactions on Robotics and Automation, vol. 13, no. 2, pp. 305­310, 1997. [21] A. Altarovici, O. Bokanowski, and H. Zidani, "A general HamiltonJacobi framework for non-linear state-constrained control problems," ESAIM: Control, Optimisation and Calculus of Variations, vol. 19, no. 2, pp. 337­357, 2013. [22] J. F. Fisac, M. Chen, C. J. Tomlin, and S. S. Sastry, "Reach-avoid problems with time-varying dynamics, targets and constraints," in Proceedings of the 18th International Conference on Hybrid Systems: Computation and Control, 2015, pp. 11­20. [23] N. Wang, Y. Gao, H. Zhao, and C. K. Ahn, "Reinforcement learningbased optimal tracking control of an unknown unmanned surface vehicle," IEEE Transactions on Neural Networks and Learning Systems, 2020, accepted (10.1109/TNNLS.2020.3009214). [24] T. Schaul, D. Horgan, K. Gregor, and D. Silver, "Universal value function approximators," in Proceedings of the 32nd International Conference on Machine Learning, vol. 37, 2015, pp. 1312­1320. [25] M. H. Lee, P. Nguyen, and J. Moon, "Leader-follower decentralized optimal control for large population hexarotors with tilted propellers: A Stackelberg game approach," Journal of the Franklin Institute, vol. 356, no. 12, pp. 6175­6207, 2019. [26] S. Rajappa, M. Ryll, H. Bu¨lthoff, and A. Franchi, "Modeling, control and design optimization for a fully-actuated hexarotor aerial vehicle with tilted propellers," in IEEE International Conference on Robotics and Automation, 2015, pp. 4006­4013. [27] A. Isidori, Nonlinear control systems. Springer Science & Business Media, 2013. [28] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in 3rd International Conference on Learning Representations, 2015, https://arxiv.org/abs/1412.6980. [29] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, "OpenAI Gym," 2016, https://arxiv.org/abs/1606.01540. [30] E. Todorov, T. Erez, and Y. Tassa, "MuJoCo: A physics engine for model-based control," in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012, pp. 5026­5033.

