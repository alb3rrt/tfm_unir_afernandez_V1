arXiv:2106.01860v1 [eess.IV] 3 Jun 2021

Noisy Labels are Treasure: Mean-Teacher-Assisted Confident Learning for
Hepatic Vessel Segmentation
Zhe Xu1,3 , Donghuan Lu2, Yixin Wang4, Jie Luo3, Jayender Jagadeesan3, Kai Ma2, Yefeng Zheng2, and Xiu Li1
1 Shenzhen International Graduate School, Tsinghua University, China li.xiu@sz.tsinghua.edu.cn 2 Tencent Jarvis Lab, China caleblu@tencent.com
3 Brigham and Women's Hospital, Harvard Medical School, USA 4 Institute of Computing Technology, Chinese Academy of Sciences, China
Abstract. Manually segmenting the hepatic vessels from Computer Tomography (CT) is far more expertise-demanding and laborious than other structures due to the low-contrast and complex morphology of vessels, resulting in the extreme lack of high-quality labeled data. Without sufficient high-quality annotations, the usual data-driven learningbased approaches struggle with deficient training. On the other hand, directly introducing additional data with low-quality annotations may confuse the network, leading to undesirable performance degradation. To address this issue, we propose a novel mean-teacher-assisted confident learning framework to robustly exploit the noisy labeled data for the challenging hepatic vessel segmentation task. Specifically, with the adapted confident learning assisted by a third party, i.e., the weightaveraged teacher model, the noisy labels in the additional low-quality dataset can be transformed from `encumbrance' to `treasure' via progressive pixel-wise soft-correction, thus providing productive guidance. Extensive experiments using two public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component.
Keywords: Hepatic Vessel · Noisy Label · Confident Learning.
1 Introduction
Segmenting hepatic vessels from Computer Tomography (CT) is essential to many hepatic surgeries such as liver resection and transplantation. Benefiting from a large amount of high-quality (HQ) pixel-wise labeled data, deep learning has greatly advanced in automatic abdominal segmentation for various structures, such as liver, kidney and spleen [9,16,13,5]. Unfortunately, due to the noises
This work was done at Tencent Jarvis Lab.

(a) Set A: 3DIRCADb

2

Z. Xu et al.

(b) Set B: MSD8

(a) Set-HQ: 3DIRCADb

(b) Set-LQ: MSD8

Fig. 1. 2D and 3D visualization of the processed example cases of (a) 3DIRCADb dataset [1] with high-quality annotations (Set-HQ), and (b) MSD8 dataset [21] with numerous mislabeled and unlabeled pixels (Set-LQ). Red represents the labeled vessels, while the yellow arrows at (b) point at some unlabeled pixels.

in CT images, pathological variations, poor-contrast and complex morphology of vessels, manually delineating the hepatic vessels is far more expertise-demanding, laborious and error-prone than other structures. Thus, limited amount of data with HQ pixel-wise hepatic vessel annotations, as exampled in Fig. 1(a), is available. Most data, as exampled in Fig. 1(b), have considerable unlabeled or mislabeled pixels, also known as "noises".
For a typical fully-supervised segmentation method, training with tiny HQ labeled dataset often results in overfitting and inferior performance. However, additionally introducing data with low-quality (LQ) annotation may provide undesirable guidance, and offset the efficacy of the HQ labeled data. Experimentally, the considerable noises become the `encumbrance' for training, leading to substantial performance degradation, as shown in Fig. 3 and Table 1. Therefore, how to robustly exploit the additional information in the abundant LQ noisy labeled data remains an open challenge.
Related Work. Due to the lack of HQ labeled data and the complex morphology, few efforts have been made on hepatic vessel segmentation. Huang et al. applied the U-Net with a new variant Dice loss to balance the foreground (vessel) and background (liver) classes [8]. Kitrungrotsakul et al. used three deep networks to extract the vessel features from different planes of hepatic CT images [11]. Neglecting the data with LQ annotation because of their potential misleading guidance, only 10 and 5 HQ labeled volumes were used for training in [8] and [11], resulting in unsatisfactory performance. To introduce auxiliary image information from additional dataset, Semi-Supervised Learning (SSL) technique [22,4,24] is a promising method. However, the standard SSL-based methods fail to exploit the potential useful information of the noisy label. To make full use of the LQ labeled data, several efforts have been made to alleviate the negative effects brought by the noisy labels, such as assigning lower weights to the noisy labeled samples [28,18], modeling the label corrupting process [7] and confident learning [17]. However, these studies focused on image-level noise identification, while the localization of pixel-wise noises is necessary for the segmentation task.
In this paper, we propose a novel Mean-Teacher-assisted Confident Learning (MTCL) framework for hepatic vessel segmentation to leverage the additional

Mean-Teacher-Assisted Confident Learning

3

`cumbrous' noisy labels in LQ labeled data. Specifically, our framework shares the same architecture as the mean-teacher model [22]. By encouraging consistent segmentation under different perturbations for the same input, the network can additionally exploit the image information of the LQ labeled data. Then, assisted by the weight-averaged teacher model, we adapt the Confident Learning (CL) technique [17], which was initially proposed for removing noisy labels in imagelevel classification, to characterize the pixel-wise label noises based on the Classification Noise Process (CNP) assumption [3]. With the guidance of the identified noise map, the proposed Smoothly Self-Denoising Module (SSDM) progressively transforms the LQ labels from `encumbrance' to `treasure', allowing the network to robustly leverage the additional noisy labels towards superior segmentation performance. We conduct extensive experiments on two public datasets with hepatic vessel annotations [1,21]. The results demonstrate the superiority of the proposed framework as well as the effectiveness of each component.

2 Methods
The detailed explanation of the experimental materials, the hepatic CT preprocessing approach, and the proposed Mean-Teacher-assisted Confident Learning (MTCL) framework are presented in the following three sections, respectively.

2.1 Materials
Two public datasets, 3DIRCADb [1] and MSD8 [21], with obviously different qualities of annotation (shown in Fig. 1) are used in this study, tersely referred as Set-HQ (i.e., high quality) and Set-LQ (i.e., low quality), respectively. 1) Set-HQ: 3DIRCADb [1]. The first dataset, 3DIRCADb, maintained by the French Institute of Digestive Cancer Treatment, serves as Set-HQ. It only consists of 20 contrast-enhanced CT hepatic scans with high-quality liver and vessel annotation. In this dataset, different volumes share the same axial slice size (512 × 512 pixels), while the pixel spacing varies from 0.57 to 0.87 mm, the slice thickness varies from 1 to 4 mm, and the slice number is between 74 and 260. 2) Set-LQ: MSD8 [21]. The second dataset MSD8 provides 443 CT hepatic scans collected from Memorial Sloan Kettering Cancer Center, serving as the SetLQ. The properties of the CT scans are similar to that of the 3DIRCADb dataset but with low-quality annotations. According to the statistics [15], around 65.5% of the vessel pixels are unlabeled and approximately 8.5% are mislabeled as vessels for this dataset, resulting in the necessity of laborious manual refinement in previous work [15].
In our experiments, the images in Set-HQ are randomly divided into two groups: 10 cases for training, and the remaining 10 cases for testing, while all the samples in Set-LQ are only used for training since their original low-quality noisy labels are not appropriate for unbiased evaluation [15].

Idea4: A Self-denoised Weakly and Semisupervised Framework for Robust Hepatic Vessel Segmentation Under Noisy Labels

ROI (liver) Extraction Intensity
Normalization Vessel Enhancement

4

Z. Xu et al.

Limited HQ Labeled Data (Set-HQ)

Img

Prob Map

... Student



Student Set-HQ Pred

Smoothly Denoised Label


Student Set-LQ Pred


SSDM

Robust Pre-process
EMA Self-loop Update

Prob Map Img



...

Abundant LQ Noisy Labeled Data (Set-LQ)

CL: Confident Learning SSDM: Smoothly Self-Denoising Module

Teacher

Teacher Set-HQ Pred

Identified Noise Map

 CL
Teacher Set-LQ Pred

Set-LQ Noisy Label

Fig. 2. Illustration of the proposed Mean-Teacher-assisted Confident Learning (MTCL) framework for hepatic vessel segmentation.

2.2 Hepatic CT Preprocessing
A standard preprocessing strategy is firstly applied to all the CT images: (1) the images are masked and cropped to the liver region based on the liver segmentation masks. Note that for the MSD8 dataset, the liver masks are obtained with the trained H-DenseUNet model [13] because no manual annotation of the liver is provided. All the cropped images are adjusted to 320 × 320 × D, where D denotes the slice number. Since the slice thickness varies greatly, we do not perform any resampling to avoid the potential artifacts [23] caused by the interpolation; (2) The intensity of each pixel is truncated to the range of [-100, 250] HU, followed by Min-Max normalization.
However, we observe that many cases have different intensity ranges (shown in Fig. 1) and intrinsic image noises [6], which could drive the model to be oversensitive to the high-intensity regions, as demonstrated in Table 1 and Fig. 3. Therefore, the vessel probability map based on the Sato tubeness filter [20] is introduced to provide auxiliary information. By calculating the Hessian matrix's eigenvectors, the similarity of the image to tubes can be obtained, so that the potential vessel regions can be enhanced with high probability (illustrated in Fig. 2). Following the input-level fusion strategy used in other multimodal segmentation tasks [27], we regard the vessel probability map as an auxiliary modality and directly concatenate it with the processed CT images in the original input space. By jointly considering the information in both the images and the probability maps, the network could perceive more robust vessel signals towards better segmentation performance (demonstrated in Table 1 and Fig. 3).

Mean-Teacher-Assisted Confident Learning

5

2.3 Mean-Teacher-assisted Confident Learning Framework
Learn from Images of Set-LQ. To additionally exploit the image information of Set-LQ, the mean-teacher model (MT) [22] is adopted as our basic architecture with the backbone network U-Net [19], as shown in Fig. 2. Denoting the weights of the student model at training step t as t, Exponential Moving Average (EMA) is applied to update the teacher model's weights t, formulated as t = t-1 + (1 - )t, where  is the EMA decay rate and set to 0.99 as recommended by [22]. By encouraging the teacher model's temporal ensemble prediction to be consistent with that of the student model under different perturbations (e.g., adding random noise  to the input samples) for the same inputs, superior prediction performance can be achieved as demonstrated in previous studies [22,25,24]. As shown in Fig. 2, the student model is optimized by minimizing the supervised loss Ls on Set-HQ, along with the (unsupervised) consistency loss Lc between predictions of the student model and the teacher model on both datasets.

Learn from Progressively Self-Denoised Soft Labels of Set-LQ. The

above MT model can only leverage the image information, while the potential

useful information of the noisy labels is still unexploited. To further leverage

the LQ annotation without being affected by the label noises, we propose a

progressive self-denoising process to alleviate the potential misleading guidance.

Inspired by the arbitration based manual annotation procedure where a third

party, e.g., the radiologists, is consulted for disputed cases, the teacher model

serves as the `third party' here to provide guidance for identifying label noises.

With its assistance, we adapt the Confident Learning [17], which was initially

proposed for pruning mislabeled samples in image-level classification, to charac-

terize the pixel-wise label noises based on the Classification Noise Process (CNP)

assumption [3]. The self-denoising process can be formulated as follows:

(1) Characterize the pixel-wise label errors via adapted CL. First, we estimate

the joint distribution Qy~,y between the noisy (observed) labels y~ and the true (latent) labels y. Given a dataset X := (x, y~)n consisting of n samples of x

with m-class noisy label y~, the out-of-sample predicted probabilities P^ can be

obtained via the `third party', i.e., our teacher model. Ideally, such a third party

is also jointly enhanced during training. If the sample x with label y~ = i has large

enough p^j(x)  tj, the true latent label y of x can be suspected to be j instead

of i. Here, the threshold tj is obtained by calculating the average (expected)

predicted probabilities p^j(x) of the samples labeled with y~ = j, which can be

formulated as

tj

:=

1 |Xy~=j |

xXy~=j p^j(x). Based on the predicted label, we

further introduce the confusion matrix Cy~,y , where Cy~,y [i][j] is the number of

x labeled as i (y~ = i), yet the true latent label may be j (y = j). Formally,

Cy~,y can be defined as:

Cy~,y [i][j] := X^ y~=i,y=j , where
(1) X^ y~=i,y=j := x  Xy~=i : p^j (x)  tj , j = arg max p^l(x) .
lM :p^l(x)tl

6

Z. Xu et al.

With the constructed confusion matrix Cy~,y , we can further estimate the m × m joint distribution matrix Qy~,y for p (y~, y):

Qy~,y [i][j] =

· |X | Cy~,y [i][j]
jM Cy~,y [i][j]

y~=i

.

iM,jM

· |X | Cy~,y [i][j]
jM Cy~,y [i][j]

y~=i

(2)

Then, we utilize the Prune by Class (PBC) [17] method recommended by [26] to identify the label noises. Specifically, for each class i  M , PBC selects the n ·
jM:j=i (Qy~,y [i][j]) samples with the lowest self-confidence p^(y~ = i; x  Xi) as the wrong-labeled samples, thereby obtaining the binary noise identification map Xn, where "1" denotes that the pixel has a wrong label and vice versa. It is worth noting that the adapted CL module is computationally efficient and does not require any extra hyper-parameters.
(2) Smoothly refine the noisy labels of Set-LQ to provide rewarding supervision. Experimentally, the CL still has uncertainties in distinguishing the label noises. Therefore, instead of directly imposing the hard-correction, we introduce the Smoothly Self-Denoising Module (SSDM) to impose a soft correction [2] on the given noisy segmentation masks y~. Based on the binary noise identification map Xn, the smoothly self-denoising operation can be formulated as follows:

y(x) = y~(x) + I(x  Xn) · (-1)y~ · ,

(3)

where I(·) is the indicator function, and   [0, 1] is the smooth factor, which is empirically set as 0.8. After that, the updated soft-corrected LQ labels of Set-LQ are used as the auxiliary CL guidance Lcl to the student model.
(3) Self-loop updating. With the proposed SSDM, we construct a self-loop updating process that substitutes the noisy labels of Set-LQ with the updated denoised ones for the next training epoch, so that the framework can progressively refine the noisy vessel labels during training.

2.4 Loss Function

The total loss is a weighted combination of the supervised loss Ls on Set-HQ, the perturbation consistency loss Lc on both datasets and the auxiliary self-denoised CL loss Lcl on Set-LQ, calculated by:

L = Ls + cLc + clLcl,

(4)

where c and cl are the trade-off weights for Lc and Lcl, respectively. We adopt the time-dependent Gaussian function [4] to schedule the ramp-up weight Lc. Meanwhile, the teacher model needs to be "warmed up" to provide reliable outof-sample predicted probabilities. Therefore, cl is set as 0 in the first 4,000 iterations, and adjusted to 0.5 during the rest training iterations. Note that the supervised loss Ls is a combination of cross-entropy loss, Dice loss, focal loss [14] and boundary loss [10] with weights of 0.5, 0.5, 1 and 0.5, respectively, as such a combination can provide better performance in our exploratory experiments

Mean-Teacher-Assisted Confident Learning

7

ResultTs:aSbelegm1.eQnutaanttiiotantivReerseusultlsts of different methods. Best results are shown in bold.

Method
Huang et al.[8] U-Net(i) U-Net(p) U-Net(c)
U-Net(c, Mix) MT(c)
MT(c)+NL w/o CL MTCL(c) w/o SSDM
MTCL(c)

Dice 
0.5991 0.6093 0.6082 0.6685 0.6338 0.6963 0.6807 0.7046 0.7245

PRE 
0.6352 0.5601 0.5553 0.6699 0.6322 0.6931 0.7270 0.7472 0.7570

ASD 
2.5477 2.7209 2.3574 2.0463 1.6040 1.4860 1.3205 1.2252 1.1718

HD 
10.5088 10.3103 10.2864 9.2078 9.2038 7.5912 8.0893 8.3667 7.2111

Img

Prob Map

GT

U-Net(i)

U-Net(p)

U-Net(c)

U-Net(c, Mix)

MTCL(c) (ours)

Fig. 3. Visualization of the fused segmentation results of different methods. The red voxels represent the ground truth, while the green voxels denote the difference between the ground truth and the segmented vessel of different methods.

with the fully supervised baseline method. The consistency loss Lc is calculated by the voxel-wise Mean Squared Error (MSE), and the CL loss Lcl is composed of cross-entropy loss and focal loss with equal weights.

3 Experiments and Results
Evaluation Metrics and Implementation. For inference, the student model segments each volume slice-by-slice and the segmentation of each slice is concatenated back into 3D volume. Then, a post-processing step that removes very small regions (less than 0.1% of the volume size) is performed. We adopt four metrics for a comprehensive evaluation, including Dice score, Precision (PRE), Average Surface Distance (ASD) and Hausdorff Distance (HD). The framework is based on the PyTorch implementation of [25] using an NVIDIA Titan X GPU. SGD optimizer is also adopted and the batch size is set to 4. Standard data augmentation, including randomly flipping and rotating, is applied. The implementation will be available at https://github.com/lemoshu/MTCL.
Comparison Study. A comprehensive qualitative and quantitative comparison study is performed on the hold-out test set of Set-HQ, as shown in Fig. 3 and Table 1. Succinctly, "i", "p" and "c" represent different input types: processed image, the vessel probability map and the concatenated one, respectively.
Surprisingly, the performance of 3D networks is far worse than the 2D ones in our experiments, which may result from inadequate training data or the thick-

8

Z. Xu et al.

ness variation [23]. Therefore, all the rest experiments are performed in 2D. The exploratory fully supervised experiments are performed on the Set-HQ. We can observe that using the concatenated slices as input (U-Net(c)) achieves superior performance. Next, we additionally introduce the Set-LQ to train the model in the fully supervised manner, denoted as U-Net(c, Mix). As predicted, the noisy labels of Set-LQ cause unavoidable performance degradation. Compared with U-Net(c) with only Set-HQ, U-Net(c, Mix)'s Dice score and PRE drop from 0.6685 to 0.6338, and from 0.6699 to 0.6322, respectively. Note that the previous learning-based studies [8,11,12] on hepatic vessel segmentation performed the evaluation on manually refined annotation without making the improved `ground truth' or their implementation publicly available, resulting in excessive lack of benchmark in this field. Here, we re-implement Huang et al.'s approach [8] in 2D as another baseline. The proposed method, denoted as MTCL(c), achieves the best performance in terms of all four metrics and more appealing visual results.
Ablation Study. To verify the effectiveness of each component, we perform an ablation study with the following variants: a) MT(c): a typical mean-teacher model that additionally uses the image information of Set-LQ, i.e., the SSL setting; b) MT(c)+NL w/o CL: extended MT(c) by leveraging the noisy labels (NL) of Set-LQ without CL; c) MTCL(c) w/o SSDM: MTCL without the proposed SSDM. As shown in Table 1, with the assistance of image information of Set-LQ, adding the perturbation consistency loss can improve the segmentation
: Effect of Self-correctepderfLoarmbaenlce, as well as alleviate the performance degradation caused by noisy
labels. Superior performance can be achieved through the self-denoising process via the adapted CL, and further improved by the SSDM.

Effectiveness of Label Self-denoising. The visualization of two example slices from MSD8 is shown in Fig. 4 to further illustrate the label self-denoising process. Some noticeable noises can be identified with the proposed framework. Moreover, an additional experiment, which uses the denoised label of Set-LQ along with Set-HQ to train a U-Net (same setting as U-Net(c, Mix)), is performed and obtains 7.67%, 8.46%, 0.61% and 3.91% improvement in terms of Dice, PRE, ASD and HD, respectively, compared to the one using the original label of Set-LQ. The extended experiment further demonstrates the capability of the proposed framework in correcting the label errors, indicating a potential application of our framework to explicably refine the label quality of large datasets by taking advantage of limited HQ labeled data for many other tasks.

Slice A

Noisy Label Identified Noise Denoised Label

Slice B

Noisy Label Identified Noise Denoised Label

Fig. 4. Illustration of the self-denoising performance for the MSD8 dataset (Set-LQ).

Mean-Teacher-Assisted Confident Learning

9

4 Conclusion

In this work, we proposed a novel Mean-Teacher-assisted Confident Learning (MTCL) framework for the challenging hepatic vessel segmentation task with a limited amount of high-quality labeled data and abundant low-quality noisy labeled data. The superior performance we achieved using two public datasets demonstrated the effectiveness of the proposed framework. Furthermore, the additional experiment with refined annotation showed that the proposed framework could improve the annotation quality of noisy labeled data with only a small amount of high-quality labeled data.

5 Acknowledgements
This project was partly supported by the National Natural Science Foundation of China (Grant No. 41876098), the National Key R&D Program of China (Grant No. 2020AAA0108303), and Shenzhen Science and Technology Project (Grant No. JCYJ20200109143041798).

References
1. 3DIRCADb Dataset, https://www.ircad.fr/research/3d-ircadb-01/ 2. Ainam, J.P., Qin, K., Liu, G., Luo, G.: Sparse label smoothing regularization for
person re-identification. IEEE Access 7, 27899­27910 (2019) 3. Angluin, D., Laird, P.: Learning from noisy examples. Machine Learning 2(4),
343­370 (1988) 4. Cui, W., Liu, Y., Li, Y., Guo, M., Li, Y., Li, X., Wang, T., Zeng, X., Ye, C.:
Semi-supervised brain lesion segmentation with an adapted mean teacher model. In: International Conference on Information Processing in Medical Imaging. pp. 554­565. Springer (2019) 5. Dou, Q., Chen, H., Jin, Y., Yu, L., Qin, J., Heng, P.A.: 3D deeply supervised network for automatic liver segmentation from CT volumes. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 149­157. Springer (2016) 6. Duan, X., Wang, J., Leng, S., Schmidt, B., Allmendinger, T., Grant, K., Flohr, T., McCollough, C.H.: Electronic noise in CT detectors: impact on image noise and artifacts. American Journal of Roentgenology 201(4), W626­W632 (2013) 7. Goldberger, J., Ben-Reuven, E.: Training deep neural-networks using a noise adaptation layer. In: International Conference on Learning Representations (2016) 8. Huang, Q., Sun, J., Ding, H., Wang, X., Wang, G.: Robust liver vessel extraction using 3D U-Net with variant Dice loss function. Computers in Biology and Medicine 101, 153­162 (2018) 9. Jin, Q., Meng, Z., Pham, T.D., Chen, Q., Wei, L., Su, R.: DUNet: A deformable network for retinal vessel segmentation. Knowledge-Based Systems 178, 149­162 (2019) 10. Kervadec, H., Bouchtiba, J., Desrosiers, C., Granger, E., Dolz, J., Ayed, I.B.: Boundary loss for highly unbalanced segmentation. In: International Conference on Medical Imaging with Deep Learning. pp. 285­296. PMLR (2019)

10

Z. Xu et al.

11. Kitrungrotsakul, T., Han, X.H., Iwamoto, Y., Foruzan, A.H., Lin, L., Chen, Y.W.: Robust hepatic vessel segmentation using multi deep convolution network. In: Medical Imaging 2017: Biomedical Applications in Molecular, Structural, and Functional Imaging. vol. 10137, p. 1013711. International Society for Optics and Photonics (2017)
12. Kitrungrotsakul, T., Han, X.H., Iwamoto, Y., Lin, L., Foruzan, A.H., Xiong, W., Chen, Y.W.: Vesselnet: A deep convolutional neural network with multi pathways for robust hepatic vessel segmentation. Computerized Medical Imaging and Graphics 75, 74­83 (2019)
13. Li, X., Chen, H., Qi, X., Dou, Q., Fu, C.W., Heng, P.A.: H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from CT volumes. IEEE Transactions on Medical Imaging 37(12), 2663­2674 (2018)
14. Lin, T.Y., Goyal, P., Girshick, R., He, K., Dolla´r, P.: Focal loss for dense object detection. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2980­2988 (2017)
15. Liu, L., Tian, J., Zhong, C., Shi, Z., Xu, F.: Robust hepatic vessels segmentation model based on noisy dataset. In: Medical Imaging 2020: Computer-Aided Diagnosis. vol. 11314, p. 113140L. International Society for Optics and Photonics (2020)
16. Livne, M., Rieger, J., Aydin, O.U., Taha, A.A., Akay, E.M., Kossen, T., Sobesky, J., Kelleher, J.D., Hildebrand, K., Frey, D., et al.: A U-Net deep learning framework for high performance vessel segmentation in patients with cerebrovascular disease. Frontiers in Neuroscience 13, 97 (2019)
17. Northcutt, C.G., Jiang, L., Chuang, I.L.: Confident learning: Estimating uncertainty in dataset labels. arXiv preprint arXiv:1911.00068 (2019)
18. Ren, M., Zeng, W., Yang, B., Urtasun, R.: Learning to reweight examples for robust deep learning. In: International Conference on Machine Learning. pp. 4334­4343. PMLR (2018)
19. Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 234­241. Springer (2015)
20. Sato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., Gerig, G., Kikinis, R.: Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images. Medical Image Analysis 2(2), 143­168 (1998)
21. Simpson, A.L., Antonelli, M., Bakas, S., Bilello, M., Farahani, K., Van Ginneken, B., Kopp-Schneider, A., Landman, B.A., Litjens, G., Menze, B., et al.: A large annotated medical image dataset for the development and evaluation of segmentation algorithms. arXiv preprint arXiv:1902.09063 (2019)
22. Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In: Advances in Neural Information Processing Systems. pp. 1195­1204 (2017)
23. Wang, S., Cao, S., Chai, Z., Wei, D., Ma, K., Wang, L., Zheng, Y.: Conquering data variations in resolution: A slice-aware multi-branch decoder network. IEEE Transactions on Medical Imaging 39(12), 4174­4185 (2020)
24. Wang, Y., Zhang, Y., Tian, J., Zhong, C., Shi, Z., Zhang, Y., He, Z.: Doubleuncertainty weighted method for semi-supervised learning. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 542­551. Springer (2020)

Mean-Teacher-Assisted Confident Learning

11

25. Yu, L., Wang, S., Li, X., Fu, C.W., Heng, P.A.: Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 605­613. Springer (2019)
26. Zhang, M., Gao, J., Lyu, Z., Zhao, W., Wang, Q., Ding, W., Wang, S., Li, Z., Cui, S.: Characterizing label errors: Confident learning for noisy-labeled image segmentation. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 721­730. Springer (2020)
27. Zhou, T., Ruan, S., Canu, S.: A review: Deep learning for medical image segmentation using multi-modality fusion. Array 3, 100004 (2019)
28. Zhu, H., Shi, J., Wu, J.: Pick-and-learn: Automatic quality evaluation for noisylabeled image segmentation. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 576­584. Springer (2019)

