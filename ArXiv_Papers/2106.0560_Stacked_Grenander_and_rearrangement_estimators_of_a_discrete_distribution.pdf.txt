arXiv:2106.00560v2 [math.ST] 8 Jun 2021

Stacked Grenander and rearrangement
estimators of a discrete distribution
Vladimir Pastukhov vmpastukhov@yahoo.com
Abstract: In this paper we consider the stacking of isotonic regression and the method of rearrangement with the empirical estimator to estimate a discrete distribution with an infinite support. The estimators are proved to be strongly consistent with n-rate of convergence. We obtain the asymptotic distributions of the estimators and construct the asymptotically correct conservative global confidence bands. We show that stacked Grenander estimator outperforms the stacked rearrangement estimator. The new estimators behave well even for small sized data sets and provide a trade-off between goodness-of-fit and shape constraints.
MSC 2010 subject classifications: 62E20, 62G07, 62G20. Keywords and phrases: Constrained inference, isotonic regression, Grenander estimator, discrete distribution, cross-validation, model stacking, smoothing.
Contents
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0 2 Statement of the problem and notation . . . . . . . . . . . . . . . . . 2 3 Data-driven selection of the mixture parameter  . . . . . . . . . . . . 4 4 Theoretical properties of the estimator . . . . . . . . . . . . . . . . . . 5
4.1 Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4.2 Rate of convergence . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.3 Asymptotic distribution and global confidence band . . . . . . . 10 5 Simulation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.1 True p.m.f. is decreasing . . . . . . . . . . . . . . . . . . . . . . . 12 5.2 True p.m.f. is not decreasing . . . . . . . . . . . . . . . . . . . . . 13 6 Conclusion and discussion . . . . . . . . . . . . . . . . . . . . . . . . . 15 7 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1. Introduction
This work is largely inspired by recent papers in the estimation of discrete distributions with shape constraints. The first paper in this area is [18], where the authors studied the method of rearrangement and maximum likelihood estimator (MLE) of probability mass function (p.m.f.) under monotonicity constraint. The MLE under monotonicity constraint is also known as Grenander estimator. Next, in the paper [12] the authors introduced the least squares estimator of a
0

V. Pastukhov/Stacked Grenander and rearrangement estimators

1

discrete distribution under the constraint of convexity and, further, its limiting distribution was obtained in [1]. Furthermore, the MLE of log-concave p.m.f. was studied in detail in [4], and in [19] the problem was generalised to the case of multidimensional discrete support. Next, in paper [3] the authors introduced the MLE of unimodal p.m.f. with unknown support, proved the consistency and obtained the asymptotic distribution. The problem of least squares estimation of a completely monotone p.m.f. was considered in papers [2, 5].
In most of the papers listed above the authors considered both the well- and the mis-specified cases and studied the asymptotic properties of the estimators in both cases. In this work we do not have the mis-specified case in a sense that we assume that the true p.m.f. can be non-monotone and our estimators are strongly consistent even if the true p.m.f. is not decreasing.
The estimators introduced and studied in this paper are in some sense similar to nearly-isotonic regression approach, cf. [31] and [22] for multidimensional case. Nearly-isotonic regression is a convex optimisation problem, which provides intermediate less restrictive solution and the isotonic regression is included in the path of the solutions.
At the same time, our approach is in some sense opposite to liso (lassoisotone), cf. [13], and to bounded isotonic regression, cf. [21]. The liso is a combination of isotonic regression and lasso penalties, and bounded isotonic regression imposes additional penalisation to the range of the fitted model.
In this paper we combine Grenander estimator and the method of rearrangement with cross-validation-based model-mix concept, cf. [29]. The estimator is constructed as a convex combination of the empirical estimator and Grenander estimator or the empirical estimator and rearrangement estimator. Following the terminology for regression and classification problems in [9, 20, 33], we call the resulting estimators as stacked Grenander estimator and stacked rearrangement estimator, respectively. Therefore, we do not impose the strict monotonic restriction and let the data decide.
There are several papers where the authors studied a convex combination of the empirical estimator with a prescribed probability vector, cf. [14, 15, 29, 32]. In particular, in [29] the authors proposed the combination of the empirical estimator and a constant p.m.f. with a mixture parameter selected by crossvalidation. Also, the minimax estimator of a p.m.f. with respect to 2-loss with a fixed known finite support and sample size n is given by a convex combination of tehqeuaelmtpoirni+canlne,stcifm. [a3t2o]r. and the uniform distribution with a mixture parameter
In the case of continuous support, the density estimation via stacking was investigated in [28], where it was shown that the method of stacking performs better than selecting the best model by cross-validation. Next, in [25] the authors studied the approach of linear and convex aggregation of density estimators and, in particular, proved that the aggregation of two estimators allows to combine the advantages of both.
To the authors' knowledge, the problem of staking of shape constrained estimators has not been studied much even in a regression setup, except for the

V. Pastukhov/Stacked Grenander and rearrangement estimators

2

paper [34]. In the paper [34] the author used a convex combination of linear regression with isotonic regression to obtain a strictly monotonic solution. Also, it is worth to mention the paper [17], where it was shown that in terms of prediction accuracy the simplified relaxed lasso (which is stacking of least squares estimator and lasso) performs almost equally to the lasso in low signal-to-noise ratio regimes, and nearly as well as the best subset selection in high signal-tonoise ratio scenarios.
The paper is organised as follows. In Section 2 we state the problem and introduce notation. The derivation of cross-validation based mixture parameter is given in Section 3. Section 4 is dedicated to the theoretical properties of the estimators such as consistency, rate of convergence and asymptotic distribution. Also, in Section 4 we construct asymptotic confidence bands. In Section 5 we do simulation study to compare the performance of the estimators with empirical, minimax, rearrangement and Grenander estimators. The article closes with a conclusion and a discussion of possible generalisations in Section 6. The ancillary results and the proofs of some statements are given in Appendix. The R code for the simulations is available upon request.

2. Statement of the problem and notation

First, let us introduce notation and several definitions. Assume that z1, z2, . . . , zn

is a sample of n i.i.d. random variables with values in N and generated by a p.m.f.

p. For a given data sample let us create the frequency data x = (x0, . . . , xt),

where xj =

n i=1

1{zi

=

j}

and

t

=

sup{j

:

xj

>

0}

denotes

the

largest

order

statistic for the sample.

The empirical estimator of p is given by

p^n,j

=

xj n

,

j  N,

and it is strongly consistent, unbiased and asymptotically normal in 2-space. The rearrangement estimator studied in [18] is defined as

r^n = rear(p^n),

(2.1)

where rear(w) denotes the reversed-ordered vector. Also, equivalently, the rearrangement estimator can be written as r^n,j = sup{u : Qn(u)  j}, where Qn(u) = #{k : p^n,k  u}.
The MLE of decreasing p.m.f., or Grenander estimator, which we denote by g^n, is equivalent to the isotonic regression of the empirical estimator, cf. [6, 18, 26], i.e.

g^n = (p^n|F decr) := argmin [p^n,j - fj]2,
f F decr j

(2.2)

where F decr is the monotonic cone in 2, i.e. F decr = f  2 : f0  f1  . . . , p^n is the empirical estimator and (p^n|F decr) denotes the 2-projection of p^n

V. Pastukhov/Stacked Grenander and rearrangement estimators

3

onto F decr 1. In our work we construct the estimator in the following way:

^n = ^nh^ n + (1 - ^n)p^n,

(2.3)

where

h^ n =

r^n, g^n,

for the stacked rearrangement estimator, for the stacked Grenander estimator,

with the data-driven selection of :

^n = argmin CV (),
[0,1]

where CV () is a cross-validation criterion, which we introduce and study below. We associate each component xj of the frequency vector x with multinomial
indicator [j]  Rt+1, given by

[j] = (0, . . . , 0, 1, 0, . . . , 0)

(2.4)

for j = 0, . . . , t, cf. [29]. All elements of [j] are zeros, except for the one with

index j.

Next, let p^\n[j] for j = 0, . . . , t denote the leave-one-out version of the empirical estimator ^n for the frequency data x = (x0, . . . , xt), i.e. for j such that xj > 0

let

p^\n[j]

=

x - [j] n-1 ,

Next, for the rearrangement estimator, the leave-one-out version is given by

r^n\[j] = rear(p^\n[j]),

and for Grenander estimator:

g^n\[j] =  p^\n[j]|F decr .
Therefore, for j such that xj > 0 the leave-one-out versions of stacked rearrangement and stacked Grenander estimators are given by

^\n[j] =  h^ \n[j] + (1 - )p^\n[j],

(2.5)

with h^ \n[j] = r^n\[j] for the case of stacked rearrangement estimator, and h^ \n[j] = g^n\[j] for the case of stacked Grenander estimator, respectively.
For an arbitrary vector f  k we define k-norm


 ||f ||k =

 j=0

|fj |k

1/k
,

if k  N\{0},

supjN |fj |,

if k = ,

1The notion of "isotonic regression" in (2.2) might be confusing. Though, for historical reasons, it is a standard notion in the subject of constrained inference, cf. the monographs [26, 27] and also papers [7, 30], dedicated to the computational aspects, where the notation "isotonic regression" is used for the isotonic projection of a general vector.

V. Pastukhov/Stacked Grenander and rearrangement estimators

4

and for v  2 and w  2 let v, w =

 j=0

vj wj

denote

the

inner

product

on

2.

For a random sequence bn  R we will use the notation bn = Op(nq) if for

any  > 0 there exists a finite M > 0 and a finite N > 0 such that

P[n-q|bn| > M ] < ,

for any n > N .

3. Data-driven selection of the mixture parameter 

Let us consider squared 2-distance between the true p.m.f. p and the stacked

estimator ^n:

Ln = ||^n - p||22 : L(n1) - 2L(n2) + L(n3),

(3.1)

where L(n1) =

t j=0

^2n,j ,

L(n2)

=

t j=0

^n,j

pj

and

L(n3)

=

t j=0

p2j .

We aim to minimise Ln. Obviously, p is unknown, and we will use the ap-

proach introduced in [23] to estimate Ln. First, note that L(n3) is a constant and

can be omitted. Next, following [23], we estimate L(n2) by

L^ (n2)

=

1 n

t

xj ^\n[,jj] =

t

p^n,j ^n\[,jj] ,

j=0

j=0

with ^\n[j] defined in (2.5). Therefore, we select the mixture parameter n to

minimise

CV () = L(n1) - 2L^(n2),

(3.2)

i.e.
n = argmin CV ().
[0,1]

This cross-validation approach for estimation of discrete distributions was first introduced in [23] for smoothing kernel estimator and was also used in, for example, [10, 11, 24]. The mixture parameter n is given in the following theorem.

Theorem 1. The leave-one-out least-squares cross-validation mixture parameter n is given by


 

bn an

,

n = 1,

0,

if an = 0 and 0  bn  an, if 0 < an  bn, otherwise,

where

t
an = (h^n,j - p^n,j )2,
j=0

V. Pastukhov/Stacked Grenander and rearrangement estimators

5

and

t

t

bn =

p^n,j(h^n\[,jj] - p^n\[,jj]) -

p^n,j (h^n,j - p^n,j ),

j=0

j=0

with h^ \n[j] = r^n\[j] for the case of stacked rearrangement estimator, and h^ \n[j] = g^n\[j] for the case of stacked Grenander estimator, respectively.

4. Theoretical properties of the estimator

In this section we study theoretical properties of stacked rearrangement and stacked Grenander estimators. First, let us assume that p  F decr, i.e. the underlying p.m.f. is decreasing. Note that from triangle inequality for ||^n-p||k,
with 1  k  , we have

||^n - p||k = ||^nh^ n + (1 - ^n)p^n - p||k  ^n||h^ n - p||k + (1 - ^n)||p^n - p||k.

From the error reduction property of the rearrangement and Grenander estima-

tors, i.e. ||h^ n - p||k  ||p^n - p||k, with 1  k  , cf. Theorem 2.1 in [18], we

have

||^n - p||k  ||p^n - p||k

(4.1)

for all 1  k  . Therefore, in a case of decreasing true p.m.f. both the
stacked rearrangement and stacked Grenander estimators also provide the error reduction property.
Next, since all g^n, r^n and p^n are strongly consistent, and 0  ^n  1 for all n, then ^n is alsostrongly consistent. Furthermore, as it follows from [18], both r^n and g^n have n-rate of convergence. Therefore, in the case of decreasing true p.m.f. p the stacked Grenander estimator ^n is strongly consistent with
n-rate of convergence.
In the rest of this section we assume that the true p.m.f. is not decreasing. Let r = rear(p) and g =  p|F decr . Note, that r = p nor g = p, if p  F decr, i.e. the vectotor r is reversed ordered vector p and g is decreasing vector in 2 which is closest in 2-norm to the true p.
Then, since isotonic regression and rearrangement, viewed as a mapping from
2 into 2, are continuous in the case of finite support, and the empirical estimator is strongly consistent, then

r^n a.s. r, and g^n a.s. g,

pointwise. Note that from the statements (i), (ii) and (iv) of Lemma 2 in Ap-
pendix it follows that g^n always exists, and it is a probability vector for all n. Clearly, the same result holds for the rearrangement estimator r^n for all n. The almost sure convergence in k-norm, for 1  k  , of r^n and g^n to r and g, respectively, now follows from Lemma C.2 in the supporting material of [4].

V. Pastukhov/Stacked Grenander and rearrangement estimators

6

4.1. Consistency

First, let us study the leave-one-out versions of the empirical, rearrangement and Grenander estimators. Recall that

p^\n[j] =

x - [j] n-1 ,

r^n\[j] = rear(p^\n[j])

and

g^n\[j] = 

p^\n[j]|F decr

,

for j such that xj > 0. Let us define vectors n  2, n  2, and n  2 as

^n,j =

p^\n[,jj], 0,

if xj > 0, otherwise,

^n,j =

r^n\[,jj] , 0,

if xj > 0, otherwise.

(4.2)

^n,j =

g^n\[,jj], 0,

if xj > 0, otherwise.

From continuous mapping theorem it follows that ^n, ^n and ^n converge pointwise a.s. to p, r, and g, respectively. Next, we prove the following important
lemma.

Lemma 1. For the vectors ^n we have

^n,j  p^n,j

for all j, and for ^n and ^n we have

^n,j



n n-

1 r^n,j

and

^n,j



n

n -

1 g^n,j

for all j.

Proof. The proof is given in Appendix.



In Lemma C.2 in the supporting material of [4] it was proved that for probability mass functions the pointwise convergence and the convergence in k for 1  k   are all equivalent. Note, in our case the sequences ^n, ^n and ^n are not probability vectors. Nevertheless, as we prove below, all n, n and n converge a.s. to p, r and g, respectively, in k-norm for 1  k  .
Theorem 2. For the vectors ^n, ^n and ^n we have
^n a.s. p,
^n a.s. r,
and ^n a.s. g
in k-norm for 1  k  .

V. Pastukhov/Stacked Grenander and rearrangement estimators

7

Proof. The proof starts in a similar way as the one for Lemma C.2 in [4].

Let us, first, study the case of ^n. Fix some  > 0. Then, we can choose K such

that

pj



1

-

 4.

jK

Since both n and the empirical estimator pn converge to p pointwise, then there exists random n0 such that for all n  n0

sup
jK

|p^n,j

-

pj |



 4(K +

1) ,

sup
jK

|^n,j

-

pj |



 4(K +

1) ,

almost surely.

This implies that for all n  n0 we have

pj |



 4

,

almost

surely.

Next, for any n

jK

p^n,j



1

-

 2

and

jK |^n,j -



|^n,j -pj| =

|^n,j -pj |+ |^n,j -pj| 

|^n,j -pj |+ ^n,j + pj .

j=0

jK

j>K

jK

j>K

j>K

Furthermore, j>K ^n,j  j>K p^n,j since 0 < ^n,j  p^n,j . Then, for all n > n0 we have proved that





|n,j - pj|  4 + 2 + 4 = ,

j=0

almost surely. This means that for any  > 0 there exists random n0, such that for all n > n0
||n - p||1  ,

almost surely. Furthermore, since k  1, for all k > 1, then a.s. convergence holds in k,
for all 1  k  . Let us prove the convergence for n. First, from Lemma 1 it follows that

n

- n

1 ^n,j



g^n,j .

Then,

since

both

n-1 n

n

and

g^n

converge

to

g

pointwise,

we

can

use

the

same

approach as for ^ above, and prove that

n

- n

1 n

a.s.

g,

in k, for 1  k  , which means that

n a.s. g,

V. Pastukhov/Stacked Grenander and rearrangement estimators

8

in k, for 1  k  .

Now, using the result of Lemma 1, we can prove the result for ^n in the same

way as we did for n.



Now we can summarize the above results in the following theorem.

Theorem 3. For any underlying distribution p, both the stacked rearrangement and stacked Grenander estimators are strongly consistent:

^n a.s. p

in k-norm for 1  k  .

Proof. The proof is given in Appendix.



4.2. Rate of convergence

In this section we study the rate of convergence of stacked estimator. In the case of bounded support the n-rate of convergence follows from pointwise convergence of the vectors ^n, ^n and ^n. In this work we assume that the support can be infinite.
 Theorem 4. Stacked rearrangement and Grenander estimators have n-rate of convergence for any underlying p.m.f. p:
n||^n - p||k = Op(1)

for 1 < k  Next, if

.
 j=0

pj

<

,

then

n||^n - p||1 = Op(1).

Proof. Let us, first, prove the case of stacked Grenander estimator. Recall

that


 

bn an

,

if an = 0 and 0  bn  an,

n = 1, if 0 < an  bn,

0, otherwise,

where

t
an = (g^n,j - p^n,j )2,
j=0

and in the notation introduced in 4.2, we can write bn as

t

t

bn = p^n,j (^n,j - ^n,j ) - p^n,j (g^n,j - p^n,j).

j=0

j=0

V. Pastukhov/Stacked Grenander and rearrangement estimators

9

First, as we proved in Theorem 3 an a.s. ||g - p||22 > 0.

(4.3)

Second, note that from Lemma 1 it follows that for all n we have

t

t

bn = p^n,j (^n,j - g^n,j ) + p^n,j (p^n,j - ^n,j ) 

j=0

j=0

n n-1

t

p^n,j g^n,j -

t

p^n,j g^n,j +

t

p^n,j (p^n,j - ^n,j ).

j=0

j=0

j=0

Next, Recall that

n n-1

t

p^n,j g^n,j -

t

p^n,j g^n,j =

j=0

j=0

t j=0
n

p^n,j -1

g^n,j

.

^n,j =

xj -1 n-1

=

n n-1

p^n,j

-

1 n-1

,

0,

if xj = 0, otherwise,

which leads to

t

p^n,j (p^n,j - ^n,j ) =

t

p^n,j (p^n,j

- ^n,j )

=

1-

n

t j=0
-1

p^2n,j

.

j=0

j=0

Therefore, the upper bound for bn is given by

bn 

t j=0

p^n,j g^n,j

+

1-

t j=0

p^2n,j

.

n-1

n-1

(4.4)

Next, since n  0, from (4.3), (4.4) and Slutsky's theorem it follows that

n^n a.s. 0.

(4.5)

Then we have

^nn||g^n - p||k a.s. 0,

for all 1  k  , and (1 - ^n)n||p^n - p||k = Op(1)

for all 1 < k   and all p, since the empirical estimator is asymptotically

normal in 2. Next, as it follows from Corollary 4.2 in [18] for 1-norm the

following result holds

 n||p^n - p||1 = Op(1),

provided

 j=0

pj

<

.

Finally, recall that

n||^n

-

p||k



^n

 n||g^n

-

p||k

+

(1

-

^n

 ) n||p^n

-

p||k,

V. Pastukhov/Stacked Grenander and rearrangement estimators

10

which finishes the prove of theorem for the case of Grenander estimator. Similarly, using the results of Lemma 1, for the case of stacked rearrangement
estimator we can show that

bn 

t j=0

p^n,j

r^n,j

n-1

+

1-

n

t j=0
-1

p^2n,j

,

for all n. Then, the rest of the proof is the same as for Grenander estimator

with g^n and g suitably changed to r^n and r, respectively.



4.3. Asymptotic distribution and global confidence band

In this section we study the asymptotic distribution of stacked rearrangement and Grenander estimators and discuss calculation of global confidence band for p. The limit distribution of rearrangement and Grenanader estimators were obtained in [18]. The asymptotic distribution of stacked Grenander estimator for the case when true p.m.f. p is not decreasing is given in the next theorem.
Theorem 5. Assume that p is not decreasing. Then stacked rearrangement and Grenander estimators are asymptotically normal
n(^n - p) d Y0,C,

in 2, where Y0,C is a Gaussian process in 2 with mean zero and the covariance operator C such that Cei, ei = pii,i - pipi , with ei  2 the orthonormal basis in 2 such that in a vector ei all elements are equal to zero but the one with the index i is equal to 1, and i,j = 1, if i = j and 0 otherwise, cf. [18].

Proof. First, note that

||n(^n

-

p)

-

 n(p^n

-

p))||2

=

n||^n

-

p^n||2



^nn||h^ n - p^n||2 + (1 - ^n)n||p^n - p^n||2 = ^nn||h^ n - p^n||2.

Then, since

||r^n - p^n||2 a.s. ||r - p||2 < , ||g^n - p^n||2 a.s. ||g - p||2 < ,

and using (4.5) we have

^nn||r^n - p^n||2 a.s. 0, ^nn||g^n - p^n||2 a.s. 0,

which leads to

n||^n - p^n||2 a.s. 0.

V. Pastukhov/Stacked Grenander and rearrangement estimators

11

The statement of the theorem now follows from Theorem 3.1 in [8].



For the process Y0,C defined in Theorem 5 let q denote the -quantile of its -norm, i.e.
P[||Y0,C || > q] = .

Then, if p is not decreasing, from Theorem 5 for stacked estimator we have

lim
n

P[n||^n

-

p||



q]

=

1

-

.

For the case of a decreasing underlying p.m.f. the limit distributions of stacked estimator remains an open problem. Nevertheless, note that from (4.1) it follows

P[n||^n

-

p||



q]



 P[ n||p^n

-

p||



q]

for all n. Therefore, in the case of a decreasing p we have

lim inf
n

P[n||^n

-

p||



q]



1

-

.

In the same way as in [3], to estimate q we can use the empirical estimator

p^n in place of p, and then each quantile can be estimated using Monte-Carlo

method. Then let q^ in the supplementary

be Monte-Carlo material of [3] it

estimated -quantile. In Proposition was proved that q^ a.s. q. Therefore,

B.7 the

following confidence band

max

(p^n,j

-

q^ n

),

0

,

p^n,j

+

q^ n

, for j  N

is asymptotically correct global confidence band if p is not decreasing and it is asymptotically correct conservative global confidence band if p is decreasing.

5. Simulation study

In this section we do simulation study to compare the performance of stacked estimators with the empirical, Grenander and the minimax estimators. For the p.m.f. with finite support size s and for a given sample size n the minimax estimator of p with respect to 2-loss is given by

p^m n m = m n m + (1 - m n m)p^n,

(5.1)

with



=

(

1 s

,

.

.

.

,

1 s

)

and

m n m

=

n n+ n

,

cf.

[32].

To

the

authors'

knowledge,

the minimax estimation with respect to 2-loss for infinitely supported p.m.f.

is an open problem. With some abuse of notation, in this and next sections for

infinitely supported distributions we refer the estimator defined in (5.1) with

s = sup{j : xj > 0} + 1 as "minimax".

V. Pastukhov/Stacked Grenander and rearrangement estimators

12

5.1. True p.m.f. is decreasing

Let us consider the following uniform and decreasing p.m.f.:
M 1 : p = U (11), M 2 : p = 0.15U (3) + 0.1U (7) + 0.75U (11), M 3 : p = 0.25U (1) + 0.2U (3) + 0.15U (5) + 0.4U (7), M 4 : p = Geom(0.25),
where U (k) denotes the uniform pmf on {0, . . . , k} and Geom() is Geometric distribution, i.e. pj = (1 - )j for j  N with 0 <  < 1.

M1 is true, n=20

M2 is true, n=20

M3 is true, n=20

M4 is true, n=20

0.0 0.2 0.4 0.6 0.8 1.0 1.2

0.0 0.2 0.4 0.6 0.8 1.0 1.2

0.0 0.2 0.4 0.6 0.8 1.0 1.2

0.0 0.2 0.4 0.6 0.8 1.0 1.2

e mm r G sr sG M1 is true, n=100

e mm r G sr sG M2 is true, n=100

e mm r G sr sG M3 is true, n=100

e mm r G sr sG M4 is true, n=100

0.0 0.2 0.4 0.6 0.8 1.0 1.2

0.0 0.2 0.4 0.6 0.8 1.0 1.2

0.0 0.2 0.4 0.6 0.8 1.0 1.2

0.0 0.2 0.4 0.6 0.8 1.0 1.2

e mm r G sr sG

e mm r G sr sG

e mm r G sr sG

e mm r G sr sG

Figure 1: The boxplots for 1-distances of the estimators: the empirical estimator (e), minimax estimator (mm), rearrangement estimator (r), Grenander estimator (G), the stacked rearrangement estimator (sr) and the stacked Grenander estimator (sG) for the models M1, M2, M3 and M4.

The models M 2, M 3 and M 4 were used in [18] to assess the performance of Grenander estimator and compare its performance with empirical and rearrangement estimators. First, we compare the performance of the estimators in 1 (Figure 1) and 2 (Figure 2) distances for small n = 20 and moderate n = 100 sample sizes with 1000 Monte Carlo simulations.
From the boxplots at Figure 1 and Figure 2 we can conclude that for both small and moderate sized data sets stacked Grenander estimator outperforms in 1 and 2 norms both the empirical estimator and minimax estimator ("minimax" for the case of Geometric distribution) and it performs almost as good as Grenander estimator.

V. Pastukhov/Stacked Grenander and rearrangement estimators

13

M1 is true, n=20

M2 is true, n=20

M3 is true, n=20

M4 is true, n=20

0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.1 0.2 0.3 0.4 0.5

e mm r G sr sG M1 is true, n=100

e mm r G sr sG M2 is true, n=100

e mm r G sr sG M3 is true, n=100

e mm r G sr sG M4 is true, n=100

0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.1 0.2 0.3 0.4 0.5

0.0 0.1 0.2 0.3 0.4 0.5

e mm r G sr sG

e mm r G sr sG

e mm r G sr sG

e mm r G sr sG

Figure 2: The boxplots for 2-distances of the estimators: the empirical estimator (e), minimax estimator (mm), rearrangement estimator (r), Grenander estimator (G), the stacked rearrangement estimator (sr) and the stacked Grenander estimator (sG) for the models M1, M2, M3 and M4.

5.2. True p.m.f. is not decreasing

Now let us consider the cases when the underlying distributions are not decreasing:

M 5 : p = T (11),

M 6 : p = N Bin(7, 0.4),

M7 : p

=

3 8

P

ois(2)

+

5 8

P

ois(15),

where T (s) stands for strictly increasing triangular function, cf. [12]; N Bin(r, ) is the negative binomial distribution with r the number of failures until the experiment is stopped and  the success probability; P ois() is Poisson distribution with rate . Therefore, we consider very non-monotonic distributions. Indeed, model M 5 is strictly increasing p.m.f., M 6 is unimodal distribution, and M 7 is bimodal.
From Figure 3 and Figure 4 we can conclude that stacked Grenander estimator outperforms in 1 and 2 norms the empirical, rearrangement and minimax estimators ("minimax" for the cases of Negative Binomial and Poisson mixture).
Next, it is interesting to note that even if the underlying distribution is not monotone, Grenander estimator can still outperform the empirical estimator in both 1 and 2 norms for small sample size. This happens because the isotonisation decreases the variance of the estimator though bias becomes larger.
Next, one can see that for n = 100 stacked Grenander estimator outperforms all the estimators.

V. Pastukhov/Stacked Grenander and rearrangement estimators

14

M5 is true, n=20

M6 is true, n=20

M7 is true, n=20

1.5

1.5

1.5

1.0

1.0

1.0

0.5

0.5

0.5

0.0

0.0

0.0

e mm r G sr sG M5 is true, n=100

e mm r G sr sG M6 is true, n=100

e mm r G sr sG M7 is true, n=100

1.5

1.5

1.5

1.0

1.0

1.0

0.5

0.5

0.5

0.0

0.0

0.0

e mm r G sr sG

e mm r G sr sG

e mm r G sr sG

Figure 3: The boxplots for 1-distances of the estimators: the empirical estimator (e), minimax estimator (mm), rearrangement estimator (r), Grenander estimator (G), the stacked rearrangement estimator (sr) and the stacked Grenander estimator (sG) for the models M5, M6 and M7.

M5 is true, n=20

M6 is true, n=20

M7 is true, n=20

0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.0 0.1 0.2 0.3 0.4 0.5 0.6

e mm r G sr sG M5 is true, n=100

e mm r G sr sG M6 is true, n=100

e mm r G sr sG M7 is true, n=100

0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.0 0.1 0.2 0.3 0.4 0.5 0.6

e mm r G sr sG

e mm r G sr sG

e mm r G sr sG

Figure 4: The boxplots for 2-distances of the estimators:the empirical estimator (e), minimax estimator (mm), rearrangement estimator (r), Grenander estimator (G), the stacked rearrangement estimator (sr) and the stacked Grenander estimator (sG) for the models M5, M6 and M7.

V. Pastukhov/Stacked Grenander and rearrangement estimators

15

6. Conclusion and discussion

In this paper we introduced and studied estimation of a discrete infinitely sup-

ported distribution by stacking the empirical estimator and Grenander estima-

tor. We can summarise the simulation results of the previous section by plotting Monte Carlo estimates of nE(||^n - p||22) (with ^n one of the estimators studied in the paper) versus the sample size n, cf. Figure 5.

The main results of the paper: the stacked Grenander estimator is compu-

tationally feasible, it outperforms the empirical estimator, and it is almost as

good as Grenander estimator for the case of decreasing true p.m.f. Also, stacked

Grenander estimator outperforms stacked rearrangement estimator, except for

the case of a strictly decreasing p.m.f. The same effect was shown in [18] for

rearrangement and Grenander estimators. We proved that even when the true

disnt-rriabtuetioofncoisnvneortgedneccer.eTashinerge,fotrhee,

estimator remains strongly consistent with the stacked Grenander estimator provides a

trade-off between goodness of fit and monotonicity.

2.5

M4 is true
empirical "minimax" Grenander stacked Grenander

2.5

M7 is true
empirical "minimax" Grenander stacked Grenander

1.5

Estimated risk

1.5

Estimated risk

0.5

0.5

50 100 150 200 n

50 100 150 200 n

Figure 5: The estimates of normalised risk nE(||^n, -p||22) of the estimators: the empirical estimator (e), minimax estimator, Grenander estimator and the
stacked Grenander estimator for the models M4 and M7.

The first natural generalisation of stacked Grenander estimator could be stacking with isotonic regression for a general isotonic constraint. Throughout the paper, in almost all the proofs we used properties of a general isotonic regression, cf. Lemma 2. However, the proof of Lemma 1 is based on the maximum upper sets algorithm, which is given in Lemma 3 in Appendix, and this algorithm is valid only for one dimensional monotonic case. Therefore, the generalisation of stacked Grenander estimator to the general isotonic case for finite support is straightforward, though the case of an infinite support remains an open problem.
Second, it is also important to consider other shape constraints, such as unimodal, convex and log-concave cases. Stacking these estimators is, in effect,

V. Pastukhov/Stacked Grenander and rearrangement estimators

16

similar to the generalisation of nearly-isotonic regression to the nearly-convex regression in [31].
Third, in this work we studied the case of discrete distribution with infinite support. The empirical estimator is closely related to estimation of probability density functions via histograms. Therefore, another direction is stacking the histogram estimators with isotonised histogram.
Another interesting direction of research concerns the stacking with a crossvalidation based on other loss functions. For the overview and theoretical properties of different loss functions for evaluation of discrete distributions we refer to the paper [16].
Finally, as we mentioned in the introduction, the problem of stacking shaped constrained regression estimators has not been studied much. Therefore, since stacked Grenander estimator performs quite well, it would be interesting to explore, for example, the prediction performance of stacked isotonic regression.

7. Appendix
In Lemma 2 we provide properties of a general isotonic regression which are referred to in the paper.
Lemma 2. [Properties of a general isotonic regression] Let vn  2 be the isotonic regressions of some set of vectors vn  2, for n = 1, 2 . . . , i.e.
vn = (vn|F is) := argmin [vn,j - fj ]2,
f F is j
where F is is some general isotonic cone in 2, not necessarily simple monotonic one, as in (2.2). Assume also that a  vn,j  b holds for some constants - < a < b < , for all n = 1, 2, . . . and j = 1, 2 . . . . Then, the following holds.
(i) vn exists and it is unique. (ii) j vn,j = j vn,j, for all n = 1, 2, . . . . (iii) vn , viewed as a mapping from 2 into 2, is continuous. (iv) vn satisfies the same bounds as the basic estimator, i.e. a  vn,j  b, for
all n = 1, 2, . . . and j = 1, 2, . . . . (v) (avn|F is) = a(vn|F is) for all a  R+.

Proof. Statements (i), (ii) and (iii) follow from Theorem 8.2.1, Corollary B

of Theorem 8.2.7 and Theorem 8.2.5, respectively, in [26], statements (iv), (v)

and (vi) follow from Corollary B of Theorem 7.9, Theorems 7.5, respectively, in

[6].



In the next lemma we describe the maximum upper sets algorithm for the solution to the isotonic regression in the monotone case.

V. Pastukhov/Stacked Grenander and rearrangement estimators

17

Lemma 3. [Maximum upper sets algorithm] For a given x  Rt++1 the solution x of a simple order isotonic regression

t

x = argmin

[xj - fj ]2

f0 f1иииft j=0

is given by the following algorithm. First, let us define m(-1) = -1. Second, we

choose m(0) > m(-1) to be the largest integer which maximizes the following

mean

m(0)

xk

k=m(-1)+1

.

m(0) - m(-1)

Next, let us choose m(1) > m(0) to be the largest integer which maximizes

m(1)

xk

k=m(0)+1
m(1) - m(0)

.

We continue this process and get

-1 = m(-1) < m(1) < и и и < m(l) = t.

The solution x is given by

m(r)

xk

xj

=

k=m(r-1)+1
m(r) - m(r - 1)

for j  [m(r - 1) + 1, m(r)] and r  [0, l].

Proof. The proof is given on p. 77 in [6] and p. 26 in [26], and, also, for

simpler explanation of the algorithm we refer to [35].



Proof of Theorem 1. Recall that the least-squares cross-validation criterion is given by

t

t

CV () =

^2n,j - 2

p^n,j^\n[,jj] =

j=0

j=0

t

t

( h^n,j + (1 - )p^n,j )2 - 2 p^n,j( h^\n[,jj] + (1 - )p^\n[,jj]).

j=0

j=0

Then, after simplification we get

CV () = an2 - 2bn + cn,

V. Pastukhov/Stacked Grenander and rearrangement estimators

18

where the term cn does not depend on , and

t
an = (h^n,j - p^n,j )2,
j=0

and

t

t

bn =

p^n,j(h^\n[,jj] - p^\n[,jj]) -

p^n,j (h^n,j - p^n,j ).

j=0

j=0

Assume, that an = 0. Then, CV () is minimised by


 

bn an

,

n = 1,

0,

if 0  bn  an, if an  bn, if bn  0.

Next, note that if p^n = h^ n, then ^n = p^n = h^ n for any 0  n  1, and,

therefore, for consistency of notation we define ^n = 0 when an = 0.



Proof of Lemma 1. First, we prove the statement for ^n. Assume that for some j we have p^\n[,jj] = 0 and recall that

^n,j

=

p^\n[,jj]

=

xj n

-1 -1

.

Next, note that

xj - 1 n-1

-

xj n

=

-n + xj n(n - 1)

< 0.

Let us study the case of ^n. To prove the statement of the lemma we will use
maximum upper sets algorithm, which is given in Lemma (3) in the Appendix. Let x = (x0, . . . , xt) be frequency data from p. Next, let x = (x0, . . . , xt ) be isotonic regression of x and assume that x has (l + 1) constant regions. Let

-1 = m(-1) < m(1) < и и и < m(l) = t

be the indices of the last elements in the constant regions of x, therefore, we

have

m(r)

xk

xj

=

k=m(r-1)+1
m(r) - m(r - 1)

for j  [m(r - 1) + 1, m(r)] and r  [0, l]. Let us consider the first constant region of x and for some integer q 
[0, m(0)] define vector y  Rt++1

yj =

xj - 1, xj ,

if j = q, otherwise.

V. Pastukhov/Stacked Grenander and rearrangement estimators

19

Recall, m(0) is the largest integer which maximizes the following mean

m(0)

xk

S1 =

k=0
m(0)

Let m(0) be the largest integer which maximizes the following mean for

vector y

m(0)

yk

S2 =

k=0
m(0)

.

and note, that since yj  xj then S1  S2, which proves ym  xm. Next, assume that m is not in the first constant region. Note that in this
case from maximum upper sets algorithm it follows that the constant regions in the isotonic regressions x and y are the same up to the region which contains
element with index m. Then, we can use the same approach as for the first region. Therefore, we have proved that ym  xm.
Next, from statement (v) of Lemma 2 for g^n and ^n we have

g^n,m

=

xm , n

and

^n,m

=

ym n-

1

,

therefore, we proved that

^n,j



n

n -

1 g^n,j.

Finally, we prove the inequality for ^n. Analogously to the case of ^n, let us consider the vectors x and y, discussed above. Note that yj  xj for all j, therefore, the same componentwise inequality holds for the sorted vectors rear(x) and rear(y). Next, using the definition of r^n and ^n we prove that

^n,j



n n-

1 r^n,j .



Proof of Theorem 3. From Theorem 2 and Slutsky's theorem for metric spaces it follows that for the case of stacked rearrangement estimator we have
an a.s. ||r - p||22,
and bn a.s. p, (r - p) - p, (r - p) = 0,

V. Pastukhov/Stacked Grenander and rearrangement estimators

20

and for the case of stacked Grenander estimator we have

an a.s. ||g - p||22,

and Therefore, Next, since

bn a.s. p, (g - p) - p, (g - p) = 0. ^n a.s. 0.

||^n - p||k  ^n||h^ n - p||k + (1 - ^n)||p^n - p||k

for all 1  k  , it follows

^n a.s. p

in k-norm for 1  k  .



References
[1] Balabdaoui, F., Durot, C., Koladjo, F. (2017). On asymptotics of the discrete convex LSE of a p.m.f. Bernoulli 23, 1449Г1480.
[2] Balabdaoui, F. and de Fournas-Labrosse, G.(2020). Least squares estimation of a completely monotone pmf: From Analysis to Statistics. Journal of Statistical Planning and Inference, 204, 55Г71.
[3] Balabdaoui, F. and Jankowski, H. (2016). Maximum likelihood estimation of a unimodal probability mass function. Statistica Sinica 26, 1061Г 1086.
[4] Balabdaoui, F., Jankowski, H., Rufibach, K., and Pavlides, M.(2013). Asymptotics of the discrete log-concave maximum likelihood estimator and related applications. Journal of the Royal Statistical Society: SERIES B: Statistical Methodology, 75, 769Г790.
[5] Balabdaoui, F. and Kulagina, Y.(2020). Completely monotone distributions: Mixing, approximation and estimation of number of species. Computational Statistics & Data Analysis, 150, 107014.
[6] Barlow, R. E., Bartholomew, D. J., Bremner, J. M. and Brunk, H. D. (1972). Statistical inference under order restrictions John Wiley & Sons, London-New York-Sydney.
[7] Best, M. J. and Nilotpal C. (1990). Active set algorithms for isotonic regression; A unifying framework. Mathematical Programming , 47, 425Г439.
[8] Billingsley, P. (2013). Convergence of probability measures.. John Wiley & Sonsc.
[9] Breiman, L. (1995). Stacked regressions. Machine Learning, 24, 49Г64. [10] Chu, C. Y., Henderson, D. J. and Parmeter, C. F. (2015). Plug-
in bandwidth selection for kernel density estimation with discrete data. Econometrics, 3, 199Г214.

V. Pastukhov/Stacked Grenander and rearrangement estimators

21

[11] Chu, C. Y., Henderson, D. J. and Parmeter, C. F. (2017). On discrete Epanechnikov kernel functions. Computational Statistics & Data Analysis, 116, 79Г105.
[12] Durot, C., Huet, S., Koladjo, F. and Robin, S. (2014). Least-squares estimation of a convex discrete distribution. Computational Statistics & Data Analysis, 67, 282Г298.
[13] Fang, Z., Meinshausen, N. (2012). Liso isotone for high-dimensional additive isotonic regression. Journal of Computational and Graphical Statistics, 21, 72Г91.
[14] Fienberg, S. E. and Holland, P. W. (1972). On the choice of flattening constants for estimating multinomial probabilities. Journal of Multivariate Analysis, 2, 127Г134.
[15] Fienberg, S. E. and Holland, P. W. (1973). Simultaneous estimation of multinomial cell probabilities. Journal of the American Statistical Association, 68, 683Г691.
[16] Haghtalab, N., Musco, M. and Waggoner, B. (2019). Toward a Characterization of Loss Functions for Distribution Learning. Tech. rep., arXiv:1906.02652v2.
[17] Hastie, T., Tibshirani, R., and Tibshirani, R. (2020) Best Subset, Forward Stepwise or Lasso? Analysis and Recommendations Based on Extensive Comparisons. Statistical Science 35, 579Г592.
[18] Jankowski, H. K. and Wellner, J. A. (2009). Estimation of a discrete monotone distribution. Electronic journal of statistics, 39, 125Г153.
[19] Jankowski, H. and Tian, Y. H. (2018). Estimating a discrete log-concave distribution in higher dimensions. Statistica Sinica, 28, 2697Г2712.
[20] LeBlanc, M. and Tibshirani, R. (1996). Combining estiamates in regression and lassification. Journal of the American Statistical Association, 91, 1641Г1650.
[21] Luss, R. and Rosset, S. (2017). Bounded isotonic regression. Electronic Journal of Statistics, 11, 4488Г4514.
[22] Minami, K. (2020). Estimating piecewise monotone signals. Electronic Journal of Statistics, 14, 1508Г1576.
[23] Ouyang, D., Li, Q., and Racine, J. (2006). Cross-validation and the estimation of probability distributions with categorical data. Journal of Nonparametric Statistics, 18, 69Г100.
[24] Racine, J. S., Li, Q. and Yan, K. X. (2020). Kernel smoothed probability mass functions for ordered datatypes. Journal of Nonparametric Statistics, 32, 563Г586.
[25] Rigollet, P., and Tsybakov, A. B. (2007). Linear and convex aggregation of density estimators. Mathematical Methods of Statistics, 16, 260Г280.
[26] Robertson, T., Wright, F. T., and Dykstra, R. L. (1988). Order restricted statistical inference. John Wiley & Sons, Ltd., Chichester.
[27] Silvapulle, M. J. and Sen, P. K. (2005). Constrained Statistical Inference. John Wiley & Sons, Ink., Hoboken, New Jersey.
[28] Smyth, P. and Wolpert, D. (1999). Linearly combined density estimators via stacking. Machine Learning, 36, 59Г83.

V. Pastukhov/Stacked Grenander and rearrangement estimators

22

[29] Stone, M. (1974). Cross-Validation and Multinomial Prediction Biometrika, 61, 509Г515.
[30] Stout, Q. F. (2013). Isotonic Regression via Partitioning Algorithmica, 66, 93Г112.
[31] Tibshirani, R. J., Hoefling, H. and Tibshirani, R. (2011). Nearlyisotonic regression. Technometrics, 53, 54Г61.
[32] Tribula, S. (1958). Some Problems of Simultaneous Minimax Estimation. The Annals of Mathematical Statistics, 29, 245Г253.
[33] Wolpert, D. (1992). Stacked Generalization. Neural Networks, 5, 241Г 259.
[34] Wright, F. T. (1978). Estimating strictly increasing regression functions. Journal of the American Statistical Association, 73, 636Г639.
[35] Wright, F. T. (1982). Monotone regression estimates for grouped observations. The Annals of Statistics, 10, 278Г286.

H l2 l2 l1 l1
0.0
H H l2 l2 l1 l1
0.0

0.2

0.4

0.6

0.8

0.1

0.2

0.3

0.4

