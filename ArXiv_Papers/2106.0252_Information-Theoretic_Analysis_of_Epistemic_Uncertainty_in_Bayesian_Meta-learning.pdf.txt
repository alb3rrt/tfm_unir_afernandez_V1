arXiv:2106.00252v1 [cs.LG] 1 Jun 2021

Information-Theoretic Analysis of Epistemic Uncertainty in Bayesian Meta-learning
Sharu Theresa Jose Department of Engineering
King's College London London, WC2R 2LS
sharu.jose@kcl.ac.uk
Sangwoo Park Department of Engineering
King's College London London, WC2R 2LS
sangwoo.park@kcl.ac.uk
Osvaldo Simeone Department of Engineering
King's College London London, WC2R 2LS
simeone.osvaldo@kcl.ac.uk
Abstract
The overall predictive uncertainty of a trained predictor can be decomposed into separate contributions due to epistemic and aleatoric uncertainty. Under a Bayesian formulation, assuming a well-specified model, the two contributions can be exactly expressed (for the log-loss) or bounded (for more general losses) in terms of information-theoretic quantities (Xu and Raginsky [2020]). This paper addresses the study of epistemic uncertainty within an information-theoretic framework in the broader setting of Bayesian meta-learning. A general hierarchical Bayesian model is assumed in which hyperparameters determine the per-task priors of the model parameters. Exact characterizations (for the log-loss) and bounds (for more general losses) are derived for the epistemic uncertainty ­ quantified by the minimum excess meta-risk (MEMR) ­ of optimal meta-learning rules. This characterization is leveraged to bring insights into the dependence of the epistemic uncertainty on the number of tasks and on the amount of per-task training data. Experiments are presented that compare the proposed information-theoretic bounds, evaluated via neural mutual information estimators, with the performance of a novel approximate fully Bayesian meta-learning strategy termed Langevin-Stein Bayesian Meta-Learning (LS-BML).
1 Introduction
Bayesian learning and epistemic uncertainty. Bayesian machine learning is well understood to have important advantages in terms of uncertainty quantification, model selection, and out-ofdistribution detection (MacKay [2003], Wilson [2020]). Bayesian learning assumes the probabilistic model illustrated in Figure 1(a), in which the training data Z is generated in an i.i.d. manner given a model parameter W that is considered to be a random variable endowed with a prior distribution PW .
Preprint. Under review.

Assuming that the model is well specified, the overall uncertainty of the optimal predictor for a test target variable Y given input X, when measured by the log-loss, is given by the conditional entropy H(Y |X, Z). This can be decomposed as (Xu and Raginsky [2020])

H(Y |X, Z) = H(Y |X, W ) + I(Y ; W |X, Z),

(1)

aleatoric uncertainty epistemic uncertainty

where the conditional entropy H(Y |X, W ) quantifies the aleatoric uncertainty in the prediction, while the conditional mutual information (MI) I(Y ; W |X, Z) accounts for the epistemic uncertainty. The aleatoric uncertainty captures the inherent randomness in the data generation process, and is independent of the amount of the available data; while the epistemic uncertainty, also known as the minimum excess risk (MER), is caused by limitations in the availability of data, and vanishes as more training data is processed. In this work, we aim at extending this decomposition, and related analysis, from conventional Bayesian learning to Bayesian meta-learning (Kim et al. [2018], Grant et al. [2018], Ravi and Beatson [2019]).
Bayesian meta-learning. In conventional Bayesian learning, the prior PW of the model parameter is fixed a priori based on knowledge about the problem or tractability. A choice of the prior that matches the data generation mechanism can reduce the amount of data required to meet accuracy requirement. Bayesian meta-learning aims to automatically infer the prior PW by observing a finite number N of "related" tasks, so that the predictive performance on a new, previously unseen task, in the same class can be improved (Kim et al. [2018], Grant et al. [2018], Ravi and Beatson [2019]). The shared statistical properties of a class of tasks can be modelled using the hierarchical Bayesian model show in Figure 1(b). In it, a latent hyperpameter U determines the prior of all tasks via the conditional distribution as PW |U . Assuming a well-specified model, knowing the hyperparameter U hence yields the correct prior PW |U for the new task. The hyperparameter U itself is assumed to be random, and endowed with a hyperprior distribution PU .
Contributions. This work aims at developing, analyzing, and evaluating information-theoretic characterizations of the epistemic uncertainty associated with a meta-learning predictor. The metalearner has access to limited labelled data from a set of related tasks, known as meta-learning tasks, as well as from the new task of interest, known as meta-test task. Intuitively, meta-learning data can help reduce the epistemic uncertainty associated with the hyperparameter U , while meta-task data is important to further reduce the epistemic uncertainty at the level of the per-task model parameter W . With this in mind, the main contributions of this work are as follows:

1. We first develop an exact information-theoretic characterization of the overall epistemic uncertainty for an optimal Bayesian meta-learner for the log-loss. The characterization is given in terms of the minimum excess meta-risk (MEMR), which generalize the notion of MER to Bayesian metalearning. The bound reveals that, under suitable assumptions, the first contribution to epistemic uncertainty ­ due to the hyperparameter U ­ scales as O(d log(N )/N ), where N is the number of meta-training tasks for fixed number of per-task data samples m; while the contribution due to model parameter uncertainty scales as O(d log(m)/m).

2. To evaluate the derived information-theoretic bounds, we leverage mutual information neural estimation (MINE) (Belghazi et al. [2018], Mukherjee et al. [2020]), and compare the obtained estimates with the performance of a fully Bayesian approximate meta-learner based on Stochastic Gradient Langevin Dynamics (SGLD) and Stein Variational Gradient Descent (SVGD). The proposed method, termed Langevin-Stein Bayesian Meta-Learning, uses SGLD to produce samples for the hyperparameter and SVGD for the model parameters. Additional material including a generalization to a broader class of loss functions and an information-theoretic comparison of Bayesian learning and meta-learning can be found in the appendix.

2 Related Work
Information-theoretic generalization analysis. The works (Russo and Zou [2016], Xu and Raginsky [2017]) have shown that the generalization error of conventional learning algorithms in the frequentist setting can be upper bounded in terms of the MI I(W ; Z) between the input training set and the output model parameter. This metric captures the sensitivity of the learning mechanism to the input training set. Various refinements of these MI-based bounds have been studied since by (Bu et al. [2019], Negrea et al. [2019], Steinke and Zakynthinou [2020]), among others.

2

Moving from the frequentist to Bayesian learning, the recent work in (Xu and Raginsky [2020]) introduces an information-theoretic analysis of the MER in Bayesian learning. The MER for a general class of loss functions, including log-loss and bounded loss, is shown to be upper bounded via functions of the ratio I(W ; Z)/m where m is the number of data samples. Under appropriate regularity assumptions on the model, this upper bound is shown to vanish in the limit as m  .
Generalization analysis of meta-learning. Originating in the work by Schmidhuber [1987] and Thrun and Pratt [1998], meta-learning has been extensively studied in recent years both in terms of algorithm design (Finn et al. [2017], Nichol et al. [2018]) and of analytical studies on the metageneralization error within a frequentist setting (Pentina and Lampert [2014], Amit and Meir [2018], Rothfuss et al. [2020] , Jose and Simeone [2021a]). While the works in (Pentina and Lampert [2014], Amit and Meir [2018], Rothfuss et al. [2020]) obtain high-probability PAC-Bayesian bounds on the meta-generalization error with respect to the meta-training data, reference (Jose and Simeone [2021a]) presents information-theoretic bounds on the average meta-generalization error, thereby extending the work of (Xu and Raginsky [2017]) for conventional learning to meta-learning. Refinements and extensions to these bounds have been studied in (Rezazadeh et al. [2020], Jose and Simeone [2021b]).
Bayesian meta-learning. Most activity on Bayesian meta-learning has modelled the hyperparameter U in Fig. 1(b) as deterministic and only captured epistemic uncertainty related to the model parameter W . The approach is akin to empirical Bayes (Grant et al. [2018]), and it has been investigated in (Finn et al. [2018], Kim et al. [2018], Ravi and Beatson [2019], Gordon et al. [2018b], Nguyen et al. [2020]). Notably, reference (Kim et al. [2018]) proposed the use of Stein Variational Gradient Descent (SVGD) (Liu and Wang [2016]) to carry out non-parametric variational inference (VI) for W . SVGD is more flexible and can be effective than standard parametric VI methods based on Gaussian distributions (Liu and Zhu [2018]). Fully Bayesian meta-learning methods were derived in (Rothfuss et al. [2020], Amit and Meir [2018]) from a PAC Bayes perspective by using parametric VI with Gaussian models.

3 Problem Setting

In this section, we first review the setting studied in (Xu and Raginsky [2020]) for conventional Bayesian learning along with the key definition of Minimum Excess Risk (MER). Then, we generalize the framework to the Bayesian meta-learning setup introduced and analyzed in this paper. Central to our analysis is the Minimum Excess Meta-Risk (MEMR) metric, which extends the MER to meta-learning. We adopt standard notations for information-theoretic quantities such as (conditional) entropy and (conditional) MI as defined in (Cover and Thomas [2006]).

3.1 Conventional Bayesian Learning

In supervised learning, each data point Z = (X, Y )  Z consists of a tuple of input feature vector X  X and target variable Y  Y, which is drawn from an unknown population distribution. The learner observes a training data set Z = (Z1, . . . , Zm), of m samples, Zi = (Xi, Yi) for i = 1, . . . , m, that are generated i.i.d. according to the underlying unknown distribution, and uses it to predict the label of a test feature input X drawn independently from the training set Z from the same distribution. Considering a parametric generative model, we assume that the unknown population distribution belongs to a model class M = {PZ|w : w  W} parametrized by a model parameter w in the set W. This implies that the model class is well-specified (Knoblauch et al.
[2019]).

In conventional Bayesian learning, the model parameter W is treated as a latent random vector, and is endowed with a prior distribution PW . Conditioned on the model parameter W , the data samples are drawn i.i.d. from the model PZ|W . Consequently, the joint distribution of model parameter W , training set Z, and test sample Z = (X, Y ) is given as the product

PW,Z,Z = PW  PZ|mW  PZ|W ,

(2)

which factorizes according to the Bayesian network illustrated in Figure 1(a).

Let A denote an action space and : Y × A  R denote a loss function. The loss accrued by action
a  A on target variable y  Y is measured by the loss function (y, a). Under the generative model (2), the Bayesian learning problem is to infer a decision rule, base : Zm × X  A, mapping the

3

input training data Z  Zm and test input X  X to an action a  A, that minimizes the expected loss E [ PY,X,Z (Y, base(X, Z))], where PY,X,Z is the marginal of (2) over Y, X, and Z.
Definition 3.1 (Xu and Raginsky [2020]) The Bayesian risk for a loss function : Y × A  R is the minimum expected loss across all possible choices of the decision rule, i.e.,

R (Y |X, Z) := min E base:Zm×X A PY,X,Z (Y, base(Z, X)) .

(3)

The Bayesian risk is lower bounded by the expected loss obtained by an ideal decision rule, base : W × X  A, that has access to the true model parameter W generating the test sample Z = (X, Y )  PZ|W .
Definition 3.2 The genie-aided Bayesian risk is defined as

R

(Y

|X, W )

:=

base

min
:W ×X

A

EPY,X,W

(Y, (W, X)) .

(4)

The difference between the Bayesian risk (3) and the genie-aided risk (4) is the minimum excess risk (MER),

MER := R (Y |X, Z) - R (Y |X, W ).

(5)

The MER defined in (5) satisfies the following properties (Xu and Raginsky [2020]): (i) MER  0; and (ii) it is non-increasing with respect to the number of data samples m. Importantly, by (5), the minimum predictive uncertainty, R (Y |X, Z), can be written as the sum R (Y |X, W ) + MER of genie-aided Bayesian risk and MER. The first term quantifies the aleatoric uncertainty resulting from the inherent presence of randomness in the data generation process; while the MER quantifies the epistemic uncertainty due to availability of insufficient data to identify the model parameter W .

3.2 Bayesian Meta-Learning

In conventional Bayesian learning, the prior distribution PW on the model parameters is con-

ventionally chosen based on prior knowledge about the problem. In contrast, in Bayesian

meta-learning, this selection is data-driven and automated. Specifically, by observing data

from a number (N ) of tasks with shared statistical characteristics, meta-learning aims at in-

ferring a suitable prior PW , to be used on a new, a priori unknown task. As we de-

tail next, the statistical relationship among different tasks is accounted for via a hierarchical

Bayesian model that includes a global latent hyperparameter U  U (Gordon et al. [2018a]).

As illustrated in Figure 1(b), the

meta-learner is given data from

N meta-training tasks. Data

for each task i is drawn from

the distribution PZ|W =Wi with the task-specific model param-

eter Wi. In particular, condi-

tioned on model parameter Wi,

the training data samples of each

ith task, Zi = (Z1i, . . . , Zmi ), are i.i.d. and drawn from the data dis-

tribution PZ|W =Wi . The model parameter Wi of each task i is

drawn from a shared prior dis-

tribution PW |U , parameterized by a common hyperparameter

Figure 1: A graphical model representation of the joint distribution of the relevant quantities for : (a) conventional Bayesian learning;

U . Both the model parameter and (b) Bayesian meta-learning.

Wi and hyperparameter U are as-

sumed to be latent random variables, with joint distribution factorizing as PU  PW |U , with PU denoting the hyper-prior distribution. The parameterized prior PW |U is assumed to be the same for all tasks, and the statistical relationship of the observed tasks is captured through the hyperparameter

U.

4

The meta-training set Z1:N = (Z1, . . . , ZN ) includes the data sets from the N meta-training tasks. The goal is to use this data to reduce the expected loss measured on a meta-test task. The latter is a
priori unknown, and is modelled as being generated by drawing an independent model parameter W  PW |U for the given hyperparameter U that is shared with the meta-training data. This model parameter underlies the generation of the meta-test training data Z  PZ|mW , and an independently generated meta-test test data Z = (X, Y )  PZ|W .

To summarize, as shown in Figure 1(b), the joint distribution of global parameter U , model parameters W1:N = (W1, . . . , WN ) for the meta-training tasks, meta-training data set Z1:N , training data Z and test data Z of the meta-test task with model parameter W , is given as

N

PU,W1:N ,Z1:N ,W,Z,Z = PU  PW |U  PZ|W

 PW |U  PZ|W  PZ|W .

(6)

meta-training

meta-testing

The meta-learning decision rule is defined as a mapping meta : ZmN × Zm × X  A from observed meta-training set Z1:N  ZNm, training set Z  Zm and test feature input X  X of the meta-test task to the action space A. In words, the meta-learning rule meta leverages metatraining data, along with the training data for the meta-test task, to predict the target variable Y of the test sample (X, Y ) for the meta-test task. Under the meta-learning generative model in (6), the Bayesian meta-learning problem is to infer a decision rule meta(Z1:N , Z, X) so as to minimize the expected loss EPZ1:N ,Z,X,Y [ (Y, meta(Z1:N , Z, X))]), where PZ1:N ,Z,X,Y is the marginal of the joint distribution (6) over (Z1:N , Z, X, Y ). Accordingly, we have the following definitions.
Definition 3.3 For a given loss function : Y × A  R, the Bayesian meta-risk is the minimum expected loss across all possible choices of the meta-learning decision rule

R (Y |X, Z1:N , Z) := min E meta:ZmN ×Zm×X A PZ1:N ,Z,X,Y (Y, meta(Z1:N , Z, X)) . (7)

The Bayesian meta-risk reduces to the conventional Bayesian risk (3) when the per-task prior distribution PW |U does not depend on the hyperparameter U , and no meta-training set is observed i.e.,

Z1:N = , and PW |U = PW .

(8)

The Bayesian meta-risk is lower bounded by the expected loss obtained by a genie-aided decision rule, meta : U × W × X  A that has access to the true shared hyperparameter U and the true model parameter W of the test task.

Definition 3.4 The genie-aided Bayesian meta-risk is defined as

R

(Y |X, W, U )

:=

min
meta:U ×W ×X

A

EPU,W,X,Y

[

(Y, meta(U, W, X))].

(9)

The difference between the Bayesian meta-risk in (7) and the genie-aided Bayesian meta-risk in (9) is the minimum excess meta risk (MEMR), i.e.,

MEMR := R (Y |X, Z1:N , Z) - R (Y |X, W, U ).

(10)

The MEMR reduces to the MER when condition (8) holds.

4 Information-Theoretic Analysis of the MEMR
In this section, we first provide some general properties of the MEMR. Then, we analyze the MEMR with log-loss as the loss function, and obtain information-theoretic upper bounds that explicitly reveal the dependence of the MEMR on the number of meta-training tasks and per-task data samples.
4.1 Exact Analysis of the MEMR
Generalizing the properties of the MER reviewed in Section 3.1 and proved in (Xu and Raginsky [2020]), the MEMR can be shown to satisfy the following properties.

5

Lemma 4.1 The minimum excess meta-risk MEMR is non-negative, i.e., MEMR  0 and it is non-increasing with respect to the number of tasks, N , and to number of data samples per task, m.
Proof : See Appendix A.
We now evaluate the MEMR explicitly in terms of information-theoretic metrics when the loss function (·, ·) is the log-loss. To this end, consider the action space A to be the space of all probability distributions q(·) on Y. We assume that all necessary measurability conditions are satisfied (Xu and Raginsky [2020]). The log-loss accrued by distribution q(·) on a given target y is defined as (y, q) = - log q(y).

MER for conventional Bayesian learning. For reference, we first review a result from (Xu and Raginsky [2020]) that expresses the MER for conventional Bayesian learning in terms of a conditional MI, and bounds it as a function of a scaled MI.

Lemma 4.2 (Xu and Raginsky [2020]) The minimum excess risk (5) for the log-loss satisfies

MERlog = I(Y ; W |X, Z) = H(Y |X, Z) - H(Y |X, W )

(11)

I(W ; Z)



=: S(Z  W ).

m

(12)

In (11), the conditional entropy H(Y |X, Z) captures the overall predictive uncertainty of the target Y when tested on the feature input X using the training data Z, while the term H(Y |X, W ) accounts for the aleatoric uncertainty. The latter results from the inherent randomness in the observations, which applies even when true model parameter W is known. The difference between the two yield the conditional mutual information I(Y ; W |X, Z), which captures the epistemic uncertainty in predicting Y . In (12), the MER is upper bounded by the term S(Z  W ), which depends on the MI I(W ; Z) between the model parameter and the training data. This term captures the sensitivity of the trained model parameter W on the training data Z (see Sec. 2).

MEMR for Bayesian meta-learning. Our first main result is the generalization of the informationtheoretic characterization (11) to Bayesian meta-learning.

Proposition 4.1 The minimum excess meta-risk (10) for the log-loss is given as

MEMRlog = I(Y ; W |X, Z, Z1:N )

= H(Y |X, Z, Z1:N ) - H(Y |X, W ).

(13)

Proof : See Appendix B.
In (13), the conditional entropy H(Y |X, W ) captures the aleatoric uncertainty in predicting Y , which applies even when the true model parameter W is known. In contrast, the term H(Y |X, Z, Z1:N ) captures the average predictive uncertainty of the optimal meta-learning decision rule. As such, the conditional MI I(Y ; W |X, Z, Z1:N ) captures the epistemic uncertainty. This uncertainty results from the availability of limited meta-training tasks and meta-test training data, which causes the true model parameter W and true hyperparameter U to be inaccurately estimated.

Dependence of MEMR on N and m. We now decouple the contributions of hyperparameter-level and per-task-level uncertainties by developing an information-theoretic upper bound on the MEMR (13). The bound will be used to relate the MEMR to the number of meta-training tasks, N , and to the number of samples of the meta-test training set, m.

Theorem 4.2 The following upper bounds on the MEMR hold under the log-loss,

MEMRlog



I(W ; Z|Z1:N ) m

(14)



I(U ; Z1:N ) Nm

+

I(W ; Z|U ) m

=: MEMRUloBg ,

(15)

=:S(Z1:N U ) =:S(ZW |U )

where the inequality (15) holds for N  1.

6

Proof : See Appendix C.
The upper bound MEMRUloBg (15) on the MEMR for the log-loss is the sum of two contributions. The first, denoted as S(Z1:N  U ), captures the sensitivity of the hyperparameter U on the metatraining set Z1:N . The second term, S(Z  W |U ), corresponds to the average sensitivity of the model parameter W on the meta-test task training data Z assuming that the hyperparameter U is known. The additive dependence of the upper bound (15) on two mutual information terms, one at the hyperparameter level and other at the per-task model parameter level, bears resemblance to the information-theoretic bounds on the generalization error of frequentist meta-learning problems obtained by (Jose and Simeone [2021a], Jose and Simeone [2021b]).
A generalization of Theorem 4.2 to any loss function can be found in Appendix F, and a comparison between meta-learning and conventional learning in terms of predictive accuracy is available in Appendix D.

Asymptotic analysis of the MEMR. The first term in (15), S(Z1:N  U ), is a function of (N, m) and the second, S(Z  W |U ), of m, obscuring the scaling of the MEMR with (N, m). To investigate
this point, we now study the asymptotic behavior of the above two terms in (15).

Lemma 4.3 Let W  W and U  U be d-dimensional vectors taking values in compact subsets W, U  Rd respectively. Assume that the data distribution PZ|W (·|w) is smooth in w  W, and that
the distribution PZ|U (·|u) is smooth in u  U . Then, under additional technical conditions listed in Appendix E, we have that for fixed m, as N  ,

d

N

mN S(Z1:N  U ) = 2 log 2e + H(U ) + EPU log |JZ|U (U )| + o(1),

(16)

and as m  , we have

d

m

mS(Z  W |U ) = log 2 2e

+ H(W |U ) + EPW,U log |JZ|W (W )| + o(1),

(17)

where H(·) denotes the differential entropy of the argument random variable and JA|B(B) is the Fisher information matrix (FIM) about B contained in A with respect to conditional distribution
PA|B, whose (j, k)th entry is

2

[JA|B(B)]j,k = Bj Bk DKL(PA|B||PA|B ) B =B .

(18)

Using Lemma 4.3, it can be seen that the epistemic uncertainty at the hyperparameter level, quantified by the sensitivity S(Z1:N  U ), scales as O(d log(N )/N ) for fixed m; while the epistemic uncertainty at the per-task level, accounted for by the sensitivity S(Z  W |U ), scales as O(d log(m)/m). Therefore, if N  , and m is finite, the MEMR depends solely on the per-task epistemic uncertainty term S(Z  W |U ) in (15). That the MEMR does not vanish as N   is a consequence of the fact that the meta-test task is a priori unknown. As a result, even an infinite amount of meta-training data does not resolve the epistemic uncertainty about the meta-test task (Gordon et al. [2018a], Jose and Simeone [2021b]).

4.2 Note on the Optimality of Bi-Level Meta-Learning

The meta-decision rule maps directly the observed meta-training set Z1:N , the training data Z of the meta-test task, and test feature input X into a predictive distribution q(y|X, Z, Z1:N ) on the space Y of target labels. By standard results in Bayesian inference (see e.g., Bishop [2006]), the optimal
predictive distribution is hence given by the posterior PY |X,Z,Z1:N . To conclude this section and prepare for the next, we recall here that the joint distribution (6) can be factorized as

PY |X,Z,Z1:N = E [P ]. PU|Z1:N PW |Z,X,U Y |X,W

(19)

This factorization reveals that the optimal meta-decision rule can be implemented as a two-step
procedure, whereby one first obtains the hyperposterior distribution PU|Z1:N using meta-training data Z1:N ; and then evaluates the per-task posterior distribution PW |Z,X,U to evaluate the ensemble predictor (19).

7

5 Langevin-Stein Bayesian Meta-Learning (LS-BML)
The bounds derived in the previous sections apply to optimal Bayesian meta-learning, i.e., to the optimal predictive distribution (19), which is only tractable in special cases. In this section, we develop an efficient, scalable, solution that combines SVGD (Liu and Wang [2016]), to account for Bayesian inference at the level of the per-task posterior PW |Z,X,U , with an application of SGLD (Welling and Teh [2011]) to approximate samples from the hyperposterior PU|Z1:N . The proposed scheme is accordingly referred to as Langevin-Stein Bayesian Meta-Learning (LS-BML). As a result, LS-BML is fully Bayesian, unlike (Kim et al. [2018], Ravi and Beatson [2019], Gordon et al. [2018b], Nguyen et al. [2020]) that treat U as (deterministic) parameter; and it benefits from the flexibility of non-parametric VI via SVGD, unlike (Rothfuss et al. [2020], Amit and Meir [2018]) that focus on Gaussian approximation. As in (Kim et al. [2018], Ravi and Beatson [2019], Gordon et al. [2018b], Nguyen et al. [2020], Rothfuss et al. [2020], Amit and Meir [2018]), we focus here on the discriminative class of models in which the input X is fixed, and the model parameter W only affects the distribution of Y through PY |X,W .
The general idea of LS-BML is to leverage the decomposition (19) by iterating between SVGD steps for the approximation of the per-task local posterior PWi|Zi,U , for i = 1, . . . , N , and SGLD steps for the approximation of the hyperposterior PU|Z1:N . Specifically, we represent PWi|Zi,U via K particles {wik}Kk=1, which are collectively updated via SVGD (Kim et al. [2018]). The SVGD particles {wik}Kk=1 are then used to estimate the gradient needed to apply SGLD at the hyperparameter level via the reparametrization trick (Kingma and Welling [2013]). Details can be found in Appendix G.
6 Examples
In this section, we first describe a toy example involving sinusoidal regression, and then a fewshot classification example for the miniImagenet dataset. Additional experiments can be found in Appendix H. We use the PyTorch library (Paszke et al. [2019]) with two GeForce RTX 3090 GPUs.
6.1 Bayesian Sinusoidal Regression
In this first example, we illustrate the validity of the derived information-theoretic bounds by leveraging LS-BML and conditional MINE (C-MINE) (Mukherjee et al. [2020]). We focus on a sinusoidal regression problem, in which we have Y = W sin(X) + , with amplitude W , and observation noise   N (0, 0.12). The prior distribution of model parameter W is determined via hyperparameter U as PW |U = N (0, U -1), with hyperprior PU = Gamma(U |, ) with fixed shape parameter  = 2 and rate parameter  = 0.2.
Fig. 2 shows the MER for conventional learning, which ignores meta-training data, as well as the MEMR for meta-learning, versus the number of meta-training tasks, N . The left-hand-side figure provides the ground-truth results obtained via analytic integration; the middle plot shows the empirical performance obtained by LS-BML; and the right plot shows the derived information-theoretic bounds estimated via C-MINE (Mukherjee et al. [2020]). The general conclusions are that LS-BML provides an efficient approximation of the expectation under the ground-truth posterior distribution; and that the information-theoretic bounds, while numerically loose (see, e.g., (Hellström and Durisi [2020]) and (Wang et al. [2021]) for similar results), reproduce well the dependence of the MEMR on N .
6.2 Bayesian Few-Shot Classification
We now consider a few-shot classification problem with convolutional neural network (CNN) based on the miniImagenet dataset (Vinyals et al. [2016]). We focus on a 5-way classification with 5-shot available data for each class, i.e., m = 25 as in (Finn et al. [2017]). In order to reduce computational cost, we implement Bayesian meta-learning only for the last fully connected layer of the CNN, while fixing the feature-extractor layers as in (Kim et al. [2018]) using ANIL (Raghu et al. [2019]). We also adopt CNN with random weights (Saxe et al. [2011]) to compress the original single 84 × 84 × 3 image (height×width×channel) into a 4-dimensional vector. Details for the experiment can be found in Appendix J.
Fig. 3 plots Bayesian risk (3) and Bayesian meta-risk (7) obtained by LS-BML, as well as the information theoretic upper bounds on MER and MEMR estimated via C-MINE. Note that we plot
8

0.2

0.215

0.195

0.21

0.19

0.205

0.185

0.2

0.18

0.195

0.175 Analytic 0 10 20 30 40 50

0.19 LS-BML 0 10 20 30 40 50

1.4 1.3 1.2 1.1
1 0.9 0.8 0.7 0.6 Information-Theoretic
Bounds 0.5
0 10 20 30 40 50

Figure 2: (Left) Analytic MER for conventional learning and MEMR for meta-learning; (Middle) Estimation of MER and MEMR via LS-BML; (Right) Upper bounds (12) for MER and (15) for MEMR. Evaluated over fixed number of samples m = 2, right two figures show average performance over three independent experiments.

0.25
3.5
0.2 3

2.5

0.15

2

0.1

0

5

10

15

20

25

30

0

5

10

15

20

25

30

Figure 3: (Left) Bayesian risk and Bayesian meta-risk via LS-BML; (Right) Upper bounds (12) for MER and (15) for MEMR, averaged over three independent experiments.

the Bayesian (meta-)risk instead of the ME(M)R for LS-BML, since in this experiment, the model is misspecified and hence there are no ground-truth variables U and W . The figure validates once more the relevance of the information-theoretic bounds in capturing the epistemic uncertainty of Bayesian meta-learning.It also suggests that the derived bounds, while obtained under the assumption of a well-specified model, may still be relevant under model misspecification.
7 Conclusion
This paper studies epistemic uncertainty for Bayesian meta-learning from an information-theoretic perspective. We show that this uncertainty can be evaluated exactly (for log-loss) or bounded (for general loss functions) using a conditional MI I(Y ; W |X, Z, Z1:N ) involving model parameter, hyperparameter, and data. A novel information-theoretic upper bound on this term is also presented that explicitly shows the dependence of epistemic uncertainty on the number of meta-training tasks, N , and per-task samples, m.
The information-theoretic analysis conducted in this work assume that the model is well-specified, and that optimal Bayesian inference is possible. However, these two assumptions may not hold in practice, potentially limiting the practical implications of the analysis, although our experiments Figure 3 mitigate this concern. Future work may try to alleviate these limitations by considering model misspecification and the impact of variational inference.
The work is of theoretical nature and it does not target any specific application, so we do not envision any negative societal impact.
9

References
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory. In Proc. of Int. Conf. Machine Learning (ICML), pages 205­214, Jul 2018.
Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12: 149­198, March 2000.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning, pages 531­540. PMLR, 2018.
Christopher M Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.
Yuheng Bu, Shaofeng Zou, and Venugopal V Veeravalli. Tightening mutual information based bounds on generalization error. In Proc. of IEEE Int. Symp. Inf. Theory (ISIT), pages 587­591, July 2019.
Bertrand S Clarke and Andrew R Barron. Jeffreys' prior is asymptotically least favorable under entropy risk. Journal of Statistical planning and Inference, 41(1):37­60, 1994.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory, 2nd Edition,. WileyInterscience, July 2006. ISBN 0471241954.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proc. of Int. Conf. Machine Learning-Volume 70, pages 1126­1135, Aug. 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. arXiv preprint arXiv:1806.02817, 2018.
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E. Turner. Decision-theoretic meta-learning: Versatile and efficient amortization of few-shot learning. CoRR, abs/1805.09921, 2018a. URL http://arxiv.org/abs/1805.09921.
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner. Metalearning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921, 2018b.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradientbased meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.
Fredrik Hellström and Giuseppe Durisi. Fast-rate loss bounds via conditional information measures with applications to neural networks. arXiv preprint arXiv:2010.11552, 2020.
Sharu Theresa Jose and Osvaldo Simeone. Information-theoretic generalization bounds for metalearning and applications. Entropy, 23(1):126, 2021a.
Sharu Theresa Jose and Osvaldo Simeone. An information-theoretic analysis of the impact of task similarity on meta-learning. arXiv preprint arXiv:2101.08390, 2021b.
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic meta-learning. arXiv preprint arXiv:1806.03836, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.
Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. Generalized Variational Inference. arXiv preprint arXiv:1904.02063, 2019.
Chang Liu and Jun Zhu. Riemannian stein variational gradient descent for Bayesian inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm. In Advances in neural information processing systems, pages 2378­2386, 2016.
10

David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.
Andreas Maurer. Algorithmic stability and meta-learning. Journal of Machine Learning Research, 6: 967­994, Jun 2005.
Sudipto Mukherjee, Himanshu Asnani, and Sreeram Kannan. Ccmi: Classifier based conditional mutual information estimation. In Uncertainty in Artificial Intelligence, pages 1083­1093. PMLR, 2020.
Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy. Information-theoretic generalization bounds for SGLD via data-dependent estimates. In Proc. of Adv. Neural Inf. Processing Sys. (NIPS), pages 11013­11023, Dec 2019.
Cuong Nguyen, Thanh-Toan Do, and Gustavo Carneiro. Uncertainty in model-agnostic meta-learning using variational inference. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3090­3100, 2020.
Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms. arXiv preprint arXiv:1803.02999, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024­8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.
Anastasia Pentina and Christoph Lampert. A PAC-Bayesian bound for lifelong learning. In Proc. of Int. Conf. on Machine Learning (ICML), pages 991­999, June 2014.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.
Sachin Ravi and Alex Beatson. Amortized Bayesian meta-learning. In ICLR (Poster), 2019.
Arezou Rezazadeh, Sharu Theresa Jose, Giuseppe Durisi, and Osvaldo Simeone. Conditional mutual information-based generalization bound for meta learning. arXiv preprint arXiv:2010.10886, 2020.
Jonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes-optimal meta-learning with PAC-guarantees. arXiv preprint arXiv:2002.05551, 2020.
Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory. In Proc. of Artificial Intelligence and Statistics (AISTATS), pages 1232­1240, May 2016.
Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng. On random weights and unsupervised feature learning. In Icml, 2011.
Jürgen Schmidhuber. Evolutionary Principles in Self-Referential Learning, or On Learning How to Learn: The Meta-meta-... Hook. PhD thesis, Technische Universität München, 1987.
Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual information. arXiv preprint arXiv:2001.09122, 2020.
Sebastian Thrun and Lorien Pratt. Learning to Learn: Introduction and Overview. In Learning to Learn, pages 3­17. Springer, 1998.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. arXiv preprint arXiv:1606.04080, 2016.
Hao Wang, Yizhe Huang, Rui Gao, and Flavio P Calmon. Learning while dissipating information: Understanding the generalization capability of sgld. arXiv preprint arXiv:2102.02976, 2021.
11

Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681­688. Citeseer, 2011.
Andrew Gordon Wilson. The case for bayesian deep learning. arXiv preprint arXiv:2001.10995, 2020.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In Proc. of Adv. in Neural Inf. Processing Sys. (NIPS), pages 2524­2533, Dec. 2017.
Aolin Xu and Maxim Raginsky. Minimum excess risk in Bayesian learning. arXiv preprint arXiv:2012.14868, 2020.
12

A Proof of Lemma 4.1
The properties are a direct consequence of the data processing inequality satisfied by the Bayesian risk (Xu and Raginsky [2020]). This states that, given jointly distributed random variables A, B and C, if the Markov chain A - B - C holds, we have the inequality R (C|A)  R (C|B). Noting that (Z1:N , Z, X) - (U, W, X) - Y forms a Markov chain, the non-negativity of the MEMR follows from the data processing inequality. Through the same argument, it can be proved that MEMR is non-increasing with the number of tasks and per-task data samples.

B Proof of Proposition 4.1

To obtain the relation in (13), we first evaluate the Bayesian meta-risk and the genie-aided Bayesian risk. Under log-loss, the Bayesian meta-risk is given by,

Rlog (Y

|X,

Z,

Z1:N )

=

min
q(·)

EPZ1:N ,Z,X PY

|X,Z,Z1:N

[-

log

q(Y

|X,

Z,

Z1:N )].

(20)

From standard results in information theory (Cover and Thomas [2006]), it can be verified that the optimal meta-decision rule q(·) that minimizes the Bayesian meta-risk corresponds to the posterior predictive distribution PY |X,Z,Z1:N , whereby we have
Rlog(Y |X, Z, Z1:N ) = EPZ1:N ,Z,X PY |X,Z,Z1:N [- log PY |X,Z,Z1:N ] = H (Y |X, Z, Z1:N ). (21)
Similarly, it can be shown that

Rlog(Y |X, W, U ) = EPU,W,X PY |X,U,W [- log PY |X,U,W ]

= H(Y |X, W, U ) = H(Y |X, W ),

(22)

where the last equality follows since U - W - Z forms a Markov chain whereby PZ|W,U = PZ|W . Together, we then have that

MEMRlog = H(Y |X, Z, Z1:N ) - H(Y |X, W )

(=a) H(Y |X, Z, Z1:N ) - H(Y |X, W, Z, Z1:N )

(23)

= I(Y ; W |X, Z, Z1:N ),

(24)

where the equality in (a) follows since conditioned on test input X and model parameter W , the test output Y is independent of (Z, Z1:N ).

C Proof of Theorem 4.2

To obtain the required bound on MEMR, we note that the following set of relations hold.

MEMRlog = I(W ; Y |X, Z, Z1:N )

 I(W ; Z|Z, Z1:N )

(25)

 I(W ; Z|Z1:N )

(26)

m

= I(W, U ; Z|Z1:N )

(27)

m

= I(W ; Z|U ) + I(U ; Z|Z1:N )

(28)

m

m

 I(W ; Z|U ) + I(U ; Z1:N ) .

(29)

m

Nm

Here, (25) follows from the chain rule of mutual information. To prove inequality (26), we use the

technique of ([Xu and Raginsky, 2020, Proof of Thm. 2]) which we now detail here. Towards this, we

define Zj = (Z1, . . . , Zj) and note that I(W ; Z|Z1:N ) =

m j=1

I (W

;

Zj

|Z j -1 ,

Z1:N

).

We

then

13

have the following set of relations

I(W ; Zj+1|Zj , Z1:N ) = H(Zj+1|Zj , Z1:N ) - H(Zj+1|Zj , W, Z1:N )

(a)
=

H(Zj+2|Zj ,

Z1:N )

-

H (Zj+1 |W

)

(b)
 H(Zj+2|Zj+1, Z1:N ) - H(Zj+2|W )

= H(Zj+2|Zj+1, Z1:N ) - H(Zj+2|Z1:N , W, Zj+1)

= I(W ; Zj+2|Z1:N , Zj+1),

(30)

where (a) follows since (Zj+1, Zj, Z1:N ) (Zj+2, Zj, Z1:N ) in distribution and that Zj+1 is conditionally independent of (Zj, Z1:N ) given W , and (b) follows since conditioning reduces

entropy, and that (Zj+1, W ) (Zj+2, W ) in distribution. Consequently, we get the inequality

m
I(W ; Z|Z1:N ) = I(W ; Zj |Zj-1, Z1:N )

j=1

 mI(W ; Zm+1|Zm, Z1:N )

= mI(W ; Z|Z, Z1:N ),

(31)

whereby I(W ; Z|Z, Z1:N )  I(W ; Z|Z1:N )/m.

The equality in (27) follows since I(U ; Z|W, Z1:N ) = 0, which results from (U, Z1:N ) - W - Z

forming a Markov chain, whereby I(W, U ; Z|Z1:N ) = I(W ; Z|Z1:N ). Finally to see (29), we follow

similar steps as in the proof of (26). Denoting Zk = (Z1, . . . , Zk), we have the mutual information

I(U ; Z1:N ) =

N k=1

I

(U

;

Zk

|Zk-1

),

each

individual

component

of

which

can

be

written

as

I(U ; Zk+1|Zk)

= H(Zk+1|Zk) - H(Zk+1|U, Zk)

(=a) H(Zk+2|Zk) - H(Zk+1|U )

(32)

(b)
 H(Zk+2|Zk+1) - H(Zk+2|U )

= H(Zk+2|Zk+1) - H(Zk+2|U, Zk+1)

= I(Zk+2; U |Zk+1).

(33)

Here, the equality in (a) follows since (Zk+1, Zk) (Zk+2, Zk) in distribution, and that Zk+1 is conditionally independent of Zk given U , and (b) follows since conditioning reduces entropy, and
that (Zk+1, U ) (Zk+2, U ) in distribution. Consequently, we have that the mutual information I(U ; Z1:N )  N I(U ; Z|Z1:N ), which results in the inequality in (29).

D Meta-Learning vs Conventional Learning

One of the advantages of the Bayesian viewpoint on meta-learning is that one can obtain general information-theoretic conclusions about the performance comparison of meta-learning and conventional learning. This is in contrast to the frequentist analyses that focus on the meta-generalization error (Baxter [2000], Maurer [2005], Pentina and Lampert [2014]), making it difficult to draw general conclusions on this comparison.
To start, it is easy to see that under the assumption (8), the MEMR (13) reduces to the MER (11) for conventional Bayesian learning. In fact, by (13), we have MEMRlog = I(W ; Y |X, Z) = MERlog.
Generalizing this observation, the following proposition quantifies the gains of meta-learning with respect to conventional learning under log-loss.

Proposition D.1 Under log-loss, the MEMRlog in (13) for meta-learning and the MERlog in (11) for conventional learning are related as

MERlog - MEMRlog = I(Z1:N ; Y |X, Z)  0.

(34)

14

Proof : The relation in (34) is obtained as follows.

MERlog - MEMRlog = I(Y ; W |X, Z) - I(Y ; W |X, Z, Z1:N )

(35)

= H(Y |X, Z) - H(Y |X, Z, Z1:N )

(36)

= I(Y ; Z1:N |X, Z).

(37)

Proposition D.1 shows that under the log-loss, meta-learning yields a lower minimum excess risk than conventional learning. The gain in minimum excess risk is quantified by the conditional MI I(Z1:N ; Y |X, Z), which grows as the meta-training set Z1:N becomes more informative about the meta-test target variable Y beyond the information already available in the meta-test training set Z and input X.

E Assumptions for the convergence rates of Lemma 4.3

In this section, we specialize the assumptions required for the convergence rate of I(W ; Z) for
Bayesian learning in (Clarke and Barron [1994]) to the case of Bayesian meta-learning, where we have two MI terms I(U ; Z1:N ) and I(W ; Z|U ). We first list the assumptions required for the convergence of I(U ; Z1:N ), and explain how these extend to I(W ; Z|U ).

Assumption E.1 The following assumptions must be satisfied for ensuring the convergence of the mutual information term I(U ; Z1:N ) in Lemma 4.3.

1. Let U  U  Rd, and that the density PZ|U exists with respect to Lebesgue measure. Moreover, U has a non-void interior and its boundary has d-dimensional Lebesgue measure
0.

2. The density PZ|U (Z|u) is twice continuously differentiable in u for almost every Z and there exists (u) so that for each j, k = 1, . . . , d,

2

2

f (u) = EPZ|U=u

sup
u :||u -u||<(u)

uj uk

log PZ|U (Z|u )

(38)

is finite and continuous.

3. For j = 1, . . . , d,



2+

EPZ|U=u uj log PZ|U (Z|u)

(39)

is finite and continuous, as a function of u, for some  > 0.

4. Fisher information matrix (FIM) and second derivative of relative entropy are equal i.e. for matrices,





[IZ|U (u)]j,k = E uj log PZ|U (Z|u) uk log PZ|U (Z|u)

(40)

and

2

[JZ|U (u)]j,k = uj uk DKL(PZ|u||PZ|u ) u =u ,

(41)

we have IZ|U (u) = JZ|U (u) and that the matrix IZ|U (u) is assumed to be positive definite.

5. For u = u , we have PZ|U=u = PZ|U=u .
6. The hyperprior PU is assumed continuous and is supported on a compact subset in the interior of U.

15

Under Assumption E.1, Theorem 1 of (Clarke and Barron [1994]) then yields the required asymptotic of the MI I(U ; Z1:N ) in (16).
To analyze the asymptotic of the MI I(W ; Z|U ), we note that I(W ; Z|U ) = EPU [I(W ; Z|U = u)]. Consequently, we specialize Assumption E.1 to ensure convergence of I(W ; Z|U = u) for each u  U . This can be done by replacing the distribution PZ|U with PZ|W , the hyperprior PU by the prior PW |U=u for each u  U , such that the resulting assumptions hold at the level of model parameter. Subsequently, Theorem 1 of (Clarke and Barron [1994]) ensures that as m  ,

d

m

I(W ; Z|U = u) = log 2 2e

+ H(W |U = u) + EPW|U=u log |JZ|W (W )| + o(1).

(42)

Taking expectation of (42) with respect to the hyperprior PU , then yields the relation in (17).

F Information-Theoretic Analysis of the MEMR for General Loss Functions

In this section, we extend the characterization in Theorem 4.2 of the MEMR from the log-loss to general loss functions : Y × A  R. We specifically show that, under suitable assumptions on the loss function, the MEMR in (10) can be upper bounded using a concave, non-decreasing, function of the conditional mutual information I(Y ; W |X, Z, Z1:N ).
To upper bound the MEMRl, we consider the performance of the following randomized metadecision rule meta(X, Z, Z1:N ). Define as meta(X, W, U ) the optimal genie-aided decision rule that minimizes the Bayesian meta-risk (9), i.e., R (Y |X, W, U ) = E[ (Y, meta(X, W, U ))]. This rule is not directly applicable since the pair (U, W ) is not known. Having computed the posterior PW,U|X,Z,Z1:N (see Section 4.2), we draw as sample (U , W ) from it. Note that conditioned on (X, Z, Z1:N ), the pairs (U, W ) and (U , W ) are independent. The meta-decision rule is chosen as meta(X, W , U ), substituting the true pair (U, W ) with the sample (U , W ). Consequently, the MEMR (10) can be upper bounded as

MEMRl



EPX,Z,Z1:N PY,W

,U

[
|X,Z,Z1:N

(Y, meta(X, W

,U

))] - EPX,Y,W,U [

(Y, meta(X, W, U ))].

(43)

We now obtain an information-theoretic upper bound on (43) under the following assumption. Towards this, we first define the following zero mean random variable

 (Y, W , U |X, Z, Z1:N ) =

(Y, meta(X, W

,U

) - EPY,W

,U

[
|X,Z,Z1:N

(Y, meta(X, W

,U

)]).

Assumption F.1 There exists function () for   (0, b] satisfying (0) =  (0) = 0 such that the cumulant generating function (CGF) of  (Y, W , U |x, z, z1:N ) is upper bounded by (), i.e., the following inequality holds

log EPY,W ,U |x,z,z1:N exp  (Y, W , U |x, z, z1:N )  ()

(44)

for all x  X , z  Zm and z1:N  ZNm.
We also define the Legendre dual of () as (x) = sup(0,b] x - (). It is a nonnegative, convex and a non-decreasing function on [0, ) with (0) = 0 (Boucheron et al. [2013][Lemma 2.4]). The inverse of this function, called inverse Legendre dual, is defined as -1(y) = inf(0,b](y + ())/, and is concave. We now state our result.

Theorem F.1 Under Assumption F.1, the following bound on MEMR holds

MEMRl  -1 I(W ; Y |X, Z, Z1:N

(45)

 -1 I(U ; Z1:N ) + I(W ; Z|U ) .

(46)

Nm

m

16

Proof : The proof follows the approach in Xu and Raginsky [2020] and we outline the main steps here. The following set of relations hold:

MEMRl



EPX,Z,Z1:N PY,W

,U

[
|X,Z,Z1:N

(Y, meta(X, W

,U

))] - EPX,Y,W,U [

(Y, meta(X, W, U ))]

= EPX,Z,Z1:N

EPY,W

,U

[
|X,Z,Z1:N

(Y, meta(X, W

,U

))] - E [ PY,W,U|X,Z,Z1:N

(Y, meta(X, W, U ))]

(a)
 EPX,Z,Z1:N -1 DKL(PY,W,U |X,Z,Z1:N ||PY,W ,U |X,Z,Z1:N )

(b)
 -1 EPX,Z,Z1:N DKL(PY,W,U |X,Z,Z1:N ||PY,W ,U |X,Z,Z1:N )

(=c) -1 I(Y ; W, U |X, Z, Z1:N )

= -1 I(Y ; W |X, Z, Z1:N )

(d)
 -1

I(U ; Z1:N ) + I(W ; Z|U )

.

(47)

Nm

m

Here, the inequality in (a) follows from Assumption F.1 and using Donsker-Varadhan inequality (see
(Xu and Raginsky [2020][Lemma A.1])). The inequality in (b) follows by using Jensen's inequality on the concave inverse Legendre dual function -1(·). The equality in (c) follows from the
observation that while the distribution PY,W,U|X,Z,Z1:N factorizes as PY |X,Z,Z1:N  PW,U|X,Y,Z,Z1:N , the distribution PY,W ,U |X,Z,Z1:N is obtained as PY |X,Z,Z1:N  PW,U|X,Z,Z1:N with (W , U ) conditionally independent of Y given (X, Z, Z1:N ). The last inequality in (d) follows since the inverse Legendre dual is a non-decreasing function.

It can be seen that if the random variable  (Y, W , U |x, z, z1:N ) is 2-sub Gaussian1 when (Y, W , U )  PY,W ,U |x,z,z1:N for all x  X , z  Zm and z1:N  ZNm, then Assumption F.1 is satisfied with b = , () = 22/2 and -1(y) = 22y. We now specialize Theorem F.1 to account for this case.
Corollary F.2 Assume that  (Y, W , U |x, z, z1:N ) is 2-sub Gaussian for all x  X , z  Zm and z1:N  ZNm. Then, the following upper bound on the MEMR (10) holds:

MEMRl 

22 I(U ; Z1:N ) + I(W ; Z|U ) .

Nm

m

(48)

G LS-BML

In this section, we detail the computation of the gradients required in LS-BML. Given K particles {wik}Kk=1 for model parameter W , each of the particles is updated via SVGD Kim et al. [2018] as

K

wik  wik + K

[k(wik , wik)wik log Pwik ,Z|U + wik k(wik , wik)]

k =1

=: SVGD({wik}Kk=1; Z)

(49)

with step size > 0, where we have w log Pw,Z|U = w log Pw|U + w log PZ|w. Given SVGD particles {wik}Kk=1 for every meta-learning task i, we then apply SGLD to obtain a sequence of sample approximately drawn from the hyperposterior PU|Z1:N . The gradient of the log-hyperposterior

1A zero-mean random variable X is said to be 2-sub-Gaussian if the cumulant generating function (CGF)

satisfies log EPX [exp((X))]



2 2 2

for all 



R.

17

can be expressed as

N

u log Pu,Z1:N = u log PZi|u + u log Pu
i=1

N

= u log

Pw,Zi|u + u log Pu

i=1

wW

N
=
i=1

w uPw,Zi|u w Pw ,Zi|u

+

u log Pu

N

=

u log Pw,Zi|u · Pw|Zi,u + u log Pu

i=1 wW

N

=

EW PW |Zi,u u log PW,Zi|u + u log Pu

i=1

1N K

 K

u log PZi|wik + u log Pwik|u + u log Pu,

i=1 k=1

(50) (51)

where (51) employs the SVGD particles {wik}Kk=1 to estimate the expectation in (50). In order to compute the required gradients in (51), we adopt reparametrization trick (Kingma and Welling [2013])
assuming Gaussian prior distribution PW =w|U=[u1,u2] = N (W = w|u1, u2), i.e., w = u1 + u2 , with   N (0, Id). The overall procedure of the proposed LS-BML scheme is summarized in Algorithm 1.

Algorithm 1: LS-BML

Input: meta-training set Z1:N , hyper-prior distribution PU , prior distribution PW |U , number K of SVGD particles, step size t
Output: hyper-posterior samples u^  PU|Z1:N

initialize u and set t = 0
while convergence criterion not met do choose N~ tasks randomly from meta-training set Z1:N and denote as Z~1:N~ the corresponding meta-training subset for each sampled task i  {1, 2, . . . , N~ } do

initialize K particles for task i model parameter Wi as wik  PW |U=u, k = 1, 2, . . . , K {wik}Kk=1  SVGD({wik}Kk=1; Z~i)

end

apply SGLD to update the hyperparameter

u



u

+

t 2

1 K

N N~

N i=1

with   N (0, Id)

tt+1

K k=1

u log PZ~i|wik + u log Pwik|u

+

t 2

u

log

Pu

+

 t,

end

return sequence of last several u

H Bayesian Logistic Regression
We first consider the problem of logistic regression under the log-loss. The model class W includes a finite, discrete, set of real valued model parameters w  R. We use |W| to denote the cardinality of the model class. Each data point Z is a tuple (X, Y ) of input feature X  R and output label Y  {0, 1}. The input features are assumed to be fixed by following the discriminative model assumed by logistic
18

regression. For a model parameter w  W, the data distribution PZ|W = PY |X,W is defined as

PY |X,W = Bern Y (a(W X + b)) ,

(52)

where Bern(·|p) represents a Bernoulli distribution with probability p, (x) = 1/(1 + exp(-x)) is the sigmoid function, and a, b  R are fixed scalars.
The hyperparameter U selects a subset WU  W of d model parameters from the model class W, which will be shared by all tasks in the given environment. The space of hyperparameters U then comprises of all possible d-size subsets of W. Knowledge of the hyperparameter U reduces the search space W of the base learner to the subset WU . For the experiment setting, we consider the prior PW |U=u to be uniform over the d selected model parameters for all u  U , and the hyper-prior PU to be uniform over the space U. Further, we set a = 4, b = 0.2, d = 2 and the number of per-task samples m = 2. Figure 4 elaborates on the advantage of meta-learning over conventional learning.

Figure 4: Plots of MER for conventional learning and of MEMR for meta-learning (lower figure) with their corresponding upper bounds (12) (for MER) and (14), (15) (for MEMR) (top figure) as a function of the number of observed tasks N . The model class W = [1.6, 1.2, 0.8, 0.4, -0.4, -0.8, -1.2, -1.6], a = 4, b = 0.2,d = 2, m = 2. Conventional learning corresponds to the case when no meta-training set Z1:N is available, and the model parameter W is selected uniformly at randomly in the entire set W. As the number of available meta-training tasks N increases, meta-learning yields a lower MER than the conventional learning, confirming the results in Lemma 4.1.
I Experimental Details for Bayesian Sinusoidal Regression
We detail the essential experimental settings for reproducibility of the results.

Settings
batch size for meta-training tasks N~ number of SGLD iteration T step size t for SGLD step size for SVGD number of SVGD iteration
number of SGLD hyperposterior samples number of SVGD particles K

Value
4 33 0.3 - 0.099t/(T - 1) 0.01 20 3 3

Table 1: LS-BML for Bayesian sinusoidal regression

J Experimental Details for Bayesian Few-Shot Classification
We detail the essential experimental settings for reproducibility of the results.

19

Settings

Value

classifier architecture for C-MINE # hidden layers # hidden units step size optimizer # epoch batch size regularizer activation

MLP + Sigmoid layer 3
(64,64,1) 0.001
Adam (1 = 0.9, 2 = 0.999) 20000 64
L2 (0.001) ReLU

Table 2: classifier details in C-MINE for Bayesian sinusoidal regression

Settings
ratio between dataset used for training C-MINE and computing estimated mutual information number of different splits

Value
1:1 (use same dataset) 1

Table 3: training details for C-MINE for Bayesian sinusoidal regression

Settings
batch size for meta-training tasks N~
step size t for SGLD step size for SVGD number of SGLD iteration number of SVGD iteration number of SGLD hyperposterior samples number of SVGD particles K

Value
4 0.001N~ /N (1 + t)0.55
0.0004 100 5 5 3

Table 4: LS-BML for Bayesian few-shot classification

Settings

Value

classifier architecture for C-MINE # hidden layers
# filters, kernel size, stride step size optimizer # epoch batch size regularizer activation

1D CNN + FC layer 3
4, 16, 5 0.001 Adam (1 = 0.9, 2 = 0.999) 100
32 L2 (0.001)
ReLU

Table 5: classifier details in C-MINE for Bayesian few-shot classification

Settings

Value

compressor architecture for C-MINE # hidden layers # filters kernel size stride pooling activation

2D CNN + Flatten layer 6
(32,32,32, 32,32,4) (5, 5, 3, 3, 3, 2) (1,2,2,2,1,1) AvgPool Tanh

Table 6: compressor (CNN with random weights) details in C-MINE for Bayesian few-shot classification

20

Settings
ratio between dataset used for training C-MINE and computing estimated mutual information number of different splits

Value
1:1 (split) 100

Table 7: training details for C-MINE for Bayesian few-shot classification

21

