arXiv:2106.00592v1 [cs.CV] 1 Jun 2021

Semi-Supervised Domain Generalization with Stochastic StyleMatch
Kaiyang Zhou Chen Change Loy Ziwei Liu S-Lab, Nanyang Technological University, Singapore {kaiyang.zhou, ccloy, ziwei.liu}@ntu.edu.sg
Abstract
Most existing research on domain generalization assumes source data gathered from multiple domains are fully annotated. However, in real-world applications, we might have only a few labels available from each source domain due to high annotation cost, along with abundant unlabeled data that are much easier to obtain. In this work, we investigate semi-supervised domain generalization (SSDG), a more realistic and practical setting. Our proposed approach, StyleMatch, is inspired by FixMatch, a state-of-the-art semi-supervised learning method based on pseudolabeling, with several new ingredients tailored to solve SSDG. Specifically, 1) to mitigate overfitting in the scarce labeled source data while improving robustness against noisy pseudo labels, we introduce stochastic modeling to the classifier's weights, seen as class prototypes, with Gaussian distributions. 2) To enhance generalization under domain shift, we upgrade FixMatch's two-view consistency learning paradigm based on weak and strong augmentations to a multi-view version with style augmentation as the third complementary view. To provide a comprehensive study and evaluation, we establish two SSDG benchmarks, which cover a wide range of strong baseline methods developed in relevant areas including domain generalization and semi-supervised learning. Extensive experiments demonstrate that StyleMatch achieves the best out-of-distribution generalization performance in the low-data regime. We hope our approach and benchmarks can pave the way for future research on data-efficient and generalizable learning systems.
1 Introduction
Conventional learning algorithms rely heavily on the i.i.d. assumption that source and target data follow the same distribution. This would typically cause substantial performance drops if a sourcetrained model is tested in a new target environment. Domain generalization (DG) [4, 46] aims to solve the distribution shift problem by learning a model using only source data gathered from multiple distinct but related domains. Over the past decade, research on DG has led to a plethora of methods, such as those based on domain alignment [11, 21, 22], meta-learning [20, 2, 9], and data augmentation [30, 48, 49], to name a few.
Most existing DG methods assume data obtained from different source domains are fully annotated. In this work, we turn to a more realistic and practical setting called semi-supervised domain generalization (SSDG). SSDG is concerned with the scenarios where the source data in each domain are only partially labeled. For instance, in medical image classification tasks [24], data annotation typically requires special expertise, which is costly and time-consuming, so learning in a semi-supervised fashion is more affordable.
SSDG is closely related to DG and semi-supervised learning (SSL) [26, 34, 3, 32], yet has its distinct challenges that differ from the two relevant areas. Figure 1 depicts the differences among DG, SSL and SSDG. Besides generalizing to out-of-distribution data as in conventional DG, SSDG also needs
Preprint. Under review.

source 1 source 2

source K

<latexit sha1_base64="Qg8riARJnh/jP+TL3NYUEVZxlHE=">AAACLHicbZDLSsNAFIYn9VbrLepShMFWaEFK0oW6LLhxWcFeoIllMp22Q2eSMDNRQ8jKp3HjQl/FjYhbn8Gl0zYL2/rDwMd/zuHM+b2QUaks68PIrayurW/kNwtb2zu7e+b+QUsGkcCkiQMWiI6HJGHUJ01FFSOdUBDEPUba3vhqUm/fEyFp4N+qOCQuR0OfDihGSls987jkJGXH48ljepeU7Up6BuMZVJy01DOLVtWaCi6DnUERZGr0zB+nH+CIE19hhqTs2lao3AQJRTEjacGJJAkRHqMh6Wr0ESfSTaZnpPBUO304CIR+voJT9+9EgriUMfd0J0dqJBdrE/O/WjdSg0s3oX4YKeLj2aJBxKAK4CQT2KeCYMViDQgLqv8K8QgJhJVObm6Lx+duSHjEFBXBQ1rQUdmLwSxDq1a1z6vWTa1Yr2Wh5cEROAFlYIMLUAfXoAGaAIMn8AxewZvxYrwbn8bXrDVnZDOHYE7G9y8ko6c9</latexit>
{(x(1)

,

y(1))}

<latexit sha1_base64="0w1fBmM6KsVftGBYSxHn6XZ6Zq8=">AAACLHicbZDLSsNAFIYn9VbrLepShMFWaEFKkoW6LLhxWcFeoKllMp22Q2eSMDNRQ8jKp3HjQl/FjYhbn8Gl0zYL2/rDwMd/zuHM+b2QUaks68PIrayurW/kNwtb2zu7e+b+QVMGkcCkgQMWiLaHJGHUJw1FFSPtUBDEPUZa3vhqUm/dEyFp4N+qOCRdjoY+HVCMlLZ65nHJTcqux5PH9C4pO5X0DMYzqLhpqWcWrao1FVwGO4MiyFTvmT9uP8ARJ77CDEnZsa1QdRMkFMWMpAU3kiREeIyGpKPRR5zIbjI9I4Wn2unDQSD08xWcun8nEsSljLmnOzlSI7lYm5j/1TqRGlx2E+qHkSI+ni0aRAyqAE4ygX0qCFYs1oCwoPqvEI+QQFjp5Oa2eHzuhoRHTFERPKQFHZW9GMwyNJ2qfV61bpxizclCy4MjcALKwAYXoAauQR00AAZP4Bm8gjfjxXg3Po2vWWvOyGYOwZyM718oDKc/</latexit>
{(x(2)

,

y(2))}

и<latexitsha1_base64="IiMkOArhMDEX9rTJ1FIQyjP1tUk=">AAACFXicbVC7TgJBFJ3FF+ILtbTZCCZWZJdCLUlsLDGRRwIbMjs7CxPmsZm5qyEbfsLGQn/Fztha+yeWDrCFgCe5yck59+bee8KEMwOe9+0UNja3tneKu6W9/YPDo/LxSduoVBPaIoor3Q2xoZxJ2gIGnHYTTbEIOe2E49uZ33mk2jAlH2CS0EDgoWQxIxis1K32SaTAVAflilfz5nDXiZ+TCsrRHJR/+pEiqaASCMfG9HwvgSDDGhjhdFrqp4YmmIzxkPYslVhQE2Tze6fuhVUiN1balgR3rv6dyLAwZiJC2ykwjMyqNxP/83opxDdBxmSSApVksShOuQvKnT3vRkxTAnxiCSaa2VtdMsIaE7ARLW0JxdIPmUg5MK2epiUblb8azDpp12v+Vc27r1ca9Ty0IjpD5+gS+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrQUnnzlFS3C+fgEhMJ+i</latexit>

ии

<latexit sha1_base64="+tTnNWiT/U5sYQjIBdMRBN5kRTA=">AAACLHicbZDLSsNAFIYn9VbrLepShMFWaEFK0oW6LLgR3FSwF2himUwn7eBMEmYmaghZ+TRuXOiruBFx6zO4dHpZ2NYfBj7+cw5nzu9FjEplWR9Gbml5ZXUtv17Y2Nza3jF391oyjAUmTRyyUHQ8JAmjAWkqqhjpRIIg7jHS9u4uRvX2PRGShsGNSiLicjQIqE8xUtrqmYclJy07Hk8fs9u0fFXJTmAygYqTlXpm0apaY8FFsKdQBFM1euaP0w9xzEmgMENSdm0rUm6KhKKYkazgxJJECN+hAelqDBAn0k3HZ2TwWDt96IdCv0DBsft3IkVcyoR7upMjNZTztZH5X60bK//cTWkQxYoEeLLIjxlUIRxlAvtUEKxYogFhQfVfIR4igbDSyc1s8fjMDSmPmaIifMgKOip7PphFaNWq9mnVuq4V67VpaHlwAI5AGdjgDNTBJWiAJsDgCTyDV/BmvBjvxqfxNWnNGdOZfTAj4/sXfU2ncQ==</latexit>
{(x(K

)

,

y(K))}

<latexit sha1_base64="XqnOIgiuh40xcc46jI3WEvDB4GE=">AAACIHicbVDLSsNAFJ3UV62vVJduBluhgpSkC3VZcOOygn1AE8pkOmmHziRhZmINIZ/ixoX+ijtxqV/i0mmbhW09cOFwzr3ce48XMSqVZX0ZhY3Nre2d4m5pb//g8MgsH3dkGAtM2jhkoeh5SBJGA9JWVDHSiwRB3GOk601uZ373kQhJw+BBJRFxORoF1KcYKS0NzHLVSWuOx9On7BImF05WHZgVq27NAdeJnZMKyNEamD/OMMQxJ4HCDEnZt61IuSkSimJGspITSxIhPEEj0tc0QJxIN52fnsFzrQyhHwpdgYJz9e9EiriUCfd0J0dqLFe9mfif14+Vf+OmNIhiRQK8WOTHDKoQznKAQyoIVizRBGFB9a0Qj5FAWOm0lrZ4fOmHlMdMURFOs5KOyl4NZp10GnX7qm7dNyrNRh5aEZyCM1ADNrgGTXAHWqANMJiCZ/AK3owX4934MD4XrQUjnzkBSzC+fwHm5KMV</latexit>
{(x,

y)}

<latexit sha1_base64="cJ34lJR0/Ei2PSpPpstNxMbP9w4=">AAACGXicbVC7TgJBFJ3FF+ILtbSZCCZWZJdCLUlsLDGRR8JuyOwwwIR5bOahIZv9DRsL/RU7Y2vln1g6wBYCnuQmJ+fcm3vviRNGtfH9b6+wsbm1vVPcLe3tHxwelY9P2lpahUkLSyZVN0aaMCpIy1DDSDdRBPGYkU48uZ35nUeiNJXiwUwTEnE0EnRIMTJOCqthGsY8tVmYVfvlil/z54DrJMhJBeRo9ss/4UBiy4kwmCGte4GfmChFylDMSFYKrSYJwhM0Ij1HBeJER+n85gxeOGUAh1K5EgbO1b8TKeJaT3nsOjkyY73qzcT/vJ41w5sopSKxhgi8WDS0DBoJZwHAAVUEGzZ1BGFF3a0Qj5FC2LiYlrbEfOmHlFtmqJJPWclFFawGs07a9VpwVfPv65VGPQ+tCM7AObgEAbgGDXAHmqAFMEjAM3gFb96L9+59eJ+L1oKXz5yCJXhfv8duoZk=</latexit>
{u}

source 1

<latexit sha1_base64="Qg8riARJnh/jP+TL3NYUEVZxlHE=">AAACLHicbZDLSsNAFIYn9VbrLepShMFWaEFK0oW6LLhxWcFeoIllMp22Q2eSMDNRQ8jKp3HjQl/FjYhbn8Gl0zYL2/rDwMd/zuHM+b2QUaks68PIrayurW/kNwtb2zu7e+b+QUsGkcCkiQMWiI6HJGHUJ01FFSOdUBDEPUba3vhqUm/fEyFp4N+qOCQuR0OfDihGSls987jkJGXH48ljepeU7Up6BuMZVJy01DOLVtWaCi6DnUERZGr0zB+nH+CIE19hhqTs2lao3AQJRTEjacGJJAkRHqMh6Wr0ESfSTaZnpPBUO304CIR+voJT9+9EgriUMfd0J0dqJBdrE/O/WjdSg0s3oX4YKeLj2aJBxKAK4CQT2KeCYMViDQgLqv8K8QgJhJVObm6Lx+duSHjEFBXBQ1rQUdmLwSxDq1a1z6vWTa1Yr2Wh5cEROAFlYIMLUAfXoAGaAIMn8AxewZvxYrwbn8bXrDVnZDOHYE7G9y8ko6c9</latexit>
{(x(1)

,

y(1))}

source 2

<latexit sha1_base64="0w1fBmM6KsVftGBYSxHn6XZ6Zq8=">AAACLHicbZDLSsNAFIYn9VbrLepShMFWaEFKkoW6LLhxWcFeoKllMp22Q2eSMDNRQ8jKp3HjQl/FjYhbn8Gl0zYL2/rDwMd/zuHM+b2QUaks68PIrayurW/kNwtb2zu7e+b+QVMGkcCkgQMWiLaHJGHUJw1FFSPtUBDEPUZa3vhqUm/dEyFp4N+qOCRdjoY+HVCMlLZ65nHJTcqux5PH9C4pO5X0DMYzqLhpqWcWrao1FVwGO4MiyFTvmT9uP8ARJ77CDEnZsa1QdRMkFMWMpAU3kiREeIyGpKPRR5zIbjI9I4Wn2unDQSD08xWcun8nEsSljLmnOzlSI7lYm5j/1TqRGlx2E+qHkSI+ni0aRAyqAE4ygX0qCFYs1oCwoPqvEI+QQFjp5Oa2eHzuhoRHTFERPKQFHZW9GMwyNJ2qfV61bpxizclCy4MjcALKwAYXoAauQR00AAZP4Bm8gjfjxXg3Po2vWWvOyGYOwZyM718oDKc/</latexit>
{(x(2)

,

y(2))}

source K

<latexit sha1_base64="+tTnNWiT/U5sYQjIBdMRBN5kRTA=">AAACLHicbZDLSsNAFIYn9VbrLepShMFWaEFK0oW6LLgR3FSwF2himUwn7eBMEmYmaghZ+TRuXOiruBFx6zO4dHpZ2NYfBj7+cw5nzu9FjEplWR9Gbml5ZXUtv17Y2Nza3jF391oyjAUmTRyyUHQ8JAmjAWkqqhjpRIIg7jHS9u4uRvX2PRGShsGNSiLicjQIqE8xUtrqmYclJy07Hk8fs9u0fFXJTmAygYqTlXpm0apaY8FFsKdQBFM1euaP0w9xzEmgMENSdm0rUm6KhKKYkazgxJJECN+hAelqDBAn0k3HZ2TwWDt96IdCv0DBsft3IkVcyoR7upMjNZTztZH5X60bK//cTWkQxYoEeLLIjxlUIRxlAvtUEKxYogFhQfVfIR4igbDSyc1s8fjMDSmPmaIifMgKOip7PphFaNWq9mnVuq4V67VpaHlwAI5AGdjgDNTBJWiAJsDgCTyDV/BmvBjvxqfxNWnNGdOZfTAj4/sXfU2ncQ==</latexit>
{(x(K

)

,

y(K))}

<latexit sha1_base64="h8kYqRQd51L1uaM55hrPjt9pTXE=">AAACIXicbVC7TsMwFHV4lvIKZWSxaJHKUiUdgLESC2OR6ENqQuW4TmvVjiPbAaoov8LCAL/ChtgQP8KI22agLUe60tE59+ree4KYUaUd58taW9/Y3Nou7BR39/YPDu2jUluJRGLSwoIJ2Q2QIoxGpKWpZqQbS4J4wEgnGF9P/c4DkYqK6E5PYuJzNIxoSDHSRurbpYqXegFPk+w+rbrnmZdV+nbZqTkzwFXi5qQMcjT79o83EDjhJNKYIaV6rhNrP0VSU8xIVvQSRWKEx2hIeoZGiBPlp7PbM3hmlAEMhTQVaThT/06kiCs14YHp5EiP1LI3Ff/zeokOr/yURnGiSYTni8KEQS3gNAg4oJJgzSaGICypuRXiEZIIaxPXwpaAL/yQ8oRpKsVjVjRRucvBrJJ2veZe1JzberlRz0MrgBNwCqrABZegAW5AE7QABk/gGbyCN+vFerc+rM9565qVzxyDBVjfv1Ryo94=</latexit>
{u(1)

}

<latexit sha1_base64="pP1249E2qjSyDAYQg+WOBrcojyQ=">AAACIXicbVC7TsMwFHV4lvIKZWSxaJHKUiUZgLESC2OR6ENqSuW4TmvVjiPbAaoov8LCAL/ChtgQP8KI22agLUe60tE59+ree4KYUaUd58taW9/Y3Nou7BR39/YPDu2jUkuJRGLSxIIJ2QmQIoxGpKmpZqQTS4J4wEg7GF9P/fYDkYqK6E5PYtLjaBjRkGKkjdS3SxU/9QOeJtl9WvXOMz+r9O2yU3NmgKvEzUkZ5Gj07R9/IHDCSaQxQ0p1XSfWvRRJTTEjWdFPFIkRHqMh6RoaIU5UL53dnsEzowxgKKSpSMOZ+nciRVypCQ9MJ0d6pJa9qfif1010eNVLaRQnmkR4vihMGNQCToOAAyoJ1mxiCMKSmlshHiGJsDZxLWwJ+MIPKU+YplI8ZkUTlbsczCppeTX3oubceuW6l4dWACfgFFSBCy5BHdyABmgCDJ7AM3gFb9aL9W59WJ/z1jUrnzkGC7C+fwFWIaPf</latexit>
{u(2)

}

и<latexitsha1_base64="IiMkOArhMDEX9rTJ1FIQyjP1tUk=">AAACFXicbVC7TgJBFJ3FF+ILtbTZCCZWZJdCLUlsLDGRRwIbMjs7CxPmsZm5qyEbfsLGQn/Fztha+yeWDrCFgCe5yck59+bee8KEMwOe9+0UNja3tneKu6W9/YPDo/LxSduoVBPaIoor3Q2xoZxJ2gIGnHYTTbEIOe2E49uZ33mk2jAlH2CS0EDgoWQxIxis1K32SaTAVAflilfz5nDXiZ+TCsrRHJR/+pEiqaASCMfG9HwvgSDDGhjhdFrqp4YmmIzxkPYslVhQE2Tze6fuhVUiN1balgR3rv6dyLAwZiJC2ykwjMyqNxP/83opxDdBxmSSApVksShOuQvKnT3vRkxTAnxiCSaa2VtdMsIaE7ARLW0JxdIPmUg5MK2epiUblb8azDpp12v+Vc27r1ca9Ty0IjpD5+gS+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrQUnnzlFS3C+fgEhMJ+i</latexit>

и

и

<latexit sha1_base64="EGDs0F5H+e4oriIlk9z/rmQ7THo=">AAACIXicbVDLTsJAFJ3iC/FVcemmEUxwQ1oW6pLEjYkbTOSR0EqmwwATZqbNPFTS9FfcuNBfcWfcGX/EpQN0IeBJbnJyzr25954wpkQq1/2ycmvrG5tb+e3Czu7e/oF9WGzJSAuEmyiikeiEUGJKOG4qoijuxAJDFlLcDsdXU7/9gIUkEb9TkxgHDA45GRAElZF6drHsJ37IEp3eJ5Wbs9RPyz275FbdGZxV4mWkBDI0evaP34+QZpgrRKGUXc+NVZBAoQiiOC34WuIYojEc4q6hHDIsg2R2e+qcGqXvDCJhiitnpv6dSCCTcsJC08mgGsllbyr+53W1GlwGCeGxVpij+aKBpo6KnGkQTp8IjBSdGAKRIOZWB42ggEiZuBa2hGzhh4RpqoiIHtOCicpbDmaVtGpV77zq3tZK9VoWWh4cgxNQAR64AHVwDRqgCRB4As/gFbxZL9a79WF9zltzVjZzBBZgff8CgDij+A==</latexit>
{u(K

)

}

(a) DG: multi-source, full labels, domain shift

(b) SSL: single-source, partial labels

(c) SSDG: multi-source, partial labels, domain shift

Figure 1: Comparison between domain generalization (DG), semi-supervised learning (SSL), and semi-supervised domain generalization (SSDG).

to solve the unlabeled data issue, which greatly challenges existing DG algorithms. Unlike SSL, the unlabeled data in SSDG are much more difficult to deal with because of their heterogeneous nature--these data are drawn from different distributions.
In this paper, we address SSDG with StyleMatch, a simple yet effective approach inspired by FixMatch [32]. Specifically, we follow the two-view consistency learning paradigm proposed in FixMatch to deal with the unlabeled source data: the predictions on strongly augmented views of images are forced to match pseudo labels obtained from the weakly augmented views. StyleMatch extends FixMatch in two crucial ways to address the specific problems faced in SSDG. First, we observe that neural networks easily overfit the small labeled source data and as a result make excessive wrong predictions with over-confidence on the pseudo labels, which hurt generalization [44]. To counteract this issue, we introduce uncertainty [10, 5] to the learning by modeling the classifier's weights, seen as class prototypes, with Gaussian distributions. This allows `infinite' classifiers to be sampled from the distributions to prevent the model from converging too quickly to a local minimum.
The second extension aims to make the model more robust under domain shift, especially with limited labels. This is achieved by upgrading FixMatch's two-view consistency learning framework to a multiview version with an additional view based on style augmentation [15]--images are mapped from one source domain to another via a pretrained style transfer model. Such a new view complements the strong augmentation, which mainly considers geometrical and color intensity transformations [7], by further expanding the source data distributions [12, 49, 47] and amplifying the regularization effect brought by multi-view learning.
For evaluation, we establish two comprehensive SSDG benchmarks based on PACS [19] and OfficeHome [35], which have been widely used for evaluating DG algorithms. For the first time, our benchmarks cover a wide range of strong baselines developed in relevant areas including DG and SSL, under two challenging settings, i.e., with 5 and 10 labels per class from each source domain, respectively. The results demonstrate that our StyleMatch achieves the best out-of-distribution generalization performance in the low-data regime. Our code and benchmarks have been released to facilitate future research on SSDG.1
2 Semi-Supervised Domain Generalization
In this section, we formally define the semi-supervised domain generalization (SSDG) problem. Let X and Y denote the input and the label space respectively, a domain is defined as a joint distribution P (X, Y ) over X О Y. P (X) and P (Y ) denote the marginal distribution of X and Y , respectively. In this work, we only consider distribution shift occurring in P (X) but not in P (Y ), i.e., all domains share the same label space.
Like the conventional DG setting, we have access to K distinct but related source domains S = {Sk}Kk=1, each associated with a joint distribution P (k)(X, Y ) and P (k)(X, Y ) = P (k )(X, Y ) for k = k . Differently, only a small portion of the source data have labels, while many of them are unlabeled for which we can only access their (empirical) marginals P (X). For each source domain Sk, the labeled set is SkL = {(x(ik), yi(k))}Ni=kL1 with (x(ik), yi(k))  P (k)(X, Y ), and the unlabeled set is SkU = {(u(ik))}Ni=kU1 with u(ik)  P (k)(X). Note that NkL NkU .
1https://github.com/KaiyangZhou/ssdg-benchmark.
2

<latexitsha1_base64="axN3G2at0UbIx1xm/9R9XlP31xk=">AAACFnicbVC7TsNAEDyHVwivACWNRYREQ2SnAMpINJRBIg+UWNH5fE5OuYd1twZFVr6ChgJ+hQ7R0vInlFwSFyRhpJVGM7va3QkTzgx43rdTWFvf2Nwqbpd2dvf2D8qHRy2jUk1okyiudCfEhnImaRMYcNpJNMUi5LQdjm6mfvuRasOUvIdxQgOBB5LFjGCw0oMBlVwMNI765YpX9WZwV4mfkwrK0eiXf3qRIqmgEgjHxnR9L4EgwxoY4XRS6qWGJpiM8IB2LZVYUBNks4Mn7plVIjdW2pYEd6b+nciwMGYsQtspMAzNsjcV//O6KcTXQcZkkgKVZL4oTrkLyp1+70ZMUwJ8bAkmmtlbXTLEGhOwGS1sCcXCD5lIOTCtniYlG5W/HMwqadWq/mXVu6tV6rU8tCI6QafoHPnoCtXRLWqgJiJIoGf0it6cF+fd+XA+560FJ585Rgtwvn4BpeigfA==</latexit>stop-grad

<latexit sha1_base64="YmeJOd6tck8lLHOI1mjqwlsUMS4=">AAACpHicfVFNTxsxEPUutND0g7QcezENraCtot09UI5IufRQVSAaQMqG1azXSSzs9dYeg6LV/oz+uP4TJC51QlQ1AXUkS09v3tN43uSVFBaj6HcQrq0/ebqx+az1/MXLV1vt12/OrHaG8T7TUpuLHCyXouR9FCj5RWU4qFzy8/yqN+ufX3NjhS5/4LTiQwXjUowEA/RU1v61myrACQNZf2/20lzVqXJNFn+mc2zFWEFzmWTxPk1/Oijoo/JkVZ78lU8KjfZ/3t6qt7e/m7U7UTeaF30I4gXokEUdZ+3btNDMKV4ik2DtII4qHNZgUDDJm1bqLK+AXcGYDzwsQXE7rOfxNfS9Zwo60sa/Eumc/ddRg7J2qnKvnC1gV3sz8rHewOHocFiLsnLIS3Y/aOQkRU1nt6CFMJyhnHoAzAj/V8omYIChv9jSlFwt7VArJ1EYfdO0fFTxajAPwVnSjQ+60UnSOUoWoW2St+Qd2SMx+UKOyFdyTPqEkbtgJ/gYfAo/hN/C07B/Lw2DhWebLFV4+QcvqdCL</latexit>
N

(х1,

2 1

)

N (х2,

2 2

)

...

N (хC ,

2 C

)

<latexit sha1_base64="6qSodmPmgPBOm6b2Il2fREWy1pc=">AAACF3icbVA9TwJBEJ3zE/ELtbTZCCZW5I5CLUlsLDHhK8KF7C0LbNjdu+zuScjl/oWNhf4VO2Nr6T+xdIErBHzJJC/vzWRmXhBxpo3rfjsbm1vbO7u5vfz+weHRceHktKnDWBHaICEPVTvAmnImacMww2k7UhSLgNNWML6b+a0nqjQLZd1MI+oLPJRswAg2Vnos1XvJhOJxWuoVim7ZnQOtEy8jRchQ6xV+uv2QxIJKQzjWuuO5kfETrAwjnKb5bqxphMkYD2nHUokF1X4yvzhFl1bpo0GobEmD5urfiQQLracisJ0Cm5Fe9Wbif14nNoNbP2Eyig2VZLFoEHNkQjR7H/WZosTwqSWYKGZvRWSEFSbGhrS0JRBLPyQi5oapcJLmbVTeajDrpFkpe9dl96FSrFay0HJwDhdwBR7cQBXuoQYNICDhGV7hzXlx3p0P53PRuuFkM2ewBOfrF9ymoJA=</latexit>

Tw

eak

<latexit sha1_base64="EREzDTmIm9tJusBRgmwcQLOBU8U=">AAACP3icbVDLTgIxFO3gC8cX6tJNIzFhRWZYqEsSNy4xkUfCENIpF2hoO5O2IyETfsOvceNC/8EvcGdc6s4CYyLgTZqcnHPuvb0njDnTxvPenNzG5tb2Tn7X3ds/ODwqHJ80dJQoCnUa8Ui1QqKBMwl1wwyHVqyAiJBDMxzdzPTmAyjNInlvJjF0BBlI1meUGEt1C14QwoDJlII0oKbuGMgIB4FLkoGw1NzlBiB7v45uoeiVvXnhdeBnoIiyqnULX0EvoslsGuVE67bvxaaTEmUY5TB1g0RDTOiIDKBtoSQCdCedXzbFF5bp4X6k7JMGz9m/HSkRWk9EaJ2CmKFe1Wbkf1o7Mf3rTspknBiQdLGon3BsIjyLCfeYAmr4xAJCFbN/xXRIFKE2g+UtoVi6IRUJN0xF46lro/JXg1kHjUrZvyx7d5VitZSFlkdn6ByVkI+uUBXdohqqI4oe0RN6Qa/Os/PufDifC2vOyXpO0VI53z9nA7Eo</latexit>

weak

augmentation

CNN <latexitsha1_base64="Lsp1XkD8Mo2+aXtMvAN3srZjb/A=">AAACEHicbVC7TgJBFL2LL8QXamkzkZhYkV0KtSShsSIY5ZHAhswOszBhZnYzM6shGz7BxkJ/xc7Y+gf+iaUDbCHgSW5ycs69ufeeIOZMG9f9dnIbm1vbO/ndwt7+weFR8fikpaNEEdokEY9UJ8CaciZp0zDDaSdWFIuA03Ywrs389iNVmkXywUxi6gs8lCxkBBsr3dfq9X6x5JbdOdA68TJSggyNfvGnN4hIIqg0hGOtu54bGz/FyjDC6bTQSzSNMRnjIe1aKrGg2k/np07RhVUGKIyULWnQXP07kWKh9UQEtlNgM9Kr3kz8z+smJrzxUybjxFBJFovChCMTodnfaMAUJYZPLMFEMXsrIiOsMDE2naUtgVj6IRUJN0xFT9OCjcpbDWadtCpl76rs3lVK1UoWWh7O4BwuwYNrqMItNKAJBIbwDK/w5rw4786H87lozTnZzCkswfn6BT0PnY4=</latexit>

S NN <latexitsha1_base64="Ug0sqPgjDqWOtBoOPzhneJel2QY=">AAACEHicbVC7TgJBFJ3FF+ILtbSZSEysyC6FWpLYWBEM8khgQ2aHWZgwM7uZuashGz7BxkJ/xc7Y+gf+iaUDbCHgSW5ycs69ufeeIBbcgOt+O7mNza3tnfxuYW//4PCoeHzSMlGiKWvSSES6ExDDBFesCRwE68SaERkI1g7GtzO//ci04ZF6gEnMfEmGioecErBSo1Gr9Yslt+zOgdeJl5ESylDvF396g4gmkimgghjT9dwY/JRo4FSwaaGXGBYTOiZD1rVUEcmMn85PneILqwxwGGlbCvBc/TuREmnMRAa2UxIYmVVvJv7ndRMIb/yUqzgBpuhiUZgIDBGe/Y0HXDMKYmIJoZrbWzEdEU0o2HSWtgRy6YdUJgK4jp6mBRuVtxrMOmlVyt5V2b2vlKqVLLQ8OkPn6BJ56BpV0R2qoyaiaIie0St6c16cd+fD+Vy05pxs5hQtwfn6BVfPnZ4=</latexit>

...<latexitsha1_base64="sNIlQgx8hIV6UFSmiPRz+aChMao=">AAACFXicbVC7TgJBFJ31ifhCLW02gokV2aVQSxIbS0zkkcCGzM7OwoR5bGbuYsiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktGVUqgltEsWV7oTYUM4kbQIDTjuJpliEnLbD0d3Mb4+pNkzJR5gkNBB4IFnMCAYrdSq9caTAVPqlslf15nDXiZ+TMsrR6Jd+epEiqaASCMfGdH0vgSDDGhjhdFrspYYmmIzwgHYtlVhQE2Tze6fupVUiN1balgR3rv6dyLAwZiJC2ykwDM2qNxP/87opxLdBxmSSApVksShOuQvKnT3vRkxTAnxiCSaa2VtdMsQaE7ARLW0JxdIPmUg5MK2epkUblb8azDpp1ar+ddV7qJXrtTy0AjpHF+gK+egG1dE9aqAmIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QJBLZ+1</latexit>

pseudo <latexitsha1_base64="HND58xq/u2MyA+NjmI4rt6ZocIk=">AAACOnicbVC7SgNBFJ31GddX1NJmMAiCEHZTqGXAxjKCSYTsEmZnb+LgPJaZWSUs+xF+jY2FfoatndhaWDqJEYx6YOBwzj3cuSfJODM2CJ69ufmFxaXlyoq/ura+sVnd2u4YlWsKbaq40pcJMcCZhLZllsNlpoGIhEM3uT4d+90b0IYpeWFHGcSCDCUbMEqsk/rVwyiBIZMFBWlBl35mIE8VjiKfkwS4H4FMv81+tRbUgwnwXxJOSQ1N0epXP6JU0Vy4OOXEmF4YZDYuiLaMcij9KDeQEXpNhtBzVBIBJi4mR5V43ykpHijtnrR4ov5MFEQYMxKJmxTEXpnf3lj8z+vldnASF0xmuQVJvxYNco6twuOGcMo0UMtHjhCqmfsrpldEE+o6mN2SiJkbCpFzy7S6LX1XVfi7mL+k06iHR/XgvFFrNqalVdAu2kMHKETHqInOUAu1EUV36B49oifvwXvxXr23r9E5b5rZQTPw3j8Bx8yu3A==</latexit> label

<latexitsha1_base64="nK9x8GIy7xu0LXRi38onVkx10hA=">AAACE3icbVC7TgJBFL3rE/GFWtpMBBMrskuhliQ2lpjIIwFCZodZGJnHZmZWQzb8g42F/oqdsfUD/BNLB9hCwJPc5OSce3PvPWHMmbG+/+2trW9sbm3ndvK7e/sHh4Wj44ZRiSa0ThRXuhViQzmTtG6Z5bQVa4pFyGkzHN1M/eYj1YYpeW/HMe0KPJAsYgRbJzVKHcNEqVco+mV/BrRKgowUIUOtV/jp9BVJBJWWcGxMO/Bj202xtoxwOsl3EkNjTEZ4QNuOSiyo6aazayfo3Cl9FCntSlo0U/9OpFgYMxah6xTYDs2yNxX/89qJja67KZNxYqkk80VRwpFVaPo66jNNieVjRzDRzN2KyBBrTKwLaGFLKBZ+SEXCLdPqaZJ3UQXLwaySRqUcXJb9u0qxWslCy8EpnMEFBHAFVbiFGtSBwAM8wyu8eS/eu/fhfc5b17xs5gQW4H39AnGRnro=</latexit> <latexitsha1_base64="nK9x8GIy7xu0LXRi38onVkx10hA=">AAACE3icbVC7TgJBFL3rE/GFWtpMBBMrskuhliQ2lpjIIwFCZodZGJnHZmZWQzb8g42F/oqdsfUD/BNLB9hCwJPc5OSce3PvPWHMmbG+/+2trW9sbm3ndvK7e/sHh4Wj44ZRiSa0ThRXuhViQzmTtG6Z5bQVa4pFyGkzHN1M/eYj1YYpeW/HMe0KPJAsYgRbJzVKHcNEqVco+mV/BrRKgowUIUOtV/jp9BVJBJWWcGxMO/Bj202xtoxwOsl3EkNjTEZ4QNuOSiyo6aazayfo3Cl9FCntSlo0U/9OpFgYMxah6xTYDs2yNxX/89qJja67KZNxYqkk80VRwpFVaPo66jNNieVjRzDRzN2KyBBrTKwLaGFLKBZ+SEXCLdPqaZJ3UQXLwaySRqUcXJb9u0qxWslCy8EpnMEFBHAFVbiFGtSBwAM8wyu8eS/eu/fhfc5b17xs5gQW4H39AnGRnro=</latexit> <latexitsha1_base64="nK9x8GIy7xu0LXRi38onVkx10hA=">AAACE3icbVC7TgJBFL3rE/GFWtpMBBMrskuhliQ2lpjIIwFCZodZGJnHZmZWQzb8g42F/oqdsfUD/BNLB9hCwJPc5OSce3PvPWHMmbG+/+2trW9sbm3ndvK7e/sHh4Wj44ZRiSa0ThRXuhViQzmTtG6Z5bQVa4pFyGkzHN1M/eYj1YYpeW/HMe0KPJAsYgRbJzVKHcNEqVco+mV/BrRKgowUIUOtV/jp9BVJBJWWcGxMO/Bj202xtoxwOsl3EkNjTEZ4QNuOSiyo6aazayfo3Cl9FCntSlo0U/9OpFgYMxah6xTYDs2yNxX/89qJja67KZNxYqkk80VRwpFVaPo66jNNieVjRzDRzN2KyBBrTKwLaGFLKBZ+SEXCLdPqaZJ3UQXLwaySRqUcXJb9u0qxWslCy8EpnMEFBHAFVbiFGtSBwAM8wyu8eS/eu/fhfc5b17xs5gQW4H39AnGRnro=</latexit>

features <latexitsha1_base64="NC+ahFZ+1YhFteHY/ZOXfb/+CoE=">AAACFXicbVA9SwNBEN2LXzF+RS1tDoNgFe5SqGXAxjKC+YDkCHubuWTJ7t6xO6eEI3/CxkL/ip3YWvtPLN0kV5jEBwOP92aYmRcmghv0vG+nsLG5tb1T3C3t7R8cHpWPT1omTjWDJotFrDshNSC4giZyFNBJNFAZCmiH49uZ334EbXisHnCSQCDpUPGIM4pW6kRAMdVg+uWKV/XmcNeJn5MKydHol396g5ilEhQyQY3p+l6CQUY1ciZgWuqlBhLKxnQIXUsVlWCCbH7v1L2wysCNYm1LoTtX/05kVBozkaHtlBRHZtWbif953RSjmyDjKkkRFFssilLhYuzOnncHXANDMbGEMs3trS4bUU0Z2oiWtoRy6YdMpgK5jp+mJRuVvxrMOmnVqv5V1buvVeq1PLQiOSPn5JL45JrUyR1pkCZhRJBn8krenBfn3flwPhetBSefOSVLcL5+ASploEA=</latexit>

.<latexitsha1_base64="hsys18mVu7amo+1M6IGIQTLmg7s=">AAACFXicbVC7TgJBFJ3FF+ILtbTZCCZWZJdCLUlsLDGRRwIbMjs7CxPmsZm5qyEbfsLGQn/Fztha+yeWDrCFgCe5yck59+bee8KEMwOe9+0UNja3tneKu6W9/YPDo/LxSduoVBPaIoor3Q2xoZxJ2gIGnHYTTbEIOe2E49uZ33mk2jAlH2CS0EDgoWQxIxis1K32R5ECUx2UK17Nm8NdJ35OKihHc1D+6UeKpIJKIBwb0/O9BIIMa2CE02mpnxqaYDLGQ9qzVGJBTZDN7526F1aJ3FhpWxLcufp3IsPCmIkIbafAMDKr3kz8z+ulEN8EGZNJClSSxaI45S4od/a8GzFNCfCJJZhoZm91yQhrTMBGtLQlFEs/ZCLlwLR6mpZsVP5qMOukXa/5VzXvvl5p1PPQiugMnaNL5KNr1EB3qIlaiCCOntErenNenHfnw/lctBacfOYULcH5+gUpm5+n</latexit>

.

.

<latexitsha1_base64="fIcSXyORw5DHJoLcneax25hp1bU=">AAACFnicbVC7TgJBFJ31ifhCLW0mgokV2aVQSxIbS0zkYWBDZocBJsxjM3NXQzZ8hY2F/oqdsbX1TywdYAsBb3KTk3Puzbn3RLHgFnz/21tb39jc2s7t5Hf39g8OC0fHDasTQ1mdaqFNKyKWCa5YHTgI1ooNIzISrBmNbqZ685EZy7W6h3HMQkkGivc5JeCoh1JHA5fMlrqFol/2Z4VXQZCBIsqq1i38dHqaJpIpoIJY2w78GMKUGOBUsEm+k1gWEzoiA9Z2UBHnEqazgyf43DE93NfGtQI8Y/9upERaO5aRm5QEhnZZm5L/ae0E+tdhylWcAFN0btRPBAaNp9/jHjeMghg7QKjh7lZMh8QQCi6jBZdILvyQykQAN/ppkndRBcvBrIJGpRxclv27SrFayULLoVN0hi5QgK5QFd2iGqojiiR6Rq/ozXvx3r0P73M+uuZlOydoobyvXwq+oCA=</latexit>

<latexit sha1_base64="wkAs00SPEGcSgXAkdv+mxCcqneM=">AAACP3icbVDLSgMxFM3UVx1fVZdugkVwVWa6UJcFNy4r2Ad0Ssmkt21oHkOSUcrQ3/Br3LjQf/AL3IlL3Zk+BNt6IHA459xc7okTzowNgjcvt7a+sbmV3/Z3dvf2DwqHR3WjUk2hRhVXuhkTA5xJqFlmOTQTDUTEHBrx8HriN+5BG6bknR0l0BakL1mPUWKd1CkEUQx9JjMK0oIe+1QZ9xOOIt8wwTjRzI78CGT3N9EpFINSMAVeJeGcFNEc1U7hK+oqmgo3TjkxphUGiW1nRFtGOYz9KDWQEDokfWg5KokA086ml43xmVO6uKe0e9Liqfp3IiPCmJGIXVIQOzDL3kT8z2ultnfVzphMUguSzhb1Uo6twpOacJdpoJaPHCHUlcAopgOiCXUdLG6JxcINmUi5ZVo9jH1XVbhczCqpl0vhRSm4LRcr5XlpeXSCTtE5CtElqqAbVEU1RNEjekIv6NV79t69D+9zFs1585ljtADv+weOT7FG</latexit>

cosine

similarity

...<latexitsha1_base64="sNIlQgx8hIV6UFSmiPRz+aChMao=">AAACFXicbVC7TgJBFJ31ifhCLW02gokV2aVQSxIbS0zkkcCGzM7OwoR5bGbuYsiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktGVUqgltEsWV7oTYUM4kbQIDTjuJpliEnLbD0d3Mb4+pNkzJR5gkNBB4IFnMCAYrdSq9caTAVPqlslf15nDXiZ+TMsrR6Jd+epEiqaASCMfGdH0vgSDDGhjhdFrspYYmmIzwgHYtlVhQE2Tze6fupVUiN1balgR3rv6dyLAwZiJC2ykwDM2qNxP/87opxLdBxmSSApVksShOuQvKnT3vRkxTAnxiCSaa2VtdMsQaE7ARLW0JxdIPmUg5MK2epkUblb8azDpp1ar+ddV7qJXrtTy0AjpHF+gK+egG1dE9aqAmIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QJBLZ+1</latexit>

<latexit sha1_base64="8T10E7u/YJ4XZvCyRN/xX0m/8S0=">AAACPnicbVC7SgNBFJ31GddX1NJmMAhWcTeFWgZsLCOYB2RDmJ3cJEPmsczMKmHJZ/g1Nhb6Ef6AndimdPIQTOKFgcO599w758QJZ8YGwYe3tr6xubWd2/F39/YPDvNHxzWjUk2hShVXuhETA5xJqFpmOTQSDUTEHOrx4HbSrz+CNkzJBztMoCVIT7Iuo8Q6qp2/jGLoMZlRkBb0yKecGIOjyE+0smqiMH4EsvM70M4XgmIwLbwKwjkooHlV2vlx1FE0FU4+3d0Mg8S2MqItoxxGfpQaSAgdkB40HZREgGllU2MjfO6YDu4q7Z60eMr+VWREGDMUsZsUxPbNcm9C/tdrprZ708qYTFILks4OdVOOrcITz7jDNFDLhw4Qqpn7K6Z9ogl1GSxeicWCh0yk3DKtnka+iypcDmYV1ErF8KoY3JcK5dI8tBw6RWfoAoXoGpXRHaqgKqLoGb2gN/TuvXqf3pf3PRtd8+aaE7RQ3vgH3nuw8w==</latexit>

class

prototypes

softmax <latexitsha1_base64="mVTwJXzlGkz5CaRd6k0JyNh7kWk=">AAACFHicbVC7SgNBFJ31GeMramkzGASrsJtCLQM2lhHMA5IlzE5mkyHzWGbuqmHJR9hY6K/Yia29f2LpJNnCJB64cDjnXu69J0oEt+D7397a+sbm1nZhp7i7t39wWDo6blqdGsoaVAtt2hGxTHDFGsBBsHZiGJGRYK1odDP1Ww/MWK7VPYwTFkoyUDzmlICTWlbHIMlTr1T2K/4MeJUEOSmjHPVe6afb1zSVTAEVxNpO4CcQZsQAp4JNit3UsoTQERmwjqOKSGbDbHbuBJ87pY9jbVwpwDP170RGpLVjGblOSWBol72p+J/XSSG+DjOukhSYovNFcSowaDz9Hfe5YRTE2BFCDXe3YjokhlBwCS1sieTCD5lMBXCjHydFF1WwHMwqaVYrwWXFv6uWa9U8tAI6RWfoAgXoCtXQLaqjBqJohJ7RK3rzXrx378P7nLeuefnMCVqA9/ULaAWf2Q==</latexit>

...<latexitsha1_base64="sNIlQgx8hIV6UFSmiPRz+aChMao=">AAACFXicbVC7TgJBFJ31ifhCLW02gokV2aVQSxIbS0zkkcCGzM7OwoR5bGbuYsiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktGVUqgltEsWV7oTYUM4kbQIDTjuJpliEnLbD0d3Mb4+pNkzJR5gkNBB4IFnMCAYrdSq9caTAVPqlslf15nDXiZ+TMsrR6Jd+epEiqaASCMfGdH0vgSDDGhjhdFrspYYmmIzwgHYtlVhQE2Tze6fupVUiN1balgR3rv6dyLAwZiJC2ykwDM2qNxP/87opxLdBxmSSApVksShOuQvKnT3vRkxTAnxiCSaa2VtdMsQaE7ARLW0JxdIPmUg5MK2epkUblb8azDpp1ar+ddV7qJXrtTy0AjpHF+gK+egG1dE9aqAmIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QJBLZ+1</latexit>

u n l ab <latexitsha1_base64="1spd4zhJeAQoWO2oBlbQKhqlvjM=">AAACPXicbVDLSgMxFM3UVx1fVZdugkVwVWa6UJcFNy4r2Ad0SslkbtvQJDMkGaUM/Qu/xo0L/Qk/wJ241aVpO4JtvRA4nAc394QJZ9p43ptTWFvf2Nwqbrs7u3v7B6XDo6aOU0WhQWMeq3ZINHAmoWGY4dBOFBARcmiFo+up3roHpVks78w4ga4gA8n6jBJjqV6pEoQwYDKjIA2oiZtKTkLgEOEgcJk1gxuAjH71XqnsVbzZ4FXg56CM8qn3St9BFNNU2DjlROuO7yWmmxFlGOUwcYNUQ0LoyC7qWCiJAN3NZndN8JllItyPlX3S4Bn7N5ERofVYhNYpiBnqZW1K/qd1UtO/6mZMJqkBSeeL+inHJsbTknDEFFDDxxYQqpj9K6ZDogi1HSxuCcXCDZlIuWEqfpi4tip/uZhV0KxW/IuKd1st16p5aUV0gk7ROfLRJaqhG1RHDUTRI3pCL+jVeXbenQ/nc24tOHnmGC2M8/UDSPGwGQ==</latexit>

eled

image

probability <latexitsha1_base64="uNsontWhKMxqbUrCoTFt/9Fzx7c=">AAACGHicbVC7TsNAEDyHVwivACWNRYREFdkpgDISDWWQyENyrOh8OSen3MO6W4MsK59BQwG/Qodo6fgTSi6JC5Iw0kqjmV3t7kQJZwY879spbWxube+Udyt7+weHR9Xjk45RqSa0TRRXuhdhQzmTtA0MOO0lmmIRcdqNJrczv/tItWFKPkCW0FDgkWQxIxisFCRaRThinEE2qNa8ujeHu078gtRQgdag+tMfKpIKKoFwbEzgewmEOdbACKfTSj81NMFkgkc0sFRiQU2Yz0+euhdWGbqx0rYkuHP170SOhTGZiGynwDA2q95M/M8LUohvwpzJJAUqyWJRnHIXlDv73x0yTQnwzBJMNLO3umSMNSZgU1raEomlH3KRcmBaPU0rNip/NZh10mnU/au6d9+oNRtFaGV0hs7RJfLRNWqiO9RCbUSQQs/oFb05L8678+F8LlpLTjFzipbgfP0CvQahoA==</latexit>

<latexit sha1_base64="M+6i4H0znQOGaOwCZWjvCOTiP3M=">AAACGXicbVC7TsNAEDzzDOEVoKSxSJCoIjsFUEaioQxSXlJiRefLOTnlHtbdGhRZ/g0aCvgVOkRLxZ9QcklckISRVhrN7Gp3J4w5M+B5387G5tb2zm5hr7h/cHh0XDo5bRuVaEJbRHGluyE2lDNJW8CA026sKRYhp51wcjfzO49UG6ZkE6YxDQQeSRYxgsFK/UpzkBrQSo6yyqBU9qreHO468XNSRjkag9JPf6hIIqgEwrExPd+LIUixBkY4zYr9xNAYkwke0Z6lEgtqgnR+c+ZeWmXoRkrbkuDO1b8TKRbGTEVoOwWGsVn1ZuJ/Xi+B6DZImYwToJIsFkUJd0G5swDcIdOUAJ9agolm9laXjLHGBGxMS1tCsfRDKhIOTKunrGij8leDWSftWtW/rnoPtXK9lodWQOfoAl0hH92gOrpHDdRCBMXoGb2iN+fFeXc+nM9F64aTz5yhJThfv8dfoZk=</latexit>

Tstr

ong

<latexit sha1_base64="x9dJkvxSbfy/URldKhhWykauq+s=">AAACQXicbVC7SgNBFJ31GddX1NJmMAipwm4KtRRsLBXMA7IhzE5uNkPmsczMKmHJf/g1Nhb6C36CnVgJNk4egkm8MHA459x755445czYIHjzVlbX1jc2C1v+9s7u3n7x4LBuVKYp1KjiSjdjYoAzCTXLLIdmqoGImEMjHlyN9cY9aMOUvLPDFNqCJJL1GCXWUZ1iNYohYTKnIC3okW+sVjLBUeSTLBGOnPj8CGT319MploJKMCm8DMIZKKFZ3XSKX1FX0Ww8jXJiTCsMUtvOibaMchj5UWYgJXRAEmg5KIkA084nt43wqWO6uKe0e9LiCfu3IyfCmKGInVMQ2zeL2pj8T2tltnfRzplMMwuSThf1Mo6twuOgcJdpoJYPHSBUM/dXTPtEE+oymN8Si7kbcpFxy7R6GPkuqnAxmGVQr1bCs0pwWy1dlmehFdAxOkFlFKJzdImu0Q2qIYoe0RN6Qa/es/fufXifU+uKN+s5QnPlff8AdY2yMQ==</latexit>

strong

augmentation

<latexit sha1_base64="rxVQ3ouoU5C5bV690F+J9WXgsYg=">AAACGHicbVC7TgJBFJ31ifhCLW02gokV2aVQSxIbS0x4JcuGzA4DTJjHZuauZrPhM2ws9FfsjK2df2LpAFsIeJJJTs65d+69J4o5M+B5387G5tb2zm5hr7h/cHh0XDo5bRuVaEJbRHGluxE2lDNJW8CA026sKRYRp51ocjfzO49UG6ZkE9KYhgKPJBsygsFKQaXZzwyknE4r/VLZq3pzuOvEz0kZ5Wj0Sz+9gSKJoBIIx8YEvhdDmGENjNgPi73E0BiTCR7RwFKJBTVhNl956l5aZeAOlbZPgjtX/3ZkWBiTishWCgxjs+rNxP+8IIHhbZgxGSdAJVkMGibcBeXO7ncHTFMCPLUEE83sri4ZY40J2JSWpkRi6YZMJByYVk/Too3KXw1mnbRrVf+66j3UyvVaHloBnaMLdIV8dIPq6B41UAsRpNAzekVvzovz7nw4n4vSDSfvOUNLcL5+AeoloSM=</latexit>

Tsty

le

<latexit sha1_base64="qh/GNBJbuDBCMrNjeHZzSYoiHFY=">AAACQHicbVDLSgMxFM34rOOr6tJNsAhdlZku1GXBjcsK9gGdUjLpbRuaZIYko5RhvsOvceNCv8E/cCfuxJXptIJtvRA4nHPuvbknjDnTxvPenLX1jc2t7cKOu7u3f3BYPDpu6ihRFBo04pFqh0QDZxIahhkO7VgBESGHVji+nuqte1CaRfLOTGLoCjKUbMAoMZbqFf0ghCGTKQVpQGWuNhMOOAhckgyF5XKbG4Ds/1p6xZJX8fLCq8CfgxKaV71X/Ar6EU2m0ygnWnd8LzbdlCjDKIfMDRINMaFjMoSOhZII0N00Py3D55bp40Gk7JMG5+zfjpQIrScitE5BzEgva1PyP62TmMFVN2UyTgxIOls0SDg2EZ7mhPtMATV8YgGhitm/YjoiilCbweKWUCzckIqEG6aih8y1UfnLwayCZrXiX1S822qpVp6HVkCn6AyVkY8uUQ3doDpqIIoe0RN6Qa/Os/PufDifM+uaM+85QQvlfP8Ah/Kxuw==</latexit>

style

augmentation

CNN <latexitsha1_base64="Lsp1XkD8Mo2+aXtMvAN3srZjb/A=">AAACEHicbVC7TgJBFL2LL8QXamkzkZhYkV0KtSShsSIY5ZHAhswOszBhZnYzM6shGz7BxkJ/xc7Y+gf+iaUDbCHgSW5ycs69ufeeIOZMG9f9dnIbm1vbO/ndwt7+weFR8fikpaNEEdokEY9UJ8CaciZp0zDDaSdWFIuA03Ywrs389iNVmkXywUxi6gs8lCxkBBsr3dfq9X6x5JbdOdA68TJSggyNfvGnN4hIIqg0hGOtu54bGz/FyjDC6bTQSzSNMRnjIe1aKrGg2k/np07RhVUGKIyULWnQXP07kWKh9UQEtlNgM9Kr3kz8z+smJrzxUybjxFBJFovChCMTodnfaMAUJYZPLMFEMXsrIiOsMDE2naUtgVj6IRUJN0xFT9OCjcpbDWadtCpl76rs3lVK1UoWWh7O4BwuwYNrqMItNKAJBIbwDK/w5rw4786H87lozTnZzCkswfn6BT0PnY4=</latexit>

S NN <latexitsha1_base64="Ug0sqPgjDqWOtBoOPzhneJel2QY=">AAACEHicbVC7TgJBFJ3FF+ILtbSZSEysyC6FWpLYWBEM8khgQ2aHWZgwM7uZuashGz7BxkJ/xc7Y+gf+iaUDbCHgSW5ycs69ufeeIBbcgOt+O7mNza3tnfxuYW//4PCoeHzSMlGiKWvSSES6ExDDBFesCRwE68SaERkI1g7GtzO//ci04ZF6gEnMfEmGioecErBSo1Gr9Yslt+zOgdeJl5ESylDvF396g4gmkimgghjT9dwY/JRo4FSwaaGXGBYTOiZD1rVUEcmMn85PneILqwxwGGlbCvBc/TuREmnMRAa2UxIYmVVvJv7ndRMIb/yUqzgBpuhiUZgIDBGe/Y0HXDMKYmIJoZrbWzEdEU0o2HSWtgRy6YdUJgK4jp6mBRuVtxrMOmlVyt5V2b2vlKqVLLQ8OkPn6BJ56BpV0R2qoyaiaIie0St6c16cd+fD+Vy05pxs5hQtwfn6BVfPnZ4=</latexit>

...<latexitsha1_base64="sNIlQgx8hIV6UFSmiPRz+aChMao=">AAACFXicbVC7TgJBFJ31ifhCLW02gokV2aVQSxIbS0zkkcCGzM7OwoR5bGbuYsiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktGVUqgltEsWV7oTYUM4kbQIDTjuJpliEnLbD0d3Mb4+pNkzJR5gkNBB4IFnMCAYrdSq9caTAVPqlslf15nDXiZ+TMsrR6Jd+epEiqaASCMfGdH0vgSDDGhjhdFrspYYmmIzwgHYtlVhQE2Tze6fupVUiN1balgR3rv6dyLAwZiJC2ykwDM2qNxP/87opxLdBxmSSApVksShOuQvKnT3vRkxTAnxiCSaa2VtdMsQaE7ARLW0JxdIPmUg5MK2epkUblb8azDpp1ar+ddV7qJXrtTy0AjpHF+gK+egG1dE9aqAmIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QJBLZ+1</latexit>

<latexit sha1_base64="EMNpZ0qvD4fEJxiAFue4vzILbJE=">AAACHnicbVC7TgJBFJ3FF+KDVUubjWBiRXYplJLExhITeSSw2cwOA0yYx2bmroZs+BIbC/0VO2Orf2Lp8CgEPMlNTs65N/feEyecGfD9bye3tb2zu5ffLxwcHh0X3ZPTllGpJrRJFFe6E2NDOZO0CQw47SSaYhFz2o7HtzO//Ui1YUo+wCShocBDyQaMYLBS5BbLPcp5lBnQSg6n5cgt+RV/Dm+TBEtSQks0Iven11ckFVQC4diYbuAnEGZYAyOcTgu91NAEkzEe0q6lEgtqwmx++NS7tErfGyhtS4I3V/9OZFgYMxGx7RQYRmbdm4n/ed0UBrUwYzJJgUqyWDRIuQfKm6Xg9ZmmBPjEEkw0s7d6ZIQ1JmCzWtkSi5UfMpFyYFo9TQs2qmA9mE3SqlaC64p/Xy3Va8vQ8ugcXaArFKAbVEd3qIGaiKAUPaNX9Oa8OO/Oh/O5aM05y5kztALn6xfhSaMz</latexit>

`strong

CNN <latexitsha1_base64="Lsp1XkD8Mo2+aXtMvAN3srZjb/A=">AAACEHicbVC7TgJBFL2LL8QXamkzkZhYkV0KtSShsSIY5ZHAhswOszBhZnYzM6shGz7BxkJ/xc7Y+gf+iaUDbCHgSW5ycs69ufeeIOZMG9f9dnIbm1vbO/ndwt7+weFR8fikpaNEEdokEY9UJ8CaciZp0zDDaSdWFIuA03Ywrs389iNVmkXywUxi6gs8lCxkBBsr3dfq9X6x5JbdOdA68TJSggyNfvGnN4hIIqg0hGOtu54bGz/FyjDC6bTQSzSNMRnjIe1aKrGg2k/np07RhVUGKIyULWnQXP07kWKh9UQEtlNgM9Kr3kz8z+smJrzxUybjxFBJFovChCMTodnfaMAUJYZPLMFEMXsrIiOsMDE2naUtgVj6IRUJN0xFT9OCjcpbDWadtCpl76rs3lVK1UoWWh7O4BwuwYNrqMItNKAJBIbwDK/w5rw4786H87lozTnZzCkswfn6BT0PnY4=</latexit>

S NN <latexitsha1_base64="Ug0sqPgjDqWOtBoOPzhneJel2QY=">AAACEHicbVC7TgJBFJ3FF+ILtbSZSEysyC6FWpLYWBEM8khgQ2aHWZgwM7uZuashGz7BxkJ/xc7Y+gf+iaUDbCHgSW5ycs69ufeeIBbcgOt+O7mNza3tnfxuYW//4PCoeHzSMlGiKWvSSES6ExDDBFesCRwE68SaERkI1g7GtzO//ci04ZF6gEnMfEmGioecErBSo1Gr9Yslt+zOgdeJl5ESylDvF396g4gmkimgghjT9dwY/JRo4FSwaaGXGBYTOiZD1rVUEcmMn85PneILqwxwGGlbCvBc/TuREmnMRAa2UxIYmVVvJv7ndRMIb/yUqzgBpuhiUZgIDBGe/Y0HXDMKYmIJoZrbWzEdEU0o2HSWtgRy6YdUJgK4jp6mBRuVtxrMOmlVyt5V2b2vlKqVLLQ8OkPn6BJ56BpV0R2qoyaiaIie0St6c16cd+fD+Vy05pxs5hQtwfn6BVfPnZ4=</latexit>

...<latexitsha1_base64="sNIlQgx8hIV6UFSmiPRz+aChMao=">AAACFXicbVC7TgJBFJ31ifhCLW02gokV2aVQSxIbS0zkkcCGzM7OwoR5bGbuYsiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktGVUqgltEsWV7oTYUM4kbQIDTjuJpliEnLbD0d3Mb4+pNkzJR5gkNBB4IFnMCAYrdSq9caTAVPqlslf15nDXiZ+TMsrR6Jd+epEiqaASCMfGdH0vgSDDGhjhdFrspYYmmIzwgHYtlVhQE2Tze6fupVUiN1balgR3rv6dyLAwZiJC2ykwDM2qNxP/87opxLdBxmSSApVksShOuQvKnT3vRkxTAnxiCSaa2VtdMsQaE7ARLW0JxdIPmUg5MK2epkUblb8azDpp1ar+ddV7qJXrtTy0AjpHF+gK+egG1dE9aqAmIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QJBLZ+1</latexit>

<latexit sha1_base64="uXS4QFWhHhVDD1T4+/ytZu1e2m4=">AAACG3icbVDLTgIxFO3gC/GFunTTCCauyAwLZUnixiUm8kgASad0oKGdTto7msmE/3DjQn/FnXHrwj9xaYFZCHiSJifn3FePHwluwHW/ndzG5tb2Tn63sLd/cHhUPD5pGRVryppUCaU7PjFM8JA1gYNgnUgzIn3B2v7kZua3H5k2XIX3kESsL8ko5AGnBKz0UO4xIQapgUSwaXlQLLkVdw68TryMlFCGxqD40xsqGksWAhXEmK7nRtBPiQZO7cBCLzYsInRCRqxraUgkM/10fvUUX1hliAOl7QsBz9W/HSmRxiTSt5WSwNisejPxP68bQ1DrpzyMYmAhXSwKYoFB4VkEeMg1oyASSwjV3N6K6ZhoQsEGtbTFl0t/SGUsgGv1NC3YqLzVYNZJq1rxriruXbVUr2Wh5dEZOkeXyEPXqI5uUQM1EUUaPaNX9Oa8OO/Oh/O5KM05Wc8pWoLz9QuGdaKM</latexit>

`style

features <latexitsha1_base64="NC+ahFZ+1YhFteHY/ZOXfb/+CoE=">AAACFXicbVA9SwNBEN2LXzF+RS1tDoNgFe5SqGXAxjKC+YDkCHubuWTJ7t6xO6eEI3/CxkL/ip3YWvtPLN0kV5jEBwOP92aYmRcmghv0vG+nsLG5tb1T3C3t7R8cHpWPT1omTjWDJotFrDshNSC4giZyFNBJNFAZCmiH49uZ334EbXisHnCSQCDpUPGIM4pW6kRAMdVg+uWKV/XmcNeJn5MKydHol396g5ilEhQyQY3p+l6CQUY1ciZgWuqlBhLKxnQIXUsVlWCCbH7v1L2wysCNYm1LoTtX/05kVBozkaHtlBRHZtWbif953RSjmyDjKkkRFFssilLhYuzOnncHXANDMbGEMs3trS4bUU0Z2oiWtoRy6YdMpgK5jp+mJRuVvxrMOmnVqv5V1buvVeq1PLQiOSPn5JL45JrUyR1pkCZhRJBn8krenBfn3flwPhetBSefOSVLcL5+ASploEA=</latexit>

probability <latexitsha1_base64="uNsontWhKMxqbUrCoTFt/9Fzx7c=">AAACGHicbVC7TsNAEDyHVwivACWNRYREFdkpgDISDWWQyENyrOh8OSen3MO6W4MsK59BQwG/Qodo6fgTSi6JC5Iw0kqjmV3t7kQJZwY879spbWxube+Udyt7+weHR9Xjk45RqSa0TRRXuhdhQzmTtA0MOO0lmmIRcdqNJrczv/tItWFKPkCW0FDgkWQxIxisFCRaRThinEE2qNa8ujeHu078gtRQgdag+tMfKpIKKoFwbEzgewmEOdbACKfTSj81NMFkgkc0sFRiQU2Yz0+euhdWGbqx0rYkuHP170SOhTGZiGynwDA2q95M/M8LUohvwpzJJAUqyWJRnHIXlDv73x0yTQnwzBJMNLO3umSMNSZgU1raEomlH3KRcmBaPU0rNip/NZh10mnU/au6d9+oNRtFaGV0hs7RJfLRNWqiO9RCbUSQQs/oFb05L8678+F8LlpLTjFzipbgfP0CvQahoA==</latexit>

(a) Stochastic neural network (SNN) based classifier

(b) Multi-view consistency learning

Figure 2: Two core components in our approach StyleMatch. (a) SNN introduces uncertainty to the learning by modeling the classifier's weights with Gaussian distributions. (b) Multi-view consistency learning essentially forces the predictions on two different views of the same unlabeled image, based on strong and style augmentation respectively, to match the output (pseudo label) obtained from the weakly augmented view, through the cross-entropy losses strong (Eq. (3)) and style (Eq. (4)).

The goal in SSDG is to learn a domain-generalizable model using both labeled and unlabeled source data. At test time, the model is directly deployed in an unseen target domain T = {x(it)} with x(it)  P (t)(X). The target domain differs from any source domain, P (t)(X, Y ) = P (k)(X, Y ) for k  {1, ..., K}.
3 Stochastic StyleMatch
Our approach, StyleMatch, is specifically designed for the semi-supervised domain generalization (SSDG) problem. There are three key components in StyleMatch: pseudo-labeling, stochastic classifier, and multi-view consistency learning. Pseudo-labeling aims to overcome the unlabeled data, for which we follow the efficient two-view consistency learning pipeline developed in FixMatch [32]. This is sketched in the top-two streams in Figure 2(b).
Since deep neural networks could easily overfit small labeled data, which in turn makes the pseudo labels inaccurate, we design a stochastic classifier that models the classifier's weights using Gaussian distributions (shown in Figure 2(a)). Therefore, instead of optimizing a fixed set of weights, we optimize their probability distributions, essentially learning an ensemble of classifiers. This is inspired by research in Bayesian deep learning [10, 5], which shows that introducing uncertainty to weights modeling can improve the robustness of deep models in the low-data regime.
To enhance domain-generalizable feature learning, we extend FixMatch's two-view consistency learning pipeline with a third view based on style/domain transfer [15], which is shown in the bottom stream in Figure 2(b). Style transfer can synthesize structural patterns to complement strong augmentation that mainly considers geometrical and color intensity transformations. These two augmentations essentially push data away from the original distribution toward different directions, and together bring a strong regularization effect. It is also worth noting that style transfer is label-free.
Below we provide the full technical details of our approach. Section 3.1 explains the design of the stochastic classifier. Section 3.2 presents the pseudo-labeling process and multi-view consistency learning.
3.1 Stochastic Classifier
Let z  RD denote the features of an image x (other indices are omitted here) and C the total number of classes, a standard linear classifier with weights W  RCОD and biases b  RC produces class-specific probabilities through an affine transformation W z +b followed by the softmax function. By ignoring the bias vector, we can view W = [w1, ..., wC ]T as a set of class prototypes. Thus, the matrix-vector multiplication of W z essentially computes the (cross-correlation) similarity between the image x and each class prototype wc with c  {1, ..., C}.
3

In our stochastic classifier, each class prototype is modeled using a Gaussian distribution parameter-

ized by N (хc, c2), from its probability

with хc, c distribution,

 RD. wc 

At each training step, N (хc, c2). To allow

we sample end-to-end

for each class optimization,

the we

prototype employ a

reparameterization trick [17, 5] to bypass the discrete sampling process,

wc = хc + softplus(c)

where  N (0, I).

(1)

Once all class prototypes are obtained, the similarity scores are computed based on cosine similarity

(denoted by sim(и, и)), which are then passed to the softmax function for generating normalized

probabilities,

p(y|x) =

exp(sim(z, wy

C c=1

exp(sim(z,

)/ ) wc)/

)

,

(2)

where  is a temperature hyper-parameter, which is fixed to 0.05. See Figure 2(a) for a graphical

representation of this stochastic classifier.

At test time, we simply use the mean parameters of the probability distributions (i.e. wc = хc) to classify images in a deterministic manner.

3.2 Multi-View Consistency Learning via StyleMatch

FixMatch We first describe the two-view consistency learning framework, which is initially
proposed in FixMatch [32] for dealing with unlabeled data using pseudo labels. The pipeline is shown in the top two streams in Figure 2(b). Let F(и) denote the entire model parameterized by , which
includes both the CNN backbone and the stochastic classifier, we first compute for an unlabeled image u(k) its pseudo label q^(k)  {1, ..., C}. This is achieved by feeding the weakly augmented image Tweak(u(k)) to the model F(и) for producing class-wise probabilities q(k) = F(Tweak(u(k))) with q(k)  RC , from which the pseudo label with the maximum confidence is selected, i.e., q^(k) = arg max(q(k)). Then, the prediction on the strongly augmented image Tstrong(u(k)) is forced to match the pseudo label q^(k) using the cross-entropy loss,

K

strong =

1 Eu(k)P (k)(X)[- (max(q(k))  ) log p(q^(k)|Tstrong(u(k)))],

(3)

k=1

where  is the confidence threshold (fixed to 0.95 as in [32]).

Following FixMatch, the weak augmentation Tweak implements random flip and crop while the strong augmentation Tstrong further adds RandAugment [7] and Cutout [8].

StyleMatch To better cope with domain shift, we introduce a third view to FixMatch, which
complements strong augmentation by performing style transfer. See the bottom stream in Figure 2(b).
This design is motivated by the observation that image style is closely related to domain [49]. We exploit recent advances in style transfer and adopt the pretrained AdaIN2 [15] to map an input image u(k) to a different source domain k , where k, k  {1, ..., K} but k = k . Similar to Eq. (3), the cross-entropy loss is used to train the model,

K

style =

1 Eu(k)P (k)(X)[- (max(q(k))  ) log p(q^(k)|Tstyle(u(k)))].

(4)

k=1

Overall Loss

As for the labeled source data, we also use the cross-entropy loss for learning,

K

labeled =

E(x(k),y(k))P (k)(X,Y )[- log p(y(k)|Tweak(x(k)))].

(5)

k=1

The final learning objective to optimize is the combination of Eqs. (5), (3) & (4),

min all = labeled + strong + style.

(6)



We do not use any balancing weights for these losses.

4

Art painting

Art

Cartoon

Clipart

Photo

Product

Sketch

Real world

(a) PACS

(b) OfficeHome

Figure 3: Example images from PACS [19] and OfficeHome [35].

Table 1: Domain generalization results in the low-data regime on PACS (averaged over 5 random splits). A: Art painting. C: Cartoon. P: Photo. S: Sketch. u: use unlabeled source data.

Model

u

# labels: 210 (10 per class) A C P S Avg

# labels: 105 (5 per class) A C P S Avg

Full-Labels

- 76.95 75.90 95.96 69.20 79.50 76.95 75.90 95.96 69.20 79.50

Vanilla CrossGrad [30] DDAIG [48] MixStyle [49] EISNet [39]

Domain generalization methods  63.09 58.49 86.56 45.56 63.42 56.71 53.87 71.87 36.92 54.84  62.56 58.92 85.81 44.11 62.85 56.39 55.11 72.61 38.08 55.55  61.95 58.74 84.44 47.48 63.15 55.09 52.31 70.53 38.89 54.20  71.11 64.04 88.99 54.62 69.69 62.00 58.40 80.43 43.58 61.10  66.84 61.33 89.16 51.38 67.18 62.08 54.75 80.66 42.68 60.04

Semi-supervised learning methods

MeanTeacher [34]  62.41 57.94 85.95 47.66 63.49 56.00 52.64 73.54 36.97 54.79

EntMin [13]

 72.77 70.55 89.39 54.38 71.77 67.01 65.67 79.99 47.96 65.16

FixMatch [32]  78.01 68.93 87.79 73.75 77.12 77.30 68.67 80.49 73.32 74.94

Semi-supervised domain generalization methods StyleMatch (ours)  79.43 73.75 90.04 78.40 80.41 78.54 74.44 89.25 79.06 80.32

4 Experiments
SSDG Benchmarks We repurpose two widely used DG datasets, PACS [19] and OfficeHome [35], for benchmarking SSDG methods. These two datasets focus on image classification. PACS consists of four distinct domains--art painting, cartoon, photo, and sketch--and contains 9,991 images of 7 classes in total. The domain shift is mainly concerned with image style changes. OfficeHome also has four domains: art, clipart, product, and real world. It contains more images than PACS: around 15,500 images of 65 classes, which are related to office and home objects, such as computer, chair, and bed. See Figure 3 for example images from these two datasets. Evaluation Metrics The common leave-one-domain-out protocol is adopted: three domains are used as the sources while the remaining one as the target. A model is first trained using only the source data, and then directly deployed in the target domain. Top-1 accuracy is used as the performance measure. Two SSDG settings are designed. In the first setting, we randomly sample 10 images per class for each source domain and use the rest as unlabeled data. The second setting tests a much more
2We use the weights provided in https://github.com/naoto0804/pytorch-AdaIN.
5

Table 2: Domain generalization results in the low-data regime on OfficeHome (averaged over 5 random splits). A: Art. C: Clipart. P: Product. R: Real world. u: use unlabeled source data.

Model

u

# labels: 1950 (10 per class)

A C P R Avg

# labels: 975 (5 per class) A C P R Avg

Full-Labels

- 58.88 49.42 74.30 76.21 64.70 58.88 49.42 74.30 76.21 64.70

Vanilla CrossGrad [30] DDAIG [48] MixStyle [49] EISNet [39]

Domain generalization methods  50.11 43.50 65.11 69.65 57.09 45.76 39.97 60.04 63.77 52.38  50.32 43.27 65.16 69.49 57.06 45.68 40.04 59.95 64.09 52.44  49.60 42.52 63.54 67.89 55.89 45.73 38.82 59.52 63.37 51.86  49.79 47.12 64.18 68.42 57.38 46.51 43.59 59.66 63.30 53.26  51.16 43.33 64.72 68.36 56.89 47.32 40.07 59.33 62.59 52.33

Semi-supervised learning methods

MeanTeacher [34]  49.92 43.42 64.61 68.79 56.69 44.65 39.15 59.18 62.98 51.49

EntMin [13]

 51.92 44.92 66.85 70.52 58.55 48.11 41.72 62.41 63.97 54.05

FixMatch [32]  50.36 49.70 63.93 67.56 57.89 48.98 47.46 60.70 64.36 55.38

Semi-supervised domain generalization methods StyleMatch (ours)  52.82 51.60 65.31 68.61 59.59 51.53 50.00 60.88 64.47 56.72

challenging scenario: only five labeled images are available for each class in each source domain. Results are averaged over five random splits.3
Baseline Methods To provide a rigorous study, we compare with strong baseline methods developed in relevant areas covering DG and SSL. These include 1) DG methods: vanilla training, CrossGrad [30], DDAIG [48], MixStyle [49], and EISNet [39]; 2) SSL methods: MeanTeacher [34], Entropy-Minimization (EntMin) [13], and FixMatch [32].
Training Details The ImageNet-pretrained ResNet18 [14] is used as the CNN backbone. We randomly sample 16 images from each source domain to construct a minibatch, for labeled and unlabeled data, respectively. Following FixMatch, the labeled minibatch is used for computing the labeled loss (Eq (5)) while both labeled and unlabeled minibatches are used to compute the two unlabeled losses (Eqs. (3) and (4)). The initial learning rate is set to 0.003 for the pretrained backbone and 0.01 for the randomly initialized stochastic classifier, both decayed by the cosine annealing rule. The number of training epochs is 40 for PACS and 20 for OfficeHome. We use a single Tesla V100 GPU for model training. Our implementation is based on the public Dassl.pytorch toolbox.4
4.1 Main Results
PACS The results are presented in Table 1. Full-Labels follows Vanilla's training strategy but uses all labels in each source domain. Most specifically designed DG methods that rely purely on the scarce labeled source data do not work well--they can hardly beat Vanilla. MixStyle performs exceptionally well compared to its fellows, suggesting that feature-level augmentation has a good potential in tackling label shortage. Nonetheless, the gap between MixStyle and Full-Labels is still huge. Among all DG methods, only EISNet can use the unlabeled source data for learning thanks to its self-supervised loss that is based on solving Jigsaw puzzles [27], and has apparently benefited from this design as reflected in its outperformance compared to most other DG methods.
The SSL methods generally outperform the DG methods. Among them, FixMatch achieves outstanding performance. This comparison emphasizes the importance of having the ability to deal with unlabeled data in SSDG, which has been largely missing in the designs of existing DG algorithms.
Finally, our approach StyleMatch demonstrates clear advantages over all baselines including FixMatch. When the target domains are art painting (A) and sketch (S), StyleMatch even surpasses Full-Labels. It is also worth noting that only StyleMatch suffers the smallest performance drop when
3We fix the random seeds to 1-5 when generating the five splits so the results of different methods can be fairly compared. Overall, the standard deviation sits around 1-2% on PACS and is generally less than 1% on OfficeHome. The splits are released along with our code.
4https://github.com/KaiyangZhou/Dassl.pytorch.
6

FixMatch + SNN + Tstyle
FixMatch + SNN
FixMatch

PACS (10 lab/cls)

PACS (5 lab/cls)

OfficeHome (10 lab/cls)

OfficeHome (5 lab/cls)

(a)

(b)

(c)

(d)

Figure 4: Ablation study on the key components in StyleMatch: the stochastic neural network (SNN) based classifier and the style augmentation Tstyle.

transitioning from 10 to 5 labels per class (80.41% vs. 80.32%), whereas the baselines' performance drops are mostly more than 5%. This suggests StyleMatch is highly competent in extreme low-data scenarios.
OfficeHome poses more challenges than PACS as it contains more classes (65 vs. 7) and cluttered images (see Figure 3(b)). The results are shown in Table 2. The gaps between different methods are smaller than those on PACS. Therefore, a small improvement in the average accuracy can be considered significant for this dataset.
Similar to PACS, most DG methods fail to beat Vanilla here, even for EISNet. The SSL methods are generally better in handling the SSDG problem in both 10- and 5-labels-per-class scenarios, with EntMin and FixMatch standing out with strong results. Our StyleMatch again achieves the best out-of-distribution generalization performance. In particular, StyleMatch gains a clear 2.5% improvement in the average accuracy over Vanilla in the 10-labels-per-class setting, and the improvement is further increased to 4.34% even with the number of labels cut down to half. These results strongly demonstrate the effectiveness of our designs in StyleMatch for tackling the low-data regime. Nonetheless, the gaps with Full-Labels are noticeable on this dataset, suggesting the need to further improve pseudo labels' accuracy in the future.
4.2 Ablation Study and Analysis
Key Components We conduct a comprehensive ablation study to examine the effectiveness of the proposed extensions to FixMatch--the SNN-based classifier and the style augmentation Tstyle. We repeat the experiments on PACS and OfficeHome by sequentially adding these two components to FixMatch. Figure 4 shows the results of this ablation study with a focus on the average accuracy over all target domains. We observe that SNN contributes around 2% and 1% increase to the performance on PACS and OfficeHome, respectively, and Tstyle further boosts the performance. In particular, by adding Tstyle to FixMatch+SNN, the improvements obtained are higher in the lower-data setting on both datasets, suggesting that Tstyle (multi-view consistency learning) is essential when dealing with extremely scarce labels.
Stochastic Classifier Reduces Overfitting To understand how the stochastic classifier improves learning, we compare FixMatch+SNN with FixMatch using two metrics: pseudo-labeling accuracy and over-confidence rate, which are measured for each minibatch data received at each training step. The first metric measures the accuracy of pseudo labels, while the over-confidence rate counts how many pseudo labels in a minibatch pass the confidence threshold. Ideally, we do not want the over-confidence rate to climb above the pseudo-labeling accuracy as this would mean the network predicts excessive incorrect pseudo labels with high confidence, which hurt generalization [44]. Figure 5 shows the comparisons. In (a), the over-confidence rate of FixMatch+SNN steadily increases and eventually converges to a similar level as the pseudo-labeling accuracy. In contrast, without SNN, the over-confidence rate overshoots the pseudo-labeling accuracy in the middle of training. In (b), the overfitting issue for FixMatch intensifies--the over-confidence rate outpaces the pseudo-labeling accuracy at the early training stage and the pseudo-labeling accuracy stops improving since then. In contrast, the curves for FixMatch+SNN look much healthier.
7

(a) The 10-labels-per-class setting on PACS with art painting as the target

(b) The 5-labels-per-class setting on PACS with art painting as the target

Figure 5: Pseudo-labeling accuracy (solid+circle) vs. over-confidence rate (dashed+triangle). Without the SNN-based classifier, the model suffers from severe overfitting, which is reflected in the over-confidence rate overshooting the pseudo-labeling accuracy.

Table 3: Strong vs. style augmentation.

Table 4: Impact on number of sources (K).

PACS StyleMatch's variants 10 lab/cls 5 lab/cls

Tstrong -only Tstyle-only Tstrong +Tstyle

79.61 72.61 80.41

76.05 69.72 80.32

PACS

10 lab/cls

5 lab/cls

K=1 K=2 K=3 K=1 K=2 K=3

FixMatch 53.55 71.42 77.12 49.91 68.52 74.94 StyleMatch 57.29 74.50 80.41 52.24 71.95 80.32

Complementarity in Augmentation Methods To provide an in-depth analysis of the role of augmentation methods, we evaluate three variants of StyleMatch: Tstrong-only, Tstyle-only, and Tstrong + Tstyle. Tstrong-only and Tstyle-only are based on the two-view consistency learning paradigm and Tstrong +Tstyle refers to the final model. Table 3 shows the results of this ablation study on PACS. We observe that 1) Tstrong is more suitable than Tstyle to be used in the two-view consistency learning framework, and 2) combining these two augmentation methods leads to a much better performance, which justifies their complementarity.
Impact on Number of Sources The previous experiments use three sources. To investigate the impact on the number of sources, we further conduct experiments by reducing the number of sources from three to two/one. For each target domain, the experiments cover all possible scenarios with different combinations of sources, each following the five random splits. For example, when sketch is used as the target and the number of sources is set to two, there are three different scenarios: 1) art painting and cartoon as the sources, 2) art painting and photo as the sources, and 3) cartoon and photo as the sources. The average accuracy is shown in Table 4. Note that when K = 1, we mix image style between random instances from the same domain for StyleMatch. The results demonstrate that StyleMatch outperforms FixMatch in all scenarios, even in the single-source case--this means mixing instance-level style also helps, which is consistent with the observation in a recent work that mixes instance-level feature statistics [49]. By increasing K from 2 to 3, StyleMatch gains 5.91% (from 74.50% to 80.41%) and 8.37% (from 71.95% to 80.32%) respectively in the 10- and 5-labels-per-class settings, while FixMatch's gains are 5.7% (from 71.42% to 77.12%) and 6.42% (from 68.52% to 74.94%) respectively, which are smaller than those of StyleMatch. Therefore, StyleMatch benefits more from having more sources, which makes sense: Tstyle can generate more diverse images given more sources.
5 Related Work
Domain Generalization Many DG methods fall into the category of domain alignment, which aims to learn a domain-invariant feature space by aligning feature distributions across source domains.
8

This is typically implemented by minimizing some distance measures such as first- and second-order moments [11], maximum mean discrepancy (MMD) [21], and adversarial losses [22]. Meta-learning has also been extensively studied for DG [20, 2, 23, 9, 31]. Most meta-learning methods construct episodes by dividing source domains into a meta-train and a meta-test set without overlap, and then learn a model on the meta-train set such that its performance on the meta-test set is improved. Most related to our work are data augmentation methods, which can be generally categorized into four groups. The first group investigates traditional label-preserving transformations [36], such as adjusting image contrast and brightness. The second group is based on adversarial gradients for augmentation in the pixel space [30, 37, 29]. A representative work is CrossGrad [30], which back-propagates adversarial gradients from a domain classifier to the input of a label classifier. The third family of methods also perform augmentation in the pixel space, but build the augmentation function using neural networks [47, 42, 48]. For example, DDAIG developed by Zhou et al. [48] learns a neural network to transform images' appearance such that a domain classifier cannot identify their source domain labels. The last group transitions from pixel- to feature-level augmentation by, e.g., mixing feature statistics [49] or learning feature perturbation networks [28]. Most existing DG methods cannot handle unlabeled data except those based on self-supervised learning [6, 39]. The intuition behind using self-supervised losses for DG is to allow a model to discover patterns that are less correlated with class labels, hence reducing overfitting to source data. Nonetheless, our experiments suggest that the proposed StyleMatch fits the SSDG problem much better than previous DG methods. For a more comprehensive review in the DG area, we refer readers to this survey [46].
Semi-Supervised Learning SSL is a well established area with a plethora of methods developed over the literature. Most related to our work are those based on consistency learning [26, 34] and pseudo-labeling [32, 41]. The basic idea in consistency learning is to force a model's predictions on two different views of the same input to be similar [45]. The consistency loss is usually imposed on the feature representations [1] or the output probabilities [32]. Recent studies have found that using a model's exponential moving average to generate the target for consistency learning can stabilize training [34]. Pseudo-labeling [18] provides soft or hard pseudo labels for unlabeled data using either a pretrained model [41] or the model being trained [32]. Recent advances [41, 3, 32, 40] have suggested that introducing strong noise to the student model can greatly improve the performance, e.g., to apply strong augmentation to the input and dropout to the model parameters. To overcome distribution shift between labeled and unlabeled data caused by sampling bias, a couple of studies [38, 1] have borrowed ideas from domain adaptation to minimize this distance. Overall, SSL is related to SSDG in the way that both problems need to deal with unlabeled data. However, the marginal data distributions of unlabeled data in SSDG are much more challenging to handle since they are collected from heterogeneous domains.
Stochastic Neural Networks Our work is also related to research in stochastic neural networks (SNNs), also known as Bayesian deep learning [10]. The key idea is to drive exploration in the parameter space via stochastic modeling, i.e., casting weights as probability distributions [5]. Blundell et al. [5] show that modeling neural networks' weights with Gaussian distributions allows the model to produce more reasonable predictions on a regression task with noisy data. Gal and Ghahramani [10] instead use Bernoulli distributions to model convolutional kernels, which are efficiently implemented using dropout at both training and test time [33]. From an application perspective, SNNs have been successfully applied to domain adaptation [25], semantic segmentation [16], and person reidentification [43]. However, their application in the DG area has not been identified before this work.
6 Conclusion
Semi-supervised domain generalization, a more realistic and practical setting, greatly challenges the design of existing DG methods. We show that with limited labels the previous top-performing DG methods fail to learn generalizable representations, while our specifically designed StyleMatch gains huge improvements in reducing the gap with full-labels training. Nonetheless, we observe that pseudo labels' accuracy is much lower in our setting compared to that in a traditional semi-supervised setting where data are sampled from a single distribution. Therefore, future work can be focused on designing new formulations to incorporate source domain shift for building more robust semi-supervised domain generalization algorithms.
9

References
[1] Abulikemu Abuduweili, Xingjian Li, Humphrey Shi, Cheng-Zhong Xu, and Dejing Dou. Adaptive consistency regularization for semi-supervised transfer learning. In CVPR, 2021.
[2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain generalization using meta-regularization. In NeurIPS, 2018.
[3] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019.
[4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011.
[5] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In ICML, 2015.
[6] Fabio M. Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In CVPR, 2019.
[7] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data augmentation with no separate search. In CVPR-W, 2020.
[8] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.
[9] Qi Dou, Daniel C Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via modelagnostic learning of semantic features. In NeurIPS, 2019.
[10] Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with bernoulli approximate variational inference. In ICLR-W, 2016.
[11] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. TPAMI, 2017.
[12] Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. Dlow: Domain flow for adaptation and generalization. In CVPR, 2019.
[13] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2004.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
[15] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017.
[16] Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla. Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding. In BMVC, 2017.
[17] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
[18] Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In ICML-W, 2013.
[19] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017.
[20] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for domain generalization. In AAAI, 2018.
[21] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, 2018.
[22] Ya Li, Xinmei Tiana, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In ECCV, 2018.
[23] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous domain generalization. In ICML, 2019.
[24] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. Feddg: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space. In CVPR, 2021.
[25] Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, and Tao Xiang. Stochastic classifiers for unsupervised domain adaptation. In CVPR, 2020.
[26] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. TPAMI, 2018.
10

[27] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016.
[28] Fengchun Qiao and Xi Peng. Uncertainty-guided model generalization to unseen domains. In CVPR, 2021. [29] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In CVPR, 2020. [30] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita
Sarawagi. Generalizing across domains via cross-gradient training. In ICLR, 2018. [31] Yang Shu, Zhangjie Cao, Chenyu Wang, Jianmin Wang, and Mingsheng Long. Open domain generalization
with domain-augmented meta-learning. In CVPR, 2021. [32] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex
Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In NeurIPS, 2020. [33] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 2014. [34] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, 2017. [35] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. [36] Riccardo Volpi and Vittorio Murino. Addressing model vulnerability to distributional shifts over image transformation sets. In ICCV, 2019. [37] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In NeurIPS, 2018. [38] Qin Wang, Wen Li, and Luc Van Gool. Semi-supervised learning by augmented distribution alignment. In ICCV, 2019. [39] Shujun Wang, Lequan Yu, Caizi Li, Chi-Wing Fu, and Pheng-Ann Heng. Learning from extrinsic and intrinsic supervisions for domain generalization. In ECCV, 2020. [40] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consistency training. In NeurIPS, 2020. [41] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In CVPR, 2020. [42] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. [43] Tianyuan Yu, Da Li, Yongxin Yang, Timothy M Hospedales, and Tao Xiang. Robust person re-identification by modelling feature uncertainty. In ICCV, 2019. [44] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017. [45] Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard SchШlkopf. Learning with local and global consistency. In NeurIPS, 2004. [46] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. arXiv preprint arXiv:2103.02503, 2021. [47] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization. In ECCV, 2020. [48] Kaiyang Zhou, Yongxin Yang, Timothy M Hospedales, and Tao Xiang. Deep domain-adversarial image generation for domain generalisation. In AAAI, 2020. [49] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In ICLR, 2021.
11

