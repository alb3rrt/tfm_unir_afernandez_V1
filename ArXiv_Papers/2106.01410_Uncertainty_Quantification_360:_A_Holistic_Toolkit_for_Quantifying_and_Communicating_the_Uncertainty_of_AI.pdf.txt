Uncertainty Quantification 360: A Holistic Toolkit for Quantifying and Communicating the Uncertainty of AI

Soumya Ghosh Q. Vera Liao Karthikeyan Natesan Ramamurthy Jiri Navratil Prasanna Sattigeri Kush R. Varshney Yunfeng Zhang

ghoshso@us.ibm.com vera.liao@ibm.com
knatesa@us.ibm.com jiri@us.ibm.com
psattig@us.ibm.com krvarshn@us.ibm.com zhangyun@us.ibm.com

arXiv:2106.01410v2 [cs.AI] 4 Jun 2021

Abstract
In this paper, we describe an open source Python toolkit named Uncertainty Quantification 360 (UQ360) for the uncertainty quantification of AI models. The goal of this toolkit is twofold: first, to provide a broad range of capabilities to streamline as well as foster the common practices of quantifying, evaluating, improving, and communicating uncertainty in the AI application development lifecycle; second, to encourage further exploration of UQ's connections to other pillars of trustworthy AI such as fairness and transparency through the dissemination of latest research and education materials. Beyond the Python package (https://github.com/IBM/UQ360), we have developed an interactive experience (http: //uq360.mybluemix.net) and guidance materials as educational tools to aid researchers and developers in producing and communicating high-quality uncertainties in an effective manner. Keywords: Uncertainty Quantification, Trustworthy AI, Transparency, Reliability
1. Introduction
Success stories of AI models are plentiful, but we have also seen prominent examples where the models behave in unexpected ways. For example, a typical failure mode of state-ofthe-art prediction models is the inability to abstain from making predictions when the test data violate assumptions made during training, potentially resulting in highly confident but incorrect predictions. Hence, there is a renewed interest in improving the reliability and transparency of AI models (Bhatt et al., 2021).
A typical AI lifecycle process consists of collecting data, pre-processing it, selecting a model to learn from the data, choosing a learning algorithm to train the selected model, and performing inference using the learned model. There are inherent uncertainties associated with each of these steps. For example, data uncertainty may arise from the inability to collect or represent real-world data reliably. Flaws in data pre-processing--whether during curation, cleaning, or labeling--also create data uncertainty. Similarly, models are only proxies for the real world and their learning and inference algorithms rely on various
. Equal contributions. Names listed in alphabetical order.
1

simplifying assumptions and thus introduce modeling and inferential uncertainties. The predictions made by an AI system are susceptible to all these sources of uncertainty.
Reliable uncertainty quantification provides a vital diagnostic for both developers and users of an AI system. For developers, it can suggest strategies for improving the system. For example, high data uncertainty may point towards improving the data representation process, while a high model uncertainty may suggest the need to collect more data. For users, accurate uncertainties, especially when combined with effective communication strategies, can add a critical layer of transparency and trust, crucial for better AI-assisted decision making (Zhang et al., 2020). Trust in AI systems is a necessary condition for their successful deployment in high-stakes applications spanning medicine, finance, and the social sciences.
The research on uncertainty quantification (UQ) is long-standing but has enjoyed ever increasing interest in recent years due to the observations made above. Numerous approaches for improved UQ in AI models have been proposed. However, choosing a particular UQ method depends on many factors: the underlying model, type of machine learning task (regression vs. classification), characteristics of the data, and the user's goal. If inappropriately used, a particular UQ method may produce poor uncertainty estimates and mislead users. Moreover, even a highly accurate uncertainty estimate may be misleading if poorly communicated. To address these issues, we introduce Uncertainty Quantification 360 (UQ360) - an open-source Python toolkit. The toolkit provides a diverse set of algorithms to quantify uncertainties, metrics to measure them, methods to improve the quality of estimated uncertainties, and approaches to communicate the uncertainties effectively. In addition, we provide a taxonomy and guidance for choosing these capabilities based on the user's needs. UQ360 makes communicating UQ simple; developers can make user-centered choices by following the psychology-based guidance on communicating uncertainty estimates, ranging from concise descriptions to detailed visualizations. Altogether, UQ360's capabilities allow quantification of uncertainties to be an integral part of the AI development lifecycle.
from sklearn.ensemble import GradientBoostingRegressor from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split
from uq360.algorithms.blackbox_metamodel import BlackboxMetamodelRegression
X, y = make_regression(random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) gbr_reg = GradientBoostingRegressor(random_state=0)
uq_model = BlackboxMetamodelRegression(base_model=gbr_reg) uq_model.fit(X_train, y_train)
y_hat, y_hat_lb, y_hat_ub = uq_model.predict(X_test)
Listing 1: Use of meta-models to augment sklearn's gradient boosted regressor with prediction interval.
2

Figure 1: Taxonomy of the uncertainty estimation algorithms included in the UQ360 toolkit.
2. UQ algorithms and evaluation metrics
UQ360 provides more than ten UQ algorithms, metrics, along with guidance to help users choose an appropriate method or metric for their use case. UQ algorithms can be broadly classified as intrinsic or extrinsic, depending on how the uncertainties are obtained from the AI models. Figure 1 lists the algorithms included in the toolkit. Intrinsic methods encompass approaches explicitly designed to produce uncertainty estimates along with their predictions. Amongst intrinsic methods, the toolkit includes variationally trained Bayesian neural networks (BNNs) (Blundell et al., 2015) with Gaussian as well as heavy-tailed, sparsity-promoting Horseshoe priors (Ghosh et al., 2019), Gaussian processes (Rasmussen and Williams, 2006), quantile regression (Koenker and Bassett, 1978) and neural networks with homoscedastic and heteroscedastic noise models (Wakefield, 2013). An infinitesimal Jackknife (IJ) based algorithm (Ghosh et al., 2020) is also included along with these. This perturbation-based approach performs uncertainty quantification by estimating model parameters under different perturbations of the original data. Crucially, the estimation only requires the model to be trained once on the unperturbed dataset.
The quality of estimation generated by a UQ algorithm also needs to be evaluated. Poorly calibrated uncertainties should neither be trusted nor presented to a user. UQ360 provides standard calibration metrics for classification and regression tasks to evaluate the quality of the estimated uncertainties. This includes expected calibration error (ECE) (Naeini et al., 2015), Brier score (Brier, 1950) for classification and prediction interval coverage probability (PICP) (Chatfield, 1993) and mean prediction interval width (MPIW) (Shrestha and Solomatine, 2006) for regression. We also include additional diagnostic tools such as reliability diagrams (DeGroot and Fienberg, 1983) and risk-vs-rejection rate curves (El-Yaniv et al., 2010) . In addition, the toolbox provides a novel operation-point agnostic
3

approaches for the assessment of prediction uncertainty estimates called the Uncertainty Characteristic Curve (UCC) (Navratil et al., 2021).
from sklearn.model_selection import GridSearchCV from uq360.utils.misc import make_sklearn_compatible_scorer from uq360.algorithms.quantile_regression import QuantileRegression
sklearn_picp = make_sklearn_compatible_scorer( task_type="regression", metric="picp", greater_is_better=True)
base_config = {"alpha":0.95, "n_estimators":20, "max_depth": 3, "learning_rate": 0.01, "min_samples_leaf": 10, "min_samples_split": 10}
configs = {"config": []} for num_estimators in [1, 2, 5, 10, 20, 30, 40, 50]:
config = base_config.copy() config["n_estimators"] = num_estimators configs["config"].append(config)
uq_model = GridSearchCV( QuantileRegression(config=base_config), configs, scoring=sklearn_picp)
uq_model.fit(X_train, y_train)
y_hat, y_hat_lb, y_hat_ub = uq_model.predict(X_test)
Listing 2: Use of UQ360 metrics for model selection. The prediction interval coverage probability score (PICP) score is used here as the metric to select the model through crossvalidation.
For methods that do not have an inherent notion of uncertainty built into them, we use extrinsic approaches to extract uncertainties post-hoc. The toolkit provides meta-models (Chen et al., 2019) that generate reliable confidence measures (in classification), prediction intervals (in regression) (Navratil et al., 2020), and predict performance metrics such as accuracy on unseen and unlabeled data (Elder et al., 2021). For pre-trained models, the toolbox also provides extrinsic algorithms for potentially improving the uncertainty quality. This includes isotonic regression (Zadrozny and Elkan, 2001), Platt-scaling (Platt, 1999), auxiliary interval predictors (Thiagarajan et al., 2020), and UCC Recalibration (Navratil et al., 2021).
3. Implementation and communication methods
The metrics and algorithms are designed to be scikit-learn compatible so that they can fit into developers' existing workflow. The implementations of the algorithms and metrics are compatible with scikit-learn functions such as GridSearchCV. The example code block in listing 1 shows how to augment scikit-learn's point estimators with uncertainty using a black-box meta-model UQ algorithm. Listing 2 shows how to use the prediction interval
4

coverage probability (PICP) metric to score and select models. The toolkit also comes with Jupyter notebook tutorials demonstrating the use of UQ in several industrial applications such as healthcare and finance.
UQ360 allows the choice of multiple styles of communication methods, from concise descriptions to detailed visualizations. We also provide guidance for communicating UQ to help practitioners make the choice, as informed by psychology and human-computer interaction research. For classification tasks, UQ360 provides functions to generate confidence scores. For regression tasks, UQ360 provides functions to generate the numerical ranges, visual confidence intervals, density plots, and quantile dot plots (Fernandes et al., 2018).
The toolkit has been engineered with a common interface for all of the different UQ capabilities and is extensible to accelerate innovation by the community advancing trustworthy and responsible AI. We are open-sourcing it to help create a community of practice for researchers, data scientists, and other practitioners that need to understand or communicate the limitations of algorithmic decisions.
References
Umang Bhatt, Javier Antor´an, Yunfeng Zhang, Q. Vera Liao, Prasanna Sattigeri, Riccardo Fogliato, Gabrielle Gauthier Melanc¸on, Ranganath Krishnan, Jason Stanley, Omesh Tickoo, Lama Nachman, Rumi Chunara, Madhulika Srikumar, Adrian Weller, and Alice Xiang. Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty. In Proceedings of the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, May 2021.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1613­1622, Lille, France, 07­09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/blundell15.html.
G. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78:1­3, 1950.
Chris Chatfield. Calculating interval forecasts. Journal of Business & Economic Statistics, 11(2):121­135, 1993.
Tongfei Chen, Jir´i Navr´atil, Vijay Iyengar, and Karthikeyan Shanmugam. Confidence scoring using whitebox meta-models with linear classifier probes. In Proceedings of the International Conference on Artificial Intelligence and Statistics, pages 1467­1475, 2019.
Morris H DeGroot and Stephen E Fienberg. The comparison and evaluation of forecasters. Journal of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12­22, 1983.
Ran El-Yaniv et al. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(5), 2010.
5

Benjamin Elder, Matthew Arnold, Anupama Murthi, and Jiri Navratil. Learning prediction intervals for model performance. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.
Michael Fernandes, Logan Walls, Sean Munson, Jessica Hullman, and Matthew Kay. Uncertainty displays using quantile dotplots or cdfs improve transit decision-making. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, page 144, April 2018.
Soumya Ghosh, Jiayu Yao, and Finale Doshi-Velez. Model selection in Bayesian neural networks via horseshoe priors. Journal of Machine Learning Research, 20(182):1­46, 2019.
Soumya Ghosh, William T. Stephenson, Tin D. Nguyen, Sameer Deshpande, and Tamara Broderick. Approximate cross-validation for structured models. In Advances in Neural Information Processing Systems, volume 33, December 2020.
Roger Koenker and Gilbert Bassett, Jr. Regression quantiles. Econometrica, 46(1):33­50, January 1978.
Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using Bayesian binning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015.
Jiri Navratil, Matthew Arnold, and Benjamin Elder. Uncertainty prediction for deep sequential regression using meta models, 2020.
Jiri Navratil, Benjamin Elder, Matthew Arnold, Soumya Ghosh, and Prasanna Sattigeri. Uncertainty characteristics curves: A systematic assessment of prediction intervals, 2021.
John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, pages 61­74. MIT Press, Cambridge, MA, 1999.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, Cambridge, MA, 2006.
Durga L Shrestha and Dimitri P Solomatine. Machine learning approaches for estimation of prediction interval for the model output. Neural Networks, 19(2):225­235, 2006.
Jayaraman J Thiagarajan, Bindya Venkatesh, Prasanna Sattigeri, and Peer-Timo Bremer. Building calibrated deep models via uncertainty matching with auxiliary interval predictors. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6005­6012, 2020.
Jon Wakefield. Bayesian and frequentist regression methods. Springer Science & Business Media, 2013.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. In Proceedings of the International Conference on Machine Learning, pages 609­616, June­July 2001.
6

Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 295­305, 2020.
7

