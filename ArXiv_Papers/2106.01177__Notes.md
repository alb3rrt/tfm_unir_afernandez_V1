
# Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck

[arXiv](https://arxiv.org/abs/2106.01177), [PDF](https://arxiv.org/pdf/2106.01177.pdf)

## Authors

- Nicolas Skatchkovsky
- Osvaldo Simeone
- Hyeryung Jang

## Abstract

One of the key challenges in training Spiking Neural Networks (SNNs) is that target outputs typically come in the form of natural signals, such as labels for classification or images for generative models, and need to be encoded into spikes. This is done by handcrafting target spiking signals, which in turn implicitly fixes the mechanisms used to decode spikes into natural signals, e.g., rate decoding. The arbitrary choice of target signals and decoding rule generally impairs the capacity of the SNN to encode and process information in the timing of spikes. To address this problem, this work introduces a hybrid variational autoencoder architecture, consisting of an encoding SNN and a decoding Artificial Neural Network (ANN). The role of the decoding ANN is to learn how to best convert the spiking signals output by the SNN into the target natural signal. A novel end-to-end learning rule is introduced that optimizes a directed information bottleneck training criterion via surrogate gradients. We demonstrate the applicability of the technique in an experimental settings on various tasks, including real-life datasets.

## Comments

Under review for conference publication

## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/learning-to-time-decode-in-spiking-neural](https://paperswithcode.com/paper/learning-to-time-decode-in-spiking-neural)

## Bibtex

```tex
@misc{skatchkovsky2021learning,
      title={Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck}, 
      author={Nicolas Skatchkovsky and Osvaldo Simeone and Hyeryung Jang},
      year={2021},
      eprint={2106.01177},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
```

## Notes

Type your reading notes here...

