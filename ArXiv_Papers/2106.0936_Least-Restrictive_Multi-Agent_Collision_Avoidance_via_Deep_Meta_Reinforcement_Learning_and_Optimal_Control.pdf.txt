Least-Restrictive Multi-Agent Collision Avoidance via Deep Meta Reinforcement Learning and Optimal Control
Salar Asayesh1, Mo Chen1, Mehran Mehrandezh2 and Kamal Gupta1

arXiv:2106.00936v1 [cs.RO] 2 Jun 2021

Abstract-- Multi-agent collision-free trajectory planning and control subject to different goal requirements and system dynamics has been extensively studied, and is gaining recent attention in the realm of machine and reinforcement learning. However, in particular when using a large number of agents, constructing a least-restrictive collision avoidance policy is of utmost importance for both classical and learning-based methods. In this paper, we propose a Least-Restrictive Collision Avoidance Module (LR-CAM) that evaluates the safety of multiagent systems and takes over control only when needed to prevent collisions. The LR-CAM is a single policy that can be wrapped around policies of all agents in a multi-agent system. It allows each agent to pursue any objective as long as it is safe to do so. The benefit of the proposed least-restrictive policy is to only interrupt and overrule the default controller in case of an upcoming inevitable danger. We use a Long ShortTerm Memory (LSTM) based Variational Auto-Encoder (VAE) to enable the LR-CAM to account for a varying number of agents in the environment. Moreover, we propose an off-policy meta-reinforcement learning framework with a novel reward function based on a Hamilton-Jacobi value function to train the LR-CAM. The proposed method is fully meta-trained through a ROS based simulation and tested on real multi-agent system. Our results show that LR-CAM outperforms the classical leastrestrictive baseline by 30 percent. In addition, we show that even if a subset of agents in a multi-agent system use LR-CAM, the success rate of all agents will increase significantly.
I. INTRODUCTION
With increasingly widespread use of autonomous vehicles such as mobile robots and unmanned aerial vehicles (UAVs), control and trajectory planning for multi-agent systems is gaining attention within the scientific community. Especially considering the current pandemic situation, the use of multiple autonomous vehicles in Giga-Factories and hospitals, where physical distancing prevents human workers from performing their regular jobs, is essential. In all these applications, multiple agents need to operate in the same work space carrying out their respective tasks, while not interfering with others. Many of these multi-agent systems are also safety critical.
Recent deep reinforcement learning (RL) methods has produced a quantum leap in solving difficult tasks. Successful applications include video games [1], [2], continuous control [3], [4], and manipulation tasks from high-dimensional image observations [5], [6]. However, there are still some key challenges in end-to-end training of precise controllers based on sensory observations using deep RL (DRL), which prevents their use in some real-world applications. Moreover, DRL tends to need lots of training episodes to learn a single task, and it is usually not trivial to transfer the learned policy to other similar tasks. Even though recent deep meta-RL
*This work was not supported by any organization 1Faculty of Applied Science, Simon Fraser University,Burnaby, BC V5A 1S6, Canada, {sasayesh, mochen, kamal} at sfu.ca 2Faculty of Engineering and Applied Science, University of Regina, Regina, SK S4S 0A2, Canada mehran.mehrandezh at uregina.ca

(a) Trajectory visualization on (b) Generated trajectory for a 4-agent real implementation - colours task through real implementation became lighter as time goes
Fig. 1: The visualization of generated trajectory in a realworld experiment.
methods are capable of adapting to new similar tasks, they usually are on-policy and require massive amount of data from different tasks [7], [8], [9].
Our motivation is to design a Least-Restrictive Collision Avoidance Module (LR-CAM) that can be added on top of any autonomous agent conducting tasks in an environment that consists of multiple other agents. Our approach adds a layer of safety to all agents co-existing in a shared environment, each of whom typically operates based on some default objective/controller. The least-restrictive policy has several benefits compared to other "do-it-all" policies that try to both achieve a goal as well as maintain safety. For example, do-it-all policies can require a lot of training data and resources to learn the both objective and safety, and therefore unable to maintain both performance and safety at the same time. In addition, if the goal or objective changes, a new policy or complicated meta-trained policy needs to be trained. In contract, the least-restrictive policy enables agents to have any classical or learning based default controller such as goal-reaching controller while providing a higher level of safety for all agents. To achieve this, we propose a distributed control approach using a single deep policy network for all agents in which the LR-CAM of each agent, based on the agent's observations, decides whether it is safe to follow its default controller. If not safe, LR-CAM takes control of the agent and attempts to resolve potential conflicts that may lead to collisions. In our algorithm, first a Long Short Term Memory (LSTM) [10] variational auto-encoder (VAE) maps variable-length observations from a varying number of observed agents into a fixed-length latent space. Then our meta-trained policy supervises the navigation based on the latent space. To remedy the need for a complicated hand-designed reward function, we propose a novel reward function based on Hamilton-Jacobi (HJ) reachability theory [11] to help LR-CAM effectively learn when to take control over each agent.
The key contributions of the proposed work are as follows: (1) We propose LR-CAM, which encompasses any goal-

oriented policy of autonomous vehicles to monitor whether collision avoidance maneuvers are needed and if so, takes over control to perform the avoidance maneuvers. (2) We propose a model-free off-policy meta-RL algorithm for training the LR-CAM; through the use of an LSTM-VAE, we map observations to a latent space to enable LR-CAM to be used in different environments with a varying number of agents. (3) We propose a novel reward function based on a safety value function computed from HJ reachability. We conduct our training and experiments within a ROSbased controller. We show that the LR-CAM outperforms the baseline classical approach.
II. RELATED WORKS
Multi-agent collision avoidance Research pertinent to the multi-agent collision-free trajectory planning is vast. The authors in [12], proposed using dynamic obstacle method in collision avoidance problems; here, the assumed dynamics are used to predict agents' future behaviours. Potential field trajectory planning is another approach [13], [14] that achieves collision avoidance by slightly changing the trajectory using a repulsive force between vehicles. The authors in [15] proposed the use of buffered Voronoi cells in which each robot computes its Voronoi cells and re-plans its trajectory within a receding horizon to safely reach a specified goal. They assume that the dynamics of the agents are known a priori. In another work, [16] proposed an Optimal Reciprocal Collision Avoidance (ORCA) method, which is a well-known classical approach for multi-agent collision avoidance. The ORCA achieves collision avoidance by optimizing a related constrained-cost in a short-time horizon. The problem of trajectory planning and collision avoidance is also studied widely in safety critical systems under the differential game framework. In [11], [17], [18], collision avoidance has been studied using HJ reachability theory in small-scale problems such as those found in a two-agent setup; the authors provide safety and goal reaching guarantees. However, HJ analysis becomes intractable as the number of agents increased. The authors in [19] proposed using Mixed Integer Programming (MIP) to establish a higher-level logic around pair-wise collision avoidance using HJ-reachability to alleviate the intractability.
The problem of multi-agent collision avoidance is also addressed in many previous works within the RL research community [20], [21], [22]. The authors in [20] proposed using RL for a two-agent collision avoidance problem. They extend their previous work in [22] and proposed an RLbased method for single agent motion planning among moving objects, treated as obstacles, without any assumptions on the agents' behavior. They used an actor-critic algorithm to train an agent with an LSTM in the first layer of their policy network. In our work, we meta-trained a separate embeddingVAE to alleviate "representation learning bottleneck" [23] and varying input sizes. The authors in [21] proposed a decentralized policy, trained using an actor-critic RL method which is capable of mapping raw sensory observations to control outputs that results in collision free trajectories. In comparison to the existing RL methods, which propose endto-end solutions that consider both the problem of goalreaching and avoidance simultaneously, our proposed LRCAM takes inspiration from HJ reachability by only interrupting the default controller when needed.

Meta Reinforcement Learning Meta RL has gained attention recently because it enables an agent to easily adapt to new tasks with some shared structures. Meta RL comprises of two main stages. In the first stage, a meta policy is trained with a huge amount of data across different tasks and in second stage, the trained policy is adopted to a specific task with relatively few training iterations. However, most of the proposed meta RL algorithms are on-policy, which means that they often need a huge amount of training data in the meta-training phase. In general, Meta-RL methods can be divided into two main categories [24]: (1) context-based, and (2) gradient-based. In the context-based approach, the experiences of tasks are mapped into a latent representation and then a conditional policy is used to adapt to previously seen tasks [8], [9], [25]. The authors in [24] proposed an off-policy context-based approach by representing the context using probabilistic latent variables. Comparing to our method, it needs an additional latent representation layer to embed the varying dimension of inputs to the latent state, which could make the training procedure inefficient. In gradient-based methods, a policy network [7], [26] or a loss function [27] is meta-trained to capture the task experience in model parameters. Our method can be considered as a gradient-based method built on top of the Model Agnostic Meta Learning (MAML) by [7]; however, we meta-train using an off-policy algorithm to improve sample efficiency.

III. PRELIMINARIES

A. Meta Reinforcement Learning

We define a set of tasks where each task Ti contains N =

3, . . . or 6 agents interacting in the same environment and

each agent has a different objective with a different default

controller. We assume each agent is a Markov Decision

Process (MDP) defined by a set of states Xi, a discrete

set of actions Ui, initial state distribution p(xi,0), transition

function pi(xi,t+1|xi,t, ui,t), reward function ri and discount factor i  (0, 1]. In addition, we assume each task has a distribution p(T ) and is an MDP defined by a tuple

(ST , AT , PT , RT , T , p0,T ) where ST is the task state set,

AT is the task action set, PT (st+1,T |st,T , At,T ) is the

transition function, RT is the vector-valued reward function, T  RN is the discount vector and p0,T is the task initial

state distribution. Our goal is to find an optimal policy

(.|st,T ) to maximize the sum of expected discounted returns

Ri = E[

T t=t

(i(T -t)ri,t)]

for

all

agents

across

all

tasks.

We

assume reward and transition functions are unknown to the

learning agents but can be sampled from the environments.

B. Safety Level Value Function

In this section we will introduce the notion of safety value

function which is used for reward calculation based on HJ

theory. We start by assuming each agent operate according

to the following approximate ordinary differential equation

(ODE) model:

x i = fi(xi, ui)

(1)

where xi  Xi = (px,i, py,i, p,i) and ui  Ui = (v, ) are the states and control inputs (linear and angular velocities) of agent i respectively. The dynamic in (1) induce the following relative dynamic between each pair of agents:

x ij = gij (xij , ui, uj ), i = j

(2)

where ui, uj are control inputs to agent i, j respectively, xij = (px,ij, py,ij, p,ij) is the relative states of agent j with respect to agent i and calculated as follows:

px,ij

cos j - sin j 0 px,i - px,j

py,ij = sin j cos j 0 py,i - py,j (3)

p,ij

0

0

1 p,i - p,j

We assume that the functions gij and fi are uniformly continuous, bounded and Lipschitz continuous in xij, xi respectively for fixed ui and uj. Furthermore, ui and uj are chosen from a set of measurable functions Ui and Uj [19]. We define the safety level value function Rij(t, xij):

Rij (t) = {xij : Rij (t, xij )  0}

(4)

Here, Rij is the backward reachable set (BRS):
Rij(t) = {xij : ui  Ui, uj  Uj, xij(.) satisfies (2), s  [0, t], xij  Dij} (5)

In (5), Dij is the danger zone between agents i and j: Dij = {xi, xj : (px,i - px,j )2 + (py,i - py,j )2  d2} (6)

where d is the collision radius. The BRS represents the set of relative states such that agent i will inevitably collide with agent j if agent j applies worst-case control policy to cause a collision. The BRS is calculated by solving the associated HJ partial differential equations by using level set methods [11]. According to (4), the BRS is the zero sublevel set of the safety value function that encodes the degree of safety based on all relative joint states. Higher is safer, and a negative value means the relative joint states are inside BRS. In this paper we consider the infinite time horizon BRS and safety value function, which can be obtained via taking the limit t   as in [19]:

Rij (xij ) = lim Rij (t, xij ).

(7)

t

IV. APPROACH

We are interested in learning the LR-CAM policy that allows all agents to follow their default controllers as long as no intervention is needed for preventing collision. The default controller could be used for goal reaching or simply be given by a human controller. However, when all, or a subset of the agents are in potential conflict with each other, the LR-CAM will intervene and take over control to attempt to allow vehicles to revert back to safety. As a result, our policy not only decides whether each agent can follow its default controller or avoid danger, but also detects possible dangers from a latent-space observation.

A. Observation, Action, and Latent Spaces
We use the relative coordinate system defined in (3). Using the relative coordinate not only reduces the dimension of observation space, but also can be extracted using on-board sensors of agents. However, as we will later explain in an ablation study, using information from only the current time step as observation is not enough to learn to predict upcoming dangers efficiently. As a result, we define the observation based on a history of states and action as follows:

Oi,t,T = (oi,t,T , oi,t-1,T , oi,t-2,T , oi,t-3,T ,

(8)

ai,t-1, ai,t-2, ai,t-3)

where ai,t is the action of agent i at time t and oi,t,T is defined by

oi,t,T = (xi1, xi2, . . . , xi,i-1, xi,i+1, . . . , xiN ) (9)
The action space of each task MDP with N agents are defines as follows:

At,T = (at,0, . . . , at,N )

(10)

where the action of each individual agent is defines as

at,i  Ui = {0 (default-controller), 1 (turning-right), (11) 2 (turning-left)}.

In (11), "default-controller" means agent is allowed to execute any default controller such as a goal-reaching controller, and this controller can be different for each agent inside a task. The other two classes of actions refers to avoidance actions that agent should execute to prevent safety violation

with the other agents. Taking inspiration from reachability theory [11], the least restrictive controller lets agents execute their own default controllers unless some upcoming danger is inevitable, in which case agents should avoid danger with their maximum actuation capacity. As a result the avoidance

actions are translated to

turning-right := [vmax, max]

(12)

turning-left := [vmax, -max].

where v and  are the linear and angular velocities respectively.

To handle the varying size of observation space in (8) across different tasks, we use an encoder part of a LSTMbased VAE to encode the observation to a fixed-size latent space as is illustrated in Fig. 2. The VAE consist of two parts: (1) an LSTM-based encoder network q(zi,t|oi,t,T ), parametrized by , which encodes the observation o (9) to a fixed size latent space, and (2) an LSTM-based decoder network q(oi,t,T |zi,t ) parametrized by . Given latent variables zi,t, we define the fixed-size augmented latent state Zi,t as follows:
Zi,t = (zi,t, zi,t-1, zi,t-2, zi,t-3, ai,t-1, ai,t-2, ai,t-3) (13)

B. Reward Engineering

In a multi-agent system with distributed control, where each agent acts independently, each agent needs a notion of safety margin with respect to other agents. While a naive distance function is widely selected as the base metric for setting up a reward function, we instead propose on using a value function derived from the HJ reachability. It encodes the degree of safety as a function of all relative state variables rather than just relative position, as defined in (3). HJ reachability also accounts for the system's dynamics. We calculate the time-invariant safety value function Rij(xij) as described in (2) to (7) and we define the reward function of agent i as follows:

-5 if j, Rij  1 AND at,i,T = 0



ri,t,T

(oi,t,T

,

at,i)

=

k × minj(Rij) -300 if collision

if j, Rij  with any agent

0 j



300 finish the task without collision

(14)

Fig. 2: Architecture of LSTM-VAE. An input observation oi,t,T is converted to the latent space using the encoder network and is sampled via reparametrization trick. A decoder network converts the sampled latent space to the observation an both networks are meta-trained by maximizing the ELBO (15)
where j = 1, ..., N, i = j and k is constant factor. This reward function is derived from the following logic. The first row implies that when there is not any possible danger (all safety values are greater than 1), the agent will receive a big punishment if it does not follow its default controller. The second row implies that if the pairwise safety between the agent and other agents is less than zero, then the reward will be the most negative safety value across all agents j with a scaling factor k. The other parts of (14) are the sparse rewards that the agent will receive at the end of episode depending on whether it finishes its own task or collides with another agent. We are not considering any continuous reward if all safety values are in the range (0, 1]. However, since we are defining discount factor  in our MDP definition, decreasing the discount factor motivate agents to finish their tasks sooner.
C. Off-Policy Meta-Reinforcement Learning
In this section, we explain how we combine our proposed LSTM-VAE with an off-policy RL algorithm, and how we train them all in a meta-training loop. Our off-policy meta RL makes the learning procedure sample efficient in both training and adaptation loops. However, efficient offpolicy meta-RL has two main challenges [24]. (1) the same distribution of data should be used for both meta-training and meta-testing. This indicates that since the meta-testing phase is usually done on on-policy data, the meta-training is also should be done on on-policy data as well. (2) The meta policy should be able to reason about the distribution of experience by optimizing the distribution of visited states. This means the policy gradient based methods are more appealing for meta reinforcement learning. To remedy these challenges, we propose using Proximal Policy Optimization (PPO) as the off-policy actor-critic method [28] and we meta-train our algorithm using MAML [7]. The PPO algorithm is not truly off-policy; however, it enables multiple epochs of update from replay buffer and is more sample efficient compared to other on-policy methods. Moreover, directly learning a policy can be more effective on stochastic exploration rather than deriving the policy from a value network, as is done value-based methods. The LSTM VAE is meta-trained via MAML [7] across different tasks using reparameterization trick by maximizing the resulting Evidence Lower Bound

(ELBO) [29]:

EOT [Ezq[log p(O|z)] - DKL(q(z|O) || p(z)) (15)

where p(z) is prior distribution over z. To prevent LSTM blocks from forgetting critical data, for the observation of each agent i in (9), we sort the (xij) in ascending order of HJ safety value function Rij, and feed them to LSTM block such that the relative state of most critical agent ­ the agent j with minimum Rij ­ is fed last. The critic is meta-trained via the following bootstrap update rule:

Lcritic = Es,r,s T ,Zq(z|s),Z q(Z |s)[Q (Z) -(r + Q¯(Z ))]2

(16)

where Q¯ is the target network. The policy which predict the categorical distribution over actions defined in (11) is metatrained using the following clipped surrogate loss with KL penalty term:

Lpolicy = Es,rT ,Zq(Z|s)[min(rt()A¯t, clip(rt(), 1 - ,
1 + )A¯t - DKL(old (.|Z)||(.|Z))] (17)

where the probability ratio rt()

=

,  (at|zt)
old (at|zt)

is the

clipping hyper parameter value,  is a constant coefficient,

DKL is shorthand of KL-divergence between new and old policy and A¯t is the Generalized Advantage Estimation

(GAE) which is computed as in [30]:

A¯t = t + ()t+1 + ... + ()T -t+1T -1

(18)

where t = rt(s, a) + Q(Zt+1) - Q(Zt)

The Alg. 1 illustrates the proposed meta-training loop.

V. SIMULATED AND REAL-WORLD EXPERIMENTS
In this section, we will explain about the implementation, training and performance of our proposed method in both simulation and experimentation. We compare our method with a classical sub-optimal approach and evaluate the specific choice of our design through ablation study. It should be noted that since the LR-CAM objective is to be least restrictive by only interrupting the default controller to avoid danger, it cannot be compared with previous collision avoidance algorithms such as ORCA [31] that solve the goal reaching and avoidance simultaneously. As a result, we chose the method presented in [19] as the classical baseline because similar to LR-CAM it tries to maintain safety by interrupting the default controller.

A. Implementation
Since there are no standard benchmark tests set for multiagent RL, we created our own simulation environments using the Gazebo and ROS open source software. We created four different tasks, with three to six agents working simultaneously. We considered a default controller for each agents, in all the aforementioned four tasks, with the objective of reaching a specific location within the environment or simply to follow a path. To show the performance of our algorithm in presence of static obstacles in the environment, we make the agents stationary under two scenarios: (1) we randomly freeze some of agents, or (2) we let agents stop-and-stay in the environment when they finish their default tasks.

Algorithm 1 proposed meta-training loop

Require: initialize parameters , , 

initialize reply buffers Bi for each training task

1: while True do

2: for k=1, ...,K do

3:

for each task {Ti} do

4:

for each agent in task {Ti} do

5:

sample zt  q(z|s)

6:

Calculate Z using (13)

7:

Roll out policy (at|Zt) to

collect data {ot, at, rt, ot} and add to Bi

8:

end for

9:

end for

10: end for

11: for each task {Ti} do

12:

pull data from Bi and calculate GAE

and update buffer with {ot, at, rt, ot, A¯t}

13: end for

14: for step in PPO off-policy training steps do

15:

for Ti do

16:

sample mini batch bi  Bi

17:

sample z  q(z|bi)

18:

Calculate Z

19:

if step < heat-up-training-steps then

20:

   - lr1LELBO(bi, Z)

21:

else

22:

   - lr1LELBO(bi, Z)

23:

   - lr2Lcritic(bi, Z)

24:

   - lr3Lactor(bi, Z)

25:

end if

26:

end for

27: end for

28: end while

In simulation We localize agents based on their odometry data and, in real experiment we localize agents using their on-board sensors. We make the localization information available to all other agents. To make the simulation/training runs a better reflection of the reality, we also add white noises to all states. The LR-CAM module is implemented in TensorFlow 2.0. For real world experiments and simulations, we used the TurtleBot3 Burger robots which are equipped with an onboard 360-degree 2D LiDAR and Gyro (Fig. 1a). We also upgrade their raspberry pi development board to Nvidia Jetson Nano to increase their computational capacity.
To calculate the reward function, we approximate the Turtlebot dynamic with simple Dubins car, where the pairwise relative dynamic of (2) is written as the following planar kinematic model:

px,ij = -v + v cos(ij ) + ipy,ij

py,ij = v sin(ij ) - ipx,ij

(19)

p,ij = j - i, |i|, |j |  ¯

For training and testing, we consider moderate and difficult scenarios. In the former, initial and target locations for each agent are randomly selected between two concentric circles with a radii r1 and r2 (r1  r2), respectively. Then, we add random perturbation to the agents' initial states. Under this setting, one agent usually comes to conflict with maximum two other agents. In the latter, however, agents' locations are

Fig. 3: Comparison with Baseline
Fig. 4: comparison of success rate when a subset of agents in a difficult-task scenario(with r = 1.7) uses LR-CAM.
randomly initialized around a circle with radius r and their objectives are chosen such that agents would pass through the center of this circle to reach their goal locations. Under this scenario, each agent can be in conflict with two or more agents. To evaluate the performance of our algorithm, we calculate the success rate to be the fraction of agents that succeeded in reaching a goal location without colliding with other agents over 100 trials for each scenario.
B. Results Fig. 3 compares the success rate of our proposed approach
with baseline across different tasks in a difficult test case scenario. To calculate the variance of success rate we repeat each success rate calculation 5-times. As the number of agents in a task increased, the success rate is decreased for both algorithms; however, the variation is negligible for our method (more than 90 percent success rate for all cases). In addition, our proposed method outperforms the baseline approach at least by 30 percent in all cases.
we also evaluate the performance of our proposed method in the case in which only some of the agents uses LR-CAM. To this end, we designed the following experiment. First, in a 5-agent task we start with no agent and increase it all the way to all agents using the LR-CAM. Results for each cases in visualized in Fig. 4. The results indicate that even only if some of agents uses LR-CAM, the success rate for agents who use LR-CAM and also for the rest of agents increased significantly.
We also tested our approach in an unseen environment involving N = 8 agents. The first row of Table I shows the success rate in a test case where N = 8 in (9). In the second row we only feed each agent the six lowest safety values while ignoring the remaining 2 safety values.
Fig. 5 and Fig. 6 show trajectory visualization for a 6agent task and an 8-agent task respectively. In Fig. 5, we chose a crowded initial condition with r = 1.5 and in Fig. 6, agents are initialized a bit farther from each other. In

Fig. 5: Trajectory visualization for 6-agent task in a crowded initial condition (r = 1.5). Agents are color coded and the level of opacity in the figures is proportional to the elapsed time. Yellow dots inside tubes indicated and all the timesteps that the LR-CAM of agent is interrupting the default controller.

value function (Rij) as follows:

ri,t,T (oi,t,T , at,i) =

-5

if i, L2ij  1 AND at,i,T = 0



k × minj((L2ij - d))

if j, L2ij  d

-300   300

if collision with any agent j finish the task without collision

(20)

where d is the collision radius defined in (6). Table II shows the comparison of success rate for using the HJ-based reward (first row) and the distance-based reward (second row) for difficult test scenarios with r = 1.7. We observed that not only did the success rate improve by approximately 10%, but also the training time reduced by 25% when using our proposed HJ-based reward. To compare how LR-

TABLE II: Ablation study for reward design - success rate comparison

Algorithm Ours with HJ reward Ours with naive distance reward

4-agent 5-agent 6-agent

0.99

0.93

0.91

0.89

0.86

0.75

CAM evaluates the safety and upcoming dangers, we define "restrictiveness factor" to be the fraction of interrupting actions over all actions in a total of 100 trials for each scenario/task. As it can be seen from Table III, using our proposed HJ-based reward improved the restrictiveness factor by 10%, which means that our proposed reward is 10% less restrictive and allows the default controller to be used more often.

Fig. 6: Trajectory visualization for an unseen 8-agent task in TABLE III: Ablation study for reward design - restrictiveness

a long range initial condition (r = 5.5)

comparison

both figures, the agents' trajectories are color coded with the opacity proportional to the elapsed time.
To visualize how exactly LR-CAM intervenes the default controller we overlay yellow dots inside the colored tubes for all time steps in which LR-CAM interrupts the default controller to perform avoidance actions. Based on these trajectories, when the agents are in dangerous configurations, LR-CAM takes over control and ,when the safety value is relatively high, it will let the agent execute its default controller. Fig. 1-b shows the trajectories of four TurtleBots in a real-world 4-agent task.
Additional experiments involving two to four agents can be found at https://bit.ly/34K8YKB .

TABLE I: Performance of LR-CAM in an unseen task with 8 agents

Test Case Observation Space with all agents Observation space with first six critical agents

Success Rate 0.85 0.91

C. Ablation Studies
Reward Design. To demonstrate the benefits of our reward function, we meta-trained a separate policy using a naive distance-function based reward. In this setting we modify (14) using the Euclidean distance (L2ij) instead of safety

Algorithm Ours with HJ reward Ours with naive distance reward

4-agent 5-agent 6-agent

0.46

0.45

0.48

0.54

0.54

0.48

Observation Space Design. To demonstrate the effectiveness of our observation space design, we trained a policy using only information from a single time step as observation. According to Table IV, using a history of observations/actions improved the success rate significantly.

TABLE IV: Ablation study for observation space design success rate comparison

Algorithm Ours with history of observations Ours with single step observation

4-agent 5-agent 6-agent

0.99

0.93

0.91

0.82

0.58

0.56

VI. CONCLUSIONS
This paper presented the least-restrictive collision avoidance module (LR-CAM) that can be added on top of autonomous agents to intervene and avoid collisions based on the joint configuration of multiple agents. We also proposed a novel off-policy meta reinforcement learning framework to train the LR-CAM. Future work includes augmenting sensory observations as the main source of input.

REFERENCES
[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., "Human-level control through deep reinforcement learning," nature, vol. 518, no. 7540, pp. 529­533, 2015.
[2] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., "Mastering the game of go with deep neural networks and tree search," nature, vol. 529, no. 7587, pp. 484­489, 2016.
[3] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement learning," arXiv preprint arXiv:1509.02971, 2015.
[4] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, "Trust region policy optimization," in International conference on machine learning, 2015, pp. 1889­1897.
[5] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection," The International Journal of Robotics Research, vol. 37, no. 4-5, pp. 421­436, 2018.
[6] C. Finn and S. Levine, "Deep visual foresight for planning robot motion," in 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 2786­2793.
[7] C. Finn, P. Abbeel, and S. Levine, "Model-agnostic meta-learning for fast adaptation of deep networks," arXiv preprint arXiv:1703.03400, 2017.
[8] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, "Fast reinforcement learning via slow reinforcement learning," arXiv preprint arXiv:1611.02779, 2016.
[9] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel, "A simple neural attentive meta-learner," arXiv preprint arXiv:1707.03141, 2017.
[10] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural Computation, vol. 9, no. 8, pp. 1735­1780, 1997. [Online]. Available: https://doi.org/10.1162/neco.1997.9.8.1735
[11] I. M. Mitchell, A. M. Bayen, and C. J. Tomlin, "A time-dependent hamilton-jacobi formulation of reachable sets for continuous dynamic games," IEEE Transactions on automatic control, vol. 50, no. 7, pp. 947­957, 2005.
[12] P. Fiorini and Z. Shiller, "Motion planning in dynamic environments using velocity obstacles," The International Journal of Robotics Research, vol. 17, no. 7, pp. 760­772, 1998. [Online]. Available: https://doi.org/10.1177/027836499801700706
[13] R. Olfati-Saber and R. M. Murray, "Distributed cooperative control of multiple vehicle formations using structural potential functions," in IFAC world congress, vol. 15. Barcelona, Spain, 2002, pp. 242­248.
[14] Y.-L. Chuang, Y. R. Huang, M. R. D'Orsogna, and A. L. Bertozzi, "Multi-vehicle flocking: scalability of cooperative control algorithms using pairwise potentials," in Proceedings 2007 IEEE international conference on robotics and automation. IEEE, 2007, pp. 2292­2299.
[15] D. Zhou, Z. Wang, S. Bandyopadhyay, and M. Schwager, "Fast, online collision avoidance for dynamic vehicles using buffered voronoi cells," IEEE Robotics and Automation Letters, vol. 2, no. 2, pp. 1047­ 1054, 2017.
[16] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha, "Reciprocal n-body collision avoidance," in Robotics research. Springer, 2011, pp. 3­19.
[17] K. Margellos and J. Lygeros, "Hamilton­jacobi formulation for reach­ avoid differential games," IEEE Transactions on Automatic Control, vol. 56, no. 8, pp. 1849­1861, 2011.
[18] J. F. Fisac, M. Chen, C. J. Tomlin, and S. S. Sastry, "Reach-avoid problems with time-varying dynamics, targets and constraints," in Proceedings of the 18th international conference on hybrid systems: computation and control, 2015, pp. 11­20.
[19] M. Chen, J. C. Shih, and C. J. Tomlin, "Multi-vehicle collision avoidance via hamilton-jacobi reachability and mixed integer programming," in 2016 IEEE 55th Conference on Decision and Control (CDC). IEEE, 2016, pp. 1695­1700.
[20] Y. F. Chen, M. Liu, M. Everett, and J. P. How, "Decentralized noncommunicating multiagent collision avoidance with deep reinforcement learning," in 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 285­292.
[21] T. Fan, P. Long, W. Liu, and J. Pan, "Distributed multi-robot collision avoidance via deep reinforcement learning for navigation in complex scenarios," The International Journal of Robotics Research, vol. 39, no. 7, pp. 856­892, 2020. [Online]. Available: https://doi.org/10.1177/0278364920916531
[22] M. Everett, Y. F. Chen, and J. P. How, "Motion planning among dynamic, decision-making agents with deep reinforcement learning," in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018, pp. 3052­3059.

[23] E. Shelhamer, P. Mahmoudieh, M. Argus, and T. Darrell, "Loss is its own reward: Self-supervision for reinforcement learning," arXiv preprint arXiv:1612.07307, 2016.
[24] K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen, "Efficient off-policy meta-reinforcement learning via probabilistic context variables," in Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. Long Beach, California, USA: PMLR, 09­15 Jun 2019, pp. 5331­5340.
[25] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick, "Learning to reinforcement learn," arXiv preprint arXiv:1611.05763, 2016.
[26] B. C. Stadie, G. Yang, R. Houthooft, X. Chen, Y. Duan, Y. Wu, P. Abbeel, and I. Sutskever, "Some considerations on learning to explore via meta-reinforcement learning," arXiv preprint arXiv:1803.01118, 2018.
[27] F. Sung, L. Zhang, T. Xiang, T. Hospedales, and Y. Yang, "Learning to learn: Meta-critic networks for sample efficient learning," arXiv preprint arXiv:1706.09529, 2017.
[28] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017.
[29] D. P. Kingma and M. Welling, "Stochastic gradient vb and the variational auto-encoder," in Second International Conference on Learning Representations, ICLR, vol. 19, 2014.
[30] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, "Highdimensional continuous control using generalized advantage estimation," arXiv preprint arXiv:1506.02438, 2015.
[31] J. Van den Berg, M. Lin, and D. Manocha, "Reciprocal velocity obstacles for real-time multi-agent navigation," in 2008 IEEE International Conference on Robotics and Automation. IEEE, 2008, pp. 1928­1935.

