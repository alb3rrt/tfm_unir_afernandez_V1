arXiv:2106.00564v1 [cs.IT] 1 Jun 2021

1
Wireless Federated Learning with Limited
Communication and Differential Privacy
Amir Sonee, Stefano Rini and Yu-Chih Huang
Abstract This paper investigates the role of dimensionality reduction in efficient communication and differential privacy (DP) of the local datasets at the remote users for over-the-air computation (AirComp)-based federated learning (FL) model. More precisely, we consider the FL setting in which clients are prompted to train a machine learning model by simultaneous channel-aware and limited communications with a parameter server (PS) over a Gaussian multiple-access channel (GMAC), so that transmissions sum coherently at the PS globally aware of the channel coefficients. For this setting, an algorithm is proposed based on applying (i) federated stochastic gradient descent (FedSGD) for training the minimum of a given loss function based on the local gradients, (ii) Johnson-Lindenstrauss (JL) random projection for reducing the dimension of the local updates and (iii) artificial noise to further aid user's privacy. For this scheme, our results show that the local DP performance is mainly improved due to injecting noise of greater variance on each dimension while keeping the sensitivity of the projected vectors unchanged. This is while the convergence rate is slowed down compared to the case without dimensionality reduction. As the performance outweighs for the slower convergence, the trade-off between privacy and convergence is higher but is shown to lessen in high-dimensional regime yielding almost the same trade-off with much less communication cost.
Index Terms Federated edge learning; Differential privacy; Random projection; Over-the-air-computation.
I. INTRODUCTION
Recently, FL has emerged as a promising paradigm for distributed edge learning over centralized networks focusing on edge computations without the need to communicate users' large datasets. This provides capability of preserving privacy for the users' datasets as well as communication-efficiency. This setting is relevant in a host of modern-day training scenarios in which some deep learning model is to be trained over big data available at a set of remote users whose privacy and anonymity has to be preserved in the course of learning process. When remote users and PS are connected wirelessly, one can exploit the properties of the radio environment for broadband over-the-air model aggregation which greatly reduces the communication latency while increasing bandwidth efficiency [1]. In the current big data-intensive applications, the model exchanged between large number of remote users and the PS through training is relatively large, so that dimensionality reduction techniques can enormously facilitate computation, storage and communication over bandwidth-limited channels.
Stefano Rini and Yu-Chih Huang are with the Department of Electrical Engineering and Computer Science, National Yang Ming-Chiao Tung University (NYCU), Hsinchu, Taiwan. Email: amir.sonee@mail.um.ac.ir,{stefano,jerryhuang}@nctu.edu.tw.

2

Literature Review: Various approaches have been proposed in the literature to address FL performance in terms of communication efficiency, privacy, and AirComp. The efficiency schemes put forth in the literature mainly fall into two categories: gradient sparsification and gradient quantization. Sparsification methods highly rely on fixed or variable rate elimination of the dimensions of the gradient vector based on a specific criterion such as magnitude or variance [2]­[5]. This is while quantization methods focus on discretizing the gradient vectors through dimension-wise [6] or vector quantization [7]. Data privacy in FL model has been mainly addressed through DP as a context-free notion evaluating the privacy loss incurred by membership attacks to extract information about the individual sample points [8]. One most common method to preserve privacy is via local perturbation of the gradients by an artificial noise of Gaussian or Laplacian distributions [9]­[12]. Finally, motivated by the use of FL in emerging technologies such as IoT, V2V and D2D communications between mobile or wireless edge devices over wireless media, the principle of AirComp has been put forth to further extend the original FL formulation presented for noiseless, dimension-unlimited channel to the AirComp FL model incorporating characteristics of the wireless radio environment in communication channel model by considering that transmissions between the clients and the PS occur over MAC [1], [13].
Contributions: This paper leverages dimensionality-reduction technique featuring its further contribution to enhance privacy in FL setting in addition to efficiency. Specifically, we propose a scheme referred to as differentially private random projection FedSGD (DPRP-FedSGD) addressing the interplay of these three ingredients in the FL problem formulation: (i) efficiency, (ii) privacy, and (iii) AirComp. We will show, in particular, that through appropriate use of a dimensionality-reduction linear random projection of JL type like Gaussian or Sub-Gaussian distributions that preserves almost isometry of the projected vectors, we can incorporate these issues through (i) reducing the communication length for efficiency (ii) bringing about more per-dimension Gaussian artificial noise with fixed noise power at clients for local DP (LDP), and (iii) inverting the aggregated vector through the transpose of the random projection matrix for AirComp-aided update of global model. Finally, we analyze the training and privacy performance of DPRP-FedSGD in terms of the convergence rate and LDP of the underlying mechanisms and show that LDP is scaled down as O( r/d) while convergence is scaled up as O(d/r). Moreover, an algorithm resulting in optimal convergence of DPRP-FedSGD is proposed exploiting the static optimal noise power allocation and reduced dimension.
Notation: [n] represents the set of integers {1, . . . , n} and x p indicates the p-norm of vector x.

II. PRELIMINARIES

A. Federated Stochastic Gradient Descent (FedSGD)

As a distributed ML model, FL consists of n clients aiming at collaborative optimization of an empirical loss function

1

L(w) = |D|

|Di| Li (w) ,

(1)

i[n]

over the model vector w  Rd and under the coordination of the PS where Li(w) is the local loss function computed over the disjoint local datasets Di at client i with D = i[n]Di. The prevalent approach for numerical optimization of (1) is through iterative application of (synchronous) distributed stochastic gradient descent, also known as federated SGD (FedSGD) over T

iterations. This is a large-scale variant of SGD wherein each client i locally computes the stochastic gradient vector at iteration

3
t  [T ], t = 1, as git = Li wt-1 with access to the global model update wt-1 of previous iteration. Subsequently, the PS aggregates the local gradients so as to obtain an unbiased estimation (stochastic gradient) of the true global gradient L(wt) as gt = i[n] git/n which is employed in global model updating as wt = wt-1 - tgt where t is the iteration-dependant learning rate. A vector gt is called a stochastic gradient of L if E [gt] = L (wt).

B. Dimensionality-reduction via Random Projection

Reducing the dimension of the transmitted local gradients (models) is generally considered as a sparsification method. A

suitable way for this approach is through the ubiquitous database-friendly random projection (RP) as proposed by Johnson-

Lindenstrauss (JL) [14]. The main idea comprises first generating a d × d random projection matrix (RPM) U with entries

drawn i.i.d. from a specific distribution satisfying the asymptotic orthogonality of the rows, and then projecting the space

of d-dimensional vectors into the subspace of r dimension using this random matrix as zti = Ugit. Among the common distributions for random matrix projection, standard Gaussian or sub-Gaussian such as Achlioptas are widely used where the

latter results in sparser random matrix projection. The overall procedure of JL transformation leads to a high probability

2-norm

unbiased

projection

to

a

lower

dimensional

vector

i.e.

|

zti

22/

git

2 2

-

1|





with

probability

1/na

so

long

as

r  (4 + 2a) 2/2 - 3/3 -1 ln n, 0 <  < 1 and a > 0 which we refer to as the JL condition [14].

C. Differential Privacy

As the distributions of the databases at clients are unknown in the ML models, DP is perhaps the most rigorous context-free criterion to quantify and measure the privacy of the learning process. When the PS is assumed to be curious but honest, one suitable way to guarantee privacy is by having clients individually apply randomized algorithm on their local updates. This latter approach is referred to as local differential privacy (LDP). More specifically, let fi : Di  Zi be the query function composed of providing the local updates, based on the local dataset Di, followed by an RPM reducing the dimension, then a mechanism Mi : Zi  Y releasing the output of the query function to the PS is said to be ( ti, t)-LDP at client i if for any zti, zit  Zi  Rr and any measurable subset S  Y,

Pr Mi

zit

S

t
 e i Pr Mi

zti

S

+ t.

(2)

The quantity

t i

can

be

equivalently

viewed

as

the

bound

on

privacy

loss

Ltp

=

ln (Pr [Mi (zit)  S] /Pr [Mi (zti)  S]),

attained with probability at least 1 - t at client i as Pr

|Ltp| 

t i

 1 - t, measuring the indistinguishability between two

sample points of its database given any observation subset of the mechanism output. Throughout this paper, we assume the

output spaces Y, S  Rr. In the context of FL, the privacy loss over T iterations referred to as T -fold LDP is considered which

is shown to guarantee the worst case

t[T ]

t i

,

t[T ] t -LDP by the composition theorem, [9]. One factor of paramount

importance in limiting privacy loss is through bounding the change in some metric quantities of the output of the query function

with the change in the input. This quantity is specified by the sensitivity of the query function which in case of a randomized

query function is defined as follows. The query function fi is said to be  q ,  sensitive w.r. to q-norm if for any two

4
neighbouring datasets Di and Di there exists two coupling random variables Zi, Zi  Zi with the same marginal distribution as fi (Di) and fi (Di), respectively, such that Pr Zi - Zi q  q  1 -  , [6].

D. MAC AirComp

In the following, we consider a form of AirComp in which clients transmit their perturbed projected local gradients simultaneously to the PS over a flat-fading MAC, described at each iteration by the input/output vector relationship

yt =

htixti + nt,

(3)

i[n]

so that the aggregated gradients can be estimated from the channel output. The channel coefficients hti are assumed to be constant over each iteration and known locally at each client and globally at the PS. nt is the additive noise, assumed standard white Gaussian. Also, the channel input xti is also subject to the average power constraint

E

xti

2 2

 Pi.

(4)

Note that it is assumed that the down-link channel has infinite capacity and for the PS to update the global model based on an estimation of the true gradient, a post-processing operation on the received vector is carried out as g^t = fr (yt).

III. PROBLEM FORMULATION AND PROPOSED APPROACH

We consider a setting combining the three components of an FL in Sec. II. In particular, we assume that FedSGD takes place in the setting in which one computation of the gradient is sent by r < d transmissions over the MAC in (3). For this scenario, we consider the problem of designing efficient communication algorithms which maximize the convergence rate of the model estimate to the optimal value under constraint on (i) the target T -fold LDP, and (ii) the communication taking place over r channel uses as the MAC in (3). The convergence performance in terms of the optimality gap (T ), defined as

(T ) = E [L (wT )] - L (w) ,

(5)

where w is the unique solution of the minimization of (1). Note that the expectation in (5) is over the randomness in

the channel noise, as well as any source of randomness in the communication scheme. In conclusion, for a given learning

problem of dimension d, and with L-smooth (having L-Lipschitz continuous gradients) and -strongly convex loss function,

the performance in (5) is a function of the time horizon T as well as (i) the number of channel transmissions for gradient

update rt, (ii) the coefficients of the MAC at each iteration t, and (iii) the target privacy level

T i

at

user

i

at

iteration

T.

A. DPRP-FedSGD Scheme Based on the techniques in Sec. II, we propose the following transmission strategy referred to as DPRP-FedSGD Scheme.

5

RPM construction: First, the random projection matrix U  Rd×d is generated with entries drawn independently from

Rademacher distribution (symmetric Bernolli taking values +1 and -1 with probability 1/2), or according to the Gaussian

distribution of zero mean and unit variance as [U]i,j  N (0, 1), or Achlioptas distribution, i, j  [d], given by



 

+

s,

1/2s



 

[U]i,j =

0, 1 - 1/s ,

(6)

    

 - s,

1/2s

and is assumed to be shared between the clients and the PS through a random seed at each iteration. Gradient projection: Each client i  [n] at iteration t  [T ] projects the local gradient git into the an 2-norm unbiased random vector zti as

zti

=

1 
rt

DrUgit

=

Tgit

(7)

where Dr is a rt × d rectangular diagonal matrix i.e. [Dr]i,i = 1, i  [rt], and [Dr]i,j = 0, j = i. Such random projection

into r-dimensional subspace, preserves the unbiasedness of the Euclidean-norm as E

zti

2 2

=

git 22.

AirComp: client i transmits a phase-compensated noisy scaled variant of the projected vector satisfying the power constraint:

xti = e-jti

itPi L

zti

+

itPi rt

mti

,

(8)

where git 2  L is the second-order bound of the local gradient, and it and it represent the fraction of the power dedicated to the transmission of the projected signal and the artificial noise, respectively, with it + it  1.
As a result of the channel model in (3), the PS receives

yt =

itti L

zti

+

itti rt

mti

+

nt,

(9)

i[n]

i[n]

at iteration t where ti = Pi|hti|2 is the individual signal to noise ratio (SNR) of client i at the PS, and then makes the following post-processing to estimate the global gradient for model updating

g^t

=

1 nct

TT

yt

=

1 nct

itti rtL

Urgit

(10)

i[n]

1 + nct
i[n]

itti rt

UTr mti

+

1  nct rt

UTrt

nt

where Ur = DrU = [Ur,1| . . . |Ur,d] and Ur = UTr Ur with E Ur = rtId for the three distributions generating U. As a result, for the global gradient estimation to remain unbiased i.e. E[g^t] = gt, it is essential to have iti/L = c for some constant c satisfying i  1, i  [n]. This corresponds to the value ct = tmin/L where tmin = mini[n] ti, and the fraction of the power allocated to the transmission of the projected gradient can be obtained as it = tmin/ti.

As a result of this post-processing by the PS, the estimated global gradient can be written as

g^t= 1 n

1 rt

Urgit

+

1 nc

i[n]

i[n]

iti rt

UTr mti

+

1  nc rt

UTr nt

(11)

6
where the first term corresponds to the true global gradient and the other two terms is the equivalent noise vector of dimension d appearing as a result of AirComp.
IV. MAIN RESULTS In this section, we first present the performance of DPRP-FedSGD algorithm in terms of the LDP analysis in Sec. IV-A. Specifically, in Theorem 1, we rely on the JL lemma to show that for a given budget on the artificial noise power, LDP scales as O r(1 + )/d providing a better privacy level with r = O(ln n) compared to the case of no reduction with high gain in high dimension regime. Furthermore, in Theorem 2, the results for general r is proved by invoking exponential concentration bounds. We then turn our focus to the convergence analysis in Sec. IV-B and show that the convergence scales almost as O (d/r) introducing slower convergence compared with the no reduction case. Also, to achieve the same performance on the convergence bound (LDP) after a specific large number of iterations, LDP (convergence) performance remains almost the same for both schemes but with less communication cost for the dimensionality reduction case. Based on the analysis in this section, numerical results in Sec. V demonstrate that in high-dimensional regime and especially with high-level privacy , the DPRP-FedSGD scheme allows us to find some operating points for r releasing a very close performance in terms of the convergence-privacy trade-off compared to the non-dimensionality-reduction case.

A. LDP analysis

Since post-processing performed by the PS to reconstruct the global gradient does not affect the privacy mechanism based on [9, Prop. 2.1], it suffices to go through the channel output (9) to investigate the LDP loss. As the equivalent noise of the signal received by the PS is Gaussian distributed, the local differential privacy loss at client i can be upper bounded to i with probability greater than 1 - ,   [0, 1], as

i

=

y nc

2 ln

1.25 ,


(12)

where n2c is the variance of the effective noise at the output of the channel n2c = i[n](itti/rt) + 1, and y is the high probability (that is,  1 -  ),   [0, 1], 2-norm sensitivity, [6], of the query function fi producing the projected vector at

client i as a randomized function of the local database Di.

Theorem 1: The DPRP-FedSGD scheme with an RPM of JL transformation type can guarantee T -fold

T i

,

T

=

t[T ]

t i

,

t[T ] t + T /na -LDP where

t i

=

2

(1 + )

2tmin ln (1.25/t) i[n] (itti/rt) + 1

(13)

provided that the reduced dimension satisfies the JL condition r  (4 + 2a) 2/2 - 3/3 -1 ln n, 0 <  < 1 and a > 0.

Proof: It should be noted that if the reduced dimension satisfies the JL condition, then 2-sensitivity of the projected

vectors lies within (1 - ) and (1 + ) of the 2-sensitivity of the local gradient git 2 with probability 1 - 1/na regardless

of the type of distribution adopted for RPM i.e. (1 - )

git - git

2 2



zti - zit

2 2



(1

+

)

git - git

22. Accordingly, as

7

the RP mapping is

 1+

git - git

2 , 1/na

sensitive and the Gaussian mechanism is

composition is

T i

,

T

- LDP by [6].

t[T ]

t i

,

t[T ] t -LDP, the

It should be noted that as we increase the precision of the sensitivity for the projected vector, almost isometry is achieved with probability close to one, i.e. z w=.p.1 g  2L, using a universal linear RP of polynomial time and independent of the datasets and gradients. Moreover, in this case, the LDP of the proposed scheme outperforms the one without dimensionality

reduction given the same level of total power for the artificial noise vector. This is roughly expected as the sensitivity is

preserved with high probability after JL transform while the amount of noise variance per dimension is increased and hence

contribute more to privacy.

However, in case the reduced dimension is not satisfying the high-probability 2-norm concentration of the projected vector as in JL condition, for any value of a and , the following result can be derived regarding the LDP.

Theorem 2: The DPRP-FedSGD scheme with an RPM generated according to Achlioptas distribution can guarantee T -fold

(

T i

,

T

)

=

(

t[T ] ti,

t[T ] t + T  )-LDP where

t i

=

2

ln(1/ )

1+8s

rt

2tmin
i[n]

ln (1.25/t) (itti/rt) +

1

,

(14)

when rt  ln(1/ ) and

t i

=

2

ln(1/ ) 1 + 8s rt

2tmin
i[n]

ln (1.25/t) (itti/rt) +

1

,

(15)

when rt < ln(1/ ). A similar result can be derived with s = 1, in case of an RPM generated according to Rademacher or

Gaussian distribution.

Proof: The proof consists of providing a tight high-probability 2-sensitivity bound for part of the channel output corresponding to the transmitted signal of an individual client by invoking the tail bound for sub-exponential random variables. The details can be found in Appendix A.

Remark 1: The per-iteration LDP result for the FedSGD without dimensionality reduction was derived in [10] as

t i

=

2

2tmin
i[n]

ln(1.25/t) (itti/d) +

1

,

(16)

where i is the fraction of the power allocated to the artificial noise. As long as the reduced dimension r satisfies

rt 

i[n] itti

1+

-1
( ( )) 1+ i[n] itti/d 1 + ( ) i[n] itti 2

i[n] (itti/d) + 32s2 ln(1/ )

(17)

then the dimensionality reduction using RPM outperforms in terms of the LDP. Taking the JL transform into account then r

should satisfy

rt < (1 + )

i[n] itti i[n] itti/d

. + 2

8

The per-iteration LDP can be further upper bounded as

t i



2

rt(1 + ) n

2tmin ln (1.25/t mini itti

)

,

(18)

which compared to the case of no dimensionality reduction, is smaller by the ratio d/rt(1 + ) for the same artificial noise

allocations.

B. Convergence analysis

Next, we present our result for the convergence rate of the FedSGD algorithm considering an L-smooth and -strongly

convex loss function L.

Theorem 3: For an L-smooth and -strongly convex loss function, the convergence rate of the DPRP-FedSGD algorithm with learning rate t = 1/t using Achlioptas RPM can be upper bounded as







2L (T )  2T 2 

L2

d+st - 2 1+ rt

d + n2(ct)2 

itti rt

+1,

t[T ]

i[n]

with the similar result for Rademacher RPM if st = 1. Also, the convergence result for a Gaussian RPM is given by







2L (T )  2T 2 

L2

d+1 1+ rt

d + n2(ct)2 

itti rt

+

1

.

t[T ]

i[n]

Proof: The proof is provided in Appendix B. Remark 2: For further interpretation of the convergence, let us consider the same reduction in dimension as rt = r and the same channel coefficients over all iterations as hti = hi and so ct = c, indicating a static power allocation it = i and it = i. Accordingly, the bound on the convergence rate can be simplified and further related to the T -fold LDP as

2L3 (T )  2T

d+s-2 1+
r

+

16dL3

ln 2

1.25 
n2(

(1

T i

)2

+

)T

,

(19)

which shows that for a given number of iterations the upper bound is decreasing with the target T -fold LDP implying the

utility-privacy trade-off. Moreover, the RHS of this bound is convex with respect to T indicating that gap is bounded for a given

level of total privacy. This has been also verified in [12] for the FedAvg algorithm without considering dimensionality-reduction.

Compared to the bound provided on the optimality gap of the FedSGD without dimensionality reduction, [10], as

(T )

2L3 2T

+

16dL3 ln 2n2(

1.25



T i

)2

T

(20)

it can be verified that, for a fixed number of iterations and when the number of clients is large, the bound on the convergence for the proposed reduction scheme in (20) differs from the non-reduction case in that the first term is scaled by the ratio of the d/r. This scaling has also been observed in [15] for the case of cyclic projection in FedAvg. However, in case of large dimensions, both schemes have the same convergence performance for a given target T -fold LDP since in this regime, the second term of (20) dominates. Note though that our scheme attains this rate more efficiently with limited communication of r instead of d dimensions per client.

9

Next, we present a strategy on the static noise power allocation at clients and the reduced dimension to achieve the optimal

convergence rate subject to power and T -fold LDP constraints at each client. More specifically, we address the following mixed

integer nonlinear programming (MINLP) problem:



2L min  T r,{i}i[n] 2

L2

d+s-2 1+
r





d + n2c2 

ii + 1 r

i[n]

s.t. i + i  1,  i  [n]

2 (1 + )

2tmin ln (1.25/)



T
i  i  [n]

i[n] (ii/r) + 1 T

r  (4 + 2a) 2/2 - 3/3 -1 ln n.

(21)

Theorem 4: For the DPRP-FedSGD algorithm, the optimal bound on the convergence subject to a given per-client LDP

level and power constraints is given by







|(T

)|



2L 2T

L2

d+s-2 1+ r

d + n2c2 

i(r)+1

(22)

i[n]

where i(r) = min

i r

(1

-

i),

-

i-1 kk + k=1 r

and



=

maxi[n](1

+

)

8min (

ln(1.25/)

T i

/T

)2

-1

and

r

is

the

largest

value such that i(r + 1) = 0, i  [n]. The optimal values for the noise allocation coefficients are i = ri(r)/i.

Proof: The proof is provided in Appendix C.

V. NUMERICAL RESULTS
In this section, we provide numerical results though evaluation of the proposed performance results based on a scenario with n = 1000 clients trying to train a strongly-convex loss function of  = 0.001 over the model parameter of dimension d = 10000 used in classifying MNIST images through T = 1000 iterations. The clients transmit their local gradient updates subject to the same power constraint Pi = 1,  i  [n] and over a channel with coefficients drawn according to complex standard Gaussian distribution as CN (0, 1). The random projection is assumed to be performed with a matrix of Achlioptas entries with s = 1 and s = 2. Also, it is assumed that the T -fold LDP of each client should hold with probability at least 0.9 and so  =  = 5 × 10-5 per-iteration.
Fig. 1 shows that for a fixed budget on the power allocated on the artificial noise at clients, the DPRP-FedSGD algorithm can surpass the scheme without dimensionality-reduction for a specific range of r, in terms of the T -fold LDP.
In terms of the convergence, as shown in Fig. 2, the DPRP-FedSGD scheme underperform the existing scheme that do not make use of reduction. This actually introduces that in terms of the trade-off between convergence and privacy, DPRP-FedSGD presents lower performance but as in high-dimensional regime and specifically for stricter level of privacy, this trade-off, as shown in Fig. 3 is close to the corresponding performance without dimensionality-reduction.

10

200

180

160

140

T-fold LDP

120

100

80

60

40

Dimensionality-reduction, s=1

20

Dimensionality-reduction, s=2 No Dimensionality-reduction

00

0.5

1

1.5

2

Reduced dimension (r)

x 104

Fig. 1.

T -fold

LDP

(

T i

)

versus

reduced

dimension

compared

with

no

dimensionality

reduction,

T

=

1000

106

Dimensionality-reduction, s=1

105

No Dimensionality-reduction

104

convergence

103

102

101

1000

0.5

1

1.5

2

Reduced dimension (r)

x 104

Fig. 2. Convergence bound versus reduced dimension compared with no dimensionality reduction, T = 1000

103

Dimensionality-reduction, r=1536(H=0.3)

Dimensionality-reduction, r=5458(H=0.15)

102

without Dimensionality-reduction

Convergence

101

100

10-1

10-20

20

40

60

80

100

T-fold LDP

Fig. 3. Convergence bound versus T -fold LDP compared with no dimensionality reduction, T = 1000

11
VI. CONCLUSION
In this paper, DPRP-FedSGD scheme as a novel strategy to attain efficiency while preserving local differential privacy in AirComp federated learning was proposed and investigated. More precisely, we considered random projection of JL transform for reducing the dimension of the local gradients at remote clients to r < d with the aim of training the model through limited communication with the PS over a flat-fading MAC with much less channel uses than the one required to transmit the model size d. The projected gradients are then corrupted with artificial noise in order to enhance privacy and sent to the PS where they are accumulated and inverted by the transpose of the projection to update the global model. We provided an analysis on the differential privacy and convergence of the DPRP-FedSGD shows that under the same total artificial noise allocation, the LDP of clients outperforms the scheme in which only artificial noise is used for privacy by scaling down to O( r/d). This is mainly a result of the projection making each dimension experience more noise while keeping the sensitivity of the projected vector almost unchanged. However, the optimality gap is scaled up by O(d/r) resulting in slower convergence. This proposes a higher utility-privacy trade-off compared to the no projection scheme which can be almost mitigated in high-dimensional regime and hence guaranteeing almost the same performance with less communication cost.
REFERENCES
[1] G. Zhu, Y. Wang, and K. Huang, "Broadband analog aggregation for low-latency federated edge learning," IEEE Trans. Wireless Comm., vol. 19, no. 1, pp. 491­506, 2020.
[2] A. F. Aji and K. Heafield, "Sparse communication for distributed gradient descent," arXiv:1704.05021. [Online]., 2017, available: https://arxiv.org/abs/1704.05021.
[3] H. Wang, S. Sievert, S. Liu, Z. Charles, D. Papailiopoulos, and S. Wright, "ATOMO: Communication-efficient learning via atomic sparsification," in Proc. 32nd Adv. Neural Inf. Process. Syst. (NIPS), 2018, pp. 9850­9861.
[4] D. Alistarh, T. Hoefler, M. Johansson, N. Konstantinov, S. Khirirat, and C. Renggli, "The convergence of sparsified gradient methods," in Proc. 32nd Adv. Neural Inf. Process. Syst. (NIPS), 2018, pp. 5973­5983.
[5] L. P. Barnes, H. A. Inan, B. Isik, and A. O¨zg¨ur, "rTop-k: A statistical estimation approach to distributed SGD," Arxiv: 2005.10761v1. [Online], 2020, available: https://arxiv.org/abs/2005.10761v1.
[6] N. Agarwal, A. T. Suresh, F. X. Yu, S. Kumar, and B. McMahan, "cpSGD: Communication-efficient and differentially-private distributed sgd," in Proc. 32nd Adv. Neural Inf. Process. Syst. (NIPS), Montr´al, Canada, Dec. 2018, pp. 7564­7575.
[7] V. Gandikota, R. K. Maity, and A. Mazumdar, "vqSGD: Vector quantized stochastic gradient descent," Available: https://arxiv.org/pdf/1911.07971.pdf, 2019.
[8] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, "Membership inference attacks against machine learning models," in Proc. 38th IEEE Symp. Security and Privacy, CA. US., May 2017, pp. 3­18.
[9] C. Dwork and A. Roth, "The algorithmic foundations of differential privacy," Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3-4, pp. 211­407, Aug. 2014.
[10] M. Seif, R. Tandon, and M. Li, "Wireless federated learning with local differential privacy," in IEEE Int. Symp. Inf. Theory (ISIT), 2020, pp. 2604­2609. [11] D. Liu and O. Simeone, "Privacy for free: Wireless federated learning via uncoded transmission with adaptive power control," IEEE Journal on Selected
Areas in Communications, vol. 39, no. 1, pp. 170­185, 2021. [12] K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang, F. Farhad, S. Jin, T. Q. Quek, and H. V. Poor, "Federated learning with differential privacy: algorithms and
performance analysis," IEEE Trans. Inf. Forensics and Security, vol. 15, no. 4, pp. 3454 ­ 3469, Apr. 2020. [13] M. M. Amiri and D. Gu¨ndu¨z, "Federated learning over wireless fading channels," IEEE Trans. Wireless Comm., vol. 19, no. 5, pp. 3546­3557, 2020. [14] W. B. Johnson and J. Lindenstrauss, "Extensions of Lipschitz mappings into a Hilbert space," in Conf. in modern analysis and probability, 1984, pp.
189­206.

12
[15] S. Rini, M. Rao, and A. Goldsmith, "Distributed sub-gradient algorithms with limited communications," in 53rd Asilomar Conf. Signals, Systems, and Computers, 2019, pp. 2171­2175.
[16] J. Honorio and T. Jaakkola, "Tight Bounds for the Expected Risk of Linear Classifiers and PAC-Bayes Finite-Sample Guarantees," in Proc. 17th Int. Conf. Artificial Intelligence and Statistics (AISTATS), ser. Proceedings of Machine Learning Research, vol. 33. Reykjavik, Iceland: PMLR, Apr. 2014, pp. 384­392.
[17] A. Rakhlin, O. Shamir, and K. Sridharan, "Making gradient descent optimal for strongly convex stochastic optimization," in Proc. 29th Int. Conf. Machine Learning, Edinburgh, Scotland, June 2012.

APPENDIX A PROOF OF THE TH. 2

we have to go through the high-probability 2-sensitivity to further bound and compute the privacy loss. This can be formulated as the following tail bound for the 2-sensitivity random variable:

Pr

itti L rt

Ur git - git

2  z   .

(23)

or equivalently as

Pr

Ur git - git

2 2



rtL22z itti

 .

(24)

This result can be further tightened by approximating the square of the

2-norm random variable S =

Ur (git - git)

2 2

through a sub-exponential random variable. To show this, we should note that S can be written as the sum of the squares of

the inner products between each row k  [rt] of the reduced matrix Ur (kth column of UTr ) and the vector git - git as



2

S=

UTr,k, git - git 2 =



[U]k,j git - git j  .

k[r]

k[rt] j[d]

(25)

As the entries of the random projection matrix (RPM) are generated independently according to Rademacher, Achlioptas or Gaussian rv which are all Sub-Gaussian with parameter Uk,j (referred to as  SubG(Uk,j )), then so is their linear combination with parameter

USk =

2
Uk,j

[git

-

git]2j

=

j[d]

U2 [git - git]2j = U git - git 2 .
j[d]

(26)

where

since

the

entries

are

identically

distributed

2
Uk,j

=

U

with

U

=

1

for

Rademacher

and

Gaussian

distributions

and

U = s for Achlioptas distribution.

As the inner product is  SubG(USk ) then its square is Sub-exponential with parameters (S, b) (referred to as SubE(S, b)) 
where Sk = 4 2U2 Sk and bk = 4U2 Sk , [16]. As a result, their sum is  SubE (S, b) where

S =

S2k =

k[rt ]

S2

=

 rS

=

 4 2rU2

k[rt ]

b= max bk
k[rt ]

=

4S2

=

4U2

git - git

2 2

git - git

2 2

(27)

13

Now, we can propose the tail bounds for the 2-sensitivity in (24) based on the tail bounds provided for sub-exponential rv

with parameters (S , b) as



 

e-2 /2S2

Pr [S - µS > ] 

0    S2/b .

(28)

 

e-/2b

  S2/b

Accordingly, the tail bound on S can be given as

Pr

S

rtL22z itti

= Pr

S - µS



rtL22z itti

- rt

git - git

2 2


       

 (rt)2

L2 2z it ti

-

git -git

exp - 64rtU4

git -git

4 2

2 2



2 ,



        


rt
exp -

L2 2z it ti

-

git -git

2 2

8U2

git -git

2 2

 ,



z  3

it ti L



z > 3

it ti L

git - git 2 git - git 2


      

 rt

L2 2z it ti

-4

exp - 64×16U4

2 ,

(a)



       


rt
exp -

L2 2z it ti

-4

32U2

 ,



z  6 z > 6

itti = 6 itti = 6

tmin tmin

(29)

where (a) follows by the L-smooth condition of the loss function indicating that the gradient is L-Lipschitz continuous i.e.  w, w  Rd, |L (w ) - L (w)|  L w - w . This indeed implies that gradient and subgradients of the loss function are bounded i.e. L(w) 2  L,  w  Rd. Hence, git - git 2  2L by the triangle inequality.
Assuming the Rademacher and Gaussian distributions for random matrix projection, then U = 1 and the high probability 2-sensitivity holding with probability at least 1 -  can be computed as



 

2





tz=

tmin

1+8

ln(1/ rt

),



  

2

tmin

1

+

8

ln(1/ rt

),

tz  6 tz > 6

tmin tmin

(30)

and so the LDP at client i can be given as



  

2







t i

=

1+8

ln(1/ ) rt

2tmin ln(1.25/t)
( ) , i[n] itti /rt +n2

rt  ln(1/ )

(31)



    

2

1

+

8

ln(1/ rt

)

2tmin ln(1.25/t)
( ) , i[n] itti /rt +n2

rt < ln(1/ )

 Assuming the Achlioptas distribution, then then U = s and the high probability 2-sensitivity holding with probability

at least 1 -  can be computed as



 

2





tz=

tmin

1 + 8s

ln(1/ rt

),



  

2

tmin

1

+

8s

ln(1/ rt

),

tz  6s tz > 6s

tmin tmin

(32)

14

and so the LDP at client i can be given as



  

2







ti =

1 + 8s

ln(1/ ) rt



    

2

1

+

8s

ln(1/ rt

)

2tmin ln(1.25/t)
( ) , i[n] itti /rt +n2

rt  ln(1/ )

2tmin ln(1.25/t)
( ) , i[n] itti /rt +n2

rt < ln(1/ )

(33)

APPENDIX B PROOF OF TH. 3

Considering the loss function L is L-smooth and -strongly convex that is  w, w  Rd,

L (w )  L(w) +

g, (w - w)

 +

w -w 2,

(34)

2

Then a formal analysis of the convergence rate for the FedSGD algorithm is given by [17],

|E [L (wT)]

-

L (w)|



2L 2T 2

E

g^t

2 2

.

(35)

t[T ]

where

g^t = 1 n

1 r

Ur git

+

1 nc

iti r

UTr mti

+

1  nc r

UTr nt

(36)

i[n]

i[n]

git

nte

Accordingly, we have to bound the second-order moment of the estimated global gradient as

E

g^t

2 2

=E

gt

2 2

+E

nte

2 2





1 = (nrt)2 E 

Urgit

2
2

+

dn2 e

i[n]





 2

1 = (nrt)2

E

UTr,j Ur,k 

git  + dn2e

j[d] k[d]

i[n]

k



2



2

1

= (nrt)2

E

j[d]

Ur,j

4 2



git

j

1 + (nrt)2

E2

i[n]

j[d] k[d]

UTr,j Ur,k 2 

git k

i[n]



2

1 - (nrt)2

E2

j[d]

UTr,j Ur,k 2 

git j  + dn2e

i[n]

where the jth element of the equivalent noise vector nte is described as



nte

j

=

1  nc rt

[Ur,j ]q 

q[r]

i[n]



itti rt

mti q +

nt q .

N 0,

i[n]

it Pi r

|hti |2+n2

(37) (38)

15

Assuming the Rademacher distribution for the entries of matrix U then nte  N 0, n2e Id where





n2 e

=

1 (nct)2



iti rt

+ n2  .

(39)

i[n]

Assuming the Achlioptas distribution for the entries of matrix U then nte  1/sN 0, sn2e Id + (1 - 1/s)d(ne) where d represents the deleta dirac function. It can be shown that for this distribution E nenTe = n2e Id.
Assuming the Gaussian distribution for the entries of matrix U, then each term of the summation contributing to the [nte]j has the following PDF

1 f (un) = ne K0

|un| ne

(40)

where K0(. ) is the modified Bessel function of second type and order zero given as



 cos tz

K0(z) =
0

cos z sinh tdt =
0



dt.

1 + t2

(41)

However, we are not able to propose an explicit closed-form expression for the distribution of the equivalent noise unless we approximate it by a Gaussian distribution of zero mean and variance n2e in case of large reduced dimension and according to Central limit Theorem (CLT).

Assuming the Rademacher distribution for random matrix projection, then UTr,jUr,k is




 UTr,j Ur,k =

Ur,j

2 2

w=.p.1

rt,

k=j

(42)

 

Ur,

k=j

where Ur is a discrete r.v., with odd or even integer values between -rt and rt (depending on rt being odd or even), having zero mean and E[ UTr,jUr,k 2] = rt. Hence, the second-order moment of the global gradient estimation can be further simplified to

E

g^t

2 2

(rt)2 + rt(d - 1)

=

(nrt)2

git

2 2

+

dn2 e

i[n]



2





(b) (rt)2 + rt(d - 1)



(nrt)2



git

2

d + (nct)2 

iti rt

+ n2 

i[n]

i[n]





(c)
 L2

d-1 1 + rt

d + (nct)2 

iti rt

+

n2  .

(43)

i[n]

where (b) follows from the triangle inequality and (c) follows by the L-smooth condition indicating that the loss function has L-Lipschitz continuous gradients and so git 2  L, i  [n].

Assuming the Achlioptas distribution for sparse random projection then




 UTr,j Ur,k =

Ur,j 22,

k=j

(44)

 

Ur,

k=j

16

where Ur,j 22/s  Bin (rt, 1/s) and Ur is a discrete r.v., taking integer values between -rt and rt scaled by s, with zero mean and E UTr,jUr,k 2 = rt. Accordingly, second-order moment of the global gradient estimation can be simplified as

E

g^t

2 2

1 = (nrt)2

rt(s - 1) + (rt)2 + rt(d - 1)

git

2 2

+

n2 e

i[n]





L2

d + s - 2)

1+

rt

d + (nct)2 

iti rt

+

n2  .

(45)

i[n]

Assuming the Gaussian distribution for random matrix projection then




 UTr,j Ur,k =

Ur,j

2 2



2(rt),

k=j

(46)

 

Ur,

k=j

where Ur is a continuous r.v. of zero mean and E UTr,jUr,k 2 = rt, corresponding to the sum of rt independent r.v. each distributed according to (40) with zero mean and 2 = 1. The second-order moment of the global gradient estimation can be simplified as

E

g^t

2 2

1 = (nrt)2

(rt)2 + 2rt + rt(d - 1)

git

2 2

+

n2 e

i[n]





L2

d+1 rt + 1

d + (nct)2 

iti rt

+

n2  .

(47)

i[n]

APPENDIX C PROOF OF TH. 4

The optimization problem under consideration can be considered equivalently as the following problem





min L2
r,{i }i[n]

d+s-2 1+
r

d + n2c2 

ii + 1 r

i[n]

s.t.

ii r



i r

(1

-

i),



i  [n]

i[n]

ii r

+

1



max
i[n]

(1

+

) 8min (

ln (1.25/)

T i

/T

)2

r  (4 + 2a) 2/2 - 3/3 -1 ln n.

(48)

For the objective function to be minimum, it suffices to first fix a value for r starting at its lower bound in third constraint and then find the minimal set of clients satisfying the first two constraints. This requires finding the minimum number of users that can guarantee the T -fold privacy level through the accumulation of their noise powers at the PS. To this end, a similar approach to that of a water-filling scheme is leveraged as also used in [10]. More precisely, based on the power constraint, we sort clients in an decreasing order with respect to their rest of powers remaining from the alignment process which was designed so as to hold the unbiasedness condition. Then, we allocate i so as to satisfy the second constraint with equality

17

releasing the following expression as the minimum of the noise power terms satisfying both constraints:



i

=

ii r

=

min



i r

(1

-

i

),

 - i-1 kk r

+ .

(49)

k=1

This in fact suggests that maximum level of noise power that can be supported by clients is allocated so long as their aggregation does not exceed the RHS of LDP constraint. If the minimal set is empty, then the value of r and the minimal set of clients obtained in previous iteration and their corresponding i coefficients yield the optimal solution of the problem. Otherwise, the process is repeated by increasing r as r = r + 1 and find the next minimal set of clients.

