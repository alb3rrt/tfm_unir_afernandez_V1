arXiv:2106.01768v1 [cs.PL] 3 Jun 2021

1
Homeostasis : Design and Implementation of a Self-Stabilizing Compiler
AMAN NOUGRAHIYA and V. KRISHNA NANDIVADA, IIT Madras, India
Mainstream compilers perform a multitude of analyses and optimizations on the given input program. Each analysis pass (such as points-to analysis) may generate a program-abstraction (such as points-to graph). Each optimization pass is typically composed of multiple alternating phases of inspection of program-abstractions and transformations of the program. An inspection phase may also invoke the respective analysis pass(es). Upon transformation of a program, the program-abstractions generated by various analysis passes may become inconsistent with the program's modified state. Consequently, the downstream transformations may be considered unsafe until the relevant program-abstractions are stabilized, i.e., the program-abstractions are made consistent with the modified program. In general, the existing compiler frameworks do not perform automated stabilization of the program-abstractions and instead leave it to the optimization writer to deal with the complex task of identifying the relevant program-abstractions to stabilize, the points where the stabilization is to be performed, and the exact procedure of stabilization. Similarly, adding new analyses becomes a challenge as one has to understand which all existing optimizations may impact the newly added program-abstractions. In this paper, we address these challenges by providing the design and implementation of a novel generalized compiler-design framework called Homeostasis.
Homeostasis can be used to guarantee the trigger of automated stabilization of relevant program-abstractions under every possible transformation of the program, in the context of object-oriented compilers, for both serial and parallel programs. Interestingly, Homeostasis provides such guarantees not only for the existing optimization passes but also for any future optimizations that may be added to the framework. To avoid the overheads due to naive automated stabilization performed eagerly after each transformation, Homeostasis supports two lazy modes of stabilization: (i) complete invalidation and recomputation (INV), and (ii) incremental update (UPD). These lazy modes ensure that the stabilization of a program-abstraction is triggered only when an attempt is made to read from its possibly inconsistent state. The simple INV mode enables self-stabilization of the program-abstraction of any analysis pass written using Homeostasis without requiring any additional code explicitly written for stabilization. To get a similar feature (zero additional code) in the context of the arguably more efficient UPD mode, for both existing and any future iterative data-flow analyses, we also present a generic, incremental, inter-thread, flow-sensitive data-flow pass, termed HIDFA , for parallel programs. HIDFA can be instantiated to write any IDFA-based analysis, which can be stabilized in an automated fashion without needing any explicit code for stabilization. To illustrate the benefits of using Homeostasis, we have implemented (i) an optimization BarrElim, which includes a set of four standard optimizations, and is used to remove redundant barriers in OpenMP programs, and (ii) a set of flow-sensitive context-insensitive inter-thread iterative analyses, as instantiations of HIDFA . Both the IDFA analyses and BarrElim required zero additional lines of code for stabilization. We have implemented our proposed ideas in the IMOP compiler framework, for OpenMP C programs. We present an evaluation, in the context of BarrElim, which shows that Homeostasis is easy to use, and the proposed lazy modes of stabilization significantly outperform the eager modes of stabilization.
1 INTRODUCTION Modern compilers often span millions of lines of code, and perform a multitude of analyses and optimizations on the given input program. Each analysis pass (such as points-to analysis, callgraph construction, and so on) may generate a program-abstraction (such as points-to graph, call-graph, etc.), which denotes some meaningful information about the program. Typical compiler optimizations involve multiple alternating phases of inspections of program-abstractions, and
Authors' address: Aman Nougrahiya, amannoug@cse.iitm.ac.in; V. Krishna Nandivada, nvk@iitm.ac.in, Department of CSE, IIT Madras, Chennai, India.
Preprint submitted to arXiv

1:2

Nougrahiya and Nandivada

transformations of the program. An inspection phase may invoke the required analysis pass(es), and inspect various program-abstractions to discover opportunities of optimization in the program. Using the results of the inspection phase, the transformation phase transforms the program by invoking appropriate writers on the intermediate representation of the program (for example, abstract syntax tree (AST), three-address codes, and so on). The transformation phases of an optimization may render various existing program-abstractions inconsistent with the modified program. Thus, unless explicit steps are taken to ensure that the program-abstractions always reflect the correct state of the program at the time of being accessed (i.e., are stabilized), the correctness of the inspection phases of the downstream optimizations cannot be ensured, which in turn can negatively impact the optimality, and even the correctness of the optimization. To ensure correct compilation, the following three key stabilization-related questions need to be addressed while adding a new (or modifying an existing) optimization or program analysis (or its associated program-abstraction).

Upon adding a new optimization O Q1. Which existing program-abstractions need to
be stabilized by O? Q2. Where to invoke the stabilization code in O,
for the existing program-abstractions?
Q3. How to stabilize each of the existing program abstractions, in O?

Upon adding a new program-abstraction A

QT
1

.

Which

existing

optimizations

need

to

stabilize

A

?

QT.
2

Where

to

invoke

the

stabilization

code

of

A ,

in

each of the existing optimizations?

QT.
3

How

to

stabilize

A

,

in

each

of

the

existing

optimi-

zations?

In conventional compilers, such as LLVM [Lattner and Adve 2004], GCC [Stallman and GCCDeveloper-Community 2009], Soot [Vallée-Rai et al. 2010], Rose [Quinlan et al. 2013], JIT compilers (OpenJ9 [IBM 2017], HotSpot [Oracle 1999], V8 [Google 2001]), and so on, the onus of addressing these three questions to ensure stabilization of program-abstractions across transformation phases of an optimization lies on the optimization/analysis pass writers. This manual process leads to complex codes and requires careful handling of different passes and their dependencies. For example, Fig. 1 shows some manually-written code (and insightful comments by the pass writers) to perform stabilization of program-abstractions: Fig. 1b and Fig. 1c show code to be invoked while performing dead-code elimination in GCC and LLVM, respectively. Fig. 1 also presents some snippets of codes from LLVM and GCC that show the explicit marking of pass dependencies: Fig. 1a specifies dependencies of dead-store elimination pass on different analyses in LLVM, and Fig. 1d specifies dependencies of CFG-expander on different analyses in GCC. Note that in LLVM, there (currently) are 1251 manually-defined dependencies among its 347 passes. With the compiler development effort spanning multiple decades, involving (sometimes) hundreds of developers, the problem compounds as no developer of an optimization might possess a clear understanding of the semantics of all the hundreds of analysis passes present in the compiler. Consequently, identifying the precise set of dependencies in the presence of such a large number of passes may lead to correctness/efficiency bugs. It is not unusual for compiler writers to forget some such dependencies, which in turn may lead to slow-downs/errors ­ such issues are not uncommon, as evident by various bug-fixing commits that frequently occur in the public codebase of LLVM and GCC; two such recent issues (and fixes) can be seen in the LLVM GitHub repository [LLVM-Developer-Community 2020a,b].
A naive solution to this approach is to recompute all (or a considerable superset of) the required program-abstractions before running each optimization. This can be highly cost prohibitive. To mitigate these issues to an extent, compilers like LLVM and GCC provide a pass manager, which requires each program-abstraction and optimization pass (say O) to explicitly declare the set of required passes that must be run before O, and the set of passes that are invalidated (or conversely, preserved) by O. The required passes are (re)run before O, and the invalidated passes (or those not

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:3

void getAnalysisUsage(AnalysisUsage &AU) const override {
AU .setPreservesCFG();
AU .addRequired< AAResultsWrapperPass >() ; AU.addRequired <TargetLibraryInfoWrapperPass >(); AU .addPreserved< GlobalsAAWrapperPass >() ; AU.addRequired <DominatorTreeWrapperPass >(); AU.addPreserved <DominatorTreeWrapperPass >(); AU.addRequired <PostDominatorTreeWrapperPass >(); ...
(a) DeadStoreElimination.cpp:2068-2077 (LLVM)

/* We do not update postdominators, so free them unconditionally.*/ free_dominance_info(CDI_POST_DOMINATORS); /*If we removed paths in the CFG, then we need to update dominators as well. I haven't investigated the possibility of incrementally updating dominators.*/ if (cfg_altered)
free_dominance_info(CDI_DOMINATORS);
(b) tree-ssa-dce.c:1693-1700 (GCC)

PreservedAnalyses ADCEPass::run(Function &F,

const pass_data pass_data_expand = {

FunctionAnalysisManager &FAM) {

RTL_PASS , /* type */

/* ADCE does not need DominatorTree, but

"expand", /* name */

require DominatorTree here to update analysis
if it is already available.*/ auto *DT=FAM.getCachedResult <DominatorTreeAnalysis >(F); auto &PDT=FAM.getResult <PostDominatorTreeAnalysis >(F); if (! AggressiveDeadCodeElimination(F, DT, PDT).
performDeadCodeElimination () ) return PreservedAnalyses::all(); PreservedAnalyses PA; /* TODO: We could track if we have actually
done CFG changes.*/ if (!RemoveControlFlowFlag)

OPTGROUP_NONE , /* optinfo_flags */ TV_EXPAND , /* tv_id */ (PROP_ssa | PROP_gimple_leh |
PROP_cfg | PROP_gimple_lcx | PROP_gimple_lvec | PROP_gimple_lva),
/* properties_required */
PROP_rtl , /* properties_provided */ (PROP_ssa | PROP_gimple),
/* properties_destroyed */...

PA.preserveSet <CFGAnalyses >();...

(c) ADCE.cpp:687-698 (LLVM)

(d) cfgexpand.c:6496-6507 (GCC)

Fig. 1. Examples from LLVM and GCC, demonstrating the challenges in manual stabilization. Snippets (a) and (d) show how pass writers are required to manually specify numerous dependencies of program-abstractions, in LLVM and GCC, respectively. Snippets (b) and (c) depict cases where the pass writers had to manually write additional code for stabilization, even for those program-abstractions that are not used in the pass.

explicitly preserved in case of LLVM) are marked invalid, to be run before a pass that requires them. Despite the advantages of this scheme, the pass managers of LLVM and GCC are not aware of the exact transformations being performed by O. Consequently, such a solution can still be potentially (i) insufficient, as the pass managers do not stabilize the program-abstractions in-between different transformation phases of O ­ hence, during the execution of O the onus of ensuring correctness and performance still falls on the writer of O; (ii) error-prone, as the writer of O may forget to specify one or more required program-abstraction (in case of both GCC and LLVM), or invalidated program-abstractions (in case of GCC); and (iii) overly conservative and hence slow, as the writer of O may conservatively mark only a small number of program-abstractions (from the hundreds present) to be preserved, or a new program analysis writer forgets to add it to the preserved list of abstractions of some of the existing optimizations. Similar issue may occur if a pass writer adds more dependencies than required. As a result, more program-abstractions, than required, may be frequently subjected to complete re-computation that can be time consuming; some examples of such an issue can be found on the LLVM GitHub repository [LLVM-Developer-Community 2019a,b,c, 2020b,c,d,e,f,g,h,i, 2021a,b,c].
These challenges become much more difficult in the case of compilers for parallel languages, where transformations done in one part of the code may warrant stabilization of program-abstractions of some seemingly unconnected part, due to concurrency relations between both the parts.
Considering the importance of the problem discussed above, there have been various attempts towards enabling automated stabilization of specific program-abstractions, in response to program
Preprint submitted to arXiv

1:4

Nougrahiya and Nandivada

transformations in serial programs. However these efforts suffer from different drawbacks. For example, Reps et al. [1983] require that program-abstractions have to be expressed as contextdependent attributes of the language constructs ­ too restrictive. Carroll and Polychronopoulos [2003] do not handle stabilization in the context of parallel programs, and automated resolution of pass dependencies. Blume et al. [1995]; Brewster and Abdelrahman [2001] handle a small set of program-abstractions ­ insufficient. To the best of our knowledge, there are no compiler designs or implementations that address the challenges discussed above and guarantee generic self-stabilization, especially in the context of parallel programs.
In this paper, we propose a novel, reliable, and efficient compiler-design framework called Homeostasis that addresses the issues discussed above in enabling self-stabilization of programabstractions in object-oriented compilers (for serial as well as parallel programs). Homeostasis takes its inspiration from the Observer pattern [Gamma et al. 1995], which helps establish a one-to-many dependency between a subject, and its dependent observers, such that the observers (program analyses, in this context) are notified of all the changes that may happen in the subject (i.e., the program). To decouple the analysis and optimization passes, Homeostasis uses a fixed set of elementary transformations that capture all program changes performed by an optimization pass, and notify them to the program analyses for stabilization. To avoid the overheads due to naïve stabilization performed eagerly after each transformation, Homeostasis supports two lazy modes of stabilization: (i) complete invalidation and recomputation (INV), and (ii) incremental update (UPD). Owing to the lazy nature of stabilizations, a program-abstraction is stabilized only when attempts are made to read its (possibly stale) value after the last program transformation. In the simple INV mode of stabilization for any program-abstraction, Homeostasis requires zero additional coding efforts from the corresponding analysis pass writers to enable self-stabilization. To get a similar feature (zero additional code) in the context of the arguably more efficient UPD mode, for both existing and any future iterative data-flow analyses, Homeostasis also provides a generic, incremental, inter-thread, flow-sensitive data-flow pass (termed HIDFA ); HIDFA can be instantiated to write any IDFA-based analysis for parallel programs which can be stabilized in an automated fashion, without needing any explicit code for stabilization. To enable UPD mode of stabilization for the program-abstraction of any arbitrary analysis, Homeostasis provides a single method interface that can be defined as per the required incremental update logic by the analysis pass writer. With Homeostasis, no additional coding efforts are required by the optimization pass writers to guarantee self-stabilization of the existing program-abstractions. Importantly, Homeostasis guarantees selfstabilization not just for the existing optimizations/program-abstractions, but also for that of all the future optimizations/program-abstractions.
An important point with Homeostasis is that pass writers need not manually specify any pass dependencies on any program analysis. As a result, no code like that shown in the snippets of Fig. 1 is required if the compiler is enabled with Homeostasis.
While for the ease of exposition, we have discussed Homeostasis using a Java-based compiler in the context of OpenMP C, the underlying principles of the Homeostasis framework are quite generic in nature, and they can be extended to any object-oriented compilers for any serial/parallel languages.
Contributions : · We present Homeostasis, a generalized compiler-design framework for self-stabilization, which
can be added to any object-oriented compiler infrastructure to ensure that all program-abstractions are automatically kept consistent with the modified program during the compilation process.
· We present the design and implementation of HIDFA , a generic inter-thread, flow-sensitive, context-insensitive iterative data-flow analysis that implicitly supports incremental update of

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:5

data-flow facts and prove that HIDFA is as sound and precise as the underlying traditional IDFA. To demonstrate the benefits of using Homeostasis, we (i) instantiated HIDFA to implement a set of four inter-thread flow-sensitive, and context-insensitive analyses for OpenMP C programs, and (ii) implemented an optimization BarrElim that removes redundant barriers for OpenMP C programs; BarrElim includes a set of four standard optimizations, and involves multiple alternating phases of inspection and transformations. As expected, both the IDFA analyses and BarrElim, required zero additional lines of code for stabilization.
· We have implemented all the key components of Homeostasis in IMOP, a source-to-source compiler infrastructure for OpenMP C programs, as a successful proof-of-concept.
· We present an evaluation to show the ease-of-use of Homeostasis and performance benefits of the chosen lazy mode of stabilization. To demonstrate the benefits of using Homeostasis, we use a set of fifteen benchmarks from four real-world benchmark suites NPB [Van der Wijngaart and
Wong 2002], SPEC OMP [Aslot et al. 2001], Sequoia [Seager, M 2008], and Parboil [Stratton et al.
2012], and show that lazy modes of stabilization are significantly better than the eager modes, both in terms of time (geomean 28.42× faster) and memory footprint (geomean 25.98% less).

2 BACKGROUND

We now give a brief description of certain relevant concepts and terminologies used in this paper.

Program-abstractions. Each analysis pass, say A, can be seen as a function from the set of

programs to a set of program-abstractions. In other words, A () =  denotes that  is the

program-abstraction generated by running the analysis A on a program . At any point during

compilation, if the current state of the input program is represented by  , and for the analysis

A,

the

stored

program-abstraction

is

 ,


then

we

term




as

stable

iff,

for

all

program

elements

,

A ( )() =  ().



Elementary transformations. Based on the grammar of the language under consideration, corre-

sponding to each type of program node with a body (such as a block in LLVM IR, or a while-statement

construct), we define a fixed set of elementary transformations which can add/delete/modify the

logical components of the program node (such as an instruction of a block, or the predicate of a while-statement). In Homeostasis, all translations of a program must be expressed, directly or

indirectly, in terms of the fixed set of elementary transformations. Any program node that does not provide an elementary transformation, is considered to be immutable. (Such nodes can be modified

by replacing them with a new modified node using the elementary transformations, if any, of their

enclosing nodes.)

Intra-thread iterative data-flow analysis. For any given iterative data-flow analysis (IDFA), we

maintain two flow facts (maps),   and  , at each executable program node. Without loss of
generality, we confine our discussions to forward data-flow analyses. The data-flow equations for intra-thread IDFA are standard, as reproduced here :   [] :=  pred()  [], and  [] := F (  []), where F denotes the transfer function of node , for the analysis under consideration. OpenMP. To demonstrate how stabilization of program-abstractions is handled by Homeostasis in compilers of parallel programs, we have discussed Homeostasis in the context of one of the

well-known APIs for parallel programming, OpenMP [Dagum and Menon 1998]. Following are the

key OpenMP constructs of our concern :

#pragma omp parallel S, denotes a parallel region that generates a team of threads, where each thread executes a copy of the code S in parallel.
#pragma omp barrier, specifies a program point where each thread of the team must reach before any thread is allowed to move forward. We use ompBarrier to abbreviate this directive.
#pragma omp flush, is used to flush a thread's temporary view of the shared memory to make it consistent with the shared memory. We use ompFlush as a shorthand for this directive.

Preprint submitted to arXiv

1:6

Nougrahiya and Nandivada

Fig. 3. Class-diagram of Homeostasis, depicting key classes and methods, in the OMT notation: class names are shown in bold; abstract methods in italics; static members are preceded with a $; inheritance is shown using a triangle on the edge, pointing towards the base class; and dashed arrows show code snippets for methods.

We assume that all implicit barriers/flushes of different OpenMP constructs have been made explicit.

Phase analysis. In a parallel region in OpenMP, all threads start their execution from the implicit

barrier at the start of the region, until they encounter a barrier on their execution paths. Once

each thread has encountered a barrier (same or different), all the threads will start executing

the code after the barrier, until they encounter the next set of barriers. All such barriers that

synchronize with each other form a synchronization set. Statically, a barrier may belong to more

than one synchronization set. All statements that exist between two consecutive synchronization

sets form a static phase. Each statement may belong to more than one phase. Two statements that

do not share any common phase may not run in parallel with each other. Precise identification of

such phases can greatly improve the precision of various static analyses of parallel programs, as

they limit the pair of statements that may communicate with each other through shared memory.

For the purpose of our discussion, any phase analysis should suffice;

we use the phase analysis derived from the concurrency analysis

provided by [Zhang and Duesterwald 2007; Zhang et al. 2008]. We

refer to this concurrency analysis as YConAn. Inter-task edges. In OpenMP parallel regions, different threads

(or tasks) may communicate with each other with the help of shared

memory. As per OpenMP standards, a valid communication can

take place between threads 1 and 2 through a shared variable, say , only if the following order is strictly maintained : (1) 1 Fig. 2. An example inter-task edge, writes to , (2) 1 flushes , (3) 2 flushes , and (4) 2 reads from .

 (dashed line), between two flush directives, 1 and 2, executed by threads 1 and 2, resp. Over , 1 can communicate with 2 using the shared variable . Solid lines denote one or more paths between two nodes in the super-graph.

Furthermore, 2 must not have written to  since its last flush of the variable. We model such perceived communications with the help of inter-task edges, as shown in the Figure 2. An inter-task edge is an edge that originates at an ompflush, say 1 and terminates at (same or different) ompFlush, say 2, such that (i) 1 and 2 share at least one common static phase, and (ii) there must exist some

shared variable which is written on an ompFlush-free path before 1, and read on an ompFlush-free path after 2. We obtain a super-graph by adding these inter-task edges to CFGs and call graphs.

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:7

3 HOMEOSTASIS : DESIGNING SELF-STABILIZING COMPILERS Considering stabilization challenges inherent in the design of conventional compilers (as discussed in Section 1), we now present a novel solution in the form of a new object-oriented compiler-design framework named Homeostasis, which requires minimal (many times zero) stabilization efforts by the writers of new optimizations and program analyses.

3.1 Overview of Homeostasis
Figure 3 shows a class-diagram (in Object Modeling Technique, or OMT, notation [Rumbaugh et al. 1991]) which gives an overview of the key entities in Homeostasis. The design of Homeostasis is inspired from the popular Observer pattern [Gamma et al. 1995] that is used to define a one-to-many dependency, where an update in one object (termed subject), is notified to all the other registered objects (termed observers). In Fig. 3, the Node class (and its subtypes) roughly correspond to the subject(s), and the BasePA class (and its subtypes) correspond to the observers.
Node. The Node class represents the super class of all the types of program nodes in the program's base representation (such as an AST, CFG, IR, and so on). Any update to any program node is notified to the program analyses via the fixed set of elementary transformations that are invoked to add/modify/delete any of the logical component(s) of the node. The figure shows one such elementary transformation, elemTransformReplaceBody defined on a specific type of program node, WhileStmt; this transformation replaces the body of the associated while-statement with the given argument. In Homeostasis, all transformations on a program must be expressed, directly or indirectly, as a sequence of elementary transformations.
BasePA. In Homeostasis, each program analysis pass inherits from the abstract class BasePA; this class contains various key data structures and methods that are critical in enabling self-stabilization
of the associated program-abstractions. For demonstration, this figure shows one concrete class of BasePA, CallGraphGenerator, which corresponds to the program-abstraction denoting the call-graph information of the program. Each concrete class of BasePA must define the method compute, which should generate the corresponding program-abstraction (such as call-graphs) by running the analysis from scratch on the current state of the program. The internal state of the generated program-abstraction can only be read through one or more getter methods (such as getCG in CallGraphGenerator), which first invoke the stabilize method, if the status of the program-abstraction is not marked STABLE (stableStatus != STABLE). Stabilization in Homeostasis. To ensure that all program-abstractions are eventually updated in response to a program transformation, each elementary transformation in Homeostasis contains the code snippet shown for the method elemTransformReplaceBody in Fig. 3, which (i) informs the class BasePA about the performed transformation (using the method notify), and (ii) marks each program-abstraction in the system as stale by setting its stableStatus flag to UNSTABLE. When an attempt is made to read the internal state of a program-abstraction, this flag is inspected
by the invoked getter method to ensure that the internal state of the program-abstraction is
stable before the getter returns; when required, the getter methods trigger stabilization of their program-abstraction by invoking the method stabilize. The stabilize method in turn calls the handleUpdate method of the program analysis to perform the stabilization with respect to all the pending updates. In Section 4, we will see that Homeostasis provides the default definition of handleUpdate for a generic iterative data-flow pass (termed HIDFA ), thereby freeing the pass writer of any IDFA written as an instantiation of HIDFA from writing code for handleUpdate.
Preprint submitted to arXiv

1:8

Nougrahiya and Nandivada

public BasePA::BasePA() { /* Constructor of the base analysis class . */ Global .allAnalyses. add ( this ); this . compute ();}
(a) Constructor of the base analysis class.

1 public V Analysis ::getData () { 2 if (this.stableStatus==STABLE) { 3 return this.data ;} 4 if (this.stableStatus==UNSTABLE){ 5 this .stabilize()); 6 return this.data ;} 7 else // stableStatus=PROCESSING 8 return this .initVal ;}
(b) Template of a getter method of a sample analysis.

1 /* Program - abstraction stabilizer in BasePA*/

2 protected final void BasePA::stabilize() {

3 this.stableStatus = PROCESSING;

4 if (this.updateMode == LZINV) {

5

this . recompute ();

6

this.stableStatus = STABLE;return;}

7 // this.updateMode = LZUPD

8 this .handleUpdate();

9 this.stableStatus = STABLE;}

(c) Implementation of a stabilizer.

Fig. 4. Constructor and stabilizer of BasePA, and template of a program-abstraction getter.

3.2 Details of Homeostasis In this section, we explain the details of Homeostasis, which can be used to enable self-stabilization in compiler frameworks written in object oriented languages. We first discuss about the required
structures of program-abstractions, and how to design the getter functions to obtain stable program-
abstractions upon each invocation. We follow up this discussion by describing the different supported modes of stabilization and how Homeostasis enables the same. For the ease of exposition, we first focus on how Homeostasis performs stabilization for serial input programs, and later in Section 3.3 we describe how we extend Homeostasis for parallel programs.
3.2.1 Structure of program-abstractions. In Homeostasis, every concrete program analysis class inherits from the abstract base class of program analyses (BasePA). The latter contains some common methods and data-structures necessary for self-stabilization of the associated program-abstractions.
To ensure that stabilization is triggered for each valid program analysis pass (A) in the system in response to a transformation, Homeostasis maintains a global set (allAnalyses) of program analysess, and adds the reference of A to allAnalyses in the constructor of BasePA; this constructor is invoked implicitly during construction of every program analysis object. In addition, the constructor invokes the compute method of the concrete analysis to (re)run the analysis from scratch and populate the corresponding program-abstraction; note that any child class of BasePA must implement the compute method. Fig. 4a shows a sketch of the constructor of BasePA.
3.2.2 Abstraction getters. Any program-abstraction can be queried only using the respective getter methods of its analysis. If the stableStatus field of the analysis is UNSTABLE, then we cannot rely on the existing values returned by its getters. These getters may have been invoked either by
an optimization, or by the stabilizer of some other dependent program analysis. To handle this scenario, Homeostasis requires that each getter invokes the stabilizer if needed, before returning the requested internal data.
The stabilization of the program-abstraction of a program analysis A1 may need the results of another program analysis A2, which is obtained via calling the getters of A2. These getters may invoke the stabilizer of A2. Now if A2 also depends on A1, the stabilizer of A2 may again invoke the getters of A1 ­ this may lead to an infinite recursion. To handle such a scenario, we check if the stableStatus flag of A1 is set to PROCESSING ­ this indicates a recursive call. If set, we avoid calling the stabilizer for A1 recursively and instead return this.initVal ; this value is provided by the program analysis writer, indicating the initial value that each element in the domain of the
Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:9

analysis is set to, at the beginning of the analysis. A sample getter is shown in Fig. 4b. Since the getter methods depend on the stableStatus flag, it is essential that this flag is correctly
set in the presence of passes that may modify the program. The mechanism to update this flag, and the detailed procedure of the stabilization, depends on the chosen stabilization-mode.
3.2.3 Modes of stabilization. An important point of self-stabilization is the time and manner in which a program abstraction is stabilized under program modifications. These two can vary along the following two dimensions :
Eager versus lazy. After an elementary transformation, the stabilization of an abstraction may either get triggered (i) immediately (eager-stabilization), or (ii) only in response to the first read request made on the abstraction after the transformation (lazy-stabilization). Considering the possible frequent calls to the stabilization routines (and the resulting overheads) in the context of eager-stabilization, Homeostasis advocates the use of lazy modes of stabilization.
Invalidate versus update. In response to one or more program modifications, the resulting stabilization of a program-abstraction may either (i) involve the complete invalidation of the program-abstraction (leads to regenerating the program-abstraction from scratch), or (ii) be able to incrementally update the program-abstraction based on the modifications. Though the update modes seem much more efficient than the invalidate modes, in practice the difference in their performance depends on a number of factors, such as the number of program modifications, the complexity of the associated incremental update, and so on. Further, note that enabling the update modes of stabilization for certain program-abstractions can be a challenging task. Considering such issues, Homeostasis supports both invalidate, as well as update modes of (lazy) stabilization. On the basis of these two dimensions, Homeostasis supports the following two modes of stabilization for any program abstraction : (i) Lazy-Invalidate (LZINV), and (ii) Lazy-Update (LZUPD). We now show how the stableStatus flag is set and stabilization is performed in different stabilization-modes.
3.2.4 Program-abstraction stabilizers. As shown in Fig. 4c, to stabilize any program-abstraction, the corresponding stabilize method is invoked. The actual stabilization procedure depends on the mode of stabilization. For LZINV mode of stabilization, the stabilize method (Fig. 4c) simply reruns the analysis from scratch if the stableStatus flag is set to UNSTABLE, by invoking the compute method that repopulates all the data structures of the program-abstraction. Upon successful stabilization, stabilize method sets the stableStatus flag to STABLE. In this way, Homeostasis realizes self-stabilization in the LZINV mode with no additional stabilization code written by the program analysis writer. The rest of this section describes how Homeostasis realizes stabilization in the LZUPD mode.
When stabilization is requested, the stabilize method performs self-stabilization of the abstraction by invoking the method handleUpdate; the program analysis writer needs to provide an implementation of this method, defining the impact of the addition/removal of a set of nodes and/or edges on the program-abstraction. To that end, the program analysis writer may utilize the information about the effective updates to the nodes and edges that have taken place since the last call to the stabilizer. This method is akin to the callbacks of Carroll and Polychronopoulos [2003].
3.2.5 Elementary transformations. Stabilization of program-abstractions in response to the modifications performed by an optimization, can intuitively be done in two ways: (i) the optimization pass directly modifies the internal representations of the program-abstraction ­ this can be complicated, and goes against the spirit/design principles of object-oriented programming, or (ii) the program analysis performs its stabilization internally ­ this requires the program analysis to be informed of the exact modifications which have been performed on the program. We use the latter option in Homeostasis.
Preprint submitted to arXiv

1:10

Nougrahiya and Nandivada

1 Function elemTransformReplaceC(newNode) // Replace component C with newNode // Step A: Save edges affected by old node removal.

2

oldC = getComponentC();

3 removedEdges = ..outgoing edges from, and incoming edges to, oldC..;

// Step B: Perform the actual program update by invoking appropriate writer(s).

4 Replace the component  with newNode;

// Step C: Save edges affected by new node addition.

5 addedEdges = ..outgoing edges from, and incoming edges to, newNode..;

// Step D: Communicate relevant information to each analysis.

6 for anl  BasePA.allAnalyses do anl.stableStatus = UNSTABLE;

7 addedNodes = {newNode};

8 removedNodes = {oldC};

9 BasePA.notify(addedNodes, removedNodes, addedEdges, removedEdges);

Fig. 5. Template of an elementary transformation in Homeostasis that replaces component  of a program node with newNode.

Homeostasis uses elementary transformations as the missing link between optimizations and program analyses. As mentioned in Section 3.1, Homeostasis prohibits the optimizations from modifying the program except via elementary transformations. Homeostasis uses this feature to capture all the program modifications via the elementary transformations. In each elementary transformation, Homeostasis (i) collects the information about addition/deletion of nodes, controlflow edges, call-edges, and inter-task edges, and (ii) makes the collected information available to every program analysis object, present in the set allAnalyses. We explain this using an example.
Fig. 5 shows the template of a generic elementary transformation function (in Homeostasis) to replace a component  with a new node. The modifications span four steps. Each of the three possible types of the elementary transformations (add, delete, replace) uses a subset of these steps.
Step A : This step applies to all those elementary transformations that may remove a node from some program point; the step captures the information about the node removal. In this step, we record (in removedEdges) all those edges that either had their source or destination in the node to be removed, and hence will be removed as a result of the node removal.
Step B : For each elementary transformation, this step performs the actual modification. For example, in Fig. 5, the component  of a program node is replaced with the provided new node.
Step C : This step applies to only those elementary transformations that add a node. Similar to Step A above, it records (in addedEdges) all those edges that either had their source or destination in the added node, and hence have been added as a result of the node addition.
Step D : In this step, Homeostasis first marks the status of each program analysis as UNSTABLE, so that the corresponding stabilization routine is invoked whenever its program-abstraction is read next (using the getter methods). Next, Homeostasis stores the information about newNode (in addedNodes), and oldC (in removedNodes). The collected information about all the added/removed nodes and edges has to be made available to every program analysis in the system (in the set allAnalyses); Homeostasis realizes this by invoking the static method notify on BasePA. Note that Homeostasis maintains these data-structures globally (instead of maintaining a copy for each analysis) in such a way that different analyses can see different subsequences of pending updates (in the form of nodes and edges that have been added/removed) depending upon when their abstraction are stabilized; to implement this efficiently, Homeostasis uses a scheme inspired by the popular idea of copy-on-write [Vahalia 1995]; we skip the details for space.
Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:11

1 Function stabilizePhInfoOnAddition(, addedEdges, removedEdges) /* may update addedEdges,

removedEdges */

2 if  contains no ompBarrier then

// Reuse neighbor's phase info.

3

phase = phase-info of any successor of ;

4

for   { }  .children do

5

.phase = phase;

6

if  is an ompFlush then

7

addedEdges  = {edges from  to other ompFlush nodes in ph};

8 else reInitPhInfo(addedEdges, removedEdges);
9 Function reInitPhInfo(addedEdges, removedEdges) // may update addedEdges, removedEdges 10 oldEdges = {all inter-task edges in the program};
11 Recalculate the phase information; 12 newEdges = {all inter-task edges in the program}; 13 addedEdges  = newEdges \oldEdges; 14 removedEdges  = oldEdges \newEdges;

Fig. 6. Stabilization of phase information; we use a mix of invalidate and update modes.

3.3 Stabilization in the Context of Parallelism We now discuss how Homeostasis helps automatic-stabilization in the presence of different parallelism-related constructs. In the context of parallel programs, program transformations in one region (R) of the program, can impact the program-abstractions pertaining to other program regions that may run in parallel with R, which can be a considerable challenge for self stabilization. This challenge becomes more difficult when the program transformation leads to addition/removal
of parallelism-related constructs, because such changes can again lead to changes in the programabstractions at non-local places. Homeostasis deals with these challenges by using phase analysis (see Section 2), which is again kept stable in the presence of different program transformations.
3.3.1 Handling elementary transformations. In the presence of parallel constructs, Homeostasis continues to capture the program modifications and communicate the same to different program analyses (via the elementary transformations) like its handling of the serial programs. In addition, Homeostasis stabilizes the phase information after each addition/deletion of nodes/edges. For example, in Fig. 5, at the end of Step A, Homeostasis invokes stabilizePhaseInfoOnRemoval(oldC, addedEdges, removedEdges), and at the end of Step C invokes stabilizePhaseInfoOnAddition(newNode, addedEdges, removedEdges). Note that both these methods take both added and removed sets of edges as their arguments: this is required as
the addition/removal of a parallelism-related construct may impact both these sets. Fig. 6 describes the working of stabilizePhaseInfoOnAddition; the stabilizePhaseInfoOnRemoval method is defined analogously and not shown here for brevity.
Description of stabilizePhaseInfoOnAddition (Fig. 6). If a node  is added to the program, and  does not internally contain any ompBarrier, then we can simply reuse the phase information of the control-flow-graph (CFG) neighbors of  to stabilize the phase information of  and its children (i.e., nodes within ). When the node being added or removed may contain an ompBarrier, it may change the phase information (globally). Thus, we recompute the phase information from scratch (by calling the function reInitPhInfo). Note that if the phase stabilization leads to addition/removal of any inter-task edges, we note those edges in the sets addedEdges/removedEdges. Correctness statement of Homeostasis. We state the correctness of the design of Homeostasis, in the form of the following theorem.
Theorem 3.1. Given a program , and an analysis A, say  is the corresponding program

Preprint submitted to arXiv

1:12

Nougrahiya and Nandivada

abstraction. That is,  = A (). Say 1, 2, 3, . . . , are a sequence of elementary transformations

on  to derive new programs 1, 2, 3, . . . and say 1, 2, 3, . . . are the corresponding program abstractions, derived from performing the complete analysis A after each transformation. If A is

implemented in a Homeostasis-enabled compiler, which takes  as its input program, performs

a subsequence of elementary transformations 1, 2, . . . ,  , and then reads the value of program

abstraction A ( ), for some program element , and (i) A is stabilized in update (UPD) mode and

the implementation of handleUpdate in A ensures correct stabilization, OR (ii) A is an instantiation

of HIDFA , OR (iii) A is stabilized using invalidate (INV) mode, then Homeostasis guarantees that

A ( ) () =  (). (Proof skipped.)



4 SELF-STABLE GENERIC INTER-THREAD IDFA USING HOMEOSTASIS
As discussed in Section 3.2.4, the simple LZINV mode enables self-stabilization for any program analysis written using Homeostasis, without requiring any additional code written specifically for stabilization. But for the arguably more efficient LZUPD mode the suggested scheme requires each newly added program analysis to provide the definition of the handleUpdate method. While this systematic approach for ensuring self-stabilization for program analyses requires much lesser efforts than the conventional approach, the compiler writer still needs to write extra code for each new IDFA. This can be repetitive and error-prone, especially in the context of parallel programs (such as OpenMP programs), where the code required to perform stabilization can be complex. In this section, we propose HIDFA , a generic, incremental, inter-thread, flow-sensitive data-flow framework for writing IDFAs for OpenMP programs. HIDFA can be instantiated to write any IDFA-based analysis (for example, points-to analysis, liveness analysis, and so on) for parallel programs, such that these analyses can be stabilized in an automated fashion, without needing any explicit code for stabilization.
We now present the design of HIDFA in Homeostasis by providing generic definition for handleUpdate method to enable self-stabilization in the LZUPD mode. We discuss the design of HIDFA in the context of OpenMP parallel programs; naturally, this design is applicable to even serial programs. It is worth noting that implementing any IDFA analysis (such as, inter-thread points-to analysis) on top of HIDFA needs no extra code (or logic) over what is needed for writing its corresponding serial IDFA analysis (such as intra-thread points-to analysis), and yet supports both (i) parallel semantics of OpenMP, and (ii) self-stabilization (even in the LZUPD mode).

4.1 Design of HIDFA We now present the design of a Homeostasis-conforming (i.e., self-stabilizing) generic IDFA pass for parallel programs; we term it HIDFA . As discussed in Section 3.2.4, for self-stabilization we only need to provide the definition for the handleUpdate method that is invoked by BasePA::stabilizer to ensure that the program-abstraction is made consistent with the current state of the program. In Homeostasis, we extend the BasePA class to create an abstract class called HIDFA that provides a concrete definition for handleUpdate. Any IDFA to be implemented in Homeostasis can be implemented as a concrete class that extends HIDFA , thereby realizing self-stabilization even in the LZUPD mode.
Figure 7 shows the steps used by the handleUpdate method of the HIDFA class in the context of forward analyses; the steps in the context of backward analysis are similarly derived (not shown). To realize self-stabilization, we maintain an internal set (seeds) of nodes, starting which the flow maps may need an update as a result of program transformations. We populate this set with those nodes
whose   (or  ) flow maps need to be recalculated due to the changes to their predecessors
(or successors), in case of forward (or backward) analyses. These nodes are: (i) destination of the
removed and added edges, (ii) added nodes, (iii) successors of the added and removed nodes. Finally,

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:13

1 Function handleUpdate 2 foreach   getRemovedEdges() do Add the destination node of the removed edge  to seeds; 3 foreach  in getRemovedNodes() do Add the (old) successors of the removed node to seeds; 4 foreach  in getAddedNodes() do Add the added node  itself, as well as its successors to seeds; 5 foreach  in getAddedEdges() do Add the destination node of the added edge  to seeds; 6 incrementalIDFA(seeds);

First-pass

Fig. 7. Definition of the handleUpdate function for forward iterative data-flow analysis.

1 Function incrementalIDFA(seeds) // seeds:Set, nodes starting which IDFA is to be recomputed

2

WL = seeds;

3 repeat /* process one SCC per iteration */

4

 = WL.removeNext();

5

sccID = .getSCCId(); // SCC ID of 

// First pass; under-approximation, for the nodes in the current SCC.

6

processedInFirstPass = ;

7

underApproximated = ;

8

repeat

9

validPreds = { :   pred(n) && (.getSCCId() ! = sccID | |   processedInFirstPass) };

10

processNode(, validPreds, WL);

11

processedInFirstPass  = { };

12

if validPreds ! = pred() then underApproximated  = { } ;

13

else underApproximated \ = { } ;

14

 =WL.removeNextWithId(sccID); // returns NULL if WL is empty.

15

until  == NULL;

16

// Second pass; stabilization, for the nodes in the current SCC.

17

WL  = underApproximated;

18

 = WL.removeNextWithId(sccID);

19

while ! = NULL do

20

processNode(, pred(n), WL);

21

 = WL.removeNextWithId(sccID);

22

until WL == ;

23 Function processNode(, selectedPreds, WL)

// :Node, whose IN() and OUT() need to be recalculated; selectedPreds:Set, contains the

selected predecessors of  to be considered for recalculation; WL:List

24 oldIN = IN(); oldOUT = OUT();

25

IN() = selectedPreds OUT();

26

OUT() = F(IN());

27 if oldIN == NULL | | oldOUT ! = OUT() then WL.addAll(succ());

28 if  is a barrier && oldIN ! = IN() then WL.addAll(siblings());

Second-pass

Fig. 8. Algorithm for incremental update of flow-facts, used by HIDFA , for a forward analysis.
these seed nodes are passed to the incrementalIDFA method for performing the stabilization (Line 6).
Figure 8 shows the incrementalIDFA method that performs self-stabilization. It is a worklist based algorithm that works on the SCC graph, which is a directed acyclic graph obtained after contracting each strongly connected component of the program's super-graph into a single node. Note that the super-graph includes the inter-task edges (see Section 2). Each program node has a unique SCC id, which corresponds to the index of its SCC node in the topological sort of the SCC graph. The worklist WL is initialized with seeds. WL is kept sorted by the SCC ids of the program nodes. For efficiency reasons, the program nodes that are part of the same SCC (i.e., have same SCC id) are maintained in any of the valid topological sort orders of the program nodes in the SCC.
The algorithm processes one SCC at a time, in the topological-sort order of the SCC graph. Within an SCC, the processing proceeds in two passes. First pass (performs under-approximation). To incrementally realize the effects of program modi-

Preprint submitted to arXiv

1:14

Nougrahiya and Nandivada

fications on data-flow-facts, one may naively perform a fixed-point based analysis by using the
standard data-flow equations, starting with the seed nodes. It is well understood [Pollock and Soffa
1989; Ryder et al. 1987] that this may lead to a state where stale information persists in strongly
connected components, thereby leading to a loss of precision as compared to the complete rerun of
the analysis.
To address this issue, we conservatively assume that the incoming information from all the
predecessors that have not been "processed" may be stale ­ and hence "unsafe" to use. Before processing a node , we calculate the set validPreds of "safe" predecessors of  ­ those that are either present in some previous SCC than that of , or which have been processed at least once
in the current pass (Line 9). This subset of predecessors is used to apply the flow functions, by invoking the function processNode.
In processNode (Fig. 8), we compute IN(n), by only considering the set of selectedPreds. We invoke the appropriate analysis specific flow function (F) to compute OUT(n). If  is a barrier, we use Ext-2 to compute OUT(n); not explicitly shown in the algorithm. Next, we check if the OUT(n) flow map of  changes; if so, the successors of  are added to WL. Similarly, if  is a barrier and IN(n) changes then we add all the sibling-barriers (see Section 2) of  to WL (Line 28).
After returning from processNode, we add  to processedInFirstPass (Fig. 8, Line 11). If some predecessors of  are not in validPreds (and hence the flow information computed for  is incomplete), we add  to the set underApproximated, so as to complete its processing in the second pass. The first pass terminates once WL has no node from the current SCC. Second pass (completes IDFA-stabilization for this SCC). In this pass, we re-initialize WL to underApproximated and process each node in the standard way, till we reach a fixed point. Note that, unlike in the first pass, here we pass all the predecessors of  to processNode.
At the completion of second pass the flow maps of the program nodes of the current SCC have been stabilized. After processing all the nodes of WL in an SCC, the algorithm proceeds with the next node in WL in the next SCC (in order). If WL is empty, the algorithm terminates.
Theorem 1. HIDFA is as sound and precise as a complete rerun of the underlying IDFA. (Proof in Appendix B.)
5 DISCUSSION Impact of Homeostasis on the key questions. We now discuss the impact of Homeostasis on answering the key questions (Q1-3 and QT1-3) presented in Section 1. In the presence of Homeostasis: (i) No optimization needs to explicitly perform stabilization of any program-abstraction. Consequently, the compiler writer does not need to consider or address questions Q1-3, while adding any new optimization. (ii) Similarly, the compiler writer does not need to consider or address questions QT1-3, while adding any new program analysis in the invalidate (INV) mode of stabilization, or a new IDFA-based program analysis in the update (UPD) mode of stabilization, as in both the cases the
compiler writer does not need to write any extra code to ensure self-stabilization. (iii) When adding
any non-IDFA based program analysis in the update (UPD) mode of stabilization, the compiler writer needs to provide the definition for only the handleUpdate method. Thus, the compiler writer has to address only QT3, in this case. This attests to the ease-of-use of Homeostasis as compared to manual stabilization, where all these key questions need to be addressed for correctness and
efficiency. Merging program changes across transformations. To stabilize a program-abstraction in update (UPD) mode, the handleUpdate method of the corresponding analysis requires net changes performed on the program (in terms of added and removed nodes/edges) since after the last stabi-
lization of that program-abstraction. A simple union of all such changes performed across multiple

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:15

transformations, would not suffice. For instance, if a program node  (or edge ), is added by
a transformation, and then deleted by another transformation before the stabilization gets trig-
gered, the node  (or edge ) should neither be considered as an added/removed node (or edge). In Homeostasis, BasePA internally merges the changes saved across all transformations to provide the relevant information via the methods getAddedNodes, getRemovedNodes, getAddedEdges, and getRemovedEdges, used in Fig. 7. Order of program-abstraction stabilization. During the stabilization of a program-abstraction Homeostasis automatically resolves the dependencies by stabilizing the program-abstractions in the topological order of their dependencies. That way, in the absence of circular dependencies,
a program-abstraction is stabilized only after all the dependee program-abstractions have been stabilized. The circular dependencies (if any) are handled internally using the flag stableStatus (see Section 3.2.2). Handling primitives, and memoization. In practice, for efficiency concerns, it is not uncommon to store information about program nodes as their primitive fields (such as integer fields for line numbers), instead of modeling them as program-abstraction objects. Similarly, compiler writers may memoize frequently-used information that has been derived from one or more program-abstractions.
For instance, the set of abstract memory locations read/written at a node may be computed using
points-to analysis, and memoized for its frequent use. It is important to note that the impact of
program transformations should be reflected in such primitive/memoized data as well. We handle
these primitive fields, and data-structures used for memoization, by requiring that the compiler writer registers them with Homeostasis; it ensures that stabilization effects are propagated to these variables. (Details skipped.) Extending Homeostasis to conventional compilers. The simple intuitive design of Homeostasis can be extended to any object-oriented compiler framework. In order to implement Homeostasis in a compiler framework, the following four constraints need to be preserved by the framework: (C1) all program analyses are a subclass of an equivalent of the BasePA class (such as the Pass class in LLVM, modified as per the details in Section 3); (C2) all program transformations are specified, directly or indirectly, as a sequence of elementary transformations (taken from a fixed set of elementary transformations); (C3) each elementary transformation conforms to the template shown in Fig. 5, which (i) collects the changes performed on the program, and (ii) notifies the changes to some equivalent of BasePA; and (C4) each getter of a program analysis conforms to the template shown in Fig. 4b, to ensure that the stabilization is triggered when required.
Based on the encouraging performance/feasibility results obtained from our study (see Section 6),
we argue that it will be interesting and useful to consider enabling real-world compilers written in OO style (such as LLVM and Soot) with Homeostasis. However considering the significantly large development/engineering efforts required for such an enablement (for example, rewriting all the
numerous existing program transformations using the identified fixed set of elementary transfor-
mations, and modifying the hundreds of analysis passes to conform to the structure prescribed by Homeostasis) it is left as a future work. Manual Stabilization. We note that it is difficult to evaluate the efficacy of self-stabilization support of Homeostasis by contrasting it with that of real world frameworks like LLVM/Cetus, as they do not provide a mechanism to perform self-stabilization across different transformation
phases within an optimization pass. This is besides the issue of large engineering efforts discussed above to implement Homeostasis in these real world compilers. Under these constraints, we find the best evaluation strategy is to compare Homeostasis against a possible approximation of manual stabilization.
Note that in order to perform manual stabilization in the context of an optimization O, a compiler writer needs to inspect the following two parameters in relation to questions Q1 and Q2 (see
Preprint submitted to arXiv

1:16

Nougrahiya and Nandivada

Section 1) : (i) critical program-abstractions: the program-abstractions that may be rendered stale by O, and read later in O or in any other downstream pass (and hence may need to be stabilized); we use  to denote the set of critical program-abstractions, and (ii) change-points: the program points in the code of the optimization O that may directly or indirectly trigger an elementary transformation (and hence may necessitate stabilization); we use  to denote the set of change-points. An inexperienced programmer may perform manual stabilization naively by re-computing each of the programabstractions (or a likely superset of , for correctness), after each change-point in  ­ a highly inefficient scheme, both in terms of its effect on performance, and the number of program points where the stabilization code needs to be invoked. An experienced programmer, on the other hand, is more likely to insert stabilization code of only a set of impacted/relevant program-abstractions, and only at the program points (say a set    ) where the stabilization is required. Note that if the union of these sets of program-abstractions stabilized at the elements of  is not a superset of , then the stabilization would be considered invalid. Thus, such a stabilization process can be complex and error-prone. Modes of manual stabilization. Besides the lazy modes of stabilization preferred by Homeostasis, a few other modes of stabilization are easily conceivable. For example, the stabilization of all program-abstractions can be triggered during each elementary transformation of the program eagerly. Like the two modes of lazy stabilization, the eager scheme leads to two modes of stabilization: eager-invalidate (EGINV) and (EGUPD). An efficient alternative to the eager modes of stabilization is to invoke stabilization only at a subset of change-points, termed relevant change-points, present in the code of the optimization pass. A relevant change-point is a program point in the optimization pass that corresponds to the last change-point in a series of transformations, after which one or more critical program-abstractions may be read; hence, one or more program-abstractions may require manual stabilization at a relevant change-point. Like before, two modes of stabilization are possible ­ RPINV and RPUPD, corresponding to the invalidate and incremental update options, respectively. These eager and RP-modes are very similar to the custom codes manually written by the compiler writers in case of conventional compilers: (i) UPD modes in the presence of incremental update, whereas INV modes otherwise, and (ii) EG-modes during naïve (and inefficient) but easy manual stabilization, whereas RP-modes during relatively efficient stabilizations performed by an experienced compiler writer. For the purpose of evaluation (see Section 6), along with the two lazy-modes of self-stabilization advocated by Homeostasis, we have implemented these four modes of stabilization (EGINV, EGUPD, RPINV, and RPUPD) to approximate different modes of manual stabilization. Instantiations of HIDFA . In order to assess the usability of HIDFA , we have implemented a set of four standard flow-sensitive context-insensitive inter-thread iterative data-flow analyses as an instantiation of the HIDFA pass: (i) points-to analysis, (ii) reaching-definitions analysis, (iii) liveness analysis (a backward IDFA), and (iv) copy propagation analysis. As expected, implementation of these analyses did not require any additional lines of code to enable their automated stabilization. BarrElim: barrier remover for OpenMP programs The Homeostasis framework described in Section 3 can be used by a compiler writer to efficiently design/implement new optimizations without having to write any extra code for stabilization of program-abstractions. To illustrate these benefits, we have implemented a set of four optimizations (function-inliner, redundantbarrier-remover, and parallel-construct-expander), collectively used to derive an optimization called BarrElim that reduces redundant barriers in OpenMP C programs. Like many similar optimizations [Aloor and Nandivada 2015; Barik et al. 2013; Gupta et al. 2017; Nandivada et al. 2013], BarrElim repeatedly invokes these component optimizations. BarrElim builds on top of prior works [Gupta and Schonberg 1996; Tseng 1995]. The details of this optimization may be found in Appendix C.

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:17

We have found that BarrElim is an impactful optimization for OpenMP codes. For example, on the NPB benchmarks [Van der Wijngaart and Wong 2002], the BarrElim optimized code yielded up to 5% improvement in execution time; both, the input benchmarks and the BarrElim optimized codes, were compiled using the -O3 switch of gcc. Considering that the obtained gains are on top of the many optimizations enabled by the -O3 switch, it can be seen that the gains are significant.
6 IMPLEMENTATION AND EVALUATION We have implemented all the key components of Homeostasis (see Fig. 3, and Section 3.2) in the IMOP compiler framework [Nougrahiya and Nandivada 2019]. IMOP is a new source-to-source
compiler framework for writing program analysis and optimization tools for OpenMP C programs.
We have also implemented HIDFA , our generic inter-thread IDFA pass with incremental update in Homeostasis. Both these implementations span around 11K lines of Java code inside IMOP. In order to assess the usability of Homeostasis, we have implemented BarrElim, an optimization pass for barrier removal (see Section 5). As expected, in BarrElim no stabilization-specific code was needed. This underscores the ease-of-use facilitated by the design of Homeostasis. We have made our implementation of Homeostasis open-source and publicly available [Anonymous 2021].
We present our evaluation on a set of fifteen real-world benchmarks taken from four popular
benchmark suites (listed in Figure 9) : (i) all the eight benchmarks of NPB-OMP 3.0 suite [Van der Wijngaart and Wong 2002], (ii) quake, the only OpenMP-C benchmark available from SPEC OMP 2012 [Aslot et al. 2001], (iii) amgmk, clomp, and stream from Sequoia benchmark suite [Seager, M 2008], and (iv) histo, stencil, and tpacf from Parboil benchmark suite [Stratton et al. 2012]. Note that these comprise some of the largest standard open-source benchmark programs for OpenMP C.
Since IMOP accepts only OpenMP C programs, we did not consider any other benchmarks from SPEC OMP 2012, or from Sequoia, as they contain a mix of C/C++/MPI code. For IS from NPB, the client optimization pass BarrElim could not find any opportunity of optimization; as no program transformation was performed by the pass, no stabilization of data-flow analyses was triggered in any mode of self-stabilization ­ hence, we skipped IS from our discussion.
Figure 9 lists a few static characteristics of the benchmarks, such as the size of each selected
benchmark, the number of parallel constructs, and the number of phases (as computed by phase analysis derived from YConAn). We have performed our experiments on a 2.3 GHz AMD Abu Dhabi system with 512 GB of memory, running CentOS 6.4. We use the Java HotSpot v1.8.0_171 64-bit Server VM to run Homeostasis. The OpenMP codes were compiled using the GCC 8.3.0 compiler. Taking inspiration from the insightful paper of Georges et al. [2007], we report the compilation-
and execution-time numbers by taking a geometric mean over 30 runs. We present the evaluation in three directions: (i) Ease-of-use of Homeostasis (in Section 6.1), (ii) Performance evaluation of the proposed lazy modes of stabilization (in Section 6.2), and (iii) Correctness of the lazy modes of
stabilization (in Section 6.3).
6.1 Self-stabilization vs. Manual Stabilization We now present an empirical study for assessing the impact of Homeostasis on writing different compiler passes, by comparing the coding efforts required to perform self-stabilization (in the presence of Homeostasis), against those required to perform manual stabilization (in the absence of Homeostasis). We perform this evaluation in the context of various components of BarrElim.
We used a simple scheme to estimate the additional coding efforts that may be required to
perform manual stabilization, as discussed below. We profiled the IMOP compiler by instrumenting the implementations of BarrElim, as well as of various program analyses and elementary transformations. By running this profiled compiler on each benchmark program, we obtain (i) the set of change-points for BarrElim, and (ii) the set of program-abstractions that may be impacted by

Preprint submitted to arXiv

1:18

Nougrahiya and Nandivada

1
Benchmark

2

3

45

6

7

8

9

10

11

12

Characteristics

STB-time (s) Total-time (s) Memory (MB)

#LOC #Node #PC #Barr #Ph RPINV LZUPD RPINV LZUPD RPINV LZUPD

1. BT (NPB)

3909 4450 9 47 558 5.96 0.53 12.23 6.04 4699.58 1944.04

2. CG (NPB)

1804 1367 14 31 31 3.39 0.37 5.66 2.75 3589.21 1653.74

3. EP (NPB)

1400 843 2 4 4 0.07 0.01 1.6 1.5 1522.07 1493.42

4. FT (NPB)

2223 1895 7 14 14 6.8 0.47 9.86 3.84 4598.49 1817.76

5. LU (NPB)

4282 4138 8 35 185 14.09 0.89 19.11 5.11 6129.66 1889.08

6. MG (NPB)

2068 2496 10 19 19 55.41 3.22 60.21 7.96 6195.26 5266.42

7. SP (NPB)

3463 4839 7 72 278 14.39

0 18.38 4.02 6025.75 1762.92

8. quake (SPEC) 2775 3068 11 22 30 11.02 0.34 14.19 3.64 6148.73 1766.11

9. amgmk (Sequoia) 1463 1804 2 5 5 4.81

1 7.45 3.68 3908.71 1702.06

10. clomp (Sequoia) 1148 3638 28 73 22268 12.54 2.15 17.02 6.59 6118.29 2870.75

11. stream (Sequoia) 214 727 10 20 12 1.01 0.07 2.85 1.91 1695.32 1534.75

12. histo (Parboil) 725 1914 1 2 2 0.9 0.08 3.05 2.23 1671.12 1588.87

13. stencil (Parboil) 641 1418 1 2 2 0.1

0 1.94 1.82 1533.26 1533.81

14. tpacf (Parboil) 774 1795 1 2 2 1.09 0.18 3.19 2.31 1654.85 1628.84

Fig. 9. Benchmark characteristics. Abbreviations: #LOC=number of lines of code, #Node=number of executable nodes in the super-graph, #PC=number of static parallel constructs, #Barr=number of static barriers (implicit + explicit), and #Ph=number of static phases. STB-Time and Total-time refer to the stabilization time and total time, respectively, taken by Homeostasis to compile the benchmark (using RPINV and LZUPD modes) with BarrElim. Memory refers to the maximum additional memory-footprint for running BarrElim (with RPINV and LZUPD modes). Entries with the maximum savings in time and memory are shown in bold.

Program-Abs. Mode |STB| #RP

Component of BarrElim LOC #CP Points-to graphs

UPD

316 17

Parallel-construct expansion 1675 66 Control-flow graphs UPD

619 70

Function inlining

463 15 Call graphs

UPD

18 60

Redundant-barrier deletion 313 2 Phase information INV

136 61

Driver

10 1 Inter-task edges

INV

70 31

Total : 2461 84
(a) Maximum number of change-points obtained within the components of BarrElim upon running it on the benchmarks under study. Abbr. : LOC=number of lines of code; #CP=number of change-points.

Symbol-/type-tables UPD Label-lookup tables UPD

78 37 477 60

(b) Program-abstractions read by BarrElim. Abbr : Mode=stabilization-mode; |STB|=LOC of stabilizationcode; #RP=number of relevant change-points in BarrElim after which the abstraction was read from.

Fig. 10. Stabilization-related information for BarrElim, obtained upon profiling/inspecting the code.

BarrElim (see Section 5). In this section, we use this data to estimate the manual coding efforts that may be required to answer Q1-3 and QT1-3 (discussed in Section 1) in the absence of Homeostasis, and thereby demonstrate the advantages of Homeostasis over manual stabilization.
Which program-abstractions to stabilize? As discussed in Section 5, since it is difficult to obtain the exact set of critical program-abstractions (to be stabilized manually), we conservatively estimate the same. We manually analyzed the code of BarrElim and found that there are seven program-abstractions that are used and/or impacted by BarrElim; these are listed in Fig. 10b. Thus, in case of manual-stabilization, on writing BarrElim, the compiler writer needs to identify these
seven program-abstractions, from the plethora of available program-abstractions ­ a daunting task.
Further, while adding any new program-abstraction A, the compiler writer needs to manually reanalyze BarrElim (and every other optimization) to check its impact on A.
In contrast, in the presence of Homeostasis, all these tasks are automated ­ the compiler writer

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:19

needs to put no effort to identify the program-abstractions (existing or new) that may be impacted by an optimization pass.
Where to invoke stabilization? In Fig. 10a, we enumerate the number of change-points discovered in the major components of BarrElim. In the absence of Homeostasis, the compiler writer would have to correctly identify these 84 change-points (i.e., on an average, almost 1 for every 28 lines of code) in BarrElim, and insert code for ensuring stabilization of the affected programabstractions. At each change-point, the compiler writer may need to handle stabilization of the impacted program-abstractions, irrespective of the chosen mode of stabilization. To identify a more aggressive baseline, we note that not all change-points (referred to in Figure 10a) may warrant stabilization, and a compiler writer may need to invoke the stabilization code only at the relevant change-points (see Section 5), for only those set of abstractions that may be modified at that point.
Fig. 10b, column 4, lists the number of relevant change-points for each program-abstraction impacted by BarrElim; this data too was obtained by profiling the IMOP compiler (profiling details discussed above), while running it on all the benchmarks under study. The figure shows that there are significant number of places in the components of BarrElim, where this stabilization code needs to be manually invoked, in the absence of Homeostasis. For example, CFG stabilization needs to be performed at 70 places, and call-graphs at 60 places ­ which can lead to cumbersome and error-prone code. Further, upon addition of any new program analysis to the compiler (or any modification to the existing analysis), the compiler writer would have to revisit all the changepoints of pre-existing optimizations (for example, 84 change-points for BarrElim) to check if the change-point may have necessitated stabilization of the newly added/modified program-abstraction.
In contrast, in the presence of Homeostasis, all the above tasks are automated ­ the compiler writer needs to spend no effort in identifying the places of stabilization, as she needs to add no additional code as part of the optimization in order to stabilize the program-abstractions.
How to stabilize? For manual stabilization of a program-abstraction, the compiler writer may choose from any of the six modes of stabilization (listed in Section 6), or a combination thereof. Out of the seven program-abstractions that require stabilization by BarrElim, phase analysis and inter-task edges have been derived from YConAn [Zhang and Duesterwald 2007; Zhang et al. 2008]. It is not clear if a straightforward approach exists to support update modes of stabilization for YConAn. Hence, to measure the manual coding efforts for these two program-abstractions we assume invalidate modes of stabilization. For the remaining program-abstractions, an experienced compiler writer would use update modes of stabilization for performing manual stabilization (see Section 3.2.3). By inspecting the code of IMOP, we estimate the amount of manual code required for stabilization of all the seven program-abstractions in column 3 of Fig. 10b. Upon adding a new program analysis, the compiler writer would need to manually write additional stabilization code. In contrast, for the analyses shown in Fig. 10b, in the presence of Homeostasis, for the case of iterative data-flow analyses, such as points-to analysis (with 316 lines of code for manual stabilization) or analyses stabilized in invalidate-mode (such as the phase-analysis), the compiler writer would not have to write any stabilization code.
Summary. In contrast to the traditional compilers, it is much easier to write optimizations or IDFAbased analyses in Homeostasis, as the compiler writer does not have to worry about stabilization.
6.2 Performance Evaluation
We conduct the performance evaluation of the proposed lazy modes of stabilization, by studying the parameters related to compilation time and memory consumption, in the context of the prior discussed BarrElim optimization. We do so by presenting a comparison to the RP-modes of stabilization. We have also done an elaborate evaluation compared to the weaker baseline of the eager-modes of stabilization - the details can be found in Appendix A.

Preprint submitted to arXiv

1:20

Nougrahiya and Nandivada

(A) Compilation time. We now present an evaluation describing the impact of the lazy modes of self-stabilization on the compilation time of the above discussed benchmark programs while running BarrElim. For reference, in Fig. 9, columns 7-10 we show the time spent in self-stabilization and the total compilation time, in the context of the RPINV and the LZUPD modes of self-stabilization, while performing BarrElim.
We illustrate the impact of LZINV, RPUPD, and LZUPD, by showing their relative speedups with respect to RPINV, in terms of speedups in the IDFA stabilization-time (see Fig. 11); the raw numbers of stabilization-time for RPINV and LZUPD are shown in Fig. 9 (columns 7 and 8) for reference. As expected, the LZUPD mode incurs the least cost for stabilization among all the cases; consequently it results in the maximum speedup with respect to RPINV ­ with speedups varying between 4.81× to 32.41× (geomean = 10.81×) in the time taken for stabilization of data-flow analyses. We have noted that the gains using a particular mode of stabilization depend on multiple stabilization-mode-
specific factors, such as (i) number of triggers of stabilization, (ii) number of times the program
nodes are re-processed during stabilization, (iii) cost incurred to process each program node per
stabilization, and so on. We illustrate our observations by comparing the performance of different
modes of stabilization. LZUPD vs. RPINV. In case of LZUPD, the maximum speedup in the IDFA stabilization-time (32.41×)
was observed for quake, consequent upon the fact that in quake, compared to RPINV, LZUPD reprocesses only a small fraction (0.23%, data not shown) of the nodes. In contrast, amgmk and clomp, where the LZUPD re-processes the highest fractions (5.45% and 1.22%, respectively) of nodes across all the benchmarks, correspond to the minimum (though still quite significant) speedups obtained ­ 4.81× and 5.83×, respectively.
LZUPD vs. RPUPD. It is clear from Fig. 11 that though RPUPD mode consistently performs better than RPINV, compared to LZUPD it performs significantly worse. This is because RPUPD processes significantly higher number of nodes compared to LZUPD across all the benchmarks (geomean 32.63% higher).
LZUPD vs. LZINV. As shown in Fig. 11, we see that in the context of IDFA stabilization-time, LZUPD performs better than LZINV for all the benchmarks (geomean 7.15× better); this observation can be attributed to the fact that the cost of invalidating and recomputing a program-abstraction (such
as the results of any instantiation of HIDFA ) is, in general, higher than the cost of incrementally updating the program-abstraction.
LZINV vs. RPUPD. From Fig. 11, note that the RPUPD mode consistently outperforms the LZINV mode in the context of IDFA-stabilization time, owing to the observation that while applying BarrElim on the benchmarks under study, the performance impact of the reduction in the number of stabilization triggers in case of LZINV mode is overpowered by the impact of reduction in the number of nodes processed per stabilization trigger in the case of RPUPD mode. This is evident from the fact the total number of times the RPUPD mode re-processes the nodes is considerably lesser than that in the case of LZINV mode (geomean 97.83% lesser).
Summary. Overall, we found that the LZUPD mode leads to maximum benefits for stabilization time, across all four modes of stabilization. This in turn leads to significant improvement in the total compilation time, with speedup (compared to RPINV) varying between 1.07× to 7.56× (geomean 2.26×), across all the benchmarks (see columns 9 and 10 in Fig. 9, for the raw numbers). It can also be seen that in most of the benchmarks LZUPD not only reduces the cost of stabilization, but also the rest of the compilation time (i.e., (column 9-column 7) > (column 10-column 8)), which we believe to be caused by the latent benefits (in cache, garbage collection and so on) arising due to
significant reduction in memory usage, as discussed next.

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:21

Fig. 11. Speedup in IDFA stabilization-time under various modes of stabilization w.r.t. the RPINV mode, when applying the client optimization, BarrElim. Higher is better.
Fig. 12. % savings in memory footprint (max resident set size) under various modes of self-stabilization w.r.t. the RPINV mode, while running the client optimization, BarrElim. Higher is better.
(B) Memory consumption. For reference, in Fig. 9 (columns 11 and 12) we show the maximum additional memory footprint (in MB), in terms of the maximum resident size, while running BarrElim. We have obtained these values by taking the difference of peak memory requirements during compilation with and without the optimization pass. The values shown are calculated with the help of /usr/bin/time GNU utility (version : 1.7). Considering imprecision in such a measurement - we believe that very small improvements or deteriorations (< 2%) should be ignored.
In Fig. 12, we illustrate the percentage savings in the memory footprint by LZINV, RPUPD, and LZUPD modes, as compared to the RPINV mode. All these three modes of stabilization perform better (or are more or less comparable), in terms of memory requirements, than the RPINV mode. The geomean improvements in memory consumption are 2.36%, 11.71%, and 20.59%, for LZINV, RPUPD, and LZUPD modes, respectively, as compared to the RPINV mode.
As discussed above, both lazy and update options minimize the number of times different transfer functions are applied during stabilization of the data-flow analyses ­ this claim is substantiated by the observation that for all the benchmarks, LZUPD requires the least amount of memory, with maximum percentage savings of 71.28% for quake, over the RPINV mode.
Summary. Overall, we see that the proposed lazy modes of stabilization lead to significant memory savings compared to the naive RPINV scheme. This in turn can improve the memory traffic and overall gains in performance.
Preprint submitted to arXiv

1:22

Nougrahiya and Nandivada

6.3 Empirical Correctness In order to empirically validate the correctness of our design/implementation of HIDFA and its four instantiations (see Section 5), we have verified the generated flow-facts under each mode of stabilization, in the context of various optimization passes provided by IMOP [Nougrahiya and Nandivada 2019]. We found that the final flow-facts across all the stabilization-modes match verbatim.
Further, to empirically validate correct stabilization in the context of BarrElim, we have also verified that for each benchmark the generated optimized code (i) does not differ across the modes of self-stabilization, and (ii) produces the same execution output as that of the unoptimized code.
Overall evaluation summary. Our evaluation shows that (i) Homeostasis makes it easy to write optimizations and program analysis passes, (ii) the lazy stabilization choices lead to faster compilation times, and (iii) our implementation leads to correct analysis and optimized code.

7 RELATED WORK Self-stabilization. There have been various attempts towards enabling automated stabilization of specific program-abstractions, in response to program transformations. For example, Carle and Pollock [1989]; Nilsson-Nyman et al. [2009]; Reps et al. [1983] rely on incremental evaluation of attribute grammars, where program-abstractions are limited to be expressed as context-dependent attributes of the language constructs. Carroll and Polychronopoulos [2003] provide an important step, where their proposed framework aims to attain automated incremental compilation for serial programs expressed in PROMIS IR. However, their approach does not address issues such as pass dependencies. Further, the techniques given by them have been devised from the perspective of compilers of serial programs. For instance, their approach tracks all program updates by storing them in the program's internal representation, locally at the point of update. However, unlike in the case of serial programs, the impact of a program update in case of parallel programs would not be localised to the point where the update has been performed. As a result, such global impacts will not be modelled by their approach, resulting in incorrect stabilization under parallel semantics. Likewise, various research compilers exist that provide automated stabilization for only a fixed small set of program-abstractions, such as symbol-information in case of zJava [Brewster and Abdelrahman 2001], and IR control-flow information in case of Polaris compiler [Blume et al. 1995]. While the base program analysis (equivalent of BasePA in Homeostasis) of Cetus compiler [ik Lee et al. 2003] validates the consistency of IR at the end of each pass, it does not stabilize the program-abstractions.
When a program undergoes edits across its multiple versions, or during the process of its development in IDEs, the full recompilation of the modified program including re-computation of all the analyses and application of optimizations from scratch can be cost-prohibitive. Various approaches have been given to reuse different program-abstractions (including object code, and IR obtained upon applying optimizations) from the prior compilations of the program to minimize the cost of recompilation. Smith et al. [1990] have developed mechanisms to perform incremental update of dependence information during interactive parallelization of Fortran programs. During recompilation of programs, Pollock and Soffa [1992] incrementally incorporate the changes into globally optimized code in an attempt to reduce redundant analysis that is performed for the purpose of optimizations. For enabling incremental symbolic executions, Person et al. [2011] have provided methods to detect and characterize the effects of program changes, using static analyses. Incrementalization of static analyses that are expressed using Prolog, is facilitated by Eichberg et al. [2007]. Various approaches, such as those by Szabó et al. [2018, 2016], exist to enable incremental update of static analyses in response to program edits in the context of Integrated Development Environments (IDE)s. Eichberg and Bockisch [2005] provide a constraint-solving based approach

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:23

for resolving dependencies (explicitly specified) among the analyses, in Eclipse. Kloppenburg [2009] discusses the notion of incremental update for static analyses, in the context of IDEs. To the best of our knowledge, there are no generic object-oriented compiler designs or implementations that address the challenges related to stabilization requirements, and guarantee self-stabilization during the process of compilation, especially in the context of parallel programs. Incremental and Parallel IDFA. There is a vast literature on the topic of incremental update of data-flow analysis. Arzt and Bodden [2014] have provided approaches to enables incremental update for IDE-/IFDS-based data-flow analyses. Ryder [1983] has given two powerful incremental update algorithms for forward and backward data-flow problems, based on Allen/Cocke interval analysis [Allen and Cocke 1976]. Sreedhar et al. [1996] have given methods to perform incremental update for elimination-based data-flow analyses. Other important approaches have been given by Carroll and Ryder [1987, 1988] for incremental update of data-flow problems based on interval, and elimination-based, analyses. Owing to the presence of inter-task edges in OpenMP programs there are a large number of improper regions (irreducible subgraphs) in our super-graph, rendering any form of structural data-flow analyses infeasible over the graph [Muchnick 1998]. HIDFA is inspired from the work by Marlowe and Ryder [1989]; Ryder et al. [1988] (referred as MR here), and from the two-phase incremental update algorithms for iterative versions of data-flow analysis given by Pollock and Soffa [1989]. The goal of both MR as well as HIDFA is same ­ to provide for an incremental update version of iterative data-flow analysis problems. MR requires that the data-flow solution be factored into two parts: internal and external. Central to the factoring of external part is the manual determination of the appropriate representative problem for each data flow problem. In contrast, HIDFA does not require any such division or manual intervention. Further, though the MR algorithm is applicable for a large class of flow problems, it is not applicable to problems like constant-propagation (due to their non-distributive nature), whereas, HIDFA is applicable to all monotone data flow problems.
Given the importance of data-flow analyses there have been numerous works that have provided analysis-specific methods for incremental update, as well as its parallelism. For instance, Yur et al. [1997] have provided incremental update mechanisms for side-effect analysis, for the case of C programs. Chen et al. [2015] have provided incremental update of inclusion-based points-to analysis for Java programs. Similarly, Liu et al. [2019] have provided an incremental and parallel version of pointer analysis. We note that all these analyses differ from HIDFA , due to its generic nature which completely hides the implementation of (i) parallelism semantics, and (ii) incremental modes of self-stabilization, from the writers of current and future IDFAs.
8 CONCLUSION
In this paper, we have presented a novel, efficient, and reliable compiler-design framework called Homeostasis, for enabling generic self-stabilization of the relevant program abstractions in response to every possible transformation of the program, in the context of object-oriented compilers, for both serial and parallel programs. Homeostasis relies on key principles of OOP, and the Observer design pattern, to ensure that no stale values are read from any program-abstraction. Homeostasis decouples the program analysis and optimization passes in a compiler: using Homeostasis, neither the optimization writers need to write any code to stabilize the (existing or future) programabstractions, nor the writers of a program analysis need to know about the set of optimization passes in the compiler in order to ensure correct stabilization of the corresponding programabstraction. We added Homeostasis to the IMOP compiler framework for OpenMP C programs. To illustrate the benefits of Homeostasis, we implemented a generic inter-thread data-flow analysis pass HIDFA . Using HIDFA , compiler writers can instantiate new IDFA-based analyses without needing to add any stabilization-specific code; we observed this in practice by implementing a set of four

Preprint submitted to arXiv

1:24

Nougrahiya and Nandivada

standard IDFAs as instantiations of HIDFA . We also implemented an optimization BarrElim, which includes a set of four standard optimizations, and is used to remove redundant barriers in OpenMP programs; we did not have to add any stabilization-specific code for BarrElim. Our evaluation of these passes on a set of real-world benchmarks has given us encouraging results concerning performance and feasibility of using Homeostasis. While Homeostasis has been discussed using a Java-based compiler for OpenMP C, we believe that the design of Homeostasis is generic enough to be applicable to even production-level object-oriented compilers (including JIT compilers), for
serial as well as parallel programs.

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:25

REFERENCES
F. E. Allen and J. Cocke. 1976. A Program Data Flow Analysis Procedure. Commun. ACM 19, 3 (March 1976), 137. https://doi.org/10.1145/360018.360025
Raghesh Aloor and V. Krishna Nandivada. 2015. Unique Worker Model for OpenMP. In Proceedings of the 29th ACM on International Conference on Supercomputing (ICS '15). ACM, New York, NY, USA, 47­56.
Anonymous. 2021. Implementation of Homeostasis in the IMOP compiler framework. https://github.com/anonymousoopsla21/ homeostasis
Steven Arzt and Eric Bodden. 2014. Reviser: Efficiently Updating IDE-/IFDS-Based Data-Flow Analyses in Response to Incremental Program Changes. In Proceedings of the 36th International Conference on Software Engineering (ICSE 2014). Association for Computing Machinery, New York, NY, USA, 288­298. https://doi.org/10.1145/2568225.2568243
Vishal Aslot, Max Domeika, Rudolf Eigenmann, Greg Gaertner, Wesley B. Jones, and Bodo Parady. 2001. SPEComp: A New Benchmark Suite for Measuring Parallel Computer Performance. In Workshop on OpenMP Applications and Tools, Rudolf Eigenmann and Michael J. Voss (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 1­10.
Rajkishore Barik, Jisheng Zhao, and Vivek Sarkar. 2013. Interprocedural strength reduction of critical sections in explicitlyparallel programs. In Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques, Edinburgh, United Kingdom, September 7-11, 2013. IEEE Computer Society, 29­40.
William Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, William Pottenger,
Lawrence Rauchwerger, Peng Tu, and Stephen Weatherford. 1995. Polaris: Improving the effectiveness of parallelizing compilers. In Languages and Compilers for Parallel Computing, Keshav Pingali, Utpal Banerjee, David Gelernter, Alex Nicolau, and David Padua (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 141­154. Neil V. Brewster and Tarek S. Abdelrahman. 2001. A Compiler Infrastructure for High-Performance Java. In High-Performance Computing and Networking, Bob Hertzberger, Alfons Hoekstra, and Roy Williams (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 675­684. Alan Carle and Lori Pollock. 1989. Modular Specification of Incremental Program Transformation Systems. In Proceedings of the 11th International Conference on Software Engineering (ICSE '89). Association for Computing Machinery, New York, NY, USA, 178­187. https://doi.org/10.1145/74587.74612 Martin Carroll and Barbara G Ryder. 1987. An Incremental Algorithm for Software Analysis. In Proceedings of the Second ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments (SDE 2). Association for Computing Machinery, New York, NY, USA, 171­179. https://doi.org/10.1145/24208.24228 M. D. Carroll and B. G. Ryder. 1988. Incremental Data Flow Analysis via Dominator and Attribute Update. In Proceedings of the 15th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL '88). Association for Computing Machinery, New York, NY, USA, 274­284. https://doi.org/10.1145/73560.73584
Steven Carroll and Constantine Polychronopoulos. 2003. A Framework for Incremental Extensible Compiler Construction. In Proceedings of the 17th Annual International Conference on Supercomputing (ICS'03). Association for Computing Machinery, New York, NY, USA, 53­62. https://doi.org/10.1145/782814.782824
Yuting Chen, Qiuwei Shi, and Weikai Miao. 2015. Incremental Points-to Analysis for Java via Edit Propagation. In Structured Object-Oriented Formal Language and Method, Shaoying Liu and Zhenhua Duan (Eds.). Springer International Publishing, Cham, 164­178.
Leonardo Dagum and Ramesh Menon. 1998. OpenMP: an industry standard API for shared-memory programming. Computational Science & Engineering, IEEE 5, 1 (1998), 46­55.
Michael Eichberg and Christoph Bockisch. 2005. . http://www.st.informatik.tu-darmstadt.de/Magellan
Michael Eichberg, Matthias Kahl, Diptikalyan Saha, Mira Mezini, and Klaus Ostermann. 2007. Automatic Incrementalization of Prolog Based Static Analyses. In Proceedings of the 9th International Conference on Practical Aspects of Declarative Languages (PADL'07). Springer-Verlag, Berlin, Heidelberg, 109­123. https://doi.org/10.1007/978-3-540-69611-7_7
Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley Longman Publishing Co., Inc., USA.
Andy Georges, Dries Buytaert, and Lieven Eeckhout. 2007. Statistically Rigorous Java Performance Evaluation. SIGPLAN Not. 42, 10 (Oct. 2007), 57­76. https://doi.org/10.1145/1297105.1297033
Google. 2001. Chrome V8. https://github.com/v8/v8 Manish Gupta and Edith Schonberg. 1996. Static Analysis to Reduce Synchronization Costs in Data-Parallel Programs. In
Proceedings of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL '96). Association for Computing Machinery, New York, NY, USA, 322­332. https://doi.org/10.1145/237721.237799
Suyash Gupta, Rahul Shrivastava, and V. Krishna Nandivada. 2017. Optimizing recursive task parallel programs. In Proceedings of the International Conference on Supercomputing, ICS 2017, Chicago, IL, USA, June 14-16, 2017, William D. Gropp, Pete Beckman, Zhiyuan Li, and Francisco J. Cazorla (Eds.). ACM, 11:1­11:11.
IBM. 2017. Eclipse OpenJ9. https://github.com/eclipse/openj9 Sang ik Lee, Troy A. Johnson, and Rudolf Eigenmann. 2003. Cetus - An Extensible Compiler Infrastructure for Source-to-

Preprint submitted to arXiv

1:26

Nougrahiya and Nandivada

Source Transformation. In Languages and Compilers for Parallel Computing, 16th Intl. Workshop, College Station, TX, USA, Revised Papers, volume 2958 of LNCS. 539­553. Sven Kloppenburg. 2009. Incrementalization of Analyses for Next Generation IDEs. Ph.D. Dissertation. Technische Universität,

Darmstadt. http://tuprints.ulb.tu-darmstadt.de/1960/

Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for Lifelong Program Analysis & Transformation. In Proceedings of the International Symposium on Code Generation and Optimization (CGO '04). IEEE Computer Society,

Washington, DC, USA.
Bozhen Liu, Jeff Huang, and Lawrence Rauchwerger. 2019. Rethinking Incremental and Parallel Pointer Analysis. ACM Trans. Program. Lang. Syst. 41, 1, Article 6 (March 2019), 31 pages. https://doi.org/10.1145/3293606
LLVM-Developer-Community. 2019a. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

8299fd9dee7df7c5f92ab2572aad04ce2fbbf83e LLVM-Developer-Community. 2019b. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

d2904ccf88e8ed487647feb90cfbf331bd888509 LLVM-Developer-Community. 2019c. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

a95d95d3922e1a24d8b9affdd570c1d8fca00129 LLVM-Developer-Community. 2020a. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

fa8c2ae76f7e4f498d29e2716233bd29025e8827 LLVM-Developer-Community. 2020b. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

de92dc2850c17259090ccf644b2f2375ab8e1664 LLVM-Developer-Community. 2020c. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

e1133179587dd895962a2fe4d6eb0cb1e63b5ee2 LLVM-Developer-Community. 2020d. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

e2fc6a31d347dc96c2dec6acb72045150f525630 LLVM-Developer-Community. 2020e. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

1ccfb52a6174816e450074f65e5f0929a9f046a5 LLVM-Developer-Community. 2020f. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

e6cf796bab7e02d2b8ac7fd495f14f5e21494270 LLVM-Developer-Community. 2020g. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

edccc35e8fa2c546e0ef1c8efde56e6b12e3c175 LLVM-Developer-Community. 2020h. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

d6b05fccb709eb38b5b4b21901cb63825faee83e LLVM-Developer-Community. 2020i. LLVM GitHub Repository.

https://github.com/llvm/llvm-project/commit/

0d90d2457c3b94760df4848941c0e7b93d07b1a2 LLVM-Developer-Community. 2021a. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

ddc4b56eef9fec990915470069a29e70bbde3711 LLVM-Developer-Community. 2021b. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

7c8b8063b66c7b936d41a0c4069c506669e13115 LLVM-Developer-Community. 2021c. LLVM GitHub Repository. https://github.com/llvm/llvm-project/commit/

2461cdb41724298591133c811df82b0064adfa6b

Thomas J. Marlowe and Barbara G. Ryder. 1989. An Efficient Hybrid Algorithm for Incremental Data Flow Analysis. In Proceedings of the 17th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL'90). Association

for Computing Machinery, New York, NY, USA, 184­196. https://doi.org/10.1145/96709.96728 Steven S. Muchnick. 1998. Advanced Compiler Design and Implementation. Morgan Kaufmann Publishers Inc., San Francisco,

CA, USA.

V. Krishna Nandivada, Jun Shirako, Jisheng Zhao, and Vivek Sarkar. 2013. A Transformation Framework for Optimizing Task-Parallel Programs. ACM Trans. Program. Lang. Syst. 35, 1, Article Article 3 (April 2013), 48 pages. https://doi.org/

10.1145/2450136.2450138

Emma Nilsson-Nyman, Görel Hedin, Eva Magnusson, and Torbjörn Ekman. 2009. Declarative Intraprocedural Flow Analysis of Java Source Code. Electronic Notes in Theoretical Computer Science 238, 5 (2009), 155­171. Proceedings of the 8th

Workshop on Language Descriptions, Tools and Applications (LDTA 2008). Aman Nougrahiya and V. Krishna Nandivada. 2019. IMOP : IIT Madras OpenMP compiler framework. https://github.com/

amannougrahiya/imop-compiler Oracle. 1999. HotSpot. https://github.com/openjdk-mirror/jdk7u-hotspot

Suzette Person, Guowei Yang, Neha Rungta, and Sarfraz Khurshid. 2011. Directed Incremental Symbolic Execution. In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '11).

Association for Computing Machinery, New York, NY, USA, 504­515. https://doi.org/10.1145/1993498.1993558 L. L. Pollock and M. L. Soffa. 1989. An incremental version of iterative data flow analysis. IEEE Transactions on Software

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:27

Engineering 15, 12 (1989), 1537­1549. Lori L. Pollock and Mary Lou Soffa. 1992. Incremental Global Reoptimization of Programs. ACM Trans. Program. Lang. Syst.
14, 2 (April 1992), 173­200. https://doi.org/10.1145/128861.128865 Daniel Quinlan, Chunhua Liao, Thomas Panas, Robb Matzke, Markus Schordan, Rich Vuduc, and Qing Yi. 2013. ROSE User
Manual: A Tool for Building Source-to-Source Translators. Technical Report. Lawrence Livermore National Laboratory. Thomas Reps, Tim Teitelbaum, and Alan Demers. 1983. Incremental Context-Dependent Analysis for Language-Based
Editors. ACM Trans. Program. Lang. Syst. 5, 3 (July 1983), 449­477. https://doi.org/10.1145/2166.357218 James Rumbaugh, Michael Blaha, William Premerlani, Frederick Eddy, and William Lorensen. 1991. Object-Oriented Modeling
and Design. Prentice-Hall, Inc., USA. B.G. Ryder, T.J. Marlowe, and M.C. Paull. 1988. Conditions for incremental iteration: Examples and counterexamples. Science
of Computer Programming 11, 1 (1988), 1­15. Barbara G. Ryder. 1983. Incremental Data Flow Analysis. In Proceedings of the 10th ACM SIGACT-SIGPLAN Symposium on
Principles of Programming Languages (POPL'83). Association for Computing Machinery, New York, NY, USA, 167­176. https://doi.org/10.1145/567067.567084 Barbara Gershon Ryder, Thomas J Marlowe, and Marvin C Paull. 1987. Incremental iteration: When will it work? Rutgers University, Department of Computer Science, Laboratory for Computer Science Research.
Seager, M. 2008. The ASC Sequoia Programming Model. (8 2008). https://doi.org/10.2172/945684
Kevin Smith, Bill Appelbe, and Kurt Stirewalt. 1990. Incremental Dependence Analysis for Interactive Parallelization. In Proceedings of the 4th International Conference on Supercomputing (ICS '90). Association for Computing Machinery, New York, NY, USA, 330­341. https://doi.org/10.1145/77726.255173
Vugranam C. Sreedhar, Guang R. Gao, and Yong-Fong Lee. 1996. A New Framework for Exhaustive and Incremental Data Flow Analysis Using DJ Graphs. In Proceedings of the ACM SIGPLAN 1996 Conference on Programming Language Design and Implementation (PLDI '96). Association for Computing Machinery, New York, NY, USA, 278­290. https: //doi.org/10.1145/231379.231434
Richard M. Stallman and GCC-Developer-Community. 2009. Using The GNU Compiler Collection: A GNU Manual For GCC Version 4.3.3. CreateSpace, Paramount, CA.
John A. Stratton, Christopher I. Rodrigues, I-Jui Sung, Nady Obeid, Li-Wen Chang, Nasser Anssari, Geng Liu, and Wen mei
W. Hwu. 2012. Parboil: A Revised Benchmark Suite for Scientific and Commercial Throughput Computing.
Tamás Szabó, Gábor Bergmann, Sebastian Erdweg, and Markus Voelter. 2018. Incrementalizing Lattice-Based Program Analyses in Datalog. Proc. ACM Program. Lang. 2, OOPSLA, Article Article 139 (Oct. 2018), 29 pages. https://doi.org/10. 1145/3276509
Tamás Szabó, Sebastian Erdweg, and Markus Voelter. 2016. IncA: A DSL for the Definition of Incremental Program Analyses. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering (ASE 2016). Association for Computing Machinery, New York, NY, USA, 320­331. https://doi.org/10.1145/2970276.2970298
Chau-Wen Tseng. 1995. Compiler Optimizations for Eliminating Barrier Synchronization. In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP '95). Association for Computing Machinery, New York, NY, USA, 144­155. https://doi.org/10.1145/209936.209952
Uresh Vahalia. 1995. UNIX Internals: The New Frontiers. Prentice Hall Press, USA. Raja Vallée-Rai, Phong Co, Etienne Gagnon, Laurie Hendren, Patrick Lam, and Vijay Sundaresan. 2010. Soot: A Java
Bytecode Optimization Framework. In CASCON First Decade High Impact Papers (CASCON '10). IBM Corp., USA, 214­224. https://doi.org/10.1145/1925805.1925818 Rob F Van der Wijngaart and Parkson Wong. 2002. NAS parallel benchmarks version 3.0. Technical Report. NAS technical report, NAS-02-007.
Jyh-Shiarn Yur, Barbara G. Ryder, William A. Landi, and Phil Stocks. 1997. Incremental Analysis of Side Effects for C Software System. In Proceedings of the 19th International Conference on Software Engineering (ICSE '97). Association for Computing Machinery, New York, NY, USA, 422­432. https://doi.org/10.1145/253228.253369
Yuan Zhang and Evelyn Duesterwald. 2007. Barrier Matching for Programs with Textually Unaligned Barriers. In Proceedings of the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP '07). Association for Computing Machinery, New York, NY, USA, 194­204. https://doi.org/10.1145/1229428.1229472
Yuan Zhang, Evelyn Duesterwald, and Guang R. Gao. 2008. Concurrency Analysis for Shared Memory Programs with
Textually Unaligned Barriers. Springer-Verlag, Berlin, Heidelberg, Chapter Languages and Compilers for Parallel
Computing, 95­109. https://doi.org/10.1007/978-3-540-85261-2_7

A PERFORMANCE COMPARISON WITH EAGER MODES OF STABILIZATION We now present an evaluation describing the impact of the lazy modes of self-stabilization on the compilation time of the above discussed benchmark programs while running BarrElim. For

Preprint submitted to arXiv

1:28

Nougrahiya and Nandivada

1
Benchmark
1. BT (NPB) 2. CG (NPB) 3. EP (NPB) 4. FT (NPB) 5. LU (NPB) 6. MG (NPB) 7. SP (NPB) 8. quake (SPEC) 9. amgmk (Sequoia) 10. clomp (Sequoia) 11. stream (Sequoia) 12. histo (Parboil) 13. stencil (Parboil) 14. tpacf (Parboil)

2

3

STB-time (s)

EGINV LZUPD

28.66 0.53

9.83 0.37

1.64 0.01

18.41 0.47

60.31 0.89

126.2 3.22

51.45

0

30.47 0.34

14.06

1

64.52 2.15

3.91 0.07

3.96 0.08

1.97

0

4.46 0.18

4

5

Total-time (s)

EGINV LZUPD

22.06 6.04

8.17 2.75

0.12

1.5

15.95 3.84

55.81 5.11

47.63 7.96

121.9 4.02

27.91 3.64

11.52 3.68

60.12 6.59

2.77 1.91

1.81 2.23

0.17 1.82

2.38 2.31

6

7

Memory (MB)

EGINV LZUPD

6518.768 1944.04

5014.351 1653.74

1539.794 1493.42

6294.874 1817.76

6537.304 1889.08

6266.910 5266.42

6436.563 1762.92

6408.975 1766.11

5431.136 1702.06

6384.106 2870.75

1918.342 1534.75

1712.794 1588.87

1583.056 1533.81

2041.487 1628.84

Fig. 13. Raw numbers for performance comparison with naive eager modes of stabilization, in the context of BarrElim. Abbreviations: STB-Time and Total-time refer to the stabilization time and total time, respectively, taken by Homeostasis to compile the benchmark (using EGINV and LZUPD modes). Memory refers to the maximum additional memory-footprint for running BarrElim (with EGINV and LZUPD modes of stabilization).

reference, in Fig. 13, columns 2-5 show the time spent in self-stabilization and the total compilation time, in the context of the EGINV and LZUPD modes of self-stabilization, while performing BarrElim. As eager mode corresponds to triggering stabilization after each transformation, EGINV is arguably the simplest (and natural) way to achieve self-stabilization.
We illustrate the impact of EGUPD, LZINV, and LZUPD, by showing their relative speedups with respect to EGINV, in terms of speedups in IDFA stabilization-time (see Fig. 14); the raw numbers of stabilization-time for EGINV and LZUPD are shown in Fig. 13 (columns 2 and 3) for reference. As expected, the LZUPD mode incurs the least cost for stabilization among all the cases; consequently it results in the maximum speedup with respect to EGINV ­ with speedups varying between 11.52× to 82.09× (geomean = 28.42×) in the time taken for stabilization of data-flow analyses. As noted in Section 6.2, the gains using a particular mode of stabilization depend on multiple stabilization-mode
specific factors, such as (i) number of triggers of stabilization, (ii) number of times the program
nodes are re-processed during stabilization, (iii) cost incurred to process each program node, per
stabilization, and so on. LZUPD vs. EGINV. In case of speedups in the IDFA stabilization-time (Fig. 14), the gains ob-
served due to LZUPD varied between 11.52× and 82.09×, across all the benchmarks. As explained above the actual gains depend on multiple factors. For example, the maximum speedup in the IDFA stabilization-time (82.09×) was observed for quake, consequent upon the fact that in quake, compared to EGINV, LGUPD re-processes only a small fraction (0.03%, data not shown) of the nodes. Similarly, the relatively lower speedups (even though still quite significant) observed in the LGUPD mode for amgmk (11.52×), and EP (12×) can be attributed to the fact that in both these benchmarks the number of nodes re-processed in the EGINV mode per invalidation was the least (data not shown) among the benchmarks under consideration.
LZUPD vs. EGUPD. It is clear from Fig. 14, that though EGUPD mode consistently performs better than EGINV, compared to LZUPD it performs significantly worse. This is because EGUPD processes significantly higher number of nodes compared to LZUPD.
LZINV vs. EGUPD. As expected the performance comparison between LZINV and EGUPD throws a

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:29

Fig. 14. Speedup in IDFA stabilization-time under various modes of stabilization w.r.t. the EGINV mode, when applying the client analysis. Higher is better.
hung verdict, owing to benefits and losses due to lazy vs. eager and update vs. invalidate operations that are split between the two modes. Overall, we found that LZINV outperformed EGUPD by a narrow margin (geomean speedup = 1.9×).
Summary. Overall, we found that the LZUPD mode leads to maximum benefits for stabilization time, across all four modes of stabilization. This in turn leads to significant improvement in in the total compilation time, with speedup (compared to EGINV) varying between 1.08× to 15.86× (geomean = 3.80×), across all the benchmarks (see columns 4 and 5 in Fig. 13, for the raw numbers). It can also be seen that in most of the benchmarks LZUPD not only reduces the cost of stabilization, but also the rest of the compilation time ((column 4-column 2) vs. (column 5-column 3)), which we believe to be caused by the latent benefits (in cache, garbage collection and so on) arising due
to significant reduction in memory usage (see columns 6 and 7).
(B) Memory consumption. Fig. 13 (columns 6 and 7) shows the maximum additional memory footprint (in MB), in terms of the maximum resident size, while running BarrElim. We have obtained these values by taking the difference of peak memory requirements during compilation with and without the optimization pass. The values shown are calculated with the help of /usr/bin/time GNU utility (version : 1.7).
In Fig. 15, we illustrate the percentage savings in the memory footprint by LZINV, EGUPD, and LZUPD modes, as compared to the EGINV mode. All these three modes of stabilization perform better (or are more or less comparable), in terms of memory requirements, than the EGINV mode. The geomean improvements in memory consumption are 4.73%, 12.49%, and 25.98%, for LZINV, EGUPD, and LZUPD modes, respectively, as compared to the EGINV mode. As discussed above, both lazy and update options minimize the number of times different transfer functions are applied during stabilization of the data-flow analyses ­ this claim is substantiated by the observation that for all the benchmarks, LZUPD requires the least amount of memory, with maximum percentage savings of 71.78% for quake, over the EGINV mode.
In EP, we see an interesting case where the peak memory usage of LZINV and EGUPD are slightly higher ( 2%) than that of EGINV. We attribute this observation to the fact that the absolute differences in the peak memory usage between these two modes and EGINV, are lesser than the standard deviation of the results obtained across 30 runs of BarrElim on EP.
Summary. Overall, we see that the proposed lazy modes of stabilization lead to significant memory savings compared to the naive EGINV scheme. This in turn can improve the memory traffic and overall gains in performance.
Preprint submitted to arXiv

1:30

Nougrahiya and Nandivada

Fig. 15. % savings in memory footprint (max resident set size) under various modes of self-stabilization w.r.t. the EGINV mode, while running the client analysis. Higher is better.
B INTER-THREAD IDFA AND THE CORRECTNESS OF HIDFA Data flow analyses of serial programs cannot be directly used in the context of OpenMP, as those analyses are oblivious to the flow of data among threads over inter-task edges (see Section 2). To address this issue, in Section B.1, we first extend the standard iterative data-flow analysis of serial programs to that for parallel programs (IDFA ). In Section B.2, we use the definition of IDFA to drive the correctness argument of HIDFA .
B.1 Design of IDFA : Generic inter-thread IDFA for Parallel Programs We now discuss an extension to the standard iterative data-flow analysis of serial programs, to make it suitable for parallel programs; we use OpenMP as the target parallel language. In OpenMP (and other such shared-memory parallelism models), there are two key portions of data environment for each thread ­ private and shared. Under the relaxed-consistency model of OpenMP, each thread is allowed to maintain its own temporary view of the shared memory, which can be made consistent with the global view of shared memory with the help of ompFlush directives. The communication between multiple threads resulting from these directives can be represented as inter-task edges, as shown in Figure 2. In order to extend serial IDFA to IDFA , we augment the serial super-graph (see Section 2) with such inter-task edges.
It is important to note that since the inter-task edges exist only to model communication via shared variables across threads, these edges cannot affect the flow-facts of the IDFAs that do not reason about the shared memory locations (for example, dominator analysis [Muchnick 1998]). Hence, in this section, we only consider flow-facts (viewed here as a flow map), whose domain is the set of abstract memory locations. For the rest of the IDFAs, IDFA matches the standard IDFA.
We distinguish the private and shared portions of data-environment, such that the domain of each flow map consists of two mutually exclusive and exhaustive subsets. Consequently the standard flow maps can be seen as, IN() := INpriv ()  INshared (), and OUT() := OUTpriv ()  OUTshared (), where the subscript to a map indicates the type (memory locations) of its domain. Now, we discuss two extensions to serial IDFA to obtain IDFA .
Ext-1 In order to model the effects of an inter-task edge (1, 2) between two ompFlush directives 1 and 2, for each shared variable  that can be communicated over the edge (1, 2), we add the following data-flow equation to the serial IDFA; this equation uses the meet operator ( ) of the underlying serial IDFA.
(INshared ( 2)) ( ) := (INshared ( 2)) ( ) (OUTshared ( 1)) ( )
Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:31

Theorem B.1. A sound IDFA for serial programs, extended with Ext-1 remains sound in the context of OpenMP.

Proof. (Sketch) OpenMP API specification [Dagum and Menon 1998, Section 1.4.4] specifies the sequence of events that must be followed for communication to happen between any two threads. Extension Ext-1 conservatively models all such possible communications that may happen over

any of the inter-task edges. Thus, since the underlying serial IDFA is sound, the extension remains

sound, as no valid flow of shared data may happen at runtime except over the edges which we

model.



Improved generic inter-thread IDFA. Even though the above proposed extension Ext-1 leads to sound analysis result, it can be both overly conservative and expensive (in terms of analysis
time). This is because it introduces additional data-flow equations for each inter-task edge in a
parallel region, but not all of those inter-task edges are required to maintain the data-flow semantics
(see below). In this section, we utilize phase information (see Section 2) to identify the required
inter-task edges and help generate data-flow equations only for those inter-task edges. Consider a sequence of runtime phases  , +1, . . .   , where  > .Consider a pair of flushes cfi
and cfj during execution that are executed in phases  and   , respectively. It may be noted that the (implicit) flushes in the barriers at the boundary of  and +1 are sufficient to reason about any perceived communication between cfi and cfj.
We utilize this observation to reduce the number of inter-task edges corresponding to which the data-flow equation from Ext-1 needs to be modelled, with the help of the following additional extension Ext-2 :

(1) We avoid introducing inter-task edges between flushes that do not share any common phase. (2) As per OpenMP semantics, at the end of each barrier, all temporary views of different threads are made consistent with each other and with the shared memory. Consider a maximal set  of barriers that synchronize with each other at the end of a phase ; this set is termed as the synchronization set of . For each barrier   , to model the consistency of temporary views at the end of phase , we add the following transfer function for the shared component of the flow map :

OUTshared () :=   INshared ( )
This equation would ensure that the  of all the barriers that may share a synchronization set would be the same. The transfer function for the private components of the flow map is kept as an identity function.
Complexity and Precision. For any given IDFA, the worst-case complexity of its corresponding IDFA (upon adding the above proposed two extensions) remains unchanged. This is because the effect of the two extensions is limited to adding extra edges in the super-graph. The precision of IDFA depends on that of the underlying phase analysis, as imprecision in phase-analysis may result in modeling extra inter-task edges than needed.

B.2 Correctness Proof of HIDFA
We now present the correctness argument for HIDFA .
Theorem B.2. HIDFA is as sound and precise as a complete rerun of the underlying IDFA (IDFA ).
Proof. (Sketch.) Notation: At any instant during the execution of IDFA , we use IN () (or OUT ()) to denote the IN (or OUT) flow map of a node  at that point. Similarly, we use IN () (or OUT ()) to denote the IN (or OUT) maps of HIDFA .
Say a program  has its set of IN and OUT maps computed for every node therein, using a fixed point analysis (for example, IDFA ). Consider a sequence of transformations  over , after which

Preprint submitted to arXiv

1:32

Nougrahiya and Nandivada

the proposed incremental update algorithm HIDFA is invoked for the analysis. This invocation
of HIDFA would be deemed unsound if upon its termination, there exists at least one program node, say , for which the flow map IN () (or OUT ()) does not match the flow map IN () (or OUT ()) corresponding to the complete rerun of IDFA on the modified program.
Note that the transfer function (F) of each program node (Line 26) is the same for IDFA and HIDFA . Furthermore, whenever HIDFA updates IN (), it also recalculates OUT () (Lines 25­26). Therefore, , IN () = IN ()  , OUT () = OUT (). Thus, it is sufficient to prove that for each program node , IN () = IN (). We prove this by induction on the topological-sort order

of the SCC graph of the modified program.

Without loss of generality, we assume that the root of the SCC graph is a special node that
contains a single Entry program node. Induction Basis: In the first element in the sorted order of the SCC-graph, after the sequence  of
transformations, the IN (Entry) = IN (Entry), as neither HIDFA , nor IDFA changes the IN map. Induction Hypothesis: Assume that  > 1, such that   ,    ( ), IN () = IN (),
where  ( ) denotes the set of nodes in  SCC, in the topological-sort order of the SCC. Inductive step: To prove that    ( + 1), IN () = IN (). To aid this process, we define
the state of IDFA (or HIDFA ) at any point of execution as the contents of IN (and consequently OUT) maps with respect to the nodes in the current SCC (at index  + 1). Note that IDFA uses a
fixed-point computation, which is independent of the order in which the nodes are processed.

Our proof has two parts: (A) We first show that with respect to the nodes in the current SCC,

the state of HIDFA at the end of the first-pass (Lines 5-15), matches the state of IDFA at a valid execution point of IDFA . Say, the state of HIDFA at Line 17 is given by  (17). We will show

that in the current SCC, there exists a valid sequence of nodes, after processing which, IDFA can reach the same state  (17). (B) Consequently, with respect to the nodes of the current SCC, we

show that the state of HIDFA after the second-pass matches the state of IDFA after completing the processing of the current SCC.

(A) We look at a possible execution of IDFA by considering two steps. Step 1: Say 1 is the set of processed by HIDFA as part of the first-pass, in the sequence given

by 1. Clearly 1 is also a valid sequence of nodes that can be processed by IDFA . Further, since

HIDFA

ignores

the

remaining

unprocessed

nodes

of

the

SCC

(say

 )
1

to

reach

at

the

fixed-point,

if IDFA is made to process the nodes in the order given by 1, then with respect to the nodes in

1, the state of IDFA will match that of HIDFA . This is because, in the first-pass, HIDFA ignores

the

predecessors

from

 ,
1

which

is

equivalent

to

how

IDFA

treats

the

unprocessed

nodes

(in

 ).
1

Step 2: Now say IDFA is made to run to a fixed-point for the nodes in the current SCC, with a

restriction

that

it

will

process

a

node



from

its

worklist

only

if





 .
1

Say

the

state

of

IDFA

at

this point is given by  (). Then we want to show that with respect to each node in the current

SCC,  (17) =  ().

Clearly for the nodes in 1 the above property continues to hold, as in Step 2 none of these nodes

are processed. We now focus on the rest of the nodes.

Since the IN maps for the program , were computed using a fixed-point computation, the IN

maps cannot be changed unless the OUT map of any of their predecessor changes. Further, each

node    , predecessors of  can be divided into three categories: (i) nodes from the previous

1

SCC,

(ii)

nodes

from

1

,

and

(iii)

nodes

from

 .
1

Note

that

for

the

predecessors

from

category

(i)

and (ii) the OUT maps would not have changed compared to  (otherwise,  would have been in 1,

not

in

 ).
1

Thus

running

the

fixed-point

analysis

of

IDFA

using

the

above

specified

restriction

(of

processing only the nodes of  ) leads us to a state, where for the nodes in  , the IN and OUT maps

1

1

match

that

of

.

Since

in

the

context

of

HIDFA ,

the

values

of

IN

maps

for

the

nodes

in


1

are

not

Preprint submitted to arXiv

Homeostasis : Design and Implementation of a Self-Stabilizing Compiler

1:33

Fig. 16. Block diagram of the selected client optimization pass for removal of barriers for OpenMP programs.

modified in the first-pass (and hence match that of ), for those nodes,  (17) =  ().

Thus,

there

exists

a

valid

sequence

of

nodes

taken

from

the

set

1




1

(that

is,

all

the

nodes

in

the current SCC), after processing which, IDFA can reach the same state  (17).

(B)

While

processing

the

nodes

in


1

till

fixed-point

in

the

above

Step

2,

IDFA

adds

to

its

worklist those nodes in 1 whose predecessors have been changed; say this subset is given by  .

After completing Step 2, to complete the execution of IDFA for the current SCC, it can continue

processing the nodes in the current SCC, by starting with  as the nodes in the worklist and using

a code that is exactly same as the fixed-point loop in the second-pass (in Fig. 8).

In

case

of

HIDFA ,

for

each

node

in

 ,
1

if

it

is

a

predecessor

to

a

node





1,

then



gets

added

to underApproximated (Line 12). Among these nodes in underApproximated, processing nodes

in the set (underApproximated -  ) in the second-pass makes no difference to their IN maps, as

otherwise those nodes would have been added to  by IDFA . Or in other words, effectively the

work-lists processed in the second-pass as part of HIDFA is exactly same as  Consequently since

HIDFA and IDFA effectively work on the same work-list, using the same piece of code, they are

guaranteed to result in the same state, for all the nodes in the current SCC.



C OPTIMIZATION PASS : BARRIER REMOVER FOR OPENMP PROGRAMS BarrElim is an optimization that extends prior works [Gupta and Schonberg 1996; Tseng 1995] on parallel region expansion and barrier removal with function inlining to realize an efficient barrier removal algorithm. BarrElim performs the following steps, as shown in the block diagram in Fig. 16 : (1) Remove redundant barriers (within a parallel region), whose removal do not violate any data dependence between the statements across them. The remaining two steps help improve the opportunities for barrier removal within each function. (2) Expand and merge the parallel regions, while possibly expanding their scope to the call-sites of their enclosing functions, wherever possible. This helps in bringing more barriers (including the implicit ones) within the resulting parallel region, thereby creating new opportunities for barrier removal. (3) Inline those monomorphic calls whose target function is (i) not recursive, and (ii) contains at least one barrier. These three steps are repeated till fixed-point (no change). An important point to note about these three steps is that all of them involve many interleaved phases of inspection and transformation, which in turn lead to a number of interleaved accesses (reads and writes) to various program-abstractions, such as phase information, points-to information, super-graph (involving CFGs, call-graphs, and inter-task edges), AST, and so on ­ this interaction is depicted in Fig.11 using dotted edges.

Preprint submitted to arXiv

