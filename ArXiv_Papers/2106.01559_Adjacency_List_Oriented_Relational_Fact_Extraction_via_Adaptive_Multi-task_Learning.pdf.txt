Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning
Fubang Zhao1, Zhuoren Jiang2, Yangyang Kang1, Changlong Sun1, Xiaozhong Liu3 1Alibaba Group, Hangzhou, China
2School of Public Affairs, Zhejiang University, Hangzhou, China 3School of Informatics, Computing and Engineering, IUB, Bloomington, USA fubang.zfb@alibaba-inc.com, jiangzhuoren@zju.edu.cn
yangyang.kangyy@alibaba-inc.com, changlong.scl@taobao.com
liu237@indiana.edu

arXiv:2106.01559v1 [cs.CL] 3 Jun 2021

Abstract

Relational fact extraction aims to extract semantic triplets from unstructured text. In this work, we show that all of the relational fact extraction models can be organized according to a graph-oriented analytical perspective. An efficient model, aDjacency lIst oRiented rElational faCT (DIRECT), is proposed based on this analytical framework. To alleviate challenges of error propagation and sub-task loss equilibrium, DIRECT employs a novel adaptive multi-task learning strategy with dynamic sub-task loss balancing. Extensive experiments are conducted on two benchmark datasets, and results prove that the proposed model outperforms a series of state-of-the-art (SoTA) models for relational triplet extraction.

Figure 1: Example of exploring the relational fact extraction task from the perspective of directed graph representation method as output data structure.

1 Introduction
Relational fact extraction, as an essential NLP task, is playing an increasingly important role in knowledge graph construction (Han et al., 2019; Distiawan et al., 2019). It aims to extract relational triplet from the text. A relational triplet is in the form of (subject, relation, object) or (s, r, o) (Zeng et al., 2019). While various prior models proposed for relational fact extraction, few of them analyze this task from the perspective of output data structure.
As shown in Figure 1, the relational fact extraction can be characterized as a directed graph construction task, where graph representation flexibility and heterogeneity accompany additional benefaction. In practice, there are three common ways to represent graphs (Gross and Yellen, 2005):
Edge List is utilized to predict a sequence of triplets (edges). The recent sequence-to-sequence based models, such as NovelTagging (Zheng et al., 2017), CopyRE (Zeng et al., 2018), CopyRL (Zeng
These two authors contributed equally to this research.  Zhuoren Jiang is the corresponding author

et al., 2019), and PNDec (Nayak and Ng, 2020), fall into this category.
Edge list is a simple and space-efficient way to represent a graph (Arifuzzaman and Khan, 2015). However, there are three problems. First, the triplet overlapping problem (Zeng et al., 2018). For instance, as shown in Figure 1, for triplets (Obama, nationality, USA) and (Obama, president of, USA), there are two types of relations between the "Obama" and "USA". If the model only generates one sequence from the text (Zheng et al., 2017), it may fail to identify the multi-relation between entities. Second, to overcome the triplet overlapping problem, the model may have to extract the triplet element repeatedly (Zeng et al., 2018), which will increase the extraction cost. Third, there could be an ordering problem (Zeng et al., 2019): for multiple triplets, the extraction order could influence the model performance.
Adjacency Matrices are used to predict matrices that represent exactly which entities (vertices) have semantic relations (edges) between them. Most early works, which take a pipeline ap-

proach (Zelenko et al., 2003; Zhou et al., 2005), belong to this category. These models first recognize all entities in text and then perform relation classification for each entity pair. The subsequent neural network-based models (Bekoulis et al., 2018; Dai et al., 2019), that attempt to extract entities and relations jointly, can also be classified into this category.
Compared to edge list, adjacency matrices have better relation (edge) searching efficiency (Arifuzzaman and Khan, 2015). Furthermore, adjacency matrices oriented models is able to cover different overlapping cases (Zeng et al., 2018) for relational fact extraction task. But the space cost of this approach can be expensive. For most cases, the output matrices are very sparse. For instance, for a sentence with n tokens, if there are m kinds of relations, the output space is n · n · m, which can be costly for graph representation efficiency. This phenomenon is also illustrated in Figure 1.
Adjacency List is designed to predict an array of linked lists that serves as a representation of a graph. As depicted in Figure 1, in the adjacency list, each vertex v (key) points to a list (value) containing all other vertices connected to v by several edges. Adjacency list is a hybrid graph representation between edge list and adjacency matrices (Gross and Yellen, 2005), which can balance space and searching efficiency1. Due to the structural characteristic of the adjacency list, this type of model usually adopts a cascade fashion to identify subject, object, and relation sequentially. For instance, the recent state-of-the-art model CasRel (Wei et al., 2020) can be considered as an exemplar. It utilizes a two-step framework to recognize the possible object(s) of a given subject under a specific relation. However, CasRel is not fully adjacency list oriented: in the first step, it use subject as the key; while in the second step, it predicts (relation, object) pairs using adjacency matrix representation.
Despite its considerable potential, the cascade fashion of adjacency list oriented model may cause problems of sub-task error propagation (Shen et al., 2019), i.e., errors from ancestor sub-tasks may accumulate to threaten downstream ones, and subtasks can hardly share supervision signals. Multitask learning (Caruana, 1997) can alleviate this problem, however, the sub-task loss balancing prob-
1More detailed complexity analyses of different graph representations are provided in Appendix section 6.3.

lem (Chen et al., 2018; Sener and Koltun, 2018) could compromise its performance.
Based on the analysis from the perspective of output data structure, we propose a novel solution, aDjacency lIst oRiented rElational faCT extraction model (DIRECT), with the following advantages:
· For efficiency, DIRECT is a fully adjacency list oriented model, which consists of a shared BERT encoder, the Pointer-Network based subject and object extractors, and a relation classification module. In Section 3.4, we provide a detailed comparative analysis2 to demonstrate the efficiency of the proposed method.
· From the performance viewpoint, to address sub-task error propagation and sub-task loss balancing problems, DIRECT employs a novel adaptive multi-task learning strategy with the dynamic subtask loss balancing approach. In Section 3.2 and 3.3, the empirical experimental results demonstrate DIRECT can achieve the state-of-the-art performance of relational fact extraction task, and the adaptive multi-task learning strategy did play a positive role in improving the task performance.
The major contributions of this paper can be summarized as follows:
1. We refurbish the relational fact extraction problem by leveraging an analytical framework of graph-oriented output structure. To the best of our knowledge, this is a pioneer investigation to explore the output data structure of relational fact extractions.
2. We propose a novel solution, DIRECT3, which is a fully adjacency list oriented model with a novel adaptive multi-task learning strategy.
3. Through extensive experiments on two benchmark datasets3, we demonstrate the efficiency and efficacy of DIRECT. The proposed DIRECT outperforms the state-of-the-art baseline models.
2 The DIRECT Framework
In this section, we will introduce the framework of the proposed DIRECT model, which includes a shared BERT encoder and three output layers: subject extraction, object extraction, and relation classification. As shown in Figure 2, DIRECT is fully adjacency list oriented. The input sentence is firstly fed into the subject extraction module to
2Theoretical representation efficiency analysis of graph representative models are described in Appendix section 6.4.
3To help other scholars reproduce the experiment outcome, we will release the code and datasets via GitHub: https://github.com/fyubang/direct-ie.

Figure 2: An overview of the proposed DIRECT framework

extract all subjects. Then each extracted subject is concatenated with the sentence, and fed into the object extraction module to extract all objects, which can form a set of subject-object pairs. Finally, the subject-object pair is concatenated with sentence, and fed into the relation classification module to get the relations between them. For balancing the weights of sub-task losses and to improve the global task performance, three modules share the BERT encoder layer and are trained with an adaptive multi-task learning strategy.

2.1 Shared BERT Encoder
In the DIRECT framework, the encoder is used to extract the semantic features from the inputs for three modules. As aforementioned, we employ the BERT (Devlin et al., 2019) as the shared encoder to make use of its pre-trained knowledge and attention mechanism.
The architecture of the shared method is shown in Figure 2. The lower embedding layer and transformers (Vaswani et al., 2017) are shared across all the three modules, while the top layers represent the task-specific outputs.
The encoding process is as follows:

ht = BERT(xt)

(1)

where xt = [w1, ..., wn] is the input text of task t and ht is the hidden vector sequence of the input. Due to the limited space, the detailed architecture of BERT please refer to the original paper (Devlin et al., 2019).

2.2 Subject and Object Extraction
The subject and object extraction modules are motivated by the Pointer-Network (Vinyals et al., 2015) architecture, which are widely used in Machine Reading Comprehension (MRC) (Rajpurkar et al., 2016) task. Different from MRC task that only needs to extract a single span, the subject and object extractions need to extract multiple spans. Therefore, in the training phase, we replace sof tmax function with sigmoid function for the activation function of the output layer, and replace cross entropy (CE) (Goodfellow et al., 2016) with binary cross entropy (BCE) (Luc et al., 2016) for the loss function. Specifically, we will perform independent binary classifications for each token twice to indicate whether the current token is the start or the end of a span. The probability of a token to be start or end is as follows:

pti,start = (Wsttart · hi + btstart)

(2)

pti,end = (Wetnd · hi + btend)

(3)

where hi represents the hidden vector of the ith token, t  [s, o] represents subject and object extraction respectively, Wt  Rh×1 represents the trainable weight, bt  R1 is the bias and  is
sigmoid function.
During inference, we first recognize all the start positions by checking if the probability pti,start > , where  is the threshold of extraction. Then, we
identify the corresponding end position with the largest probability pti,end between two neighboring start positions. Concretely, assuming posj,start is the start position of the jth span, the corresponding

end position is:

posj,end =

argmax

pti,end (4)

posj,start <=i<posj +1,start

Though the overall structure is similar, the inputs for subject and object extraction are different. When extracting the subject, only the original sentence needs to be input:

x = [w1, ..., wn]

(5)

inputs = [[cls], x, [sep]]

(6)

where wi represents the ith token of the original sentence.
Meanwhile, the object extraction is based on the corresponding subject. To form the input, the subject s and the original sentence x are concatenated with [sep] as follows:
inputo = [[cls], s, [sep], x, [sep]] (7)

2.3 Relation classification
The output layer of relation classification is relatively simple, which is a normal multi-label classification model. The [cls] vector obtained by BERT encoder is used as the sentence embedding. A fully connected layer is used for the nonlinear transformation, and perform multi-label classification to predict relations of the input subject-object pair. The detailed operations of relation classification are as follows:

Pr = (Wr · h[cls] + br)

(8)

where Pr  Rc is the predicted probability vector of relations,  is sigmoid function, Wr  Rh×c and br  Rc are the trainable weights and bias, h is the hidden size of encoder, c is the num-
ber of relations, and h[cls] denotes the hidden vector of the first token [cls]. The input for relation classi-
fication task is as follows:

inputr = [[cls], s, [sep], o, [sep], x, [sep]] (9)

2.4 Adaptive Multi-task Learning
In DIRECT, subject extraction module, object extraction module, and relation classification module can be considered as three sub-tasks. As aforementioned, if we train each module directly and separately, the error propagation problem would

Algorithm 1: Adaptive Multi-task Learning with Dynamic Loss Balancing
Initialize model parameters  randomly; Load pre-trained BERT parameters for shared encoder; Prepare the data for each task t and pack them into mini-batch: Dt, t  [s, o, r] ; Get the number of batch for each task: nt; Set the number of epoch for training: epochmax; for epoch in 1, 2, ..., epochmax do
1. Merge all the datasets: D = Ds  Do  Dr;
2. Shuffle D; 3. Initialize EMA for each task vt = 1
and its decay = 0.99 ; for bt in D do
// bt is a mini-batch of Dt ; 4. Compute loss: lt() ; 5. Update EMA:
vt = (1 - ) · (lt) + · vt ; 6. Calculate and normalize the weights: wt = (vt/nt)/(vr/nr) ; 7. Update model  with gradient:
(wt · l¯t) ; end end
reduce the task performance. Meanwhile, three independent encoders would consume more memory. Therefore, we use multi-task learning to alleviate this problem, and the encoder layer is shared across three modules.
However, applying multi-task learning could be challenging in DIRECT, due to the following problems:
· The input and output of the three modules are different, which means we cannot simply sum up the loss of each task.
· How should we balance the weights of losses for three sub-task modules?
These issues can affect the final results of multitask training (Shen et al., 2019; Sener and Koltun, 2018).
In this work, based on the architecture of MTDNN (Liu et al., 2019b), we propose a novel adaptive multi-task learning strategy to address the above problems. The algorithm is shown as Algorithm 1. Basically, the datasets are firstly split into mini-batches. A batch is then randomly sampled

to calculate the loss. The parameters of the shared encoder and its task-specific layer are updated accordingly. Especially, the learning effect of each task t is different and dynamically changing during training. Therefore, an approach of adaptively adjusting the weights of task losses is applied. The sum of sub-task's loss lt is utilized to approximate its optimization effect. The adaptive weight adjusting strategy ensures that the more room a sub-task has to be optimized, the more weight its loss will receive. Furthermore, an exponential moving average (EMA) (Lawrance and Lewis, 1977) is maintained to avoid the drastic fluctuations of loss weights. Last but not least, to make sure that each task has enough influence on the shared encoder, the weight of the sub-task will be penalized according to the training data amount of each sub-task.
3 Experiments
3.1 Dataset and Experiment Setting
Datasets. Two public datasets are used for evaluation: NYT (Riedel et al., 2010) is originally produced by the distant supervision approach. There are 1.18M sentences with 24 predefined relation types in NYT. WebNLG (Gardent et al., 2017) is originally created for Natural Language Generation (NLG) tasks. (Zeng et al., 2018) adopts this dataset for relational triplet extraction task. It contains 246 predefined relation types. There are different versions of these two datasets. To facilitate comparison evaluation, we use the datasets released by (Zeng et al., 2018) and follow their data split rules.
Besides the basic relational triplet extraction, recent studies are focusing on the relational triplet overlapping problem (Zeng et al., 2018; Wei et al., 2020). Follow the overlapping pattern definition of relational triplets (Zeng et al., 2018), the sentences in both datasets are divided into three categories, namely, Normal, EntityPairOverlap (EPO), and SingleEntityOverlap (SEO). The statistics of the two datasets are described in Table 1.
Baselines: the following strong state-of-the-art (SoTA) models have been compared in the experiments.
· NovelTagging (Zheng et al., 2017) introduces a tagging scheme that transforms the joint entity and relation extraction task into a sequence labeling problem. It can be considered as edge list oriented.
· CopyRE (Zeng et al., 2018) is a seq2seq based model with the copy mechanism, which

Category
Normal EPO SEO ALL

NYT Train Test 37013 3266 9782 978 14735 1297 56195 5000

WebNLG Train Test 1596 246 227 26 3406 457 5019 703

Table 1: Statistics of Dataset NYT and WebNLG. Note that a sentence can belong to both EPO class and SEO class.

can effectively extract overlapping triplets. It has

two variants: CopyREone employs one decoder; CopyREmul employs multiple decoders. CopyRE is also edge list oriented.

· GraphRel (Fu et al., 2019) is a GCN (graph

convolutional networks) (Kipf and Welling, 2017)

based model, where a relation-weighted GCN is uti-

lized to learn the interaction between entities and

relations. It is a two phases model: GraphRel1p denotes 1st-phase extraction model; GraphRel2p denotes full extraction model. GraphRel is adja-

cency matrices oriented.

· CopyRL (Zeng et al., 2019) combines the re-

inforcement learning with a seq2seq model to au-

tomatically learn the extraction order of triplets.

CopyRL is edge list oriented.

· CasRel (Wei et al., 2020) is a cascade binary

tagging framework, where all possible subjects are

identified in the first stage, and then for each iden-

tified subject, all possible relations and the cor-

responding objects are simultaneously identified

by a relation specific tagger. This work recently

achieves the SoTA results. As aforementioned, Cas-

Rel is partially adjacency list oriented.

Evaluation Metrics: following the previous

work (Zeng et al., 2018; Wei et al., 2020), differ-

ent models are compared by using standard micro Precision (Prec.), Recall (Rec.), and F1-score4. An

extracted relational triplet (subject, relation, object)

is regarded as correct only if the relation and the

heads of both subject and object are all correct.

Implementation Details.

The hyper-

parameters are determined on the validation

set. To avoid the evaluation bias, all reported

results from our method are averaged results for 5

runs. More implementation details are described in

4In this study, the results of baseline models are all selfreported results from their original papers. Meanwhile, the experimental results of our proposed model are the average of five runs.

Appendix section 6.1.
3.2 Results and Analysis Relational Triplet Extraction Performance. The task performances on two datasets are summarized in Table 2. Based on the experiment results, we have the following observations and discussions:
· The proposed DIRECT model outperformed all baseline models in terms of all evaluation metrics on both datasets, which proved DIRECT model can effectively address the relational triplet extraction task.
· The best-performed model (DIRECT) and runner-up model (CasRel) were both adjacency list oriented model. These two models overwhelmingly outperformed other models, which indicated the considerable potential of adjacency list (as the output data structure) for improving the task performance.
· To further compare the relation extraction ability of DIRECT and CasRel, we took a closer look at the extraction performance of relational triplet elements from these two models. As shown in Table 35, DIRECT outperformed CasRel in terms of all relational triplet elements on both datasets. These empirical results suggested that, for relational triplet extraction, a fully adjacency list oriented model (DIRECT) may have advantages over a partially oriented one (CasRel).
Figure 3: F1 score of extracting relational triples from sentences with different overlapping patterns on NYT dataset.
Ability in Handling The Overlapping Problem. The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. To verify the ability of our models in handling the overlapping problem,
5More detailed results with Precision and Recall are provided in Appendix section 6.2.

we conducted further experiments on NYT dataset. Figure 3 illustrated of F1 scores of extracting relational triplets from sentences with different overlapping patterns. DIRECT outperformed all baseline models in terms of all overlapping patterns. These results demonstrated the effectiveness of the proposed model in solving the overlapping problem.
Ability in Handling Multiple Relation Extraction. We further compared the model's ability of extracting relations from sentences that contain multiple triplets. The sentences in NYT and WebNLG were divided into 5 categories. Each category contained sentences that had 1,2,3,4 or  5 triplets. The triplet number was denoted as N . As shown in Table 4:
· DIRECT achieved the best performance for all triplet categories on both datasets. These experimental results demonstrated our model had an excellent ability in handling multiple relation extraction.
· In both NYT and WebNLG datasets, when the sentences contained more triplets, the leading advantage of DIRECT became greater. This observation indicated that DIRECT was good at solving complex relational fact extraction.
3.3 Ablation Study
To validate the effectiveness of components in DIRECT, We implemented several model variants for ablation tests6. The results of the comparison on NYT dataset are shown in Table 5. In particular, we aim to address the following two research questions:
RQ1: Is it possible to improve the model performance by sharing the parameters of extraction layers?
RQ2: Did the proposed adaptive multi-task learning strategy improve the task performance?
Effects of Sharing Extraction Layer Parameters (RQ1). As described in Section 2, the structures of subject extraction and object extraction output layers are exactly the same. To answer RQ1, we merged the subject extraction and object extraction layers into one entity extraction layer by sharing the parameters of output layers of these two modules, denoted as DIRECTshared. From the results of Table 5, we can observe that, sharing the parameters of output layers of two extraction modules would reduce the performance of the model.
6Due to the length limitation, we list two main ablation experiments, the rest will be provided in the Appendix section 6.2.

Method
NovelTagging(Zheng et al., 2017) CopyREOne(Zeng et al., 2018) CopyREMul(Zeng et al., 2018) GraphRel1p(Fu et al., 2019) GraphRel2p(Fu et al., 2019) CopyRL(Zeng et al., 2019) CasRel(Wei et al., 2020) DIRECT(Ours)

Category
EL EL EL AM AM EL ALP ALF

Prec. 62.4 59.4 61.0 62.9 63.9 77.9 89.7 92.3±0.32

NYT Rec. 31.7 53.1 56.6 57.3 60.0 67.2 89.5 92.8±0.26

F1 42.0 56.0 58.7 60.0 61.9 72.1 89.6 92.5±0.09

Prec. 52.5 32.2 37.7 42.3 44.7 63.3 93.4 93.6±0.1

WebNLG Rec. 193. 28.9 36.4 39.2 41.1 59.9 90.1
92.7±0.24

F1 28.3 30.5 37.1 40.7 42.9 61.6 91.8 93.2±0.07

Table 2: Results of different methods on NYT and WebNLG datasets. EL: Edge List; AM: Adjacency Matrices; ALP: Adjacency List (Partially); ALF: Adjacency List (Fully).

Method CasRel
DIRECT(Ours)

Element s o r s o r

NYT 93.5 93.5 94.9 95.4 96.4 97.8

WebNLG 95.7 95.3 94.0 97.3 96.4 97.4

Table 3: F1-score for extracting elements of relational triplets on NYT and WebNLG datasets.

A possible explanation is that, although the output of these two modules is similar, the semantics of subject and object are different. Hence, directly sharing the output parameters of two modules could lead to an unsatisfactory performance.
Effects of Adaptive Multi-task Learning (RQ2). As described in Section 2, the adaptive multi-task learning strategy with the dynamic subtask loss balancing approach is proposed for improving the task performance. To answer RQ2, we replaced the adaptive multi-task learning strategy with an ordinary learning strategy. In this strategy, the losses of three sub-tasks were computed with equal weights, denoted as DIRECTequal. From the results of Table 5, we can observe that, by using adaptive multi-task learning, DIRECT was able to get a 1.5 percentage improvement on the F1-score. This significant improvement indicated that adaptive multi-task learning played a positive role in the balance of sub-task learning and can improve the global task performance.
3.4 Graph Representation Efficiency Analysis
Based on the amount estimation of predicted logits7, we conduct a graph representation efficiency
7Numeric output (0/1) of the last layer

analysis to demonstrate the efficiency of the proposed method8.
For each graph representation category, we choose one representative algorithms. Edge List: CopyRE (Zeng et al., 2018); Adjacency Matrices: MHS (Bekoulis et al., 2018); Adjacency List: CasRel (partially) (Wei et al., 2020) and the proposed DIRECT (fully).
The averaged predicted logits estimation for one sample9 of different models on two datasets are shown in Table 6. MHS is adjacency matrices oriented, it has the most logits that need to be predicted. Since CasRel is partially adjacency list oriented, it needs to predict more logits than DIRECT. Theoretically, as an edge list oriented, the predicted logits of CopyRE should be the least. But, as described in Section 1, it needs to extract the entities repeatedly to handle the overlapping problem. Hence, its graph representation efficiency could be worse than our model. The structure of our model is simple and fully adjacency list oriented. Therefore, from the viewpoint of predicted logits estimation, DIRECT is the most representative-efficient model.
4 Related Work
Relation Fact Extraction. In this work, we show that all of the relational fact extraction models can be unified into a graph-oriented output structure analytical framework. From the perspective of graph representation, the prior models can be divided into three categories. Edge List, this type of model usually employs sequence-to-sequence fashion, such as NovelTagging (Zheng et al., 2017), CopyRE (Zeng et al., 2018), CopyRL (Zeng et al.,
8From the graph representation perspective, when a method requires fewer logits to represent the graph (set of triples), it will reduce the model fitting difficulty.
9The theoretical analysis of predicted logits for different models are described in Appendix section 6.4.

Method
Count CopyREOne CopyREMul GraphRel1p GraphRel2p
CopyRL CasRel DIRECT(Ours)

N =1 3244 66.6 67.1 69.1 71.0 71.7 88.2 90.4

N =2 1045 52.6 58.6 59.5 61.5 72.6 90.3 93.1

NYT N =3
312 49.7 52.0 54.4 57.4 72.5 91.9 94.3

N =4 291 48.7 53.6 53.9 55.1 77.9 94.2 95.8

N 5 108 20.3 30.0 37.5 41.1 45.9 83.7 93.1

N =1 268 65.2 59.2 63.8 66.0 63.4 89.3 90.3

WebNLG

N =2 N =3 N =4

174

128

89

33.0 22.2 14.2

42.5 31.7 24.2

46.3 34.7 30.8

48.3 37.0 32.1

62.2 64.4 57.2

90.8 94.2 92.4

92.8 94.8 94.0

N 5 44 13.2 30.0 29.4 32.1 55.7 90.9 92.9

Table 4: F1-score of extracting relational triplets from sentences with different number (denoted as N) of triplets.

Method

NYT Prec. Rec. F1

DIRECTshared 92.1 91.6 91.9 DIRECTequal 90.6 91.3 91.0
DIRECT 92.3 92.8 92.5

Table 5: Results of model variants for ablation tests.

Method CopyRe
MHS CasRel DIRECT

Category EL AM ALP ALF

NYT 329 57369 3084 238

WebNLG 712 26518 15836 542

Table 6: Graph representation efficiency estimation based on the predicted logits amount. EL: Edge List; AM: Adjacency Matrices; ALP: Adjacency List (Partially); ALF: Adjacency List (Fully).

2019), and PNDec (Nayak and Ng, 2020). Some models of this category may suffer from the triplet overlapping problem and expensive extraction cost. Adjacency Matrices, many early pipeline approaches (Zelenko et al., 2003; Zhou et al., 2005; Mintz et al., 2009) and recent neural network-based models (Bekoulis et al., 2018; Dai et al., 2019; Fu et al., 2019), can be classified into this category. The main problem for this type of model is the graph representation efficiency. Adjacency List, the recent state-of-the-art model CasRel (Wei et al., 2020) is a partially adjacency list oriented model. In this work, we propose DIRECT that is a fully adjacency list oriented relational fact extraction model. To the best of our knowledge, few previous works analyze this task from the output data structure perspective. GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while we analyze it from the perspective of output structure. Our work is a pioneer investigation to analyze the output data

structure of relational fact extraction. Multi-task Learning. Multi-task Learning
(MTL) can improve the model performance. (Caruana, 1997) summarizes the goal succinctly: "it improves generalization by leveraging the domainspecific information contained in the training signals of related task." It has two benefits (Vandenhende et al.): (1) multiple tasks share a single model, which can save memory. (2) Associated tasks complement and constrain each other by sharing information, which can reduce overfitting and improve global performance. There are two main types of MTL: hard parameter sharing (Baxter, 1997) and soft parameter sharing (Duong et al., 2015). Most of the multi-task learning is done by summing the loses directly, this approach is not suitable for our case. When the input and output are different, it is impossible to get two losses in one forward propagation. MT-DNN (Liu et al., 2019b) is proposed for this problem. Furthermore, MTL is difficult for training, the magnitudes of different task-losses are different, and the direct summation of losses may lead to a bias for a particular task. There are already some studies proposed to address this problem (Chen et al., 2018; Guo et al., 2018; Liu et al., 2019a). They all try to dynamically adjust the weight of the loss according to the magnitude of the loss, the difficulty of the problem, the speed of learning, etc. In this study, we adopt MT-DNN's framework, and propose an adaptive multi-task learning strategy that can dynamically adjust the loss weight based on the averaged EMA (Lawrance and Lewis, 1977) of the training data amount, task difficulty, etc.
5 Conclusion
In this paper, we introduce a new analytical perspective to organize the relational fact extraction models and propose DIRECT model for this task. Unlike existing methods, DIRECT is fully adja-

cency list oriented, which employs a novel adaptive multi-task learning strategy with dynamic sub-task loss balancing. Extensive experiments on two public datasets, prove the efficiency and efficacy of the proposed methods.
Acknowledgments
We are thankful to the anonymous reviewers for their helpful comments. This work is supported by Alibaba Group through Alibaba Research Fellowship Program, the National Natural Science Foundation of China (61876003), the Key Research and Development Plan of Zhejiang Province (Grant No.2021C03140), the Fundamental Research Funds for the Central Universities, and Guangdong Basic and Applied Basic Research Foundation (2019A1515010837).
References
Shaikh Arifuzzaman and Maleq Khan. 2015. Fast parallel conversion of edge list to adjacency list for large-scale graphs. In Proceedings of the Symposium on High Performance Computing, pages 17­ 24.
Jonathan Baxter. 1997. A bayesian/information theoretic model of learning to learn via multiple task sampling. Machine learning, 28(1):7­39.
Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018. Joint entity recognition and relation extraction as a multi-head selection problem. Expert Systems with Applications, 114:34­ 45.
Rich Caruana. 1997. Multitask learning. Machine learning, 28(1):41­75.
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. 2018. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning, pages 794­803. PMLR.
Dai Dai, Xinyan Xiao, Yajuan Lyu, Shan Dou, Qiaoqiao She, and Haifeng Wang. 2019. Joint extraction of entities and overlapping relations using position-attentive sequence labeling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6300­6308.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171­4186.

Bayu Distiawan, Gerhard Weikum, Jianzhong Qi, and Rui Zhang. 2019. Neural relation extraction for knowledge base enrichment. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 229­240.
Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser. In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers), pages 845­ 850.
Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019. Graphrel: Modeling text as relational graphs for joint entity and relation extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1409­1418.
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 179­188.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep learning, volume 1. MIT press Cambridge.
Jonathan L Gross and Jay Yellen. 2005. Graph theory and its applications. CRC press.
Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. 2018. Dynamic task prioritization for multitask learning. In European Conference on Computer Vision, pages 282­299. Springer.
Xu Han, Tianyu Gao, Yuan Yao, Deming Ye, Zhiyuan Liu, and Maosong Sun. 2019. Opennre: An open and extensible toolkit for neural relation extraction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pages 169­174.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR).
Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR).
AJ Lawrance and PAW Lewis. 1977. An exponential moving-average sequence and point process (ema1). Journal of Applied Probability, pages 98­113.
Shikun Liu, Edward Johns, and Andrew J Davison. 2019a. End-to-end multi-task learning with attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1871­1880.

Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019b. Multi-task deep neural networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487­4496.
Pauline Luc, Camille Couprie, Soumith Chintala, and Jakob Verbeek. 2016. Semantic segmentation using adversarial networks. arXiv preprint arXiv:1611.08408.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003­1011.
Tapas Nayak and Hwee Tou Ng. 2020. Effective modeling of encoder-decoder architecture for joint entity and relation extraction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8528­8535.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383­2392.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 148­163. Springer.
Ozan Sener and Vladlen Koltun. 2018. Multi-task learning as multi-objective optimization. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 525­ 536.
Tao Shen, Xiubo Geng, QIN Tao, Daya Guo, Duyu Tang, Nan Duan, Guodong Long, and Daxin Jiang. 2019. Multi-task learning for conversational question answering over a large-scale knowledge base. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2442­ 2451.
Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000­6010.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2, pages 2692­ 2700.
Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2020. A novel cascade binary tagging framework for relational triple extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1476­ 1488.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Journal of machine learning research, 3(Feb):1083­1106.
Daojian Zeng, Haoran Zhang, and Qianying Liu. 2020. Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9507­9514.
Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu, Shengping Liu, and Jun Zhao. 2019. Learning the extraction order of multiple relational facts in a sentence with reinforcement learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 367­377.
Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu, and Jun Zhao. 2018. Extracting relational facts by an end-to-end neural model with copy mechanism. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 506­514.
Ranran Haoran Zhang, Qianying Liu, Aysa Xuemo Fan, Heng Ji, Daojian Zeng, Fei Cheng, Daisuke Kawahara, and Sadao Kurohashi. 2020. Minimize exposure bias of seq2seq models in joint entity and relation extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 236­246.
Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017. Joint extraction of entities and relations based on a novel tagging scheme. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1227­1236.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proceedings of the 43rd annual meeting of the association for computational linguistics (acl'05), pages 427­434.

6 Appendix
6.1 Implementation Details
We adopted the pre-trained BERT model [BERTBase-Cased]10 as our encoder, where the number of Transformer layers was 12 and the hidden size was 768. The token types of input were always set to 0.
We used Adam as our optimizer and applied a triangular learning rate schedule as suggested by original BERT paper. In addition, we adopted a lazy mechanism for optimization. Different from the momentum mechanism of ordinary Adam optimizer (Kingma and Ba, 2015) that updated the output layer parameters for all tasks, this lazy-Adam mechanism wouldn't update the parameters of noncurrent tasks.
The dacay rate of EMA was set to 0.99 as default. The max sequence length was 128.
The other hyper-parameters were determined on the validation set. Notably, considering our special decoding strategy, we raised the threshold of extraction to 0.9 to balance the precision and the recall. The threshold of relation classification was set to 0.5 as default. The hyper-parameter setting was listed in Table 7.
Our mthod were implemented by Pytorch11 and run on a server configured with a Tesla V100 GPU, 16 CPU, and 64G memory.

Hyper-parameter Learning Rate Epoch Num. Batch Size

NYT 8e-5 15 32

WebNLG 1e-4 60 16

Table 7: Hyper-parameter setting for NYT and WebNLG datasets.

6.2 Supplementary Experimental Results
Ablation Study. To validate the effectiveness of components in DIRECT, We implemented several model variants for ablation tests respectively. For experimental fairness, we kept the other components in the same settings when modifying one module.
· DIRECTshared, we merged the subject extraction and object extraction layers into one
10Available at: https://storage.googleapis.com/bert models/ 2018 10 18/cased L-12 H-768 A-12.zip
11https://pytorch.org/

entity extraction layer by sharing the parameters of output layers of these two modules.
· DIRECTequal, we replaced the adaptive multi-task learning strategy with an ordinary learning strategy. In this strategy, the losses of three sub-tasks were computed with equal weights, denoted as DIRECTequal.
· DIRECTthreshold, we simply recognized all the start and end positions of entities by checking if the probability pti,start/end > , where  was the threshold of extraction.
· DIRECTadam, we used ordinary Adam as optimizer.

Method

NYT Prec. Rec. F1

DIRECTshared 92.1 91.6 91.9

DIRECTequal 90.6 91.3 91.0

DIRECTthreshold 92.8 92.0 92.4

DIRECTadam 92.1 92.9 92.5

DIRECT

92.9 92.1 92.5

Table 8: Results of model variants for ablation tests.

From the results of Table 8, we can observe that:
1. Sharing the parameters of output layers of subject and object extraction modules would reduce the performance of the model.
2. Compared to ordinary multi-task learning strategy, by using adaptive multi-task learning, DIRECT was able to get a 1.5 percentage point improvement on F1-score.
3. There would be a slight drop in performance, if we just used a simple threshold policy to recognize the start and end positions of an entity.
4. Despite the difference in precision and recall, there was no significant difference between these two optimizers (ordinary-Adam & lazyAdam ) for the task.
Results on Extracting Elements of Relational Triplets. The complete extraction performance of relational triplet elements from DIRECT and CaslRel are listed in Table 9. DIRECT outperformed CasRel in terms of all relational triplet elements on both datasets. These empirical results

Method

Element

NYT Prec. Rec.

F1

WebNLG Prec. Rec. F1

s

94.6 92.4 93.5 98.7 92.8 95.7

CasRel

o

94.1 93.0 93.5 97.7 93.0 95.3

r

96.0 93.8 94.9 96.6 91.5 94.0

s

95.1 95.1 95.1 97.1 96.8 96.9

Ours

o

97.2 96.3 96.7 96.4 96.3 96.3

r

98.6 98.3 98.5 97.6 97.3 97.4

Table 9: Results on extracting elements of relational triplets

Method MHS (Bekoulis et al., 2018)

NYT Prec. Rec. F1 60.7 58.6 59.6

CopyMTLone(Zeng et al., 2020) 72.7 69.2 70.9 CopyMTLmul(Zeng et al., 2020) 75.7 68.7 72.0
WDec (Nayak and Ng, 2020) 88.1 76.1 81.7

PNDec (Nayak and Ng, 2020) 80.6 77.3 78.9

Seq2UMTree (Zhang et al., 2020) 79.1 75.1 77.1

DIRECT(ours)

90.2 90.2 90.2

Table 10: Results of different methods under Exact-Match Metrics. * marks results reproduced by official implementation.

suggest that, for relational triplet extraction, a fully adjacency list oriented model (DIRECT) may have advantages over a partially oriented one (CasRel).
Results of Different Methods under ExactMatch Metrics. In experiment section, we followed the match metric from (Zeng et al., 2018), which only required to match the first token of entity span. Many previous works adopted this match metric (Fu et al., 2019; Zeng et al., 2019; Wei et al., 2020).
In fact, our model is capable of extracting the complete entities. Therefore, we collected papers that reported the results of exact-match metrics (requiring to match the complete entity span). The following strong state-of-the-art (SoTA) models have been compared:
· CopyMTL (Zeng et al., 2020) is a multi-task learning framework, where conditional random field is used to identify entities, and a seq2seq model is adopted to extract relational triplets.
· WDec (Nayak and Ng, 2020) fuses a seq2seq model with a new representation scheme, which enables the decoder to generate one word at a and can handle full entity names of different length and overlapping entities.
· PNDec (Nayak and Ng, 2020) is a modification of seq2seq model. Pointer networks are used in the

decoding framework to identify the entities in the sentence using their start and end locations.
· Seq2UMTree (Zhang et al., 2020) is a modification of seq2seq model, which employs an unordered-multi-tree decoder to to minimize exposure bias.
The task performances on NYT dataset are summarized in Table 10. The proposed DIRECT model outperformed all baseline models in terms of all evaluation metrics. This experimental results further confirmed the efficacy of DIRECT for relational fact extraction task.
6.3 Complexity Analysis of Graph Representations
For a graph G = (V, E), |V | denotes the number of nodes/entities and |E| denotes the number of edges/relations. Suppose there are m kinds of relations, d(v) denotes the number of edges from node v.
· Edge List Complexity
- Space: O(|E|)
- Find all edges/relations from a node: O(|E|)
· Adjacency Matrices Complexity
- Space: O(|V | · |V | · m)

Category Edge List Adjacency Matrices Adjacency List (Partially) Adjacency List (Fully)

Method CopyRe
MHS CasRel DIRECT

Theoretical 4kl + kr
llr 2l + 2slr 2l + 2sl + or

NYT 329 57369 3084 238

WebNLG 712 26518 15836 542

Table 11: Graph representation efficiency based on the theoretical logits amount and the estimated logits amount on two benchmark datasets.

- Find all edges/relations from a node: O(|V | · m)
· Adjacency List Complexity
- Space: O(|V | + |E|)
- Find all edges/relations from a node: O(d(v))
6.4 Graph Representation Efficiency Analysis
Based on the amount estimation of predicted logits12 (0/1), we conduct a graph representation efficiency analysis to demonstrate the efficiency of proposed method13.
For each graph representation category, we choose one representative model algorithms. Edge List: CopyRE (Zeng et al., 2018); Adjacency Matrices: MHS (Bekoulis et al., 2018); Adjacency List: CasRel (partially) (Wei et al., 2020) and DIRECT (fully).
Formally, for a sentence whose length is l (l tokens), there are r types of relations, k denotes the number of triplets. Suppose there are s keys (subjects) and o values (corresponding amount of object-based lists) in adjacency list. The theoretical logits amount and the estimated logits amount on two benchmark datasets (NYT and WebNLG) are shown in Table 11. From the viewpoint of predicted logits estimation, DIRECT is the most representative-efficient model.

12Numeric output of the last layer 13As aforementioned, from the graph representation perspective, when a method requires fewer logits to represent the graph (set of triples), it will reduce the model fitting difficulty.

