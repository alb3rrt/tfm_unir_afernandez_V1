arXiv:2106.00885v2 [stat.ML] 3 Jun 2021

Robustifying Algorithms of Learning Latent Trees with Vector Variables
Fengzhuo Zhang Department of Electrical and Computer Engineering
National University of Singapore fzzhang@u.nus.edu
Vincent Y. F. Tan Department of Electrical and Computer Engineering
Department of Mathematics National University of Singapore
vtan@nus.edu.sg
Abstract
We consider learning the structures of Gaussian latent tree models with vector observations when a subset of them are arbitrarily corrupted. First, we present the sample complexities of Recursive Grouping (RG) and Chow-Liu Recursive Grouping (CLRG) without the assumption that the effective depth is bounded in the number of observed nodes, significantly generalizing the results in Choi et al. (2011). We show that Chow-Liu initialization in CLRG greatly reduces the sample complexity of RG from being exponential in the diameter of the tree to only logarithmic in the diameter for the hidden Markov model (HMM). Second, we robustify RG, CLRG, Neighbor Joining (NJ) and Spectral NJ (SNJ) by using the truncated inner product. These robustified algorithms can tolerate a number of corruptions up to the square root of the number of clean samples. Finally, we derive the first known instance-dependent impossibility result for structure learning of latent trees. The optimalities of the robust version of CLRG and NJ are verified by comparing their sample complexities and the impossibility result.
1 Introduction
Latent graphical models provide a succinct representation of the dependencies among observed and latent variables. Each node in the graphical model represents a random variable or a random vector, and the dependencies among these variables are captured by the edges among nodes. Graphical models are widely used in domains from biology [1], computer vision [2] and social networks [3].
This paper focuses on the structure learning of latent tree Gaussian graphical models (GGM) in which the node observations are random vectors and a subset of the observations can be arbitrarily corrupted. This classical problem, in which the variables are clean scalar random variables, has been studied extensively in the past decades. The first information distance-based method, NJ, was proposed in [1] to learn the structure of phylogenetic trees. This method makes use of additive information distances to deduce the existence of hidden nodes and introduce edges between hidden and observed nodes. RG, proposed in [4], generalizes the information distance-based methods to make it applicable for the latent graphical models with general structures. Different from these information distance-based methods, quartet-based methods [5] utilize the relative geometry of every four nodes to estimate the structure of the whole graph. Although experimental comparisons of these algorithms were conducted in some works [4, 6, 7], since there is no instance-dependent impossibility
Preprint. Under review.

result of the sample complexity of structure learning of latent tree graphical models, no thorough theoretical comparisons have been made, and the optimal dependencies on the diameter of graphs and the maximal distance between nodes max have not been found.

The success of the previously-mentioned algorithms relies on the assumption that the observations are i.i.d. samples from the generating distribution. The structure learning of latent graphical models in presence of (random or adversarial) noise remains a relatively unexplored problem. There are some works studying the problem of structure learning of graphical models with noisy samples, where all the nodes in the graphical models are observed and not hidden. Several assumptions on the additive noise are made in these works, which limit the use of these proposed algorithms. For example, the covariance matrix of the noise is specified in [8], the independence and/or distribution of the noise is assumed in [9­11, 7]. In contrast, we consider the structure learning of latent tree graphical models with arbitrary corruptions, where the boundness and independence of noise are not required [12]. Furthermore, the corruptions are allowed to be presented at any position in the data; they do not appear solely as outliers. In this work, we derive a bound on the maximum number of corruptions that can be tolerated and yet structure learning can succeed with high probability.

Firstly, we derive the sample complexities of RG and CLRG where each node represents a random vector; this differs from previous works where each node is scalar random variable (e.g., [4, 13]). We explore the dependence of the sample complexities on the parameters. The vanilla CLRG takes in

the mutual informations and builds a max-weight spanning tree to learn the tree. However, for latent

trees, we work with information distances and the mutual information is, in general, not monotone in

the distance (unless the variables are scalar). To implement CLRG on GGMs with vector variables, we consider a class of GGMs in which parents and children nodes are connected by linear Gaussian

channels. Sufficient conditions are derived to ensure that the mutual information is monotone in the

distance, ensuring that CLRG is correctly implemented. Our sample complexity analysis proves the

effectiveness of the Chow-Liu initialization in CLRG; this has been only verified experimentally [4].

In the HMM, we show that the Chow-Liu initialization reduces the sample complexity of RG which

is O

(

9 2

)Diam(T)

to O

log Diam(T) , where Diam(T) is the tree diameter.

Secondly, we robustify RG, CLRG, NJ and SNJ by using the truncated inner product [14] to es-

timate the information complexities and show

distances that they

ciannthtoeleprraetseennc1e=ofOarbloitgrnna2r2y

corruptions. corruptions,

We derive their sample where n2 is the number

of clean samples.

Finally, we derive the first known instance-dependent impossibility result for learning latent trees. The dependencies on the number of observed nodes and the maximum distance max are delineated. The comparison of the sample complexities of the structure learning algorithms and the impossibility
result demonstrates the optimality of Robust Chow-Liu Recursive Grouping (RCLRG) and Robust Neighbor Joining (RNJ) in Diam(T) for some archetypal latent tree structures.

Notation We use san-serif letters x, boldface letters x, and bold uppercase letters X to denote
variables, vectors and matrices, respectively. The notations [x]i, [X]ij , [X]:,j and diag(X) are respectively the ith entry of vector x, the (i, j)th of entry X, the jth column of X, and the diagonal
entries of matrix X. The notation x(k) represents the kth sample of x. For a tree T = (V, E), the
internal (non-leaf) nodes, the maximal degree and the diameter of T are denoted as Int(T), Deg(T)
and Diam(T), respectively. We denote the closed neighborhood and the degree of xi as nbd[xi; T] and deg(i), respectively. The length of the path connecting xi and xj is denoted as dT(xi, xj).

2 Preliminaries and problem statement

A GGM [15] is a multivariate Gaussian distribution that factorizes according to an undirected graph

G = (V, E). More precisely, a lsum-dimensional random vector x = [x1, . . . , xd]T, where xi  Rli

and lsum =

d i=1

li,

follows

a

Gaussian

distribution

N (0, ),

and

it

is

said

to

be

Markov

on

a

graph G = (V, E) with vertex set V = {x1, . . . , xd} and edge set E 

V 2

and (xi, xj)  E if

and only if the (i, j)th block ij of the precision  = -1 is not the zero matrix 0. We focus on

tree-structured graphical models, which factorize according to acyclic and connected (tree) graphs.

A special class of graphical models is the set of latent graphical models G = (V, E). The vertex set V is decomposed as V = Vhid  Vobs. We only have access to n i.i.d. samples drawn from

2

the observed set of nodes Vobs. The two goals of any structure learning algorithm are to learn the identities of the hidden nodes Vhid and how they are connected to the observed nodes.

2.1 System model for arbitrary corruptions

We consider tree-structured GGMs T = (V, E) with observed nodes Vobs = {x1, · · · , xo} and

hidden nodes Vhid = {xo+1, · · · , xo+h}, where V = Vhid  Vobs and E 

V 2

.

Each node xi

represents a random vector xi  Rli. The concatenation of these random vectors is a multivariate

Gaussian random vector with zero mean and covariance matrix  with size lsum × lsum.

We have n i.i.d. samples Xj = [x(1j)T, · · · , x(oj)T]T  Rlsum , j = 1, . . . , n drawn from the observed nodes Vobs = {x1, · · · , xo}. Furthermore, the data matrix Xn1 = [X1, · · · , Xn]T  Rn×lsum may contain some corrupted elements. We allow n1/2 samples of a variable to be arbitrarily corrupted, which means that there are at most n1/2 corrupted terms in each column of Xn1 , and the remaining n - n1/2 samples in this column are clean. In particular, the corrupted samples in different columns need not be in the same rows. If the corruptions in different columns lie in the same rows, as shown
in (the left of) Fig. 3, all the samples in the corresponding rows are corrupted; these are called
outliers. Obviously, outliers form a special case of our corruption model. Since each variable has at most n1/2 corrupted samples, the sample-wise inner product between two variables has at least n2 = n - n1 clean samples. There is no constraint on the statistical dependence or patterns of the corruptions. Unlike fixing the covariance matrix of the noise [8] or keeping the noise independent
[9], we allow arbitrary corruptions on the samples, which means that the noise can have unbounded
amplitude, can be dependent, and even can be generated from another graphical model (as we will
see in the experimental results in Section 3.6).

2.2 Structural and distributional assumptions

To construct the correct latent tree from samples of observed nodes, it is imperative to constrain
the class of latent trees to guarantee that the information from the distribution of observed nodes p(x1, . . . , xo) is sufficient to construct the tree. The distribution p(x1, . . . , xo+h) of the observed and hidden nodes is said to have a redundant hidden node xj if the distribution observed nodes p(x1, . . . , xo) remains the same after we marginalize over xj . To ensure that a latent tree can be constructed with no ambiguity, we need to guarantee that the true distribution does not have any
redundant hidden node(s), which is achieved by following two conditions [16]: (C1) Each hidden node has at least three neighbors; the set of such latent trees is denoted as T3; (C2) Any two variables connected by an edge are neither perfectly dependent nor independent.
Assumption 1. The dimensions of all the random vectors are all equal to lmax.

In fact, we only require the random vectors of the internal (non-leaf) nodes to have the same length. However, for ease of notation, we assume that the dimensions of all random vectors are lmax. Assumption 2. For every xi, xj  V, the covariance matrix ij = E xixTj has full rank, and the smallest singular value of ij is lower bounded by min, i.e.,

lmax (ij )  min for all xi, xj  V ,

(1)

where i() is the ith largest singular value of .

This assumption is a strengthening of Condition (C2) when each node represents a random vector.

Assumption 3. The determinant of the covariance matrix of any node ii = E xixTi is lower bounded by min, and the diagonal terms of the covariance matrix are upper bounded by m2 ax, i.e.,

min det(ii)  min
xiV

and

max diag ii
xiV

 m2 ax.

(2)

Assumption 3 is natural; otherwise, ii may be arbitrarily close to a singular matrix. Assumption 4. The degree of each node is upper bounded by dmax, i.e., Deg(T)  dmax.

2.3 Information distance
We define the information distance for Gaussian random vectors and prove that it is additive for trees.

3

Definition 1. The information distance between nodes xi and xj is

d(xi, xj) = - log

 lmax
k=1 k

ij

.

(3)

det ii det jj

Condition (C2) can be equivalently restated as constraints on the information distance.

Assumption 5. There exist two constants 0 < min  max <  such that.

min  d(xi, xj )  max for all xi, xj  V.

(4)

Assumptions 2 and 5 both describe the properties of the correlation between random vectors from different perspectives. In fact, we can relate the constraints in these two assumptions as follows:

 e min max/lmax  m1/ilnmax .

(5)

Proposition 1. If Assumptions 1 and 2 hold, d(·, ·) defined in Definition 1 is additive on the tree-
structured GGM T = (V, E). In other words, d(xi, xk) = d(xi, xj) + d(xj , xk) holds for any two nodes xi, xk  V and any node xj on the path connecting xi and xk in T.

This additivity property is used extensively in the following algorithms. It was first stated and proved in Huang et al. [17]. We provide an alternative proof in Appendix G.

3 Robustifying latent tree structure learning algorithms

3.1 Robust estimation of information distances

Before delving into the details of robustifying latent tree structure learning algorithms, we first
introduce the truncated inner product [14], which estimates the correlation against arbitrary corrup-
tion effectively and serves as a basis for the robust latent tree structure learning algorithms. Given a, b  Rn and an integer n1, we compute qi = aibi for i = 1, 2, . . . , n and sort {|qi|}. Let  be the index set of the n - n1 smallest |qi|'s. The truncated inner product is a, b n1 = i qi. Note that the implementation of the truncated inner product requires the knowledge of corruption level n1.

To estimate the information distance defined in Definition 1, we implement the truncated inner

product to estimate each term of ij , i.e., [^ ij]st =

1 n-n1

[Xn1 ]:,(i-1)lmax+s, [Xn1 ]:,(j-1)lmax+t

n1 .

Then the information distance is computed based on this estimate of ij as

lmax
d^(xi, xj) = - log k ^ ij

+

1 2

log

det

^ ii

+

1 2

log

det

^ jj

.

(6)

k=1

The truncated inner product guarantees that ^ ij converges in probability to ij , which further ensures the convergence of the singular values and the determinant of ij to their nominal values.
Proposition 2. If Assumptions 1 and 2 hold, the estimate of the information distance between xi and xj based on the truncated inner product d^(xi, xj) satisfies

P

d^(xi, xj) - d(xi, xj )

>

2lm2 ax min

(t1

+ t2)



2l e 2
max

-

3n2 16n1

t1

+

lm2 ax

e-c

n2 2

t22

,

(7)

where t2 <  = max{m2 ax, min}, and c is an absolute constant.

The first and second parts of (7) originate from the corrupted and clean samples respectively.

3.2 Robust Recursive Grouping algorithm
The RG algorithm was proposed by [4] to learn latent tree models with additive information distances. We extend the RG to be applicable to GGMs with vector observations and robustify it to learn the tree structure against arbitrary corruptions. We call this robustified algorithm Robust Recursive Grouping (RRG). RRG makes use of the additivity of information distance to identify the relationship between nodes. For any three nodes xi, xj and xk, the difference between the information distances d(xi, xk) and d(xj , xk) is denoted as ijk = d(xi, xk) - d(xj , xk).

4

Lemma 3. [4] For information distances d(xi, xj) for all nodes xi, xj  V in a tree T  T3, ijk has following two properties: (1) ijk = d(xi, xj) for all xk  V\{xi, xj} if and only if xj is a leaf node and xi is the parent of xj and (2) -d(xi, xj ) < ijk = ijk < d(xi, xj) for all xk, xk  V\{xi, xj} if and only if xi and xj are leaves and share the same parent.

RRG initializes the active set 1 to be the set of all observed nodes. In the ith iteration, as shown in Algorithm 1, RRG adopts Lemma 3 to identify relationships among nodes in active set i, and it removes nodes identified as siblings from i and adds newly introduced hidden nodes to form the active set i+1 in the (i + 1)st iteration. The procedure of estimating the distances between the
newly-introduced hidden node xnew and other nodes is as follows. For the node xi which is the child of xnew, i.e., xi  C(xnew), the information distance is estimated as

d^(xi, xnew) =

2

1 |C(xnew)| - 1

d^(xi, xj)
jC(xnew )

+

1 |Kij |

kKij

^ ijk

,

(8)

where Kij = xk  V\{xi, xj} : max d^(xi, xk), d^(xj , xk) <  for some threshold  > 0. For xi / C(xnew), the distance is estimated as


d^(xi, xnew) =  

. d^(xk ,xi )-d^(xk ,xnew )

xkC(xnew )

|C (xnew )|

d^(xk,xj )-d^(xk,xnew)-d^(xj ,yi)

(xk,xj )C(xnew)×C(i)

|C (xnew )||C (i)|

if xi  Vobs . otherwise

(9)

The set Kij is designed to ensure that the nodes involved in the calculation of information distances are not too far, since estimat-
ing long distances requires a large number of samples. The maximal cardinality of Kij over all nodes xi, xj  V can be found, and we denote this as N , i.e., |Kij|  N .

The observed nodes are placed in the 0th layer. The hidden nodes introduced in ith iteration are placed in ith layer. The nodes in the ith layer are in the active set i+1 in the (i + 1)st iteration, but nodes in i+1 can be nodes created in the jth iteration, where j < i. For example, in Fig. 1, nodes x12, x14 and x15 are created in the 1st iteration, and they are in 2. Nodes x1, x2 and x5 are also in 2, which are observed nodes. Eqns. (8) and (9) imply that the estimation error in the 0th layer will propagate to the nodes in higher layers, and it is necessary to derive concentration
results for the information distance related to the nodes in higher
layers. To avoid repeating complicated expressions in the various
concentration bounds to follow, we define the function

Figure 1: An illustration of the active set. The shaded nodes are the observed nodes and the rest are hidden nodes.

f (x)

2l e 2
max

-

3n2 32n1

x

+ l e 2
max

-c

n2 42 2

x2

=:

ae-wx

+ be-ux2 ,

where 

=

2lm2 axemax/lmax /m1/ilnmax , w

=

3n2 32n1

,

u

=

c n2
42 2

,

a

=

2lm2 ax

and b

=

lm2 ax.

To assess

the proximity of the estimates d^(xi, xnew) in (8) and (9) to their nominal versions, we define

h(l)(x) slf (mlx) = sl ae-wmlx + be-um2lx2 for all l  N  {0}.

(10)

where s = d2max + 2d3max(1 + 2N ) and m = 2/9. The following proposition yields recursive estimates for the errors of the distances at various layers of the learned latent tree.
Proposition 4. With Assumptions 1­5, if we implement the truncated inner product to estimate the information distance among observed nodes and adopt (8) and (9) to estimate the information distances related to newly introduced hidden nodes, then the information distance related to the hidden nodes xnew created in the lth layer d^(xi, xnew) satisfies
P d^(xi, xnew) - d(xi, xnew) >  < h(l)() for all xi  l+1 and l  N  {0}. (11)

We note that Proposition 4 demonstrates that the coefficient of exponential terms in (11) grow exponentially with increasing layers (i.e., ml and m2l in (10)), which requires a commensurately large number of samples to control the tail probabilities.

5

Theorem 1. Under Assumptions 1­5, RRG learns the correct latent tree with probability 1 -  if

n2 = 

lm4 axe2max/lmax 2 m2/ilnmax 2min

9 2

2LR log |Vobs|3 

and

n1 = O

n2 log n2

,

(12)

where LR is the number of iterations of RRG needed to construct the tree.

Theorem 1 indicates that the number of clean samples n2 required by RRG to learn the correct structure grows exponentially with the number of iterations LR. Specifically, for the full m-tree illustrated in Fig. 5, n2 is exponential in the depth of the tree with high probability for structure learning to succeed. The sample complexity of RRG depends on e , 2max/lmax and the exponential

relationship with max will be shown to be unavoidable in view of our impossibility result in The-

orem trees

5. but

Huang et al. [17, Lemma the algorithm is based on

7.2 [5]

] also derived instead of RG.

aRsRaGmpisleabcloemtoplteoxlietryatreesnu1lt=foOr l(earnn2in/glolgatnen2)t

corruptions. This tolerance level originates from the properties of the truncated inner product; sim-

ilar tolerances will also be seen for the sample complexities of subsequent algorithms. We expect

this is also the case for [17], which is based on [5], though we have not shown this formally. In

addition, the sample complexity is applicable to a wide class of graphical models that satisfies the

Assumptions 1 to 5, while the sample complexity result [4, Theorem 11], which hides the depen-

dencies on the parameters, only holds for a limited class of graphical models whose effective depths

(the maximal length of paths between hidden nodes and their closest observed nodes) are bounded

in |Vobs|.

3.3 Robust Neighbor Joining and Spectral Neighbor Joining algorithms

The NJ algorithm [1] also makes use of additive distances to identify the existence of hidden nodes. To robustify the NJ algorithm, we adopt robust estimates of information distances as the additive distances in the so-called RNJ algorithm. We first recap a result by Atteson [18].

Proposition 5. If all the nodes have exactly two children, NJ will output the correct latent tree if

max
xi,xj Vobs

d^(xi, xj ) - d(xi, xj)

 min/2.

(13)

Unlike RG, NJ does not identify the parent relationship among nodes, so it is only applicable to binary trees in which each node has at most two children.

Theorem 2. If Assumptions 1­5 hold and all the nodes have exactly two children, RNJ constructs

the correct latent tree with probability at least 1 -  if

n2 = 

lm4 axe2max/lmax 2 m2/ilnmax 2min

log

|Vobs|2 

and

n1 = O

n2 log n2

.

(14)

Theorem 2 indicates that the sample complexity of RNJ grows as log |Vobs|, which is much better than RRG. Similarly to RRG, the sample complexity has an exponential dependence on max.
In recent years, several variants of NJ algorithms have been proposed. The additivity of information distances results in certain properties of the rank of the matrix R  R , |Vobs|×|Vobs| where R(i, j) = exp(-d(xi, xj )) for all xi, xj  Vobs. Jaffe et al. [6] proposed SNJ which utilizes the rank of R to deduce the sibling relationships among nodes. We robustify the SNJ algorithm by implementing the robust estimation of information distances, as shown in Algorithm 2.

Although SNJ was designed for discrete random variables, the additivity of the information distance proved in Proposition 1 guarantees the consistency of Robust Spectral NJ (RSNJ) for GGMs with vector variables. A sufficient condition for RSNJ to learn the correct tree can be generalized from [6].

Proposition 6. If Assumptions 1­5 hold and all the nodes have exactly two children, a sufficient condition for RSNJ to recover the correct tree from R^ is

R^ - R 2  g(|Vobs|, min, max),

(15)

where

g(x, min, max) =

e21-(23em-axm(a1x

)log2 (x/2) e-max - e-2min ),

(1

-

e-2min

),

e-2max  0.5 e-2max > 0.5

.

6

Similar with RNJ, RSNJ also does not identify the parent relationship between nodes, so it only

applies to

binary trees.

To state

the next

result

succinctly, we

assume

that max



1 2

log 2; this is

the

regime of interest because we consider large trees which implies that max is typically large.

Theorem 3.

If Assumptions 1­5 hold, max



1 2

log 2, and

RSNJ reconstructs the correct latent tree with probability at

all the least 1

nodes -  if

have

exactly

two

children,

n2 = 

lm4 axe2max(1/lmax+log2(|Vobs|/2)+1)2

 e 2/lmax
min

2min

log

|Vobs|2 

and

n1 = O

n2 log n2

.

(16)

Theorem 3 indicates that the sample complexity of RSNJ grows as poly(|Vobs|). Specifically, in the

binary tree case, the sample complexity grows exponentially with the depth of the tree. Also, the de-

pendence of sample complexity on max is exponential, i.e., O e2(1/lmax+log2(|Vobs|/2)+1)max , but

the coefficient of max is larger than those of RRG and RNJ, which are O e2max/lmax . Compared

to the sample complexity of SNJ in [6], the sample complexity of RSNJ has the same dependence

on the number truncated inner

porfodoubcset rivseadblneotdoetsol|eVroabtes|,Owlhoigcnnh22

means that the corruptions.

robustification

of

SNJ

using

the

3.4 Robust Chow-Liu Recursive Grouping

In this section, we show that the exponential dependence on LR in Theorem 1 can be provably mitigated with an accurate initialization of the structure. Different from RRG, RCLRG takes Chow-Liu algorithm as the initialization stage, as shown in Algorithm 3. The Chow-Liu algorithm [19] learns the maximum likelihood estimate of the tree structure by finding the maximum weight spanning tree of the graph whose edge weights are the mutual information quantities between these variables. With the monotonicity between mutual information and information distance, we can construct the Chow-Liu tree as the minimum spanning tree (MST) of the graph whose weights are information distances. However, this monotonicity property is violated in general for random vectors, since the determinants of I - X and X are not monotonic functions of each other for general X  Rlmax×lmax . We now show that the required monotonicity property is guaranteed by a wide class of GGMs.

We choose any node in the tree as the root node xr, and define the parent node and set of children nodes (in the rooted tree) of any node xi as pa(i) and C(xi) respectively. The depth of a node xi is
dT(xi, xr). We specify the model where

xi = Axpa(i) + ni for all xi  V

(17)

where A  Rlmax×lmax is non-singular, ni  N (0, i) and ni's are mutually independent. Since the root node has no parent, it is natural to set xpa(r) = 0 and nr  N (0, r). It is easy to verify that the model specified by (17) and this initial condition is an undirected GGM. To guarantee that
the mutual information is a monotonic function of the information distance, it is natural to consider
the situation in which the covariance matrices of all variables are the same up to a constant scale
factor.

Proposition 7. If ni's for the variables at depth l are distributed as N (0, l-1n), and

ArAT + n = r

(18)

where  > 0 is a constant, then the covariance matrix of the variable at depth l is lr.

We call (18) the (A, r, n)-homogenous condition, which guarantees that covariance matrices of the random vectors in the tree are same up to a scale factor. We now provide a sufficient condition on A, r, n to achieve the monotonicity between mutual information and information distance.
Proposition 8. In the GGM specified by (17), if (i) the ni's for the variables at depth l are distributed as N (0, l-1n) for some  > 0; (ii) the (A, r, n)-homogeneous condition in (18) is satisfied; and (iii) r and A commute, then the mutual information is a monotonically decreasing function of the information distance. Furthermore, the mutual information and the information distance can be expressed in closed form in terms of (, dT(xi, xj), A). See (J.27) and (J.28) in Appendix J.

This property is trivially satisfied in the scalar case [4], but is more subtle in the vector case. With this property, the Chow-Liu algorithm [19] can be implemented by finding the MST with information distances as edge weights.

7

Lemma 9. If the three conditions in Proposition 8 are satisfied, the Chow-Liu tree reduces to the MST where edge weights are the information distances, i.e.,

TCL = MST(Vobs; D) := arg min

d(xi, xj ),

(19)

TTVobs (xi,xj )T

where TVobs is the set of all the trees with node set V = Vobs.

Definition 2. Given the latent tree T = (V, E) and any node xi  V, the surrogate node [4] of xi is Sg(xi; T, Vobs) = arg minxjVobs d(xi, xj ).

We introduce a new notion of distance that quantifies the sample complexity of RCLRG.

Definition 3. Given the latent tree T = (V, E) and any node xi  V, the contrastive distance of xi with respect to Vobs is defined as

dct(xi; T, Vobs) =

min

d(xi, xj ) - min d(xi, xj).

xj Vobs\Sg(xi;T,Vobs)

xj Vobs

(20)

Definitions 2 and 3 imply that the surrogate node Sg(xi; T, Vobs) of any observed node xi is itself xi, and its contrastive distance is the information distance between the closest observed node and itself.

It is shown that the Chow-Liu tree TCL is equal to the tree where all the hidden nodes are contracted to their surrogate nodes [4], so it will be difficult to identify the surrogate node of some node if

its contrastive distance is small. Under this scenario, more accurate estimates of the information

distances are required to construct the correct Chow-Liu tree.

Proposition 10. The Chow-Liu tree MST(Vobs; D^ ) is constructed correctly if

d^(xi, xj) - d(xi, xj) < MST/2 for all xi, xj  Vobs,

(21)

where MST := minxjInt(T) dct(xj ; T, Vobs).

Hence, the contrastive distance describes the difficulty of learning the correct Chow-Liu tree.

Theorem 4. With Assumptions 1­4 and the conditions of Proposition 8, RCLRG constructs the

correct latent tree with probability at least 1 -  if

n2 =  max

1 2min

9 2

2LC ,

1 2MST

lm4 axe2max/lmax 2 m2/ilnmax

log

|Vobs|3 

and n1 = O

n2 log n2

, (22)

where LC is the maximum number of iterations of RRG (over each internal node of the constructed

Chow-Liu tree) in RCLRG needed to construct the tree.

If we implement RCLRG Theorem 4 indicates that

with true information the sample complexity

doifstRanCcLesR,GLCgrowse12xDpeogn(eMntSiaTll(yVionbsL; DC^ )) -L1R..

Compared with [4, Theorem 12], the sample complexity of RCLRG in Theorem 4 is applicable

to a wide class of graphical models that satisfy Assumptions 1 to 5, while the [4, Theorem 12]

requires the assumption that the effective depths of latent trees are bounded in |Vobs|, which is rather restrictive.

3.5 Comparison of robust latent tree learning algorithms

Since the sample complexities of RRG, RCLRG, RSNJ and RNJ depend on different parameters and
different structures of the underlying graphs, it is instructive to compare the sample complexities of these algorithms on some representative tree structures. These trees are illustrated in Fig. 5. RSNJ
and RNJ are not able to identify the parent relationship among nodes, so they are only applicable to trees whose maximal degrees are no larger that 3, including the double-binary tree and the HMM. In particular, RNJ and RSNJ are not applicable to the full m-tree (for m  3) and the double star. Derivations and more detailed discussions of the sample complexities are deferred to Appendix K.

n2

Algorithm

RRG

RCLRG

RSNJ

RNJ

Tree

Double-binary tree HMM
Full m-tree Double star

O

(

9 2

)Diam(T)

O

(

9 2

)Diam(T)

O

(

9 2

)Diam(T)

O( log dmax)

O

(

9 2

)

1 2

Diam(T)

O  log Diam(T)

O Diam(T)

O  log dmax

O e2tmax Diam(T) O e2tmax log Diam(T)
N.A. N.A.

O Diam(T) O  log Diam(T)
N.A. N.A.

Table 1: The sample complexities of RRG, RCLRG, RSNJ and RNJ on the double-binary tree, the

HMM, the full m-tree and the double star. We set  := e2max/lmax and t = O(lm-1ax + log |Vobs|).

8

3.6 Experimental results

We present simulation results to demonstrate the efficacy of the robustified algorithms. Samples are generated from a HMM with lmax = 3 and Diam(T) = 80. The three conditions in Proposition 8 are satisfied with  = 1. The Robinson-Foulds distance [20] between the true and estimated trees is
adopted to measure the performances of the algorithms. For the implementations of CLRG and RG,
we use the code from [4]. Other settings and more extensive experiments are given in Appendix L.

We consider three corruption patterns here. (i) Uniform corruptions are independent additive noises

in [-2A, 2A]; (ii) Constant magnitude corruptions are also independent additive noises but taking

values in {-A, +A} with probability 0.5. These two types of noises are distributed randomly in

Xn1 ; (iii) HMM corruptions are generated HMM but has different parameters. They

by a HMM replace the

which entries

has the same structure as the original in Xn1 with samples generated by the

variables in the same positions. In our simulations, A is set to 60, and the number of corruptions n1

is 100.

160

160

160

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

RNJ

NJ

128

128

128

Robinson-Foulds Metric Robinson-Foulds Metric Robinson-Foulds Metric

96

96

96

64

64

64

32

32

32

0

500

1000 1500 2000

10000

Number of samples

(a) Uniform corruptions

20000

0

500

1000 1500 2000

5000

10000

20000

Number of samples

(b) Constant magnitude corruptions

0

500

1000 1500 2000

10000

20000

Number of samples

(c) HMM corruptions

Figure 2: Robinson-Foulds distances of robustified and original algorithms averaged over 100 trials

Fig. 2 (error bars are in Appendix L.1) demonstrates the superiority of RCLRG in learning HMMs compared to other algorithms. The robustified algorithms also result in smaller estimation errors (Robinson-Foulds distances) compared to their unrobustified counterparts in presence of corruptions.

4 Impossibility result

Definition 4. Given a triple (|Vobs|, max, lmax), the set T (|Vobs|, max, lmax) consists of all multivariate Gaussian distributions N (0, ) such that: (1) The underlying graph T = (V, E) is a tree
T  T3, and the size of the set of observed nodes is |Vobs|. (2) The distribution N (0, ) satisfies Assumptions 1 and 5 with parameters lmax and max.

For the given class of graphical models T (|Vobs|, max, lmax), nature chooses some parameter  =  and generates n i.i.d. samples Xn1 from P. The goal of the statistician is to use the observations Xn1 to learn the underlying graph T, which entails the design of a decoder  : Rn|Vobs|lmax  T|Vobs|, where T|Vobs| is the set of trees whose size of the node set is at least |Vobs|.
Theorem 5. Consider the class of graphical models T (|Vobs|, max, lmax), where |Vobs|  3. If there exists a graph decoder learns from n i.i.d. samples such that

(T)T

max
(|Vobs |,max ,lmax )

P(T)((Xn1 )

=

T)

<

,

(23)

then (as max   and |Vobs|  ),

n = max



(1 - )e max log3 |Vobs |lmax

log |Vobs|

,

(1

-

)e

2max 3lmax

.

(24)

Theorem 5 implies that the optimal sample complexity grows as (log |Vobs|) as |Vobs| grows. Table 1 indicates that the sample complexity of RCLRG when the underlying latent tree is a full m-tree
(for m  3) or a HMM is optimal in the dependence on |Vobs|. The sample complexity of RNJ is also optimal in |Vobs| for double binary trees and HMMs. In contrast, the derived sample complexi-
ties of RRG and RSNJ are suboptimal in relation to Theorem 5. However, one caveat of our analyses

9

of the latent tree learning algorithms in Section 3 is that we are not claiming that they are the best possible for the given algorithm; there may be room for improvement.

When the maximum information distance max grows, Theorem 5 indicates that the optimal sample

complexity

grows

as

(e 2max 3lmax

).

Table 1 shows the sample complexities of RRG, RCLRG and

RNJ

grow

as

O(e2

max lmax

),

which

has

the

alike

dependence

as

the

impossibility

result.

However,

the

sample complexity of RSNJ grows as O e2tmax , which is larger (looser) than that prescribed by

Theorem 5.

References
[1] N. Saitou and M. Nei. The neighbor-joining method: a new method for reconstructing phylogenetic trees. Mol. Bio. Evol., 4(4):406­425, 1987.
[2] D. Tang, H. J. Chang, A. Tejani, and T. Kim. Latent regression forest: Structured estimation of 3d articulated hand posture. In Proc. IEEE Conf. Computer Vision and Pattern Recognition, pages 3786­3793, 2014.
[3] J. Eisenstein, B. O'Connor, N. A. Smith, and E. Xing. A latent variable model for geographic lexical variation. In Proc. Conf. Empirical Methods in Natural Language Processing, pages 1277­1287, 2010.
[4] M. J. Choi, V. Y. F. Tan, A. Anandkumar, and A. S. Willsky. Learning latent tree graphical models. Journal of Machine Learning Research, 12:1771­1812, 2011.
[5] A. Anandkumar, K. Chaudhuri, D. Hsu, S. M. Kakade M, L. Song, and T. Zhang. Spectral methods for learning multivariate latent tree structure. arXiv preprint arXiv:1107.1283, 2011.
[6] A. Jaffe, N. Amsel, Y. Aizenbud, B. Nadler, J. T. Chang, and Y. Kluger. Spectral neighbor joining for reconstruction of latent tree models. SIAM Journal on Mathematics of Data Science, 3(1):113­141, 2021.
[7] M. Casanellas, M. Garrote-Lopez, and P. Zwiernik. Robust estimation of tree structured models. arXiv:2102.05472v1 [stat.ML], Feb. 2021.
[8] A. Katiyar, J. Hoffmann, and C. Caramanis. Robust estimation of tree structured gaussian graphical models. In International Conference on Machine Learning, pages 3292­3300. PMLR, 2019.
[9] K. E. Nikolakakis, D. S. Kalogerias, and A.D. Sarwate. Learning tree structures from noisy data. In Proc. Artificial Intelligence and Statistics, pages 1771­1782. PMLR, 2019.
[10] A. Tandon, V. Y. F. Tan, and S. Zhu. Exact asymptotics for learning tree-structured graphical models: Noiseless and noisy samples. IEEE Journal on Selected Areas of Information Theory, 1(3):760­776, 2020.
[11] A. Tandon, A. H. J. Yuan, and V. Y. F. Tan. SGA: A robust algorithm for partial recovery of tree-structured graphical models with noisy samples. In International Conference on Machine Learning. PMLR, 2021.
[12] L. Wang and Q. Gu. Robust gaussian graphical model estimation with arbitrary corruption. In International Conference on Machine Learning, pages 3617­3626. PMLR, 2017.
[13] A. P. Parikh, L. Song, and E. P. Xing. A spectral algorithm for latent tree graphical models. In International Conference on Machine Learning, pages 1065­1072, Jun. 2011.
[14] Y. Chen, C. Caramanis, and S. Mannor. Robust high dimensional sparse regression and matching pursuit. arXiv preprint arXiv:1301.2725, 2013.
[15] S. L. Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.
[16] P. Judea. Probabilistic reasoning in intelligent systems: Networks of plausible inference. Elsevier, 2014.

10

[17] F. Huang, N. U. Naresh, I. Perros, R. Chen, J. Sun, and A. Anandkumar. Guaranteed scalable learning of latent tree models. In Uncertainty in Artificial Intelligence, pages 883­893. PMLR, 2020.
[18] K. Atteson. The performance of neighbor-joining methods of phylogenetic reconstruction. Algorithmica, 25(2):251­278, 1999.
[19] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Trans. Inform. Theory, 14(3):462­467, 1968.
[20] D. F. Robinson and L. R. Foulds. Comparison of phylogenetic trees. Mathematical Biosciences, 53(1-2):131­147, 1981.
[21] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Cambridge University Press, New York, NY, 2010.
[22] G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Maryland, US, 2013.
[23] G. W. Stewart. Perturbation theory for the singular value decomposition. SVD and Signal Processing, II: Algorithms, Analysis and Applications, pages 99­109, 1991.
[24] R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge University Press, New York, NY, 2012.
[25] S. Pettie and V. Ramachandran. An optimal minimum spanning tree algorithm. J. ACM, 49(1):16­34, 2002.
[26] W. Wang, M. J. Wainwright, and K. Ramchandran. Information-theoretic bounds on model selection for Gaussian markov random fields. In Proc. IEEE Int. Symp. on Inf. Theory, pages 1373­1377, Austin, Texas, USA, Jun. 2010. IEEE.
[27] M. Marcus and W. Gordon. An extension of the Minkowski determinant theorem. Proceedings of the Edinburgh Mathematical Society, 17(4):321­324, 1971.
11

Supplementary materials for "Robustifying Algorithms of Learning Latent Trees with Vector
Variables"
A Illustrations of corruption patterns in Section 2.1

Figure 3: The left figure shows the corruption pattern that corrupted terms lie in the same rows. This corruption patterm is known as outliers. The right figure shows an arbitrary corruption pattern where corrupted entries in each column can be in any n1/2 rows.
B Illustrations of active sets defined in Section 3.2

(a) Illustration of 1

(b) Illustration of 2 Figure 4: Illustration of active sets.

(c) Illustration of 3

12

C Pseudo-code of RRG in Section 3.2

Algorithm 1 RRG

Input: Data matrix X, corruption level n1, threshold  Output: Adjacency matrix A

Procedure:

1: Active set 1  all the observed nodes 2: Implement truncated inner product to compute d^(xi, xj) for all xi, xj  Vobs. 3: while |i| > 2 do

4: Update d^(xnew, xi) for all xi  i for all new hidden nodes.

5: Compute ^ ijk = d^(xi, xk) - d^(xj, xk) for all xi, xj, xk  i

6: for all nodes xi and xj in i do

7:

if |^ ijk - ^ ijk | <  for all xk, xk  i then

8:

if |^ ijk - d^(xi, xj)| <  for all xk  i then

9:

xj is the parent of xi.

10:

Eliminate xi from i

11:

else

12:

xj and xi are siblings.

13:

Create a hidden node xnew as the parent of xj and xi

14:

Add xnew and eliminate xj and xi from i

15:

end if

16:

end if

17: end for

18: end while

D Pseudo-code of RSNJ in Section 3.3

Algorithm 2 RSNJ Input: Data matrix X, corruption level n1 Output: Adjacent matrix A Procedure: 1: Implement truncated inner product to compute d^(xi, xj) for all xi, xj  Vobs. 2: Compute the symmetric affinity matrix R^ as R^ (i, j) = exp(-d^(xi, xj)) for all xi, xj  Vobs 3: Set Bi = {xi} for all xi   4: Compute the matrix S as S^(i, j) = 2(R^ BiBj ) 5: while The number of Bi's is larger than 3 do 6: Find (^i, ^j) = arg mini,j S^(i, j). 7: Merge B^i and B^j as B^i = B^i  B^j and delete B^j . 8: Update S^(k,^i) = 2(R^ BkB^i ). 9: end while
13

E Pseudo-code of RCLRG in Section 3.4
Algorithm 3 RCLRG Input: Data matrix X, corruption level n1, threshold  Output: Adjacency matrix A Procedure: 1: Construct a Chow-Liu tree with d^(xj , xk) for observed nodes xj , xk  Vobs 2: Identify the set of internal nodes of the Chow-Liu tree 3: for all internal nodes xi of the Chow-Liu tree do 4: Implement RRG algorithm on the closed neighborhood of xi 5: Replace the closed neighborhood of xi with the output of RRG 6: end for
F Illustrations of representative trees in Section 3.5

(a) Double-binary tree

(b) HMM

(c) Full m-tree, m = 3

(d) Double star

Figure 5: Representative tree structures.

G Proofs of results in Section 3.1

Proof of Proposition 1. For the sake of brevity, we prove the additivity property for paths of length 2. The proof for the general cases can be derived similarly. We consider the case xj is on the path connected xi and xk and xi, xj, xk  V.
For any square matrix A  Rn×n, the determinant of A is denoted as |A| = det(A).

Then we can write information distance as

d(xi,

xk )

=

-

1 2

log

ik Tik

+

1 4

log

iiTii

+

1 4

log

kk Tkk

Note that E[xi|xj] = ij-jj1xj and ij is of full rank by Assumption 2, and

Ai|j = ij -jj1

is also of full rank.

(G.1) (G.2)

Furthermore, we have ik = Ai|j jj ATk|j and ik Tik = Ai|j jj ATk|j Ak|j jj ATi|j .

(G.3)

14

Then we have Furthermore,

ikTik = Ai|j jj ATk|j Ak|j jj ATi|j

= ATi|j Ai|j jj ATk|j Ak|j jj

= Tjj ATi|j Ai|j jj jj

Tjj ATk|j Ak|j jj . jj

Tjj ATi|j Ai|j jj = Ai|j jj Tjj ATi|j = ij Tij , Tjj ATk|j Ak|j jj = kj Tkj .

(G.4) (G.5) (G.6)
(G.7) (G.8)

Substituting (G.4) and (G.7) into (G.1), we have

d(xi,

xk )

=

-

1 2

log

ij Tij

+

1 4

log

iiTii

+

1 4

log

jj Tjj

-

1 2

log

kj Tkj

+

1 4

log

kk Tkk

+

1 4

log

jj Tjj

= d(xi, xj ) + d(xj , xk),

as desired.

(G.9) (G.10)

Lemma 11. (Bernstein-type inequality [21]) Let X1, . . . , Xn be n centered sub-exponential random variables, and K = max1in Xi 1, where · 1 is the sub-exponential norm and is defined as

X 1 := sup p-1 E|X |p 1/p.
p1

(G.11)

Then for every a = (a1, . . . , an)  Rn and every t > 0, we have

P

n
aiXi  t
i=1

 2 exp

- c min

t2 K2 a

2 2

,

K

t a



(G.12)

Lemma 12. Let the estimate of the covariance matrix ij based on the truncated inner product be ^ ij. If t2 <  = max{m2 ax, min}, we have

P

^ ij - ij , > t1 + t2



2l e 2
max

-

3n2 16n1

t1

+

l e 2
max

-c

t22 n2 2

xi, xj  Vobs.

(G.13)

Proof of Lemma 12. Let Iisjt,1 be the set of indexes of the uncorrupted samples of [xi]s[xj ]t. Without loss of generality, we assume that |Iisjt,1| = n2. Let Iisjt,2 and Iisjt,3 be the sets of the indexes of truncated uncorrupted samples and the reserved corrupted samples, respectively.

Figure 6: Illustration of the truncated inner product.

Then,

[^ ij ]st

=

1 n2

[xi](sm)[xj ]t(m) -

[xi](sm)[xj ](tm) +

[xi](sm)[xj ](tm)

mIisjt,1

mIisjt,2

mIisjt,3

(G.14)

15

The (s, t)th entry of the error covariance matrix ~ ij = ^ ij - ij  Rd×d is defined as

[~ ij ]st

=

1 n2

-

[xi](sm)[xj ]t(m) +

[xi](sm)[xj ](tm) .

mIisjt,2

mIisjt,3

(G.15)

From the definition of the truncated inner product, we can bound the right-hand side of (G.15) as

[~ ij ]st

2 n2 mIisjt,2

[xi](sm)[xj ](tm) .

(G.16)

Equipped with the expression of the moment-generating function of a chi-squared distribution, the moment-generating function of each term in the sum of (G.16) can be upper bounded as

E e  E e |[xi]s(m)[xj ]t(m)|



([xi

]s(m)

)2

+([xj 2

]t(m)

)2

(G.17)



E e E e ([xi]s(m))2

([xj ]t(m))2

(G.18)

 Using the power mean inequality, we have

1

.

1 - 2m2 ax

(G.19)

Thus,

2
e n2

 mIisjt,2 |[xi]s(m)[xj ]t(m)|

1 |Iisjt,2 |



e mIisjt,2

2 n2

|[xi

]s(m)

[xj

]t(m)

|

|Iisjt,2 |

e mIisjt,2

2n1 n2

|[xi]s(m)

[xj

]t(m)

|

|Iisjt,2 |

1 n1
.

(G.20) (G.21)

E e|[~ ij ]st|

2
 E e n2

mIisjt,2 [xi]s(m)[xj ]t(m)

 max E e 2n1 n2

[xi]s(m)[xj ]t(m)

mIisjt,2



1

1

-

 4m 2 ax n1
n2

and

E e maxs,t [~ ij ]st

= E max e [~ ij ]st
s,t

 lm2 axE e [~ ij ]st



lm2 ax

.

1

-

 4m 2 ax n1
n2

Thus,

(G.22) (G.23)
(G.24)

P

~ ij

, > t

=P

max
s,t

[~ ij ]st

>t

= P e maxs,t [~ ij ]st > et

(G.25)

 e-tE e maxs,t [~ ij ]st

 e-t

lm2 ax

.

1

-

 4m 2 axn1
n2

(G.26)

Let 

=

, 3n2
16m 2 axn1

then we

have

P

~ ij

, > t

 2l e . 2
max

-

3n2 16m 2 ax

n1

t

(G.27)

According to Lemma 11 (since the involved random variables are sub-exponential), we have

P

1 n2 mIisjt,1

[xi]s(m)[xj ](tm) - [ij ]st

> t  exp

- c min

t2n2 K2

,

tn2 K

,

where K = m2 ax.

(G.28)

Thus, if t < , we have

as desired.

P ^ ij - ij , > t1 + t2



2l e 2
max

-

3n2 16n1

t1

+

l e 2
max

-c

t22 n2 2

,

(G.29)

16

Proof of Proposition 2. From the definition of the information distance, we have

lmax
d(xi, xj) = - log n ij

+

1 2

log

det

ii

+

1 2

log

det

jj .

n=1

(G.30)

According to the inequality A 2  A 1 A  which holds for all A  Rn×m [22], we have

k(^ ij ) - k(ij )  ^ ij - ij 2

(G.31)

 ^ ij - ij  ^ ij - ij 1  lmax ^ ij - ij ,. (G.32)

Using the triangle inequality, we arrive at

d^(xi, xj ) - d(xi, xj)

lmax


log n(^ ij ) - log n(ij )

+ 1 dim(xi) 2

log n(^ ii) - log n(ii)

n=1

n=1

+ 1 dim(xj ) 2

log n(^ jj ) - log n(jj ) .

n=1

(G.33)

Furthermore, since the singular value is lower bounded by min, using Taylor's theorem and (G.31), we obtain

log n(^ ij) - log n(ij)



1 min

n(^ ij ) - n(ij )



lmax min

^ ij - ij

,.

(G.34)

Finally,

d^(xi, xj) - d(xi, xj ) 

lmax

+

dim(xi)

+ 2

dim(xj )

 2lm2 ax min

^ ij - ij

,.

lmax min

^ ij - ij

,

From Lemma 12, the proposition is proved.

(G.35)

H Proofs of results in Section 3.2

Lemma 13. Consider the optimization problem

n

P : max f (x) = xi(xi - 1)

{xi }

i=1

N

s.t.

xi  N 0  xi  k

i=1

i = 1, . . . , N.

(H.1)

Assume

nk



N.

An

optimal solution

is

given

by

xi

=

k

for

all

i

=

1,

.

.

.

,



N k



and

x

N k

+1

=

N

-

k

N k

,

and

xi

=

0

for

i

=



N k



+

2,

.

..

,

n.

This lemma can be verified by direct calculation, and so we will omit the details.

Proof of Proposition 4. We prove the proposition by induction. Proposition 2 and Eqn. (5) show that at the 0th layer [23]

P(|ij | > ) < f () = h(0)().

(H.2)

Now suppose that the distances related to the nodes created in the (l - 1)st iteration satisfy

P d^(xi, xh) - d(xi, xh) >  < h(l-1)().

(H.3)

17

Since s > 1 and m < 1, it is obvious that h(l)()  h(l+k)() for all l, k  N and for all  > 0.

(H.4)

Then we can deduce that P(|d^(xi, xj) - d(xi, xj )| > ) < h(l-1)()
From the update equation of the distance in (8), we have

for all

xi, xj  l.

(H.5)

d^(xi, xh)

=

2(

1 C(h)

-

1|

(d(xi, xj)
jC(h)

+

ij )

+

1 |Kij |

(ijk
kKij

+

ik

-

jk )

and

d^(xi, xh)

=

2(

1 C(h)

-

1|

ij
jC(h)

+

1 |Kij |

(ik
kKij

-

jk )

+ d(xi, xh).

Using the union bound, we find that

(H.6) (H.7)

P d^(xi, xh) - d(xi, xh) > 

P
jC(h)

ij

+

1 |Kij |

(ik
kKij

-

jk )

> 2



P

jC(h)

ij

+

1 |Kij |

(ik
kKij

-

jk )

> 2



P

|ij | >

2 3



+

P

|ik |

>

2 3



+P

|jk |

>

2 3



.

jC(h)

kKij

The estimates of the distances related to the nodes in the lth layer satisfy

P

d^(xi, xh) - d(xi, xh) > 

< |C(h)|

1 + 2|Kij|

h(l-1)

2 3



 dmax(1 + 2N )h(l-1)

2 3



.

Similarly, from (9), we have

(H.8) (H.9) (H.10)
(H.11) (H.12)

P d^(xk, xh) - d(xk, xh) > 



iC(h) P

|ik| >

1 2



+P

|ih| >

1 2



,

(i,j)C(h)×C(k) P

|ij | >

1 3



+P

|ih |

>

1 3



+P

|jk |

>

1 3



,

if k  Vobs otherwise.
(H.13)

Using the concentration bound at the (l - 1)st layer in inequality (H.3), we have

P d^(xk, xh) - d(xk, xh) > 



dmaxh(l-1)(

1 2

)

+

d2max(1

+

2N

)h(l-1) (

1 3

),

if k  Vobs

d2maxh(l-1)(

2 3

)

+

2d3max(1

+

2N

)h(l-1)(

2 9

),

otherwise.

(H.14)

Summarizing the above three concentration bounds, we have that for the nodes at the lth layer,

estimates of the information distances (based on the truncated inner product) satisfy

P

d^(xk, xh) - d(xk, xh) > 

< d2max + 2d3max(1 + 2N ) h(l-1)

2 9



= h(l)().

(H.15)

Proposition 14. The cardinalities of the active sets in lth and (l + 1)st iterations admit following relationship

|l| dmax



|l+1|



|l| - 2.

(H.16)

18

Proof of Proposition 14. Note that at the lth iteration, the number of families is |l+1|, and thus we have

|l+1 |
ni = |l|,
i=1

(H.17)

where ni

is the number of nodes in l

in each family.

Since 1



ni

 dmax, we have

l dmax



|l+1|.

Figure 7: Illustration of RRG. The shaded nodes are the observed nodes and the rest are hidden nodes. 1 = {x1, x2, . . . , , x9}, and 2 is the nodes in the dotted lines. If we delete the nodes in 2, the remained unknown hidden nodes are x10, x11 and x13. Nodes x10 and x13 are at the end of the chain formed by these two nodes, and x11 is at the end of the degenerate chain formed by itself.

We next prove that there are at least two of ni's not less than 2. If we delete the nodes in active set l, the remaining hidden nodes form a single tree or a forest. There will at least two nodes at the end of the chain, which means that they only have one neighbor in hidden nodes, as shown in Fig. 7. Since they at least have three neighbors, they have at least two neighbors in l. Thus, there are at least two of ni's not less than 2, and thus |l+1|  |l| - 2.

Corollary 1. The maximum number of iterations of Algorithm 1, LR, is bounded as

log

|Vobs | 2

log dmax

 LR  |Vobs| - 2.

(H.18)

Proof. When Algorithm 1 terminates, ||  2. Combining Proposition 14 and ||  2 proves the corollary.

Theorem 6. Under Assumptions 1­5, RRG algorithm constructs the correct latent tree with probability at least 1 -  if

n2



6422 c2

9 2

2LR-2

log

17lm2 axsLR-1|Vobs|3 

n2  128 9 LR-1 log 34lm2 axsLR-1|Vobs|3 ,

n1

3 2



(H.19) (H.20)

where



=

2lm2 axemax/lmax m1/ilnmax

 = max{m2 ax, min}

s = d2max + 2d3max(1 + 2N )



=

min 2

,

(H.21)

c is an absolute constant, and LR is the number of iterations of RRG needed to construct the tree.

19

Proof of Theorem 6. It is easy to see by substituting the constants , , s and  into (H.19) and (H.20) that Theorem 6 implies Theorem 1, so we provide the proof of Theorem 6 here.

The error events of learning structure in the lth layer of the latent tree (the 0th layer consists of the observed nodes, and the (l + 1)st layer is the active set formed from lth layer). The error events could be enumerated as: misclassification of families Efl, misclassification of non-families Enl f , misclassification of parents Epl and misclassification of siblings Esl. We will bound the probabilities of these four error events in the following.

The event representing misclassification of families Efl represents classifying the nodes that are not in the same family as a family. Suppose nodes xi and xj are in different families. The event that classifying them to be in the same family Efl,ij at layer l can be expressed as

Efl,ij = |^ ijk - ^ ijk | <  for all xk, xk  l .

(H.22)

We have

P(Efl,ij ) = P

|^ ijk - ^ ijk | < 

xk,xk 

 min P |^ ijk - ^ ijk | <  ,
xk,xk 

(H.23)

P(Efl) = P

Efl,ij = P

Efl,ij .

xi,xjnot in same family

(xi,xj )lf

We enumerate all possible structural relationships between xi, xj, xk and xk

(H.24)

Figure 8: Enumerating of four-node topology and the corresponding |ijk - ijk |.

Let  < 2min, by decomposing the estimate of the information distance as d^(xi, xj) = d(xi, xj ) + ij , we have
P |^ ijk - ^ ijk | <  = P |ijk - ijk + ik - jk - ik + jk | < 

 P ik - jk - ik + jk <  - (ijk - ijk )

 P ik - jk - ik + jk <  - 2min

P

|ik |

>

2min 4

-



+P

|jk| >

2min -  4

+P

jk |

>

2min 4

-

+P

|ik |

>

2min -  4

.

(H.25)

20

The event representing misclassification of the parents Epl represents classifying a sibling relationship as a parent relationship. Following similar procedures, we have

P(Epl ) = P

Epl ,ij = P

Epl ,ij

xi,xj are siblings

(xi,xj )lp

(H.26)

P(Epl ,ij) = P P

^ ijk - d^(xi, xj) < 
xk l

 min P
xkl

|ij |

>

2min 3

-

+P

|ik| >

2min -  3

^ ijk - d^(xi, xj ) < 

+P

|jk |

>

2min 3

-



(H.27) (H.28)

The event representing misclassification of non-families Enl f represents classifying family members as non-family members. We have

P(Enl f) = P

Enl f,ij = P

Enl f,ij

xi,xj in the same family

(xi,xj )lnf

P(Enl f) = P

|^ ijk - ^ ijk | > 

xi,xj in the same family xk,xk 

(H.29) (H.30)

and P |^ ijk - ^ ijk |  

P

|ik |

>

 4

+P

|jk |

>

 4

+P

|jk |

>

 4

+P

|ik |

>

 4

(H.31)

The event representing misclassification of siblings Esl represents classifying parent relationship as sibling relationship. Similarly, we have

P(Esl) = P

Esl,ij
xi is the parent of xj

=P

Esl,ij
(xi,xj )ls

P(Esl,ij ) = P

^ jik - d^(xi, xj ) > 

xk

and

P

^ jik - d^(xi, xj ) > 

P

|ij |

>

 3

+P

|ik |

>

 3

+P

|jk| >

 3

(H.32) (H.33) (H.34)

To bound the probability of error event in lth layer, we first analyze the cardinalities of lf , lp, lnf and ls. Note that the definitions of these four sets are

lf = lp = lnf = ls =

(xi, xj) : xi and xj are not in the same family xi, xj  l (xi, xj) : xi and xj are siblings xi, xj  l (xi, xj) : xi and xj are in the same family xi, xj  l (xi, xj) : xi and xj is the parent of xi, xj  l .

(H.35) (H.36) (H.37) (H.38)

Clearly, we have

|lf | 

|l| 2

and |ls|  |l|.

The cardinality of lp can be bounded as

|l+1 |
|lp| 

ni 2

i=1

where ni is the size of each family in l.

(H.39) (H.40)

21

From Lemma 13, we deduce that

Similarly, we have

|lp|



1 2

dmax(dmax

-

1)

|l| dmax

=

1 2

|l

|(dmax

-

1).

|lnf |



1 2

|l|(dmax

-

1).

The probability of the error event in lth layer can be bounded as

P(El) = P(Efl  Epl  Enl f  Esl)

 P(Efl) + P(Epl ) + P(Enl f ) + P(Esl)

4

|l| 2

h(l)

2min -  4

+

3 2

|l

|(dmax

-

1)h(l)

+ 3|l|2h(l)

 3

+ 2|l|3(dmax - 1)h(l)

 4

.

2min -  3

The probability of learning the wrong structure is

(H.41) (H.42)
(H.43)

P(E) = P El  P(El)

(H.44)

l

l



4

|l| 2

h(l)

2min -  4

+

3 2

|l

|(dmax

-

1)h(l)

l

+ 3|l|2h(l)

 3

+ 2|l|3(dmax - 1)h(l)

 4

With Proposition 14, we have

2min -  3

(H.45)

L-1
P(E)  4

|Vobs| - 2l 2

h(l)

2min -  4

+

3 2

(|Vobs|

-

2l)(dmax

-

1)h(l)

l=0

+ 3(|Vobs| - 2l)2h(l)

 3

+ 2(|Vobs| - 2l)3(dmax - 1)h(l)

 4

,

where L is the number of iterations of RRG.

2min -  3
(H.46)

We can separately bound the two parts of the first term in the summation

|Vobs |-2l 2

h(l)

2min - 4

as

4

4

|Vobs |-2l 2

sl ae-wml x

|Vobs |-2L 2

sL-1ae-wmL-1x



|Vobs| - 2l |Vobs| - 2l - 1 2sL-1-l



|Vobs|2 2sL-1-l

for x

>

0

and

4

4

|Vobs |-2l 2

slbe-um2lx2

|Vobs |-2L 2

sL-1be-um2L-2x2



|Vobs| - 2l |Vobs| - 2l - 1 2sL-1-l



|Vobs|2 2sL-1-l

.

(H.47)

These bounds imply that

L-1
4

|Vobs| - 2l 2

h(l)(x) 

1

+

|Vobs|2 2



1 si

4h(L-1)(x) = 4

1

+

|Vobs|2 2(s - 1)

4h(L-1)(x) for x > 0

l=0

i=1

Similar procedures could be implemented on other terms, and we will obtain

P(E) 

4

1

+

|Vobs|2 2(s - 1)

+

3 2

(dmax

-

1)

1

+

|Vobs| 2(s - 1)

h(L-1)

2min -  4

+

3

1

+

|Vobs|2 4(s - 1)

+ 2(dmax - 1)

1

+

|Vobs|3 8(s - 1)

h(L-1)

 4

(H.48)

=

4

1

+

|Vobs|2 2(s - 1)

+

3 2

(dmax

-

1)

1

+

|Vobs| 2(s - 1)

ae + be -wmL-1

2min 4

-

-um2L-2(

2min 4

-

)2

+

3

1

+

|Vobs|2 4(s - 1)

+ 2(dmax - 1)

1

+

|Vobs|3 8(s - 1)

ae + be -wmL-1

 4

-um2L-2

(

 4

)2

 .

22

Upper bounding each of the four terms in inequality (H.48) by /4, we obtain the following sufficient conditions of n1 and n2 to ensure that P(E)  :

n2  max

6422 c(2min - )2

9 2

2L-2 log 4lm2 axsL-1

4(1

+

) |Vobs |2
2(s-1)

+

3 2

(dmax

-

1)(1

+

|Vobs | 2(s-1)

)



,

6422 c2

9 2

2L-2
log

4lm2 axsL-1

3(1 +

|Vobs |2 4(s-1)

)

+

2(dmax

-

1)(1

+



) |Vobs|3
8(s-1)

,

n2 n1



max

128 3(2min - )

9 2

L-1 log 8lm2 axsL-1

4(1

+

) |Vobs |2
2(s-1)

+

3 2

(dmax

-

1)(1

+

|Vobs | 2(s-1)

)

,



128

9

L-1 log 8lm2 axsL-1

3(1 +

|Vobs |2 4(s-1)

)

+

2(dmax

-

1)(1

+

) |Vobs|3
8(s-1)

.

3 2



Note that

max

4

1

+

|Vobs|2 2(s - 1)

+

3 2

(dmax

-

1)

1

+

|Vobs| 2(s - 1)

,

3

1

+

|Vobs|2 (s - 1)

+ 2(dmax - 1)

1

+

|Vobs|3 8(s - 1)

<4

1

+

|Vobs|2 2(s - 1)

+ 2(dmax - 1)

1

+

|Vobs|3 2(s - 1)

(H.49)

 2(dmax - 1)

2

+

|Vobs|3 + 2|Vobs|2 2(s - 1)

(H.50)

< 2(dmax - 1)

2

+

|Vobs|3

+ 2|Vobs|2 s

(a)
<

2(dmax

-

1) |Vobs|3

+

2|Vobs|2 s

+

7N |Vobs|3

<

17dmaxN

|Vobs|3 s

(H.51) (H.52) (H.53)

(b)
<

17 4

|Vobs

|3

,

(H.54)

where inequality (a) and (b) result from s < 7N |Vobs|3 and dmaxN <  < min, we then can derive the sufficient conditions to ensure that P(E)

s4,

respectively.  as

Choosing

n2



6422 c2

9 2

2L-2

log

17lm2 axsL-1|Vobs|3 

,

(H.55)

n2 n1



128 3

9 2

L-1

log

34lm2 axsL-1|Vobs|3 

.

In Theorem 1,

we

choose 

=

. min
2

(H.56)

Then the following conditions

n2 n1

 =

6422 9

O

c2 n2 2 log n2

.

2L-2

log

17lm2 axsL-1|Vobs|3 

,

are sufficient to guarantee that P(E)  .

(H.57) (H.58)

We are going to prove that there exists C > 0, such that

C



n2 log n2



128 3

(

9 2

)L-1

n2 log 34l2maxsL-1|Vobs|3


,

(H.59)

23

which is equivalent to

C



128 3

9 2

L-1
log

34lm2 axsL-1|Vobs|3 

 n2 log n2.

(H.60)

Since n2 is lower bounded as in (H.57), it is sufficient to show that there exists C > 0, such that

(C  )2

128 3

9 2

L-1

log

34lm2 axsL-1|Vobs|3 

2



6422 c2

9 2

2L-2

log

17lm2 axsL-1|Vobs|3 

log

6422 c2

9 2

2L-2

log

17lm2 axsL-1|Vobs|3 

,

which is equivalent to

(C  )2



9 log 256c log

17l2max sL-1 |Vobs |3 
34l2max sL-1 |Vobs |3 

log

642 2 c2

(

9 2

)2L-2

+

log

log

17l2max sL-1 |Vobs |3 

log 34l2maxsL-1|Vobs|3


2
. (H.61)

We have

log

17lm2 axsL-1 

|Vobs

|3

/

log

34lm2 axsL-1|Vobs|3 

>

1 2

and

log

642 2 c2

(

9 2

)2L-2

+

log

log

17l2max sL-1 |Vobs |3 

log 34l2maxsL-1|Vobs|3


2
>

log

642 2 c2

(

9 2

)2L-2

log 34l2maxsL-1|Vobs|3


2
.

(H.62) (H.63)

Since

lim
L

log

642 2 c2

(

9 2

)2L-2

log 34l2maxsL-1|Vobs|3


2
= +,

we can see that there exists C > 0 that satisfies inequality (H.59).

(H.64)

I Proofs of results in Section 3.3

Theorem 7. If Assumptions 1 to 5 hold and all the nodes have exactly two children, RNJ constructs the correct latent tree with probability at least 1 -  if

n2

>

1622 c2min

log

2|Vobs|2lm2 ax 

n2 n1

>

64 3min

log

4|Vobs|2lm2 ax 

(I.1) (I.2)

where



=

2lm2 axemax/lmax m1/ilnmax

and c is an absolute constant.

and  = max{m2 ax, min},

(I.3)

Proof of Theorem 7. It is easy to see by substituting the constants  and  into (I.1) and (I.2) that Theorem 7 implies Theorem 2, so we provide the proof of Theorem 7 here.

With the sufficient condition in Proposition 5, we can bound the probability of error event by the

union bound as follows

P(E)  P

max d^(xi, xj ) - d(xi, xj)
xi,xj Vobs

>

min 2

(I.4)

 |Vobs|2P

d^(xi, xj ) - d(xi, xj)

>

min 2

.

(I.5)

24

We bound two terms in the tail probability separately as

2l e 2
max

-

3n2 64n1

min

<

 2|Vobs|2

l e 2
max

-c

n2 162 2

2min

<

 2|Vobs|2

.

(I.6) (I.7)

Then we have

n2

>

1622 c2min

log

2|Vobs|2lm2 ax 

,

(I.8)

The proof that n1

=

O(n2/

n2 n1

>

64 3min

log

4|Vobs|2lm2 ax 

.

log n2) can be derived by following

the

similar

procedures

(I.9) in the

proof of Theorem 1.

Proposition 15. If Assumption 1 to 5 hold and the truncated inner product is adopted to estimate the information distances,

P

R^ - R 2 > t

 |Vobs|2f

emin

t |Vobs|

,

(I.10)

where the function f is defined as

f (x)

2l e 2
max

-

3n2 32n1

x

+ l e 2
max

-c

n2 42 2

x2

=

ae-wx

+ be-ux2,

(I.11)

with



=

2lm2 axemax/lmax /m1/ilnmax ,

w

=

3n2 32n1

,

u

=

c

n2 42 2

,

a

=

2lm2 ax

and

b

=

lm2 ax.

Proof of Proposition 15. Noting that Rij = exp - d(xi, xj) , we have

P |R^ ij - Rij| > t = P exp - d^(xi, xj) - exp - d(xi, xj ) > t

(I.12)

(a)
P

d^(xi, xj ) - d(xi, xj ) > emin t

< f (emin t),

where inequality (a) is derived from Taylor's Theorem.

(I.13) (I.14)

Since

R^ - R

2



|Vobs|

max
i,j

|R^ ij

-

Rij |,

we have P

R^ - R 2 > t

P

max
i,j

|R^ ij

- Rij |

>

t |Vobs|

 |Vobs|2f

emin

t |Vobs|

as desired.

(I.15) (I.16)

Theorem 8. If Assumptions 1 to 5 hold and all the nodes have exactly two children, RSNJ constructs the correct latent tree with probability at least 1 -  if

n2



1622|Vobs|2 ce2min g(|Vobs|, min, max)2

log

2|Vobs|2lm2 ax 

n2 n1



64|Vobs| 3emin g(|Vobs|, min,

max)

log

4|Vobs|2lm2 ax 

(I.17) (I.18)

where

g(x, min, max) =

e21-(23em-axm(a1x

)log2 (x/2) e-max - e-2min ),

(1

-

e-2min

),

e-2max  0.5 e-2max > 0.5

(I.19)



=

2lm2 axemax/lmax m1/ilnmax

and c is an absolute constant.

 = max{m2 ax, min},

(I.20)

25

Proof of Theorem 8. It is easy to see by substituting the constants  and  into (I.17) and (I.18) that Theorem 8 implies Theorem 3, so we provide the proof of Theorem 8 here.

Proposition 6 shows that the probability of learning the wrong tree P(E) could be bounded as

P(E)  P

R^ - R 2 > g(|Vobs|, min, max)

 |Vobs|2f

emin

g(|Vobs|, min, |Vobs|

max)

.

(I.21)

Substituting the expression of f and bounding the right-hand-side of inequality (I.21) by , we have

n2



1622|Vobs|2 ce2min g(|Vobs|, min, max)2

log

2|Vobs|2lm2 ax 

and

(I.22)

n2 n1



64|Vobs| 3emin g(|Vobs|, min,

max)

log

4|Vobs|2lm2 ax 

.

(I.23)

The proof that n1 = O(n2/ log n2) can be derived by following the similar procedures in the

proof of Theorem 1.

J Proofs of results in Section 3.4

Lemma 16. If r and n are positive definite matrices, A is a nonsingular square matrix, ho-

mogenous condition (18) is satisfied for k > 1, and A and r commute, then all the eigenvalues of

1 k

AAT

and

1 k

ATA

are

in

the

interval

(0, 1).

Proof. By transposing Ar = rA, we have

ATr = rAT.

(J.1)

Furthermore, we have

1 k

AAT

r

=

r

1 k

AAT.

(J.2)

Then

1 k

AAT

and

r

are

simultaneously

diagonalizable

[24]:



nonsingular

matrix

U

s.t.

1 Uk

AATU-1

=



UrU-1 = r

(J.3)

where  and r are diagonal matrices.

Since (18) is satisfied and A and r commute, we have

r

1 k

AAT

+

1 k n

=

r.

(J.4)

Then, by multiplying U and U-1, we have

r

+

U-1

1 k

nU

=

r

(J.5)

Thus,

U-1

1 k

nU

must

be

a

diagonal

matrix

n,

and

we

can

deduce

that

its

diagonal

entries

are

positive, since n is positive definite matrix.

r + n = r

(J.6)

Because

1 k

AAT

is

a

positive

definite

matrix,

diagonal

entries

of



is

larger

than

0.

Then (J.6)

implies that the diagonal entries of  is less than 1.

Thus, the eigenvalues of

1 k

AAT

are in the

interval

(0,

1).

Since

1 k

AAT

and

1 k

AT

A

have

the

same

eigenvalues,

the

argument

is

proved.

Lemma 17. The MST of a weighted graph T has the following properties:

(1) For any cut C of the graph, if the weight of an edge e in the cut-set of C is strictly smaller than the weights of all other edges of the cut-set of C, then this edge belongs to all MSTs
of the graph.

26

(2) If T is a tree of MST edges, then we can contract T into a single vertex while maintaining the invariant that the MST of the contracted graph plus T gives the MST for the graph before contraction [25].
Proposition 18. The undirected graphical model specified by (17) and the initial condition xpa(r) = 0, nr  N (0, r) is GGM.

Proof of Proposition 18. To prove that the specified model is a GGM, we need to prove that the joint distribution of all variables is Gaussian and that the conditional independence relationship induced by the edges is achieved.

According to (17) and the initial condition, it is easy to see that any linear combination of variables is the linear combination of independent Gaussian variables, which is Gaussian. Thus, the joint distribution of all variables is indeed Gaussian.

To show that the conditional independence is guaranteed, we show that

A  B | S for any S separates A and B.

(J.7)

where S, A and B are all sets of nodes, and S separates A and B means that any path connected nodes in A and B goes through a node in S.
Without loss of generality, we consider the case where S, A and B consist of a single node for conciseness of the proof. The case where these sets consist of multiple nodes can be easily proved by generalizing the proof we show here.

Figure 9: Illustration of the relationship among xn, xm and xs.

We first consider the case where xn and xm belong to different branches, as shown in Fig. 9, and the depths of xn and xm are n and m, respectively. The separator node xs can be anywhere along the path connecting xn and xm. Without loss of generality, we assume it sits in the same branch as xn, and its depth is s, where s < n. Then we have

n
E[xnxTn ] = Anr(An)T + An-ii(An-i)T
i=1 m
E[xmxTm] = Amr(Am)T + Am-ii(Am-i)T
i=1
t
E[xtxTn ] = Atr(An)T + At-ii(An-i)T,
i=1

(J.8) (J.9) (J.10)

where i and i are the covariance matrices of the independent noises in each branch.

Then we calculate the distribution of conditional distribution

xn xm

| xt  N (µ~, ~ ),

(J.11)

27

where

~ =

~ 11 ~ 21

~ 12 ~ 22

.

(J.12)

We have

~ 12 = Anr(Am)T - ×

t
Anr(At)T + An-iiA(t-i)T
i=1
Atr(At)T + t At-iiA(t-i)T -1Atr(Am)T = 0.
i=1

(J.13)

Thus, the conditional independence of xn and xm given xs is proved.

When xn and xm are on the same branch, a similar calculation can be performed to prove the conditional independence property.

Proof of Proposition 7. The statement in Proposition 7 is equivalent to

l
Alr(Al)T + i-1Al-in(Al-i)T = lr.
i=1

We prove (J.14) by induction. When l = 1, the homogenous condition guarantees that ArAT + n = r. If (J.14) holds for l = 1, . . . , n, then for l = n + 1

n+1

An+1r(An+1)T +

i-1 An+1-i n (An+1-i )T

i=1

n+1
= A(Anr(An)T + i-1An-in(An-i)T)AT

i=1

= A(nr + nA-1nA-T)AT

= n(ArAT + n)

= n+1r

as desired.

(J.14)
(J.15) (J.16) (J.17) (J.18)

Proof of Proposition 8. The mutual information between two jointly Gaussian random vectors xT yT T  N (0, ) where

=

xx yx

xy yy

(J.19)

is

I (x;

y)

=

h(x)

+

h(y)

-

h(x,

y)

=

-

1 2

log

det() det(xx )det(yy )

=

-

1 2

log

det(yy - yx-xx1xy) det(yy )

=

-

1 2

log

det(-yy1)det(yy - yx-xx1xy)

=

-

1 2

log

det(I - -yy1yx-xx1xy)

(J.20) (J.21) (J.22) (J.23)

From the Definition 1, the information distance is

d(x,

y)

=

-

1 2

log

det(xy )det(yx ) det(xx )det(yy )

=

-

1 2

log

det(-yy1 yx -xx1 xy )

(J.24)

28

Figure 10: Illustration of different topological relationships.

For the case where xn and xm are on the two different branches from the root, such as nodes x10 and x11 in Fig. 10, assuming their depths are n and m, we have

I (xn ;

xm)

=

-

1 2

log

det

I-

1 m

-r 1

Amr(An

)T

1 n

-r 1

Anr(Am

)T

(J.25)

Since rA = Ar, by transposing and multiplying -r 1 on both sides, we have

ATr = rAT, -r 1A = A-r 1, and AT-r 1 = -r 1AT.

(J.26)

Then we obtain

I (xn ;

xm)

=

-

1 2

log

det

I

-

1 m+n

(AT

A)n+m

.

(J.27)

Similarly,

d(x,

y)

=

-

1 2

log

det

1 m+n

(ATA)n+m

.

(J.28)

Note that n + m is the graph distance between these nodes.

For the case where xm is the ancestor of xn, such as x10 and x12 in Fig. 10, following a similar calculation, we have

I (xn ;

xm)

=

-

1 2

log

d(x,

y)

=

-

1 2

log

det

I

-

1 n-m

(AT

A)n-m

det

1 n-m

(AT

A)n-m

.

(J.29) (J.30)

Note that now n - m is the graph distance between these nodes.

For the case where xn and xm share the same ancestor xa which is not the root node, we have

I (xn ;

xm)

=

-

1 2

log

det

I

-

1 n+m-2a

(AT A)n+m-2a

(J.31)

d(x,

y)

=

-

1 2

log

det

1 n+m-2a

(AT

A)n+m-2a

.

(J.32)

Note that n + m - 2a is the graph distance between these nodes.

Lemma

16

shows

that

all

the

eigenvalues

of

1 

AT

A

lie

in

(0,

1).

Thus,

the

information

distance

is

a

monotonically increasing function of the graph distance, and the mutual information is a monotonic

decreasing function of the graph distance.

Proof of Proposition 10. We prove this argument by induction. Choosing any node as the root node, we first prove that the edges which are related to the observed nodes with the largest depth are identified or contracted correctly.

29

Since we consider the edges which involve at least one observed node, we only need to discuss the edges formed by two observed nodes and one observed node and one hidden node. We first consider the identification of the edges between two observed nodes.

Figure 11: Two kinds of edges related at least one observed node.

To correctly identify the edge (xk, xj) in Fig. 11, we consider the cut of the graph which splits the nodes into {xj} and all the other nodes. Lemma 17 says that the condition that

d^(xk, xj ) < d^(xl, xj ) xl  Vobs, xl = xk, xj

(J.33)

is sufficient to guarantee that this edge is identified correctly. This condition is equivalent to

kj < lj + d(xl, xk) xl  Vobs, xl = xk, xj ,

(J.34)

which is guaranteed by choosing MST = dct(xk; T, Vobs).

Furthermore, we need to guarantee that xj is not connected to other nodes except xk. We consider the cut of the graph which split the nodes into {xj, xk} and all the other nodes. Lemma 17 says that
the condition that

d^(xl, xk) < d^(xl, xj ) xl  Vobs, xl = xk, xj

(J.35)

is sufficient to guarantee xj is not connected to other nodes. This condition is equivalent to

lk < lj + d(xk, xj ) xl  Vobs, xl = xk, xj ,

(J.36)

which is guaranteed by choosing MST = dct(xk; T, Vobs).

A similar proof can be used to guarantee (xi, xk) can be identified correctly. Then we can contract xi, xj to xk to form a super node in the subsequent edges identification for Lemma 17.

Now we discuss the edges involving one observed node and one hidden node. There are two cases:
(i) The hidden node xk should be contracted to either xi or xj . (ii) The hidden node xk should be contracted to xl  Vobs, xl = xi, xj.

We first consider the case (i). Without loss of generality, we assume that xk should be contracted to xj. Contracting xk to xj is equivalent to that xi is not connected to other nodes except xj. Lemma
17 shows that

d^(xi, xj ) < d^(xi, xl) d^(xj , xl) < d^(xi, xl)  xl  Vobs, xl = xi, xj

(J.37)

is sufficient to achieve that xi is not connected to other nodes except xj . This condition is equivalent to

ij + d(xk , xj ) < lj + d(xk , xl) and jl + d(xk , xj ) < il + d(xk , xi)

xl  Vobs, xl = xk, xj ,

(J.38)

which is guaranteed by choosing MST = dct(xk ; T, Vobs). Then we can contract xi to xj to form a super node in the subsequent edges identification for Lemma 17.

Then we consider the case (ii). Here we need to prove that xk will not be contracted to xi or xj . Without loss of generality, we assume that xk is contracted to xl, which guaranteed by that there is no edge between xi and xj . Lemma 17 shows that

d^(xi, xl) < d^(xi, xj ) d^(xj , xl) < d^(xi, xj)

(J.39)

30

is sufficient to guarantee that there is no edge between xi and xj. This condition is equivalent to

il + d(xk , xl) < ij + d(xk , xj ) jl + d(xk , xl) < ij + d(xk , xi),

(J.40)

which is guaranteed by choosing MST = dct(xk ; T, Vobs).

Assume that all the edges related to the nodes with depths larger than l are identified or contracted correctly. We now consider the edges related to the nodes with depths l. For the edges between
two observed nodes and edges of case (i) and (ii), similar procedures can be adopted to prove the
statements. Here we discuss the case where xl should contract the hidden nodes which are its descendants. Contracting xk to xl is equivalent to that there are edges between (xl, xi) and (xl, xj), and there is no other edges related to xi and xj. Recall that condition (J.39) is satisfied by the induction hypothesis. Lemma 17 shows that

d^(xi, xl) < d^(xi, xk) d^(xk, xl) < d^(xk, xi) xk  Vobs, xk = xi, xj, xl d^(xj , xl) < d^(xj, xk) d^(xk, xl) < d^(xk, xj) xk  Vobs, xk = xi, xj, xl

(J.41) (J.42)

is sufficient to guarantee that xk is contracted to xl. This condition is equivalent to

il < ik + d(xk, xl) kl < ki + d(xi, xl) xk  Vobs, xk = xi, xj , xl jl < jk + d(xk, xl) kl < kj + d(xj , xl) xk  Vobs, xk = xi, xj , xl

(J.43) (J.44)

which is guaranteed by choosing MST = dct(xl; T, Vobs). Then we can contract xi and xj to xl to form a super node in the subsequent edges identification for Lemma 17.

Thus, the results are proved by induction.

Theorem 9. If Assumptions 1­4 as well as the conditions in Proposition 8 hold, RCLRG constructs the correct latent tree with probability at least 1 -  if

n2  max

n2 n1



max

4 2

9 2

2LC -2
,

1 2MST

1622 c

log

17lm2 axsLC-1|Vobs|3 

+

lm2 ax|Vobs|2

,

2 

9 2

LC-1
,

1 MST

64 3

log

34lm2 axsLC-1|Vobs|3 

+

2lm2 ax|Vobs|2

,

(J.45) (J.46)

where



=

2lm2 axemax/lmax m1/ilnmax

 = max{m2 ax, min}

s = d2max + 2d3max(1 + 2N )



=

min 2

,

(J.47)

c is an absolute constant, and LC is the number of iterations of RCLRG needed to construct the tree.

Proof of Theorem 9. It is easy to see by substituting the constants , , s and  into (J.45) and (J.46) that Theorem 9 implies Theorem 4, so we provide the proof of Theorem 9 here.

The RCLRG algorithm consists of two stages: Calculation of MST and implementation of RRG on internal nodes. The probability of error of RCLRG could be decomposed as

P(E ) = P EMST  (EMc ST  ERRG) = P(EMST) + P(EMc ST  ERRG)  P(EMST) + P(ERRG)

We define the correct event of calculation of the MST as

CMST =
xi,xj Vobs

d^(xi, xj) - d(xi, xj)

<

MST 2

=

Cij

xi,xj Vobs

(J.48)

Proposition 10 shows that

P(EMST)  1 - P(CMST) = P (

Cij )c

xi,xj Vobs

=P

Cicj

xi,xj Vobs



P(Cicj ) 

xi,xj Vobs

|Vobs| 2

f

MST 2

(J.49) (J.50)

31

We define the event that RRG yields the correct subtree based on nbd[xi, T]

CRRG =

{Output of RRG is correct with input nbd[xi, T]}

xiInt(MST(Vobs;D^ ))

(J.51)

=

Ci

xiInt(MST(Vobs;D^ ))

(J.52)

Then we have

P(ERRG) = 1-P(CRRG) = P

c
Ci
xiInt(MST(Vobs;D^ ))

=P

Cic . (J.53)

xiInt(MST(Vobs;D^ ))

By

defining

LC

=



Deg(MST(Vobs 2

;D^ ))

-

1,

we

have

P(E)  P(EMST) + P(ERRG)



|Vobs| 2

f

MST 2

+ |Vobs| - 2

4

1

+

|Vobs|2 2(s - 1)

(J.54)

+

3 2

(dmax

-

1)

1

+

|Vobs| 2(s - 1)

× h(LC-1)

2min -  4

+

3

1

+

|Vobs|2 4(s - 1)

+ 2(dmax - 1)

1

+

|Vobs|3 8(s - 1)

h(LC-1)(  ) 4
(J.55)

To derive the sufficient conditions of P(E)  , we consider the following conditions

P(EMST)  (1 - r) and P(ERRG)  r for some r  (0, 1)

Following the same calculations with inequalities (12), we have

n2  max

n2 n1



max

6422 c2

9 2

2LC -2

log

17lm2 axsLC-1|Vobs|3 r

,

1622 c2MST

log

lm2 ax|Vobs|2 (1 - r)

128 3

9 2

LC -1

log

34lm2 axsLC-1|Vobs|3 , r

64 3MST

log

2lm2 ax|Vobs|2 (1 - r)

By choosing r

=

, we have 17sLC-1|Vobs|3
17sLC -1 |Vobs |3 +|Vobs |2

n2  max

n2 n1



max

4 2

9 2LC-2 1

2

, 2MST

1622 c

log

17lm2 axsLC-1|Vobs|3 

+

lm2 ax|Vobs|2

,

2

9

LC-1
,

1

64 log 34lm2 axsLC-1|Vobs|3 + 2lm2 ax|Vobs|2

2

MST 3



(J.56) (J.57) (J.58)
(J.59) (J.60)

Following a similar proof as that for RRG, we claim that

n2  max

4 2

n1 = O

n2 log n2

9 2

2LC -2
,

1 2MST

1622 log 17lm2 axsLC-1|Vobs|3 + lm2 ax|Vobs|2 ,

c



are sufficient to guarantee P(E)  .

(J.61) (J.62)

K Discussions and Proofs of results in Section 3.5

In this section, we provide more discussions of the results in Table 1. We also provide the proofs of results listed in Table 1.

The sample complexities of RRG and RCLRG are achieved w.h.p., since the number of iterations

LR and LC depend on the quality of the estimates of the information distances. The parameter t for

RSNJ

scales

as

O(

1 lmax

+ log |Vobs|).

For

the

dependence on

Diam(T),

RRG

and

RSNJ

have

the

32

worst performance. This is because RRG constructs new hidden nodes and estimates the information

distances related to them in each iteration (or layer), which results in more severe error propagation

on larger and deeper graphs. In contrast, our impossibility result in Theorem 5 suggests that RNJ

has the optimal dependence on Diam(T). RCLRG also has the optimal dependence on the diameter

of graphs on HMM, which demonstrates that the Chow-Liu initialization procedure greatly reduces

the

sample

complexity

from O

(

9 2

)Diam(T)

to O

log Diam(T) . Since the dependence on max

only relies on the parameters, the dependence of max of all these algorithms remains the same

for graphical models with different underlying structures. RRG, RCLRG and RNJ have the same

dependence

O(e2

max lmax

),

while

RSNJ

has

a

worse

dependence

on

max.

K.1 Proofs of entries in Table 1

Double-binary tree For RRG, the number of iterations needed to construct the tree

LORe2=lmmaaxx12((92D)Diaimam((TT))

- .

1)

with

high

probability.

Thus, the sample complexity of RRG is

For RCLRG, as mentioned previously, the MST can be obtained by contracting the hidden nodes to

its closest observed node. For example, the MST of the double-binary tree with Diam(T) = 5 could

be

derived

by

contracting

hidden

nodes

as

Fig.

12.

Then

LC

=



Diam(T)+1 4



-

1,

and

the

number

of

observed

nodes

is

|Vobs|

=

2

Diam(T)+1 2

.

Thus,

the

sample

complexity

is

O

e ( ) 2

max lmax

9 2

Diam(T) 2

.

Figure 12: The contraction of hidden nodes in double-binary trees.

For

RSNJ,

the

number of

observed nodes

is

|Vobs|

=

2 , Diam(T)+1 2

so

the

sample

complexity

is

O e2tmax Diam(T) .

For RNJ,

the

number of observed nodes is |Vobs|

=

2 , Diam(T)+1 2

so the sample

complexity is

O

e2

max lmax

Diam(T)

.

HMM

For

RRG,

the

number

of

iterations

needed

to

construct

the

tree

LR

=



DiamT 2

-

1

with

high probability. Thus, the sample complexity of RRG is O

e ( ) 2

max lmax

9 Diam(T) 2

.

For RCLRG, MST could be derived as contracting hidden nodes as shown in Fig. 13. Then LC = 1

and |Vobs| = Diam(T) + 1. The sample complexity is thus O

e2

max lmax

log Diam(T)

.

Figure 13: The contraction of hidden nodes in HMMs.

For RSNJ, the number of observed nodes is |Vobs| = Diam(T) + 1, so the sample complexity is O e2tmax log Diam(T).

For RNJ, the number of observed nodes is |Vobs| = Diam(T) + 1, so the sample complexity is

O

e2

max lmax

log Diam(T)

.

33

Full m-tree For RRG, the number of iterations needed to construct the tree LR

Thus, the sample complexity of RRG is O

e ( ) 2

max lmax

9 Diam(T) 2

.

=

1 2

Diam(T).

For RCLRG, the MST can be derived by contracting hidden nodes as shown in Fig. 14. Then LC = 2

and |Vobs| = mDiam(T)/2.

Thus, its sample complexity is O

e2

max lmax

Diam(T)

.

Figure 14: The contraction of hidden nodes in full m-trees.

Double star For RRG, the number of iterations needed to construct the tree LR = 1. Thus, the

sample

complexity

of

RRG

is

O(e2

max lmax

).

For RCLRG, the maximum number of iterations over each RRG step (over each internal node of the

constructed Chow-Liu tree) in RCLRG is. LC = 1 and |Vobs| = 2dmax, so the sample complexity

of RCLRG is O

e2

max lmax

log dmax

.

L Additional numerical details and results
L.1 Standard deviations of results in Fig. 2
We first report the standard deviations of the results presented in Fig. 2 in the main paper. All results are averaged over 100 independent runs. Constant magnitude corruptions (Fig. 2(a))

/(/AVG)×100 Algorithm

# Samples 500

1000

1500

2000

5000

10000

20000

RRG

9.7/9.3 4.4/5.0

3.7/4.5

4.0/5.0

5.0/6.8 14.4/24.1 21.0/75.0

RSNJ

3.3/3.8 3.0/7.0 3.9/28.8 0.3/703.5 0.0/0.0

0.0/0.0

0.0/0.0

RCLRG

2.1/52.0 0.5/229.1 0.0/0.0

0.0/0.0

0.0/0.0

0.0/0.0

0.0/0.0

RNJ

5.6/4.3 9.0/7.1 12.3/10.0 17.1/15.1 28.4/28.3 35.4/47.9 32.2/68.1

RG

9.2/9.5 8.8/8.8

8.3/8.3

7.8/7.8

5.7/6.2

4.1/4.9

1.9/2.3

SNJ

0.4/0.3 0.6/0.4

1.4/0.9

2.7/1.8

3.2/2.6

3.6/3.7

3.2/5.9

CLRG

3.0/2.2 3.5/2.6

3.4/2.5

4.0/3.0 11.2/19.9 4.7/22.4 2.1/43.5

NJ

1.8/1.3 2.2/1.6

2.2/1.5

3.1/2.3

6.0/4.8 11.5/9.8 17.9/16.35

Table 2: The standard deviations and standard deviations divided by the means of the Robinson-

Foulds distances for different algorithms

Uniform corruptions (Fig. 2(b)) 34

/(/AVG)×100 Algorithm

# Samples 500

1000

1500

2000

5000

10000

20000

RRG

4.5/5.0 3.3/4.0 3.8/4.6

3.1/4.0

4.3/5.9 10.9/17.4 23.0/103.2

RSNJ

3.3/4.0 2.9/6.7 5.0/30.1 0.7/230.3 0.0/0.0

0.0/0.0

0.0/0.0

RCLRG

4.6/9.7 2.5/35.8 0.6/197.1 0.1/1971.0 0.0/0.0

0.0/0.0

0.0/0.0

RNJ

9.2/6.9 11.5/9.4 16.4/14.1 18.7/16.6 31.1/35.0 31.4/50.9 33.7/74.5

RG

9.2/9.0 9.8/9.6 8.0/7.8

8.1/8.0

9.0/9.0

7.8/7.4

5.9/6.1

SNJ

0.0/0.0 0.0/0.0 0.0/0.0

0.2/0.1

0.5/0.3

4.4/3.0

3.5/3.0

CLRG

3.3/2.4 3.4/2.5 3.3/2.4

3.5/2.5

3.0/2.2

6.0/4.5

8.0/17.5

NJ

1.7/1.2 1.9/1.3 2.0/1.4

2.0/1.4

2.1/1.5

3.9/2.8

6.1/4.8

Table 3: The standard deviations and standard deviations divided by the means of the Robinson-

Foulds distances for different algorithms

HMM corruptions (Fig. 2(c))

/(/AVG)×100 Algorithm

# Samples 500

1000

1500

2000

5000

10000

20000

RRG

4.0/4.5 5.3/5.8 3.8/4.5

3.3/4.0

3.4/4.6 7.5/10.9 21.1/49.6

RSNJ

5.9/6.0 3.5/6.4 3.4/9.7 3.6/35.2 0.0/0.0

0.0/0.0

0.0/0.0

RCLRG

13.1/17.4 4.9/14.1 3.4/29.1 1.6/54.6 0.0/0.0

0.0/0.0

0.0/0.0

RNJ

6.7/4.5 11.3/8.9 12.7/10.5 19.2/16.7 29.3/30.7 38.0/48.7 32.3/65.4

RG

9.3/9.1 8.4/8.3 8.7/8.5

8.6/8.3

9.0/8.8

8.6/8.2

5.5/5.7

SNJ

0.3/0.2 0.4/0.3 0.4/0.3

0.5/0.3

2.0/1.2

4.8/3.4

3.9/3.2

CLRG

3.4/2.5 3.3/2.4 3.2/2.3

3.1/2.3

3.5/2.6 15.0/12.7 8.2/13.7

NJ

1.8/1.3 1.6/1.2 2.0/1.4

1.9/1.3

2.8/2.0

4.5/3.3

5.7/4.4

Table 4: The standard deviations and standard deviations divided by the means of the Robinson-

Foulds distances for different algorithms

We note that most of the standard deviations (relative to the means) are reasonably small. However, some entries in Tables 2­4 appear to be rather large, for example 0.5/229.1. The reason is that the mean value of the errors are already quite small in these cases, so any deviation from the small means result in large standard deviations. This, however, seems unavoidable.
L.2 More simulation results complementing those in Section 3.6
In the following more extensive simulations, we consider eight corruption patterns:
· Uniform corruptions: Uniform corruptions are independent additive noises in [-2A, 2A] and distributed randomly in the data matrix Xn1 .
· Constant magnitude corruptions: Constant magnitude corruptions are independent additive noises but taking values in {-A, +A} with probability 0.5 and distributed randomly in Xn1 .
· Gaussian corruptions: Gaussian corruptions are independent additive Gaussian noises N (0, A2) and distributed randomly in Xn1 .
· HMM corruptions: HMM corruptions are generated by a HMM which shares the same structure as the original HMM but has different parameters. They replace the entries in Xn1 with the samples generated by the variables in the same positions.
· Double binary corruptions: Double binary corruptions are generated by a double binary tree-structured graphical model which shares the same structure as the original double binary graphical model but has different parameters. They replace the entries in Xn1 with the samples generated by the variables in the same positions.
· Gaussian outliers: Gaussian outliers are outliers that are generated by independent Gaussian random variables distributed as N (0, A2).
· HMM outliers: HMM outliers are outliers that are generated by a HMM that shares the same structure as the original HMM but has different parameters.
· Double binary outliers: Double binary outliers are outliers that are generated by a double binary tree-structured graphical model which shares the same structure as the original HMM but has different parameters.
35

In all our experiments, the parameter A is set to 60 and the number of corruptions n1 is set to 100. Samples are generated from two graphical models: HMM (Fig. 5(b)) and double binary tree (Fig. 5(a)). The dimensions of the random vectors at each node are lmax = 3. The RobinsonFoulds distance [20] between the nominal tree and the estimate and the error rate (zero-one loss) are adopted to measure the performance of learning algorithms. These are computed based on 100 independent trials. We use the code for RG and CLRG provided by Choi et al. [4]. All our experiments are run on an Intel(R) Xeon(R) CPU E5-2697 v4 @ 2.30 GHz.
L.2.1 HMM
Just as in the experiments in Choi et al. [4], the diameter of the HMM (Fig. 5(b)) is chosen to be Diam(T) = 80. The matrices (A, r, n) are chosen so that the three conditions in Proposition 8 are satisfied with  = 1. The information distances between neighboring nodes are chosen to be the same value 0.24, which implies that min = 0.24 and max = 0.24 · Diam(T) = 19.2.

Robinson-Foulds Metric

160

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

RNJ

NJ

128

96

64

32

0

500

1000 1500 2000

5000

10000

20000

Number of samples

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

500

1000 1500 2000

10000

20000

Number of samples

(b) Structure recovery error rate

Figure 15: Performances of robustified and original learning algorithms with constant magnitude corruptions

Robinson-Foulds Metric

160

128

96

64

32

0

500

1000 1500 2000

10000

20000

Number of samples

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

RRG

RG

0.9

RSNJ

SNJ

RCLRG

CLRG

0.8

RNJ

NJ

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

500

1000 1500 2000

10000

Number of samples

(b) Structure recovery error rate

20000

Figure 16: Performances of robustified and original learning algorithms with uniform corruptions

36

Robinson-Foulds Metric

160

128

96

64

32

0

500

1000 1500 2000

10000

20000

Number of samples

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

RRG

RG

0.8

RSNJ

SNJ

RCLRG

CLRG

RNJ

NJ

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

500

1000 1500 2000

10000

20000

Number of samples

(b) Structure recovery error rate

Figure 17: Performances of robustified and original learning algorithms with HMM corruptions

Robinson-Foulds Metric

160

128

96

64

32
0 500

RRG RSNJ RCLRG RNJ

RG SNJ CLRG NJ

1000 1500 2000

5000

Number of samples

10000

20000

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

500

1000 1500 2000

10000

20000

Number of samples

(b) Structure recovery error rate

Figure 18: Performances of robustified and original learning algorithms with Gaussian corruptions

Robinson-Foulds Metric

160

128

96

64

32

0

500

1000 1500 2000

10000

20000

Number of samples

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

RRG

RG

RSNJ

SNJ

0.8

RCLRG

CLRG

RNJ

NJ

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

500

1000 1500 2000

10000

Number of samples

(b) Structure recovery error rate

20000

Figure 19: Performances of robustified and original learning algorithms with double binary corruptions

37

Robinson-Foulds Metric

160

128

96

64

32
0 500

RRG RSNJ RCLRG RNJ

RG SNJ CLRG NJ

1000 1500 2000

5000

Number of samples

10000

20000

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

500

1000 1500 2000

10000

20000

Number of samples

(b) Structure recovery error rate

Figure 20: Performances of robustified and original learning algorithms with Gaussian outliers

Robinson-Foulds Metric

160

128

96

64

32

0

500

1000 1500 2000

10000

20000

Number of samples

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

RRG

RG

RSNJ

SNJ

0.8

RCLRG RNJ

CLRG NJ

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

500

1000 1500 2000

10000

Number of samples

(b) Structure recovery error rate

20000

Figure 21: Performances of robustified and original learning algorithms with HMM outliers

Robinson-Foulds Metric

160

128

96

64

32

0

500

1000 1500 2000

10000

20000

Number of samples

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

RRG

RG

RSNJ

SNJ

0.8

RCLRG

CLRG

RNJ

NJ

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

500

1000 1500 2000

10000

Number of samples

(b) Structure recovery error rate

20000

Figure 22: Performances of robustified and original learning algorithms with double binary outliers

These figures show that for the HMM, RCLRG performs best among all these algorithms. The reason is that the Chow-Liu initialization greatly reduces the effective depth of the original tree, which mitigates the error propagation. These simulation results also corroborate the effectiveness
38

of the truncated inner product in combating any form of corruptions. We observe that the errors of robustified algorithms are significantly less that those of original algorithms.
Table 1 shows that for the HMM, RCLRG and RNJ both have optimal dependence on the diameter of the tree. In fact, by changing the parameters min and max, we find that RNJ can sometimes perform better than RCLRG when min and max are both very small. In the experiments shown above, the parameters favor RCLRG.
Finally, it is also instructive to observe the effect of the different corruption patterns. By comparing the simulation results of HMM (resp. Gaussian and double binary) corruptions and HMM (resp. Gaussian and double binary) outliers, we can see that the algorithms perform worse in the presence of HMM (resp. Gaussian and double binary) corruptions. Since the truncated inner product truncates the samples with large absolute values, if corruptions appear in the same positions for all the samples, i.e., they appear as outliers, it is easier for the truncated inner product to identify these outliers and truncate them, resulting in higher quality estimates.
L.2.2 Double binary tree
The diameter of the double binary tree (Fig. 5(a)) is Diam(T) = 11. The matrices (A, r, n) are chosen so that the three conditions in Proposition 8 are satisfied with  = 1. The information distance between neighboring nodes is 1, which implies that min = 1 and max = Diam(T) = 11.

Robinson-Foulds Metric

126

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

105

RNJ

NJ

84

63

42

21

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(b) Structure recovery error rate

Figure 23: Performances of robustified and original learning algorithms with constant magnitude corruptions

Robinson-Foulds Metric

126

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

105

RNJ

NJ

84

63

42

21

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(b) Structure recovery error rate

Figure 24: Performances of robustified and original learning algorithms with uniform corruptions

39

Robinson-Foulds Metric

126

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

105

RNJ

NJ

84

63

42

21

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(b) Structure recovery error rate

Figure 25: Performances of robustified and original learning algorithms with Gaussian corruptions

Robinson-Foulds Metric

126

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

105

RNJ

NJ

84

63

42

21

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(b) Structure recovery error rate

Figure 26: Performances of robustified and original learning algorithms with HMM corruptions

Robinson-Foulds Metric

126

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

105

RNJ

NJ

84

63

42

21

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(b) Structure recovery error rate

Figure 27: Performances of robustified and original learning algorithms with double binary corruptions

40

Robinson-Foulds Metric

126

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

105

RNJ

NJ

84

63

42

21

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(b) Structure recovery error rate

Figure 28: Performances of robustified and original learning algorithms with Gaussian outliers

Robinson-Foulds Metric

126

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

105

RNJ

NJ

84

63

42

21

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(b) Structure recovery error rate

Figure 29: Performances of robustified and original learning algorithms with HMM outliers

Robinson-Foulds Metric

126

RRG

RG

RSNJ

SNJ

RCLRG

CLRG

105

RNJ

NJ

84

63

42

21

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(a) Robinson-Foulds distances

Structure Recovery Error Rate

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 1k 1.5k 2k

10k 20k Number of samples

100k 200k

(b) Structure recovery error rate

Figure 30: Performances of robustified and original learning algorithms with double binary outliers

These figures reinforce that the robustification procedure is highly effective in combating the corruptions. Furtheremore, we observe that RNJ performs the best among all these algorithms for the double binary tree. However, the simulation results in Jaffe et al. [6] shows that SNJ performs better
41

than NJ. This does not contradict our observations here. The reason lies on the choice of the parameters of the model min and max. In the simulations of [6], the parameter  (defined in therein) is set to 0.9, but in our simulation, the equivalent parameter e-2max/Diam(T) is 0.1. The exponential dependence on max of RSNJ listed in Table 1 explains the difference between simulation results in [6] and our simulation results.

M Proofs of results in Section 4

Proposition 19. For a tree graph T = (V, E) where V = {x1, x2, . . . , xp} and any symmetric matrix A  Rd×d whose absolute values of all the eigenvalues are less than 1, the determinant of the matrix D¯ (T, A), which is defined below, is det(I - A2) p-1

AdT(x1,x1) AdT(x1,x2) · · · AdT(x1,xp) 

AdT(x2,x1) AdT(x2,x2) · · · AdT(x2,xp) 

D¯ (T, A) =   

...

...

...

...

,  

AdT(xp,x1) AdT(xp,x2) · · · AdT(xp,xp)

(M.1)

Proof of Proposition 19. Since the underlying structure is a tree, we can always find a leaf and its neighbor. Without loss of generality, we assume xp is a leaf and xp-1 is xp's neighbor, otherwise we can exchange the rows and columns of D¯ (T, A) to satisfy this assumption. Then we have

dT(xp-1, xp) = 1 and dT(xp, xi) = dT(xp-1, xi) + 1 for all i  [p - 2].

(M.2)

Thus, we have



A0

AdT(x1,x2)

· · · AdT(x1,xp-1) AdT(x1,xp-1)+1

 AdT(x2,x1)

A0

· · · AdT(x2,xp-1) AdT(x2,xp-1)+1

D¯ (T,

A)

=

 



...

...

...

...

...

  . (M.3) 

 

AdT (xp-1 ,x1 )

AdT (xp-1 ,x2 )

···

A0

A1

 

AdT(xp-1,x1)+1 AdT(xp-1,x2)+1 · · ·

A1

A0

Subtracting A times the penultimate row of D¯ (T, A) from the last row of D¯ (T, A), we have

 A0

AdT(x1,x2)

· · · AdT(x1,xp-1) AdT(x1  ,xp-1)+1

 AdT(x2,x1)

A0

· · · AdT(x2,xp-1) AdT(x2  ,xp-1)+1

  

...

...

...

...

...

 . 

AdT(xp-1,x1) AdT(xp-1,x2) · · ·

A0

A1

 

0

0

···

0

A0 - A2

(M.4)

Applying the similar column transformation, we have

 A0

AdT(x1,x2)

· · · AdT(x1,xp-1)

0

 AdT(x2,x1)

A0

· · · AdT(x2,xp-1)

0

  

...

...

...

...

...

 . 

AdT(xp-1,x1) AdT(xp-1,x2) · · ·

A0

0

 

0

0

···

0

A0 - A2

(M.5)

By repeating these row and column transformations, we will acquire diag(I, I - A2, . . . , I - A2),

(M.6)

which has the same determinant as D¯ (T, A). Thus, det(D¯ (T, A)) = det(I - A2) p-1.

The proof of Theorem 5 follows from the following non-asymptotic result.

Theorem 10. Consider the class of graphs T (|Vobs|, max, lmax), where |Vobs|  3. If the number of i.i.d. samples n is upper bounded as follows,

n < max

2(1

- ) log 31/3log3(|Vobs|) - 1 -

-l log 1 - e max

-

max log3(|Vobs |)lmax

2 |Vobs |

,

(1

-

)/5

-

2 |Vobs |

-lmax log

1 - e-

2max 3lmax

(M.7)

42

then for any graph decoder  : Rn|Vobs|lmax  T (|Vobs|, max, lmax)

(T)T

max
(|Vobs |,max ,lmax )

P(T)((Xn1 )

=

T)



.

(M.8)

Proof of Theorem 5. To prove Theorem 5, we simply implement the Taylor expansion log(1 + x) =

 k=1

(-1)k+1

xk k

on (M.7)

in Theorem 10

taking

max





and

|Vobs|



.

It remains to prove Theorem 10.

Proof of Theorem 10. To prove this non-asymptotic converse bound, we consider M models in T (|Vobs|, max, lmax), whose parameters are enumerated as {(1), (2), . . . , (M)}. We choose a model K = k uniformly in {1, . . . , M } and generate n i.i.d. samples Xn1 from P(k). A latent tree learning algorithm is a decoder  : Rn|Vobs|lmax  {1, . . . , M }.
Two families are built to derive the converse bound. We separately describe the families of M graphical models we consider here.

Graphical model family A We specify the structure of trees as full-m trees, except the top layer,

as shown in Fig. 31. All the observed nodes are leaves. The parameters of each tree are set to satisfy

the conditions in Proposition 7. Additionally, we set  = 1 in the homogeneous condition (18) and

set A nodes

to is

be a r=

symmetric matrix. We |Vobs| - 3L. All these

set m = residual

3 and L = log3(|Vobs|), then the number of residual nodes are connected to one of parents of the observed

nodes.

To derive the converse result, we use the Fano's method. Namely, Fano's method says that if the sample size

n

<

(1 - ) log M I(X1; K)

,

(M.9)

then for any decoder

max
k=1,...,M

P(k)

(Xn1 ) = k





-

1 log M

.

(M.10)

We first evaluate the cardinality of this family of graphical models. We first count the number of graphical models with depth 1  k  L in Fig. 31. For a specific order of labels (e.g., 1, 2, . . . , mL),
exchanging the labels in a family does not change the topology of the tree. For instance, exchanging
the position of node 1 and node m, we obtain an identical tree. By changing the orders in the last layer, it is obvious that there are (m!)mL-1 different orders representing the same structure. For the penultimate layer, there are (m!)mL-2 different orders represent an identical structure. Thus, for a specific graphical model with depth k, there are

L-k+1

(mL-k+1 )!

(m!)mi

=

(mL-k+1

)!(m!)

mL

-mL-k+1 m-1

i=L-1

(M.11)

graphical models with the same distribution. Then the number of different structures of graphical models with depth k can be calculated as

(mL)!

(mL-k+1

)!(m!)

mL

-mL-k+1 m-1

.

(M.12)

The total number of different graphical models in the family we consider is

M

=

L k=1

(mL)!

(mL-k+1

)!(m!)

mL

-mL-k+1 m-1

.

(M.13)

43

(a) The full 3-tree. All the observed nodes are leaves, and residual nodes are connected to one of parents of the observed nodes.

(b) The full tree with depth k, where all the internal nodes have three children except the root node. Figure 31: The family A of graphical models considered in the impossibility result.

Using Stirling's formula, we have the following simplification of M :

M



L k=1

2 (mL )mL +1/2 e-mL e(mL-k+1 )mL-k+1 +1/2 e-mL-k+1

1 (e-(m-1) mm+1/2 )(mL -mL-k+1 )/(m-1)

=

L



2 e

mLmL-(L-k+1)mL-k+1+(k-1)/2-(m+1/2)(mL

-mL-k+1

)/(m-1)

k=1



=

2 e

m(L-(m+1/2)/(m-1))mL

L

m-(L-k+1-(m+1/2)/(m-1))mL-k+1+(k-1)/2



k=1

> 2 m(L-(m+1/2)/(m-1))mL m3m/(2m-2)+(L-1)/2

e

(M.14) (M.15) (M.16) (M.17)

and

log M > log

 2 e

+ mL

L

-

m+ m-

1 2
1

log m +

3m 2m -

2

+

L

- 2

1

log m

(M.18)

44

Thus,

log M |Vobs|

>

1 |Vobs|

log

 2 e

+

mL |Vobs|

L

-

m m

+ -

1 2
1

>

log m m

L

-

m+ m-

1 2
1

(a)
>

log 3

3

L

- 1,

+

1 |Vobs|

log

 2 e

log

m

+

1 |Vobs|

3m 2m -

2

+

L

- 2

1

log m (M.19) (M.20)
(M.21)

where inequality (a) is derived by substituting m = 3.

Next we calculate an upper bound of I(X1; K). Since PTk = N (0, obs(Tk)), where obs is the covariance matrix of observed variables, we have [26]

I(X1; K)  ETk D(PTk Q) ,

for any distribution Q. By choosing Q = N (0, I ), lmax|Vobs|×lmax|Vobs| we have

D(PTk

Q)

=

1 2

=

1 2

log det obs(Tk) + trace obs(Tk) - lmax|Vobs| - log det obs(Tk) + trace obs(Tk) - lmax|Vobs|

(M.22)
(M.23) (M.24)

Since we consider models that satisfy the conditions in Proposition 7, the covariance matrix of any two variables is

E xixTj = rAdT(xi,xj).

(M.25)

The covariance matrix (Tk) for all the observed variables and latent variables Vobs  Vhid is

 rAdT(x1,x1)

···

 A r dT(x1,x|Vobs|)

r AdT (x1 ,y1 )

···

 A  r dT(x1,y|Vhid|)

 

...

...

...

...

...

...

 

 A · · ·  A  A · · ·  A  r dT(x|Vobs|,x1)

r dT(x|Vobs|,x|Vobs|)

r dT(x|Vobs |,y1)

r dT(x|Vobs| ,x|Vhid| )

 

r AdT (y1 ,x1 )

···

 A r dT(y1,x|Vobs|)

r AdT (y1 ,y1 )

···

 A r dT(y1,y|Vhid|)

 

  

...

...

...

...

...

...

  

 A · · ·  A  A · · ·  A r dT(y|Vhid|,x1)

r dT(y|Vhid| ,x|Vobs|)

r dT(y|Vhid|,y1)

r dT(y|Vhid|,y|Vhid|)

= I|V|×|V|  r

VB BT H

(M.26)

= I|V|×|V|  r D¯ (Tk, A)

(M.27)

where A  B is the Kronecker product of matrices A and B. Letting r = I, it is obvious that D¯ (T, A) is a positive definite matrix. Furthermore, H - BTV-1B is positive semi-definite matrix, since it is the inverse of the principal minor of D¯ -1. Thus we have

det D¯ (Tk, A) = det(V) det H - BTV-1B

(a)
 det(V) det

H

(=b) det(V)

det(I - A2) , |Vhid|-1

(M.28) (M.29)

where inequality (a) is derived from Minkowski determinant theorem [27], and (b) comes from the fact that all the latent variables themselves form a tree. Also, we have that

det D¯ (Tk, A) = det(I - A2) . |Vhid|+|Vobs|-1

(M.30)

Thus, we have

det(V)  det(I - A2) , |Vobs|

(M.31)

45

which implies that log det obs(Tk)

 log det(r) |Vobs| det(I - A2) |Vobs| = |Vobs| log det(r) det(I - A2) .

(M.32) (M.33)

The mutual information can thus be upper bounded as

I(X1; K)



1 2

|Vobs|(-

log

det(I - A2)

+ trace(r) - log

det(r)

- lmax)

(M.34)

Combining inequalities (M.14) and (M.34), we can deduce that the any decoder will construct the wrong tree with probability at least  if

2(1 - ) log 31/3L - 1 n < - log det(I - A2) + trace(r) - log det(r) - lmax

(M.35)

By choosing r = I and letting the eigenvalues of A are all the same, we have

max

=

-

2L 2

log

det(A2)

= -2lmaxL log

(A)

and

(M.36)

trace(r) - log det(r) - lmax = 0.

(M.37)

Furthermore, we have

log

det(I - A2)

= lmax log 1 - (A)2

= lmax log

1 - e-

max Llmax

.

(M.38)

By

choosing



=



+

1 log(M

)

,

we

have

that

the

condition

n

<

2(1 - ) log 31/3L - 1 -

-lmax log

1 - e-

max Llmax

2 |Vobs|

(M.39)

guarantees that

(T)T

max
(|Vobs |,max ,lmax )

P(T)((Xn1 )

=

T)





(M.40)

Graphical model family B We consider the family of graphical models with double-star substruc-

tures, as shown in Fig. 32. Then the number of graphical models M in this family is lower bounded

as



M

>

1 2 

|Vobs| |Vobs|/2

>

2 |Vobs ||Vobs |+1/2 e-|Vobs |

2 enn+1/2e-n e(|Vobs| - n)|Vobs|-n+1/2e-(|Vobs|-n)

=

2

|Vobs ||Vobs |+1/2

2e2 nn+1/2(|Vobs| - n)|Vobs|-n+1/2

(M.41)

when n = |Vobs|/2. Since n  |Vobs| - n, we further have



M>

2 |Vobs||Vobs|+1/2

2e2

n|Vobs|+1

=

 2e4|Vobs|

|Vobs| n

|Vobs |+1

and

log(M ) |Vobs|

>

|Vobs| + 1 |Vobs|

log

|Vobs| |Vobs|/2

+

1 |Vobs|

log

>

|Vobs| + |Vobs|

1

log 2

+

1 |Vobs|

log

 2e4|Vobs|



2e4|Vobs|

>

1 10

By choosing r = I and letting the eigenvalues of A to be the same, we have

max

=

-

3 2

log

det(A2)

= -3lmax log

(A)

(M.42) (M.43) (M.44) (M.45)

46

Figure 32: The family B of graphical models considered in the impossibility result.

and

trace(r) - log det(r) - lmax = 0.

Furthermore, we have

log

det(I - A2)

= lmax log 1 - (A)2

= lmax log

1 - e-

2max 3lmax

.

By

choosing



=



+

1 log(M

)

,

we

have

that

the

condition

n

<

(1

-

)/5

-

2 |Vobs |

-lmax log

1 - e-

2max 3lmax

guarantees that

(T)T

max
(|Vobs |,max ,lmax )

P(T)((Xn1 )

=

T)





as desired.

(M.46) (M.47) (M.48) (M.49)

47

