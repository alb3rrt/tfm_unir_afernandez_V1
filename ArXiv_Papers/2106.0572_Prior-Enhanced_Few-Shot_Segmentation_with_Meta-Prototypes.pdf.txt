Prior-Enhanced Few-Shot Segmentation with Meta-Prototypes

Jian-Wei Zhang1

Lei Lv1 Yawei Luo1 Hao-Zhe Feng1 1Zhejiang University
2University of Technology Sydney

Yi Yang2

{zjw.cs, lvlei, fenghz}@zju.edu.cn yaweiluo329@gmail.com

Yi.Yang@uts.edu.au chenwei@cad.zju.edu.cn

Wei Chen1*

arXiv:2106.00572v1 [cs.CV] 1 Jun 2021

Abstract
Few-shot segmentation (FSS) performance has been extensively promoted by introducing episodic training and class-wise prototypes. However, the FSS problem remains challenging due to three limitations: (1) Models are distracted by task-unrelated information; (2) The representation ability of a single prototype is limited; (3) Class-related prototypes ignore the prior knowledge of base classes. We propose the Prior-Enhanced network with Meta-Prototypes to tackle these limitations. The prior-enhanced network leverages the support and query (pseudo-) labels in feature extraction, which guides the model to focus on the task-related features of the foreground objects, and suppress much noise due to the lack of supervised knowledge. Moreover, we introduce multiple meta-prototypes to encode hierarchical features and learn class-agnostic structural information. The hierarchical features help the model highlight the decision boundary and focus on hard pixels, and the structural information learned from base classes is treated as the prior knowledge for novel classes. Experiments show that our method achieves the mean-IoU scores of 60.79% and 41.16% on PASCAL-5i and COCO-20i, outperforming the state-of-the-art method by 3.49% and 5.64% in the 5-shot setting. Moreover, comparing with 1-shot results, our method promotes 5-shot accuracy by 3.73% and 10.32% on the above two benchmarks. The source code of our method is available at https://github.com/ Jarvis73/PEMP.
1. Introduction
Deep convolutional neural networks have extensively promoted semantic segmentation accuracy in past years [16, 20, 1, 2, 36]. The inspiring performances usually profit from a supervised manner at the price of huge amounts of densely annotated images [5, 14]. However, collecting
*Corresponding author

pixel-wise object labels at scale is notoriously known as cumbersome, expensive, and sometimes impractical when developing models for novel scenarios. To tackle this challenge, few-shot segmentation (FSS) [21, 19, 22] is developed for image segmentation with a few labeled samples and can generalize to unseen classes quickly.
FSS is derived from the few-shot learning (FSL) [28, 24, 25]. Unlike the standard supervised training scheme that models are trained and evaluated on the same classes, FSL models are trained on base classes with fully labeled images and evaluated on novel classes with only a few labeled samples. Recent FSS methods [33, 29, 35, 7, 31] generally follow an episodic meta-training scheme where the training and testing samples are formulated as tasks, i.e., episodes. Precisely, each episode consists of a few labeled support images and a query image. Support images and labels are adopted to extract task-related features, also known as the "prototypes", which are then leveraged to segment the query image.
Nevertheless, despite the achieved improvements, the meta-learning based approaches particularly suffer from three limitations: (1) Models are distracted by taskunrelated information. In the inference stage of FSS, as the support labels are utilized after the feature extraction, models have no knowledge about the novel class when extracting features. Therefore, the features contain much noise, which distracts models from task-related information. (2) The representation ability of a single prototype is limited. Using a single prototype to represent a class is widely adopted in recent work [35, 29, 26]. However, the prototype is obtained by global average pooling, which deteriorates the feature distribution [31] and degenerates the decision boundary. (3) Class-related prototypes ignore the prior knowledge. In the inference stage, previous methods [33, 29, 31] generate prototypes only utilizing the support features of the novel class. Therefore, the classrelated prototypes do not benefit from the prior knowledge of the base classes.
This paper proposes the Prior-Enhanced network with

multiple Meta-Prototypes (PEMP), which contains two parts to overcome the above limitations. The priorenhanced network is designed to handle the first limitation by leveraging support labels in support feature extraction, which guides the model to focus on foreground objects and reduces task-unrelated information. We introduce a prior network to generate query pseudo-labels, which allows the query features also benefit from the prior-enhanced network. Moreover, since the support and query features are extracted in two separated branches, a Communication Module is designed to establish communication between the two branches for selecting the most discriminate features [10] of the foreground objects. To tackle the remaining two limitations, we introduce multiple meta-prototypes to encode hierarchical features for highlighting the decision boundary and learn structural information for aggregating basic knowledge. Concretely, features in the embedding space are represented by multiple prototypes according to the relative distances between the features and the decision boundary. Therefore, models are encouraged to pay more attention to the features close to the decision boundary, improving the accuracy of hard pixels. We extend standard class-related prototypes by class-agnostic meta-prototypes, which aggregate knowledge of base classes by learning structural information from the training set. The structural information corresponds to the relative distances between features and the decision boundary existing in all the base classes.
We evaluate the proposed method on two benchmarks PASCAL-5i [21] and COCO-20i [14]. The results demonstrate that our method achieves state-of-the-art performance in the 1-shot setting and outperforms state-of-the-arts with a large margin in the 5-shot setting. To sum up, the main contributions of the work are summarized as follows:
· We develop PEMP for few-shot segmentation. We propose a prior-enhanced network to leverage support and query (pseudo-) labels in feature extraction, which guides the model to focus on task-related features and suppress noise information.
· We propose a communication module to break the isolation between the support branch and query branch, which help select the most discriminative features.
· We introduce multiple meta-prototypes that encode hierarchical features to highlight the decision boundary and aggregate prior knowledge of base classes.
· Experiments on PASCAL-5i and COCO-20i show that our method significantly outperforms the state-of-theart performance in the 5-shot setting and can effectively improve segmentation accuracy with more support images.

2. Related Work
Semantic segmentation. Semantic segmentation problem has been extensively investigated for decades. Stateof-the-art methods extend the fully convolutional networks (FCN) [16] by leveraging skip connections [20], atrous convolution [2] or pyramid pooling [36], which are widely adopted in segmentation problems. Based on the atrous convolution, Chen et al. [1] propose an Atrous Spatial Pyramid Pooling (ASPP) layer [2], which is applied in semantic segmentation [1, 2] and FSS [33, 31]. In this work, we follow the structure of FCN with atrous convolution to learn features in the few-shot semantic segmentation. With the extracted features, the prediction is generated based on a metric function as in PANet [29], which is different from the scheme of standard semantic segmentation.
Few-shot segmentation. Shaban et al. [21] introduce few-shot learning into semantic segmentation tasks with a dense prediction strategy, formulating the few-shot segmentation scheme. To sufficiently utilize the supervised knowledge, Zhang et al. [35] propose to generate prototypes by Masked Average Pooling on support features with corresponding labels. Besides, the places of support and query features can be exchanged in feature space because they belong to the same class. Therefore, Wang et al. [29] implement prototype alignment between support and query features by adding an extra loss on support predictions. Zhang et al. [33] propose to refine segmentation results by an iterative optimization module, which can recurrently fine-tune the segmentation results. Yang et al. [31] insert a prototype mixture model on top of the feature extractor. The mixture model generates multiple prototypes representing semantic parts of the foreground objects. Nevertheless, the concrete meaning of each prototype is too ambiguous to explain.
Prior knowledge. Prior knowledge has been widely used in semantic segmentation. Yu et al. [32] propose Context Prior to encode the relationships between pixels. The Context Prior Map is supervised with an Ideal Affinity Map computed from segmentation labels. Cheng et al. [3] address the high-resolution segmentation problem with cascade refinement modules. The input of each module is the prior knowledge from the previous module. Prior knowledge is also investigated in the FSS problem. Tian et al. [26] propose to generate a prior map from high-level features based on a pre-trained feature extractor. Then, the prior map is concatenated with middle-level features and fed into a feature enhancement module. Pambala et al. [18] introduce a semantic meta-learning method that incorporates classlevel semantic description into the prototypes. However, the semantic learning scheme requires an extra embedding dataset that is not always available in practice. Some ef-

Support Label Support Image Query Image

Feature Extractor

Purifier

weights sharing

Feature Extractor

Purifier

MPM

Query Label

cos max

(a) Prior Network
Feature Extractor (Ext)

Support Label

CM

CM

CM

Query Pseudo-Label

Feature Extractor (Ext)
(c) Extended Feature Extractor

Feature Extractor (Ext)

Purifier

weights sharing &
Communicaon

Feature Extractor (Ext)

Purifier

MPM cos max

(b) Segmentation Network

Masked Features Mean avg

Concat Linear & Expand

avg Max
(d) Communication Module

Figure 1. Overview of the proposed two-stage Prior-Enhanced network with Meta-Prototypes. (a) The Prior network (the first stage) is employed to provide query pseudo-labels. (b) The Segmentation network (the second stage) is responsible for making the final prediction by leveraging (pseudo-) labels in feature extraction. (c) Details of the extended feature extractor with (pseudo-) labels. (d) The Communication Module for fusing support features and query features. MPM: Meta-Prototype Module; CM: Communication Module.

forts have been made to utilize the prior knowledge of the base classes [22, 9, 15], where a classifier encodes the classspecific knowledge. However, the classifier size linearly increases with the number of base classes, which results in unlimited model size. Therefore, we propose the metaprototypes to summarize the knowledge of base classes into a fixed number of feature vectors, which efficiently controls the classifier size.
3. Method
Problem Setting Following the training scheme of meta-learning [27], FSS methods perform training with a series of episodes. Each episode is formed as an N -class segmentation task with K support (image, label) pairs (xs, ys) in each class and a query (image, label) pair (xq, yq). The problem is formulated as an N -way K-shot FSS problem where K is a small number. In this work, we focus on the 1-way K-shot problem.
3.1. Baseline
We first set a Baseline model, which is a vanilla model of the metric-based FSS methods. Given a support image xs and a query image xq, we embed them into a feature space F with a feature extractor f, where  denotes trainable parameters and f(xs), f(xq)  F  Rc×h×w, where c, h, w denote the channel, height and width of the features, respectively. Let hsi , hqi  Rc be the feature vectors

of f(xs) and f(xq), respectively, at the spatial location i  {1, 2, · · · , HW }. With the support labels yis  {0, 1},
we have foreground and background prototypes as follows:

pF G =

i

hsi · yis i yis

,

pBG =

i
H

hsi W

· (1 -

-
i

yis) yis

.

The query prediction is obtained by computing the cosine similarity between the hqi and pr, where r  {F G, BG}.
Then we get prediction maps by

y^i

=

softmax(
r

ds(hqi

,

pr )),

where ds(·, ·) denotes the cosine similarity function, and  is a hyper-parameter controlling the distance scale.

3.2. Prior-Enhanced Network

We present the prior-enhanced network to boost the task-related features by capitalizing on support and query (pseudo-) labels in feature extraction. As shown in Fig. 1, the prior-enhanced network is a two-stage model, consisting of a prior network and a segmentation network. Moreover, we design a Communication Module to exchange information between the support and query branches, which further enhances the foreground object features.

Two-stage networks Since query labels are unavailable in the inference stage, we propose the prior network to gen-

erate prior maps, which can be transformed into pseudolabels, as shown in Fig. 1 (a). We first embed support and query images into a low-dimension feature space according to a weight-shared feature extractor and a purifier. Then, we learn multiple prototypes from support features by the Meta-Prototype Module (MPM) (cf. §3.3). The cosine similarity function measures the distances of the prototypes and query features, and the resulting distance maps are merged as a query prior map. Finally, we binarize the prior map as a pseudo-label that is consequently fed into the segmentation network.
From the prior network, we get the pseudo-labels for query images. The segmentation network takes both images and (pseudo-) labels as input for paying attention on taskrelated objects, as shown in Fig. 1 (b). We extend the feature extractor by inserting extra convolutional kernels into multiple layers to leverage (pseudo-) labels, as shown in Fig. 1 (c). The (pseudo-) labels are incorporated into the input layer as an extra channel, which is a strong prior for extracting crucial features. They are also used in Communication Modules to extract the most discriminative foreground features.
When the feature extractor (e.g., Resnet [12]) includes residual paths, the generated features contain too much noise because the residual paths bring low-level features into high layers. Therefore, we introduce a purifier on top of the feature extractor to alleviate this problem. The purifier consists of two convolutional layers and an ASPP layer [2] for reducing noise and enlarging the receptive field of the output features.

Communication Modules The idea of feature communication is inspired by the work of low-light enhancement where the features of multi-exposure images are fused [37]. Communication Modules are applied before each convolutional block for guiding the model to extract the most discriminative features. As shown in Fig. 1 (d), given the intermediate support and query features h~si , h~qi , and the downsampled (pseudo-) labels y~s, y~q, we take the mean and max features on the spatial dimensions within object regions:

otmean

=

1 HW

h~ti · y~it,

otmax

=

max
i

h~ ti

·

y~it,

i

where t  {s, q}. We perform a "merge-and-distribute" strategy to achieve communication between features. Support and query features are first merged and processed by

u = W osmean + oqmean osmax + oqmax + b,

2

2

where W and b are weights and biases of a linear layer, and [ ] is the concatenate operation. The linear layer compresses hundreds of features to only two features, which avoids affecting the original features too much. Then, we copy u

Base Classes Training
Training

Novel Classes Feature Extractor

Meta-Prototypes & Decision Boundary
Adapve Prototypes
Figure 2. Illustration of the proposed multiple meta-prototypes. Meta-prototypes summarize prior knowledge from all the base classes and then are combined with features of novel classes to generate adaptive prototypes. Multiple prototypes are used to encode hierarchical features to highlight the decision boundary.

back to the support and query branches to finish the distribution step.

3.3. Meta-Prototypes Module

We introduce multiple meta-prototypes to better identifying the decision boundary and aggregate prior knowledge from the base classes. As shown in Fig. 2, the knowledge of training images is learned in the feature extractor while the prior knowledge of base classes is aggregated into multiple meta-prototypes. With a specific task, meta-prototypes are transformed into adaptive prototypes and the hierarchical features highlight the decision boundary.
Let qmr denote meta-prototypes with r  {F G, BG} and m = 1, 2, · · · , M , where M is the number of prototypes for each class. We first measure the similarity between support features hsi and each meta-prototype by the Euclidean distance de(hsi , prm). Then, we apply softmax to get attention coefficients of the support features with each meta-prototype as soft assignments:

ir,m =

exp (de(hsi , qmr ))

M l=1

exp

(de

(hsi ,

qlr

))

,

which push feature vectors to the closest meta-prototype. The support features are scaled with the coefficients and averaged over the class-specific spatial regions, yielding adaptive prototypes:

p^rm

=

1 |I r |

xsi · ir,m,

iIr

where Ir denotes the index set of either the objects or the backgrounds, and | · | gives the number of elements.

Finally, multiple query predictions are obtained by measuring the similarities between the query features and multiple adaptive prototypes. We utilize a max operation to merge the predictions:

y^i

=

softmax(
r

max
m

ds(hqi ,

p^rm)),

which is either the query prior map for the prior network or the query prediction for the segmentation network.

3.4. Training Objective
The proposed PEMP is optimized with the cross-entropy loss in two stages. We first train the prior network to predict query prior maps. Then, we fix the prior network parameters, and train the segmentation network for final segmentation results. We introduce a boundary-enhanced weight map for the cross-entropy loss. We assign large weights to the pixels close to object boundaries and small weights to other pixels with an exponential decay function, . Specifically, let B denote a binary map where only object boundary pixels are set to the positive value. The weight map is formulated as

-EDT(¬B)

w = exp

2

+ 1,

(1)

where EDT is the Euclidean distance transform [30], and ¬ is the boolean not operation. The  is a hyper-parameter that adjusts the decay rate of weights from boundary pixels to other pixels. The constant one is added to avoid zero weights for the pixels far from the boundaries. The final loss function is

1

L= HW

wi(yi log y^i + (1 - yi) log(1 - y^i)),

i

which is used to train PEMP.

4. Experiments
4.1. Setup
Datasets We conduct experiments on two datasets: PASCAL-5i [21] and COCO-20i [17]. PASCAL-5i is built from the PASCAL VOC 2012 [6] and extended annotations from BSDS [11]. It consists of more than 15,000 annotated images of 20 classes. The dataset is divided into four splits (5 classes per split) following the label order for cross-validation. COCO-20i is built from MS COCO 2014 dataset [14]. It contains more than 80,000 images for training and more than 40,000 images for evaluation of 80 classes. Similar to PASCAL-5i, we divide the set of classes into four splits. For the details of the classes in each split, please refer to the work of Nguyen et al. [17].

Evaluation metrics We use the mean-IoU and binaryIoU [29] to evaluate our method. The mean-IoU computes average intersection-over-union (IoU) over all the novel classes, while the binary-IoU combines all the novel classes into a single foreground class and takes the average value between the foreground class and the background class. Following PANet [29], we evaluate the model with 1000 random episodes at each testing run. The average score of 5 runs are reported for a stable result.
Implementation details We evaluate our method with VGG-16 [23] and Resnet-50 [12] as the feature extractor. We omit the purifier when using the VGG-16 feature extractor as no residual paths exist. The weights of the feature extractor are pre-trained on ImageNet [4] and fine-tuned on the training set as in previous work [29, 31]. Input images are resized to 401 × 401 and augmented with random flipping, random resizing, and color jitter. SGD is used to train the model with the momentum of 0.9. To accelerate and stabilize the training process, we apply gradient clipping with a norm of 1.1 before updating parameters in each training step [34]. We train the prior network for 90 epochs with a fixed learning rate of 0.001 [29] and the segmentation network for 200 epochs with a fixed learning rate of 0.0035 [31]. The weight decay is 0.0005, and the batch size is 4. We set the hyper-parameter  to 20 as in PANet [29]. The number of prototypes M is 3, and the scale factor  is 5.0 by default.
Baseline We implement a Baseline model for highlighting the contribution of the proposed method. The Baseline model is a vanilla FSS method without the prior-enhanced strategy, the purifier, and multiple meta-prototypes. Details of the Baseline is described in Sec. 3.1.
4.2. Results and Discussion
4.2.1 PASCAL-5i
Quantitative results Table 1 displays the comparison among PEMP and previous works. In the 1-shot setting, compared with the Baseline, PEMP achieves significant improvements by 6.24% and 6.49% with VGG-16 and Resnet50 feature extractor, respectively. Compared with stateof-the-arts, PEMP achieves modest improvements. In the 5-shot setting, PEMP (VGG-16) significantly outperforms the Baseline by 5.01% and outperforms the state-of-the-art method RPMMs (Resnet-50) by 3.49%. Meanwhile, PEMP consistently improves the accuracy in all the splits. The gains from 1-shot to 5-shot are limited for most previous methods. But PEMP gains by 7.32% and 3.73% with the above two feature extractors, respectively. It benefits from the guidance of the supervised knowledge and the metaprototypes that allows the integration of basic knowledge.

Method

FE

split-0

split-1

1-shot split-2

split-3

Mean

split-0

split-1

5-shot split-2

split-3

Mean



OSLSM[21]

33.60 55.30 40.90 33.50 40.80 35.90 58.10 42.70 39.10 43.95 3.15

co-FCN [19]

36.70 50.60 44.90 32.40 41.10 37.50 50.00 44.10 33.90 41.40 0.30

VGG-16

SG-One [35]

40.20 58.40 48.40 38.40 46.30 41.90 58.60 48.60 39.40 47.10 0.80

PANet [29]

42.30 58.00 51.10 41.20 48.10 51.80 64.60 59.80 46.50 55.70 7.60

FWB [17] RPMMs [31] Baseline PEMP (Ours) CaNet [33] PFENet [26] RPMMs [31] Baseline PEMP (Ours)

Resnet-50

47.04 47.14 42.69 50.05 53.21 55.89 53.86 45.48 55.74

59.64 65.82 56.43 64.22 64.07 65.89 66.45 59.97 65.88

52.61 50.57 47.59 53.65 49.58 49.03 52.76 51.35 54.12

48.27 48.54 42.19 45.97 50.98 53.10 51.31 43.31 50.34

51.90 53.02 47.23 53.47 54.46 55.98 56.10 50.03 56.52

50.87 50.00 51.91 57.10 55.12 58.00 56.28 52.47 58.59

62.86 66.46 63.98 68.46 66.57 67.21 67.34 66.31 69.10

56.48 51.94 58.84 62.54 50.42 49.31 54.52 59.85 60.31

50.09 55.08 3.18 47.64 54.01 0.99 48.40 55.78 8.55 55.06 60.79 7.32 53.00 56.28 1.82 52.73 56.81 0.83 51.00 57.30 1.20 51.02 57.41 7.38 53.01 60.25 3.73

Table 1. Mean-IoU results of 1-shot and 5-shot segmentation on the PASCAL-5i. The results marked by  are re-evaluated on the same validation pairs as ours.  denotes the differences between the 1-shot and 5-shot results. FE: Feature extractor.

Method

FE

1-shot 5-shot 

OSLSM[21]

61.30 61.50 0.20

co-FCN [19]

60.10 60.20 0.10

SG-One [35] PANet [29]

VGG-16

63.90 66.50

65.90 2.00 70.70 4.20

Baseline

65.51 70.24 4.73

PEMP (Ours)

70.50 75.69 5.19

CaNet [33]

67.23 69.58 2.35

PFENet [26]

71.42 72.63 1.21

RPMMs [31] Resnet-50 70.32 -

-

Baseline

67.58 71.90 4.32

PEMP (Ours)

71.41 73.84 2.43

Table 2. Binary-IoU results of 1-shot and 5-shot segmentation on the PASCAL-5i. The results marked by  are re-evaluated on the same validation pairs as ours.  denotes the differences between
the 1-shot and 5-shot results. FE: Feature extractor.

Table 2 display the results in binary-IoU. PEMP outperforms state-of-the-arts in most of the settings. For the 1-shot setting with Resnet-50, PEMP still achieves a comparable result with PFENet [26].
PEMP makes prediction with a non-parametric cosine similarity function as in PANet [29]. Previous methods achieving the top performance, such as CaNet [33] and RPMMs [31], make prediction by relation networks. In this view, the experiments indicate that non-parametric predictors can also achieve state-of-the-art performance, suggesting that learning a good embedding is important for FSS.
Qualitative results We present some qualitative results of our method in Fig. 3. The first row gives support and query images from different classes, and the second row is the predictions, which indicates PEMP can produce accu-

rate segmentation results even with one support image. The last row presents the response maps to multiple foreground prototypes and background prototypes. They are generated from the indices of the max operation. From the response maps, one can observe that an object can be captured by one prototype (e.g., the boat) or multiple prototypes (e.g., the bird), which depends on the similarity of the support and query objects. The response maps reflect the hierarchical features encoded by multiple meta-prototypes. Specifically, for query pixels determinedly belonging to the object (e.g., inner pixels of the bird), they have a higher response to foreground prototypes FG-1 and FG-2. For those pixels less similar to the object (e.g., border pixels of the bird and all the boat pixels), they are close to the decision boundary and have a higher response to prototype FG-3. Background pixels also suggest the hierarchical structure (e.g., the train). With multiple meta-prototypes, PEMP can recognize easy pixels with high confidence and pay more attention to hard pixels close to the decision boundary.
4.2.2 MS COCO-20i
The results of the COCO-20i dataset are shown in Table 3. COCO-20i has more fine-grained categories than PASCAL-5i, and is more challenging. For the results in mean-IoU, PEMP marginally outperforms state-of-the-arts in the 1-shot setting and makes a significant improvement of 5.64% in the 5-shot setting. We list the results in binaryIoU for reference. Like the observation in PASCAL-5i experiments, PEMP achieves a remarkable improvement by 10.32% when more support images are available.

cow

chair

bird

bole

horse

boat

train

car

response map query pred supp. & query

FG-1 FG-2 FG-3 BG-1 BG-2 BG-3
Figure 3. Samples of the segmentation results with the proposed method on the Pascal-5i dataset and 1-shot setting. The first row shows the support images (at the corner) and query images. The second row shows the query predictions, and the last row presents response maps of the query features to multiple prototypes. The meaning of the colors in response maps is denoted in the right bottom corner.

Met. Method

1-shot

5-shot



split-0 split-1 split-2 split-3 Mean split-0 split-1 split-2 split-3 Mean

b-IoU m-IoU

PANet [29]

-

-

-

- 20.90 -

-

-

- 29.70 8.80

FWB [17]

16.98 17.98 20.96 28.85 21.19 19.13 21.46 23.93 30.08 23.65 2.46

RPMMs [31] 29.53 36.82 28.94 27.02 30.58 33.82 41.96 32.99 33.33 35.52 4.94

PEMP (Ours) 29.28 34.09 29.64 30.36 30.84 39.08 44.59 39.54 41.42 41.16 10.32

A-MCG [13] -

-

-

- 52.00 -

-

-

- 54.70 2.70

PANet [29]

-

-

-

- 59.20 -

-

-

- 63.50 4.30

PEMP (Ours) 60.41 64.46 64.49 63.14 63.13 67.73 72.48 70.05 72.56 70.71 7.58

Table 3. Comparison of the proposed method and state-of-the-art methods on the MS COCO-20i dataset. All the results are acquired based on the Resnet-50 feature extractor.  denotes the difference between the 5-shot results and the corresponding 1-shot results. Met.: Metric.

4.3. Ablation Study
Structure analysis We conduct ablation experiments to display the contribution of each module as shown in Table 4. The output of the prior network is not only a pseudo-label but a query prediction. With the purifier, the prior network achieves an improvement of 2.03% comparing with the Baseline. When introducing the segmentation network, the accuracy is further promoted by 1.69%. Meta-prototypes achieve apparent improvements by 1.77% and 2.34% in the prior network and the segmentation network, respectively. Moreover, with Communication Modules, the accuracy is further promoted by 0.43% and achieves 56.52%. These experiments suggest that the proposed modules consistently improve prediction accuracy, and their contributions are effectively accumulated.

Meta-prototypes In Fig. 4, we conduct experiments to compare the performance with different meta-prototype numbers M and the performance with an alternative fusion strategy--the sum operation for merging the predictions from multiple prototypes. The experiments are conducted in the prior network, and the number of foreground prototypes and background prototypes are changed simultaneously. One can observe that the model reaches the highest score when M = 3. Using only one meta-prototype gives the worse result as the hierarchical information cannot be properly represented. Besides, excessive prototypes have no further contribution.
Yang et al. [31] merge multiple probability maps by directly summing them up. We explore the fusion strategy of multiple prototypes in Fig. 4. One can observe that the max operation has better performance than that of the sum operation. We conjecture that the softmax in MPM forces the model to push multiple meta-prototypes encod-

Accuracy Accuracy

Model PN SN MP CM Accuracy

Baseline

50.03

1-stage

52.06 53.83

53.75

2-stage

56.09 56.52

Table 4. Ablation experiments of the proposed modules. The mean-IoU results are reported in the 1-shot setting and the Resnet50 feature extractor. PN: Prior Network; SN: Segmentation Network; MP: Meta-Prototypes; CM: Communication Modules.

54.0 53.6 53.2 52.8 52.4 52.0
1

2

3

4

Number of Prototypes

max sum
5

Figure 4. Ablation experiments of the number of meta-prototypes and the fusion strategy.

ing discrepant information. Merging multiple predictions by the max operation better conforms to the design of metaprototypes.
Complexity analysis We compare the model complexity of PEMP with previous methods in Table 5. Although PEMP is a two-stage model, the number of parameters does not increase too much comparing with one-stage methods, such as CaNet and RPMMs, and is even fewer than PFENet. The parameter number of the prior network is also listed for reference. To demonstrate that enlarging a model has no extra help for the performance, we fine-tune the prior network with a Resnet-101 feature extractor with more parameters than PEMP. The results suggest that it gets worse than PEMP or the prior network using Resnet-50, which indicates the importance of adequately leveraging the supervised knowledge in feature extraction.
The last column of Table 5 shows the inference speed of different methods. We unify the input size of all the methods to 401 × 401 for a fair comparison. PEMP is computationally efficient with the highest performance, achieving a real-time inference speed of 26 FPS. The prior network (Resnet-50) and CaNet have a higher inference speed of 49 FPS and 42 FPS, but their performance is limited. The prior network (Resnet-101) and PFENet have more parameters than PEMP, and their speeds are slower with 22

Model

Resnet #Param. Accuracy Speed

Prior network

12.0M 53.83

49

CaNet [33]

19.0M 54.46

42

PFENet [26] 50 34.4M 55.98

22

RPMMs [31]

19.6M 56.10

23

PEMP (Ours)

23.9M 56.52

26

Prior network 101 27.5M 53.77

22

Table 5. Model complexity and inference speed comparison with state-of-the-arts. The prior network is a one-stage model. The unit of the speed is frame per second.

Method

Accuracy

w/o weight map 52.63 with weight map 53.83
Table 6. Results comparison when using or omitting the weight map in the model training. The experiments are conducted with the prior network in the 1-shot setting.

56.5

56.0 1-shot 5-shot
54.0

53.5

3

4

5

6

7

Figure 5. Comparison of different  values that are used in the weight map of the loss function.

FPS. Although RPMMs have fewer parameters than PEMP, it embeds a mixture model and multiple residual branches in the network, slowing down the inference speed.
Loss weights Table. 6 displays the segmentation results of using or omitting the weight map in the model training. It indicates that increasing the weights of boundary pixels helps the model improve segmentation accuracy. Fig. 5 shows the results using different  values, which are described in Eqn. (1). One can observe that the best performances of 1-shot and 5-shot are obtained at  = 4 and  = 6, respectively. Therefore, we take the average value of  = 5 by default.
5. Conclusion
We propose PEMP for few-shot segmentation based on learning more accurate features. The prior-enhanced network leverages the supervised knowledge in feature extraction, which guides the model to focus on the task-related ob-

jects and suppress noise information. With multiple metaprototypes, the model aggregates prior knowledge from the base classes to enhance the prototypes for novel classes and encodes hierarchical features to highlight the decision boundary. Experiments on PASCAL-5i and COCO-20i demonstrate that the proposed method improves the 5-shot results with a large margin comparing with either state-ofthe-arts or the corresponding 1-shot results.
References
[1] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834­848, Apr. 2018.
[2] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision ­ ECCV 2018, Lecture Notes in Computer Science, pages 833­851, Cham, 2018. Springer International Publishing.
[3] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung Tang. CascadePSP: Toward Class-Agnostic and Very HighResolution Segmentation via Global and Local Refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
[4] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li FeiFei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248­255, June 2009.
[5] Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The Pascal Visual Object Classes Challenge: A Retrospective. International Journal of Computer Vision, 111(1):98­136, Jan. 2015.
[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.
[7] Siddhartha Gairola, Mayur Hemani, Balaji Krishnamurthy, and Ayush Chopra. SimPropNet: Improved Similarity Propagation for Few-shot Image Segmentation. In Twenty-Ninth International Joint Conference on Artificial Intelligence, volume 1, pages 573­579, July 2020.
[8] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. DropBlock: A regularization method for convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31, pages 10727­10737. Curran Associates, Inc., 2018.
[9] Spyros Gidaris and Nikos Komodakis. Dynamic Few-Shot Visual Learning Without Forgetting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
[10] Fusheng Hao, Fengxiang He, Jun Cheng, Lei Wang, Jianzhong Cao, and Dacheng Tao. Collect and select: Semantic alignment metric learning for few-shot learning. In

The IEEE International Conference on Computer Vision (ICCV), Oct. 2019. [11] B. Hariharan, P. Arbela´ez, L. Bourdev, S. Maji, and J. Malik. Semantic contours from inverse detectors. In 2011 International Conference on Computer Vision, pages 991­998, Nov. 2011.
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770­778, 2016.
[13] Tao Hu, Pengwan Yang, Chiliang Zhang, Gang Yu, Yadong Mu, and Cees G. M. Snoek. Attention-Based Multi-Context Guiding for Few-Shot Semantic Segmentation. Proceedings of the AAAI Conference on Artificial Intelligence, 33:8441­ 8448, July 2019.
[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision ­ ECCV 2014, Lecture Notes in Computer Science, pages 740­755, Cham, 2014. Springer International Publishing.
[15] Lizhao Liu, Junyi Cao, Minqian Liu, Yong Guo, Qi Chen, and Mingkui Tan. Dynamic Extension Nets for Few-shot Semantic Segmentation. In Proceedings of the 28th ACM International Conference on Multimedia, pages 1441­1449, Seattle WA USA, Oct. 2020. ACM.
[16] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431­3440, 2015.
[17] Khoi Nguyen and Sinisa Todorovic. Feature weighting and boosting for few-shot segmentation. In The IEEE International Conference on Computer Vision (ICCV), Oct. 2019.
[18] Ayyappa Kumar Pambala, Titir Dutta, and Soma Biswas. SML: Semantic Meta-learning for Few-shot Semantic Segmentation. arXiv:2009.06680 [cs], Sept. 2020.
[19] Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alyosha Efros, and Sergey Levine. Conditional networks for few-shot semantic segmentation. ICLR 2018 Workshop, 2018.
[20] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234­241. Springer, 2015.
[21] Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation. BMVC 2017 arXiv preprint arXiv:1709.03410, 2017.
[22] Mennatullah Siam, Boris N. Oreshkin, and Martin Jagersand. AMP: Adaptive masked proxies for few-shot segmentation. In The IEEE International Conference on Computer Vision (ICCV), Oct. 2019.
[23] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
[24] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical Networks for Few-shot Learning. In I. Guyon,

U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4077­4087. Curran Associates, Inc., 2017.
[25] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales. Learning to Compare: Relation Network for Few-Shot Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1199­1208, 2018.
[26] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Prior Guided Feature Enrichment Network for Few-Shot Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1­1, 2020.
[27] Joaquin Vanschoren. Meta-Learning: A Survey. arXiv:1810.03548 [cs, stat], Oct. 2018.
[28] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching Networks for One Shot Learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3630­3638. Curran Associates, Inc., 2016.
[29] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. PANet: Few-shot image semantic segmentation with prototype alignment. In The IEEE International Conference on Computer Vision (ICCV), Oct. 2019.
[30] Dong Xu and Hua Li. Euclidean Distance Transform of Digital Images in Arbitrary Dimensions. In Yueting Zhuang, Shi-Qiang Yang, Yong Rui, and Qinming He, editors, Advances in Multimedia Information Processing - PCM 2006, Lecture Notes in Computer Science, pages 72­79, Berlin, Heidelberg, 2006. Springer.
[31] Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang Ye. Prototype Mixture Models for Few-Shot Semantic Segmentation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision ­ ECCV 2020, volume 12353, pages 763­778. Springer International Publishing, Cham, 2020.
[32] Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, and Nong Sang. Context Prior for Scene Segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
[33] Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. CANet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
[34] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity. In International Conference on Learning Representations, Sept. 2019.
[35] X. Zhang, Y. Wei, Y. Yang, and T. S. Huang. SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation. IEEE Transactions on Cybernetics, 50(9):3855­3865, Sept. 2020.
[36] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid Scene Parsing Network. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
[37] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. EEMEFN: Low-Light Image Enhancement via EdgeEnhanced Multi-Exposure Fusion Network. Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):13106­13113, Apr. 2020.

6. Supplementary Material
We present more details of the proposed Prior-Enhanced network with Meta-Prototypes (PEMP) in Sec. 6.1. We conduct extra ablation experiments about prior map transformation strategy and show the results in Sec. 6.2. Extra qualitative results of PASCAL-5i and COCO-20i are shown in Sec. 6.3.
6.1. Details of the Purifier
We adopt modified VGG-16 and Resnet-50 as the feature extractor of the prior-enhanced network. Following previous work [33, 29], the last three convolutional layers of VGG-16 are replaced by dilated convolutional layers with a dilation rate of 2. For Resnet-50, only the first three residual blocks are reserved. Dilated convolution is applied to all the 3 × 3 convolutional layers in the third block. Different from CaNet [33], we do not fuse the multi-scale features of the second block and the third block, as no obvious improvement is observed in the experiments of the proposed method.
For Resnet, since residual paths bring low-level features into high-level layers, the feature extractor's outputs contain much noise. We introduce a purifier on top of the feature extractor to improve the smoothness of features. The purifier consists of a 1 × 1 convolutional layer used to reduce the number of channels, a 3 × 3 convolutional layer for smoothing the features, and an Atrous Spatial Pyramid Pooling (ASPP) [2] for exploring multi-scale information. Details of the purifier are shown in Fig. 6 and Fig. 7. We implement the prior network's purifiers and the segmentation network with sightly different structures to alleviate the overfitting problem. Concretely, for the prior network, the DropBlock layer [8] randomly drop image blocks of size 4 × 4 with a drop rate of 0.1. For the segmentation network, the Dropout layer has a drop rate of 0.5 for a stronger regularization. For VGG-16, we do not add the extra purifier for reducing the number of parameters, which still gives a good performance.
6.2. Ablation Study
Strategy split-0 split-1 split-2 split-3 Mean
Identity 54.90 65.65 54.82 48.41 55.95 Binary 55.74 65.88 54.12 50.34 56.52 Table 7. Mean-IoU of PEMP on PASCAL-5i with different strategies that how the query prior map is integrated into the segmentation network. Identity: the segmentation network directly takes the prior map as the pseudo-label of the query image; Binary: The prior map is binarized with a threshold of 0.5 and then treated as the pseudo-label.

Conv 1×1@256 ReLU
DropBlock
Conv 3×3@256 ReLU
DropBlock
ASPP

Image Pooling

Dilation {1, 6, 12, 18}

BatchNorm DropBlock Conv 1×1@256
ReLU

BatchNorm

BatchNorm

DropBlock

DropBlock

l

Conv 1×1@256 D1

Conv 3×3@256 D18

ReLU

ReLU

Concatenate

Conv 1×1@512

Figure 6. Detailed structure of the purifier in the prior network.

Conv 1×1@256 ReLU+Dropout
Conv 3×3@256 ReLU+Dropout
ASPP

Image Pooling

Dilation {1, 6, 12, 18}

Conv 1×1@256 ReLU+Dropout

Conv 1×1@256 D1

Conv 3×3@256 D18

ReLU+Dropout l ReLU+Dropout

Concatenate

Conv 1×1@512

Figure 7. Detailed structure of the purifier in the segmentation network.
Prior map transformation strategy The prior network produces a two-channel prior map which is the output of Softmax. We compare two strategies that how the prior map is transformed into the pseudo-label.
· Identity The prior map is directly copied to the input layer as a pseudo-label. The range of prior map values is [0, 1], while the range of the support label values is {0, 1}.
· Binary The prior map is binarized with a threshould of 0.5, and threfore "1" denotes foreground pixels and "0" denotes background pixels. Then, the pseudo-label has the same meaning with support labels.
Table. 7 shows the mean-IoU of two strategies in the 1shot setting. Binary strategy gives the best performance on average. The Identity strategy has a lower score due to the misalignment of ranges between the support label and the pseudo-label. Although the binary pseudo-label has errors comparing with the real label, we treat the errors as data augmentation, which plays a role of regularizer. This regularization help the model avoid overfitting to the pseudolabels but learn to fine-tune the pseudo-labels.

6.3. More qualitative results
Fig. 8 shows the results in the 5-shot setting on PASCAL-5i. Fig. 9 shows the results in the 1-shot setting on COCO-5i.

support images

query image query pred response map

bird

aeroplane

boat

horse

car

bus

person

sheep

bole

FG-1 FG-2 FG-3 BG-1 BG-2 BG-3 Figure 8. Qualitative results of our method in the 5-shot setting on PASCAL-5i.

tv

zebra

umbrella

stop sign

frisbee

vase

tennis racket suitcase

response map query pred Query image support image

FG-1 FG-2 FG-3 BG-1 BG-2 BG-3
Figure 9. Qualitative results of our method in the 1-shot setting on COCO-20i.

