Leveraging Pre-Images to Discover Nonlinear Relationships in Multivariate Environments

M. Ali Vosoughi

Axel Wismu¨ller1,2,3,4

Graduate Student Member, IEEE

1Dept. of Imaging Sciences, 2Electrical and Comp. Eng., 3Biomed. Eng.,

Dept. of Electrical and Comp. Eng.

University of Rochester Medical Center, Rochester, NY, USA

University of Rochester, Rochester, NY, USA 4Faculty of Medicine, Ludwig Maximilian University, Munich, Germany

Email: mvosough@ece.rochester.edu

arXiv:2106.00842v1 [cs.LG] 1 Jun 2021

Abstract--Causal discovery, beyond the inference of a network as a collection of connected dots, offers a crucial functionality in scientific discovery using artificial intelligence. The questions that arise in multiple domains, such as physics, physiology, the strategic decision in uncertain environments with multiple agents, climatology, among many others, have roots in causality and reasoning. It became apparent that many real-world temporal observations are nonlinearly related to each other. While the number of observations can be as high as millions of points, the number of temporal samples can be minimal due to ethical or practical reasons, leading to the curse-of-dimensionality in largescale systems. This paper proposes a novel method using kernel principal component analysis and pre-images to obtain nonlinear dependencies of multivariate time-series data. We show that our method outperforms state-of-the-art causal discovery methods when the observations are restricted by time and are nonlinearly related. Extensive simulations on both real-world and synthetic datasets with various topologies are provided to evaluate our proposed methods.
Index Terms--causal graphical models, discovering nonlinear interactions, network inference, causality, large-scale time-series
I. INTRODUCTION
Causal relationships are meaningful and apply to many problems, as evidenced by the DARPA's recent challenge on machine common-sense (MCS) [1]. Such applications require an AI machine with a mechanism to understand and reason the system's underlying relations, which solo statistics cannot plead (see [2] for the condition that statistics fail). One may argue that such AI machines are causal because AI that finds rules is equivalent to AI that finds causality: when a causal relationship is strong enough, we call it a rule [3, Chapter 1]. An inherent advantage of learning reasons is learning with few samples. Causality gained the most traction recently to build models with sufficient robustness, generalizability, interpretability, and the capability to discover semantically meaningful representations.
The completeness of identifiability and development of docalculus paved the way for causal inference using observa-
This research was partly funded by an Ernest J. Del Monte Institute for Neuroscience Award from the Harry T. Mangurian Jr. Foundation. This work was conducted as a Practice Quality Improvement (PQI) project related to American Board of Radiology (ABR) Maintenance of Certificate (MOC) for Prof. Dr. Axel Wismu¨ller. We thank Dr. John Foxe, Institute for Neuroscience, University of Rochester, for his support.

tional data [2]. However, using solo temporal measurements, the generative process underneath data is unknown, making the use of do-calculus unsettled. Granger causality (GC) is widely used to infer causality in the time-series data given the faithfulness assumption. GC initially stated for linear relations [4], and states that a time-series A is a cause for time-series B if A causes a better prediction for B.
Although there is a rich body of literature on discovering nonlinear relations from time-series data in the recent decade, yet it is a challenging problem for two reasons: 1) the detected nonlinear relations can be spurious, which requires a multivariate approach, and 2) the curse-of-dimensionality and illposedness becomes the main problem with an increasing number of nodes [5], [6]. Various algorithms have been proposed to recover nonlinear relations in networks, including Ancona's work [7], where they present conditions to detect nonlinear relationships, and in [8] Marinazzo develops a method, called kernel Granger causality (KGC) to consider spurious relations using kernel functions. Unfortunately, their approach in [7] is bivariate and cannot be extended to multivariate settings, and [8] cannot be applied to large-scale problems due to over-fitting. In parallel with the Marinazzo approach, another method called transfer entropy (TE) was proposed by Schreiber [9], which formulates the causal relationship among time-series from the information theory's perspective. In his 2009 paper, Barnett proved that the TE and GC were equivalent for Gaussian variables [10]. Therefore, the KGC under the Gaussian assumption is equal to TE. These two lines of work (TE and KGC) have received considerable attention.
Several algorithms have been proposed to extended the previously discussed approaches to multivariate time-series [5], [11]­[15]; however, the curse-of-dimensionality of the large-scale problem yet is an open problem [5]. Note that in real-world studies such as climatology, the dimensions of the problem can reach millions of variables [16], so in the presence of short time-series (just a few decades of temperature measurement), the inverse problems quickly become ill-posed due to the presence of redundant variables. In large-scale problems, TE-based methods are costly based on their dependency on estimating the probability distribution functions. Our analysis of a recent TE algorithm based on the Kraskov estimation [12] for a graph with 34 nodes takes 26 hours on an Nvidia GeForce

1080-Ti GPU and tens of times more on a CPU, while the cost increases drastically with the number of nodes.
The closest study to our work in terms of the ongoing open problem is the PCMCI method [5]. Peter and Clark Momentary Conditional Independence (PCMCI) is a two-step algorithm, with an statistical independence test in the PC stage, and obtaining causality strengths of the significant links in the MCI stage [5]. Besides, because our method is a nonlinear Granger causality under Gaussian assumption, it is equivalent to the TE [10]­[12] for Gaussian variables. However, our proposed method differs from the above methods in two ways. First, since it is a nonlinear GC method, it is equivalent to the TE method for Gaussian variables, while it is also suitable for large-scale problems. On the other hand, although PCMCI is proposed for large-scale problems, the method we present provides the ability to apply to short time-series on a large scale and provides all steps without examining nonlinear independencies using a partial correlation as is in the PCMCI. We believe our contribution adds numerous capabilities to the existing literature, affirmed through extensive simulations on synthetic and real-world datasets.
II. PRELIMINARIES
In an abuse of notation, we use i, j and Yi, Yj to point to the nodes and random variables on the nodes. Scalars are small letters, vectors are in small bold letters, and matrices are in bold capital letters. Calligraphic letters N , E, H, and Y represent the set of nodes, set of weighted edges, feature space (Hilbert subspace), and input space, correspondingly.
a) Causality assumptions: A variable i is said to be a cause of a variable j if j can change in response to change in i. Consider a directional graphical model G = (N , E) consisting of the set of nodes N = {i|i = 1, . . . , N } and the set of directed edges E, whose topology is unknown, but time-series {yit}Tt=1 are observed per node i, over T time intervals. We assume that edges are causal, meaning that every parent i is a direct cause of all its children C(i), and therefore i is a parent of j, denoting as i  P(j). Furthermore, we assume that measurements {yit}Tt=1 are causally sufficient, meaning that there are no unobserved confounders of any of the variables in the graph1. Our goal is to estimate causal quantities ij between each pair of the nodes i and j in the presence of all other nodes Z = N \ {i, j}, and derive the topology of the graph as E = {ij|i, j  N }, such that i  P(j) if ij > 0 else i  P(j). We assume that ij is identifiable, therefore we can compute it from a purely statistical quantity2 [2]. We use faithfulness assumption to obtain causal relations among the nodes N , which allows us to infer d-separations in the graph from in dependencies in the distributions YiGYj|Z  YiP Yj|Z. The notion

 implies independence in the graph G, or the distribution P (Y1, . . . , YN ) where Yi denotes the random variable corresponding to node i. Note that faithfulness is a weak
assumption; however, without it, we cannot infer the topology of the causal graph from the observational time-series3.
b) Causal quantities: Linear vector autoregressive model
(VARM) is widely adopted framework to infer Granger causal-
ity in multivariate time-series, in which a Granger causality index ij can be defined as a quantification of prediction quality as follows. VARM states that each observed yt := [y1t, . . . , yNt] is a linear combination of the time-lagged versions of the measurements {{yi(t- )}Ni=1}L=1. Let A  RN×N denote the "time-lagged" VARM parameters matrix, with [A ]ij = aij, and aij as model coefficients over a lag of time points. Given the time-lagged multivariate timeseries {y(t- )}Lt= , where , the goal is to estimate the model parameter matrices {A }L=1 in:

L

yt = A y(t- ) + et,

(1)

=1

and minimize the residual errors et using an optimization method, such as ordinary least squares (OLS), the lasso, the
ridge, or the elastic-net regressions [18].
Let E := [e , . . . , eT ] be the matrix of residuals of VARM for the full system including all nodes N , and let Ei- := [ei-, . . . , eiT-] to be the same matrix when the node i is excluded N \ {i} from the VARM, where each eit- is obtained using yti- := [y1t, . . . , y(i-1)t, y(i+1)t, . . . , yNt] in expression (1). Then, the derivation of error covariance matrices  = cov(E, E) and i- = cov(Ei-, Ei-) will be straightforward. Based on  of the full VARM and i- of the
VARM without i, the degree of information flow from node i to node j can be quantified by ln(ij-/j), where ij- and j denote the diagonal entries of i- and  associated to node j, respectively. Consequently, the topology of the causal
graph can be inferred using:

ij =

ln(ij-/j ), 0,

ln(ij-/j) > 0 ln(ij-/j)  0,

(2)

as E = {ij|i, j  N , i  P(j) if ij > 0 else i  P(j)}. c) Problem Statement: Given nodal measurements in (1),
the aim is to solve VARM in the reproducing kernel Hilbert space (RKHS) and then revert the solution (residuals) to the input space to infer the topology in the input space. The benefit is to encompass nonlinearities while reducing dimensionality to address the curse-of-dimensionality.

III. THE PROPOSED METHOD
The proposed method consists of three steps using three offthe-shelf modules, as shown in Fig. 1. The first step involves a

1Note that we cannot test unconfoundedness, and cannot guarantee that it is satisfied. In other words, no unobserved confounding is unrealistic assumption, and we will always have unobserved confounders [17]
2Identifiablity requires to have four assumptions: unconfoundness, positivity, consistency, and no interference.

3The reason is related to global Markov assumption, where given that P is Markov with respect to graph G one can use G to infer independencies in P . However, in case of the unstructured time-series data, the graph topology is priory unknown and we cannot use Markov assumption before obtaining the graph topology.

Reproducing kernel Hilbert space

2-Logistic

3-Fan Out

3-Fan In

Input space

Input space

(a) 2- 5-LLineaor gistic

(b) 3-Fan 5-Nonlinear out.

(c) 3-Fan in.

Impossible due to curse of dimensionality

Topology inference without curse-of-dimensionality

Fig. 1: The proposed method involves three steps: kernel PCA, solving linear vector autoregressive model (VARM) in feature space, and using pre-images to infer topology in input space.

transformation of the input space to the dimensionality reduced (or lifted) feature space using kernel principal component (kernel PCA), the second step involves solving the VARM of (1) in the feature space, and the third step which is the essential part of the method, is to revert the residuals of the VARM in the feature space into the input space using preimages, and finally infer the topology of the graph E in the input space.
a) Dimensionality reduction using kernel PCA: The aim is to transfer the equation (1) to the space of the principal components with lower dimensions, encompassing nonlinear dependencies. We define RKHS function (.) as (yjt) : Y  H to project time-series measurements into the feature space. A feature space related to the principal components with the maximum explained variance of data would be desired; therefore, kernel PCA is of interest. Altough P < N as the dimension of the feature space is desired, N  P <  can be also used to to lift the space (rather than reducing). Therefore, {p(.)}Pp=1 builds the basis of feature space, and (yit) := [1(yit), . . . , P (yit)] . A projection of input data yjt onto the p -th principal component would be desired, such that the major proportion of data variation is explained by a few principal components [19].
b) Solving the VARM in the reproducing kernel Hilbert space: Using the RKHS function transformation, the feature space representation of (1) will be:

LN

pt =

jpp(yj(t- )) + pt

(3)

=1 j=1

where {pt}Tt=1 for p = 1, . . . , P is the transformed timeseries into the feature space, and {pt}Pp=1 is the error at time slot t in the feature space. The expression (3) is a linear VARM
and can be derived with the same approach as discussed in section II. Once the {pt}Pp=1 in (3) are obtained , the predictions {^pt}Pp=1 will be transformed into the input space to infer the graph topology.
c) Estimating pre-images: Let Y := [y1, . . . , yT ], (pt) := [1(pt), . . . , N (pt)] , p := [(p1), . . . , (pT )] ,  := [1, . . . , P ], j := [(yj1), . . . , (yjT )] ,

(d) 5-linear.

(e) 5-nonlinear.

Fig. 2: The topologies of datasets, which have a different number of nodes {2, 3, 5}, and different characteristics {chaotic (a), linear (d), nonlinear (b, c, e)}. Equations governing these networks are detailed in [25].

and  := [1, . . . , P ], and the notations Yj- :=
[y~1, . . . , y~j-1, y~j+1, . . . , y~N ], ~p := [p1, . . . , pT ] , H := [~1, . . . , ~P ] for the sake of simplicity. To transform {^pt}Pp=1 into the input space and obtain the pre-image (^pt) : H  Y, one needs to estimate y^jt = arg min yjt - ((y^jt)) 2,
y^j t Y
which is a nonlinear optimization problem, and is not inter-

esting as a solution to find pre-images [19], [20]. However, interestingly, both the inputs {yit}Tt=1 and their corresponding mappings are present for (yjt) : Y  H, easing the

pre-image problem to a simple learning problem. Therefore, we use the learning problem L = arg min Y -  2

instead, which is adopted from [20] to directly obtain co-

efficients jp := []jp, and consequently the predictions

y^jt =

P p=1

jp^pt

will

be

obtained.

Simultaneously,

the

topology of the graph can be obtained as discussed in section

II. The complete algorithm is shown in Algorithm. 1. Note that

novel statistical frameworks such as [21]­[24] can be used to

formulate the problem in a neural network, which can be as a

future work.

Algorithm 1 The proposed method leveraging pre-images

Require: L, P, Y, (.)

Ensure:

E



{ln(

ij- j

)}Nj=1



ln(

diag{cov(Yi--Y^ i- diag{cov(Y-Y^ )}

)}

)

  L  Y  normalize(Y)

for j = 1 to N do
H - Y, and Hj- - Yj- for = 1 to L do Y^ - H^ and Y^ j- - H^ j-

end for

end for

IV. NUMERICAL TESTS
Extensive simulations on a benchmark of the synthetic datasets with known ground-truth and a real-world dataset in a downstream task are presented in this section.

ROC-AUC

1.0

0.9

0.8

0.7

0.6

0.5

0.4

PCMCI

2-Logistic

MI

TE

KGC

3-Fan Out

Ours
3-Fan In

5-Linear

5-Nonlinear

Fig. 3: Five different datasets vs. ROC-AUC of different methods are shown: a benchmark to compare PC-momentary conditional independence (PCMCI, 2019), mutual information (MI, 2018), transfer entropy (TE, 2018), kernel Granger causality (KGC, 2011) with our method. The number of time samples is T=500. Synthetic datasets with known ground truth and various topologies and dependencies (Fig. 2) each are replicated for 50 different random implementations. Our proposed method's performance (ROC-AUC) (light blue, first on the right side of each dataset) is significantly better than the competitor methods for all datasets. Boxplots represent [0.25,0.75] quartiles and median.

a) Tests on synthetic datasets: We used five various networks with various topologies, each with 50 random realizations, to test our method and some of the state-of-theart methods, namely PCMCI (2019) [5], mutual information using Kraskov estimation (2018) [12], transfer entropy (TE) using Kraskov method (2018) [11], [12], and kernel Granger causality 2011 [13]. The topology of the datasets are shown in Fig. 2. We used one 2-nodes chaotic master-slave, two 3-nodes with fork and immorality topologies, and two 5-nodes datasets in the linear and nonlinear dependencies mode. Datasets are detailed in [25].
We used datasets to address two challenges: 1) Fig. 3 shows the case of which datasets have enough time samples for different topologies, and 2) Fig. 4 shows when the trend of different algorithms by reducing the number of time samples. ROC-AUC is used to compare the performance in the box plots. It is clearly seen in Fig. 3 and 4 that our algorithm is more robust to the size of the network, nonlinearity in dependencies, and short time-series. Note that PCMCI is also proposed for large-scale problems, but in our simulations, we show that our algorithm's robustness for short time-series is better than the PCMCI.
b) Tests on real datasets: We examine our method on climatology data of average daily discharges of rivers in the upper Danube basin by three stations located on the Iller at Kempten (IK), the Danube at Dillingen (DD), and the Isar at Lenggries (IL). The data are available through Bavarian Environmental Agency at https://www.gkd.bayern.de, and we use the measurements of three years (2017-2019). As shown in Fig. 5, there is a causal relationship IK - DD since IK discharges into the DD upstream after a day, while there is no causal relationship between IL- IK and IL- DD (and vice versa). Due to confounder variables such as rainfall or other weather conditions, the statistical analyses are vulnerable to detecting spurious connections. Our method correctly detects a causal relationship between IK and DD while detecting no relationships between IL-IK and IL-DD,

unconfounding spurious variables. In contrast, the Kraskov's TE [9], [12] wrongly detects the spurious connections as the DD- IK, DD- IL, IL- IK, and IL- DD altogether, for the best parameters. Our analysis of rivers' underlying dynamics conforms to [26], which shows our method's capability to distinguish between confounding and causal variables.
V. CONCLUSIONS
This paper puts forth a novel method for discovering nonlinear dependencies between time-series in multivariate environments. The method aims to solve curse-of-dimensionality while preserving nonlinear dependencies using combination of kernel principal component analysis, linear vector autoregressive model, and estimating pre-images to obtain topology of the graph in the input space. The method's advantage is its simplicity, while the results on the synthetic and real-world datasets show the merit of the method compared to state-ofthe-art causality inference methods.
REFERENCES
[1] M. Turek. Machine Common Sense. June 3, 2021. [Online]. Available: https://www.darpa.mil/program/machine-common-sense
[2] I. Shpitser and J. Pearl, "Complete Identification Methods for the Causal Hierarchy," Journal of Machine Learning Research, Vol. 9, pp. 1941­ 1979, September 2008.
[3] J. Pearl and D. Mackenzie, The Book of Why: the New Science of Cause and Effect. Basic Books, 2018.
[4] C. Granger, "Some Recent Development in a Concept of Causality," Journal of Econometrics, Vol. 39, No. 1-2, pp. 199­211, September 1988.
[5] J. Runge et al., "Detecting and Quantifying Causal Associations in Large Nonlinear Time Series Datasets," Science Advances, Vol. 5, No. 11, pp. 1­15, November 2019.
[6] Y. Antonacci, L. Astolfi, and L. Faes, "Testing Different Methodologies for Granger Causality Estimation: A Simulation Study," in European Signal Processing Conference, January 2021, pp. 940­944.
[7] N. Ancona, D. Marinazzo, and S. Stramaglia, "Radial Basis Function Approach to Nonlinear Granger Causality of Time Series," Physical Review E, Vol. 70, No. 5, pp. 1­7, November 2004.
[8] D. Marinazzo, M. Pellicoro, and S. Stramaglia, "Kernel-Granger Causality and the Analysis of Dynamical Networks," Physical review E, Vol. 77, No. 5, pp. 1­9, May 2008.

2-Logistic
1.0

3-Fan Out

3-Fan In

5-Linear

5-Nonlinear

0.9

0.8

ROC-AUC

0.7

0.6

PCMCI

PCMCI

PCMCI

PCMCI

PCMCI

0.5

MI TE

MI TE

MI TE

MI TE

MI TE

KGC

KGC

KGC

KGC

KGC

Ours

Ours

Ours

Ours

Ours

100 2T0im0e sa3m00ples400 500 100 2T0im0e sa3m00ples400 500 100 2T0im0e sa3m00ples400 500 100 2T0im0e sa3m00ples400 500 100 2T0im0e sa3m00ples400 500

Fig. 4: Simulations of various algorithms on different datasets are shown. Each column shows a different dataset (see titles). From left to right: 2-Logistic, 3-Fan out, 3-Fan in, 5-Linear, and 5-Nonlinear. Five different methods (PCMCI, MI, TE, KGC, and the proposed method) are tested for various time samples T= {50, 100, 200, 500} to compare the performance on the short time-series. Each dataset has 50 different replications. The confidence intervals of 95 percent are shown as shaded areas around the means. The time samples are increasing from left to right. Our proposed method (light blue, denoted by Ours) is significantly more robust than the counterparts to the short time series and for all topologies.

DD

IK

IL

Fig. 5: The average daily discharges of rivers in the upper Danube basin, the Iller at Kempten (IK), the Danube at Dillingen (DD), and the Isar at Lenggries (IL) are nonlinearly related. Only IK causes DD, which is correctly detected by the proposed method.
[9] T. Schreiber, "Measuring Information Transfer," Physical Review Letters, Vol. 85, No. 2, pp. 1­4, July 2000.
[10] L. Barnett, A. B. Barrett, and A. Seth, "Granger Causality and Transfer Entropy are Equivalent for Gaussian Variables," Physical Review Letters, Vol. 103, No. 23, pp. 1­4, December 2009.
[11] A. Kraskov, H. Sto¨gbauer, and P. Grassberger, "Estimating Mutual Information," Physical Review E, Vol. 69, No. 6, pp. 1­16, June 2004.
[12] L. Novelli et al., "Large-Scale Directed Network Inference with Multivariate Transfer Entropy and Hierarchical Statistical Testing," MIT Press, Vol. 3, No. 3, pp. 827­847, July 2019.
[13] D. Marinazzo et al., "Nonlinear Connectivity by Granger Causality," Neuroimage, Vol. 58, No. 2, pp. 330­338, September 2011.
[14] J. Runge et al., "Inferring Causation from Time Series in Earth System Sciences," Nature Communications, Vol. 10, No. 1, pp. 1­13, June 2019.
[15] M. A. Vosoughi and A. Wismu¨ller, "Large-scale extended Granger

causality for classification of marijuana users from functional MRI," in Medical Imaging 2021: Biomedical Applications in Molecular, Structural, and Functional Imaging, Vol. 11600, International Society for Optics and Photonics. SPIE, February 2021, pp. 72 ­ 84. [16] G. Camps-Valls, D. Sejdinovic, J. Runge, and M. Reichstein, "A Perspective on Gaussian Processes for Earth Observation," National Science Review, Vol. 6, No. 4, pp. 616­618, July 2019. [17] C. Manski, Partial Identification of Probability Distributions. Springer Science & Business Media, 2003. [18] J. Friedman, T. Hastie, and R. Tibshirani, "Regularization Paths for Generalized Linear Models via Coordinate Descent," Journal of Statistical Software, Vol. 33, No. 1, p. 1, August 2010. [19] B. Scho¨lkopf et al., Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT press, 2002. [20] G. Bakir, J. Weston, and B. Scho¨lkopf, "Learning to Find Pre-Images," pp. 449­456, December 2003. [21] N. Mohammadi, M. M. Doyley, and M. Cetin, "Finite element reconstruction of stiffness images in mr elastography using statistical physical forward modeling and proximal optimization methods," arXiv preprint arXiv:2103.14632, 2021. [22] N. Mohammadi et al., "Ultrasound elasticity imaging using physicsbased models and learning-based plug-and-play priors," in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 1165­1169. [23] N. Mohammadi, M. M. Doyley, and M. Cetin, "A statistical framework for model-based inverse problems in ultrasound elastography," arXiv preprint arXiv:2010.10729, 2020. [24] N. Mohammadi, M. M. Doyley et al., "Mr elasticity reconstruction using statistical physical modeling and explicit data-driven denoising regularizer," arXiv preprint arXiv:2105.12922, 2021. [25] A. Wismu¨ller et al., "Large-scale nonlinear Granger causality for inferring directed dependence from short multivariate time-series data," Scientific Reports, Vol. 11, No. 1, pp. 1­11, 2021. [26] L. Mhalla, V. Chavez-Demoulin, and D. J. Dupuis, "Causal Mechanism of Extreme River Discharges in the Upper Danube Basin Network," Journal of the Royal Statistical Society: Series C (Applied Statistics), Vol. 69, No. 4, pp. 741­764, August 2020.

