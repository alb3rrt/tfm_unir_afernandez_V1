arXiv:2106.01567v1 [cs.DS] 3 Jun 2021

Deterministic Weighted Expander Decomposition in Almost-linear Time

Jason Li CMU

Thatchaphol Saranurak University of Michigan
June 4, 2021

Abstract
In this note, we study the expander decomposition problem in a more general setting where the input graph has positively weighted edges and nonnegative demands on its vertices. We show how to extend the techniques of [CGL+20] to this wider setting, obtaining a deterministic algorithm for the problem in almost-linear time.

supported in part by NSF awards CCF-1907820, CCF-1955785, and CCF-2006953

1 Introduction

An ( , )-expander decomposition of a graph G = (V, E) is a partition P = {V1, . . . , Vk} of the

set V of vertices, such that for all 1  i  k, the conductance of graph G[Vi] is at least , and

k i-1

G(Vi)



Vol(G). This decomposition was introduced in [KVV04, GR99] and has been

used as a key tool in many applications, including the ones mentioned in this paper.

Spielman and Teng [ST04] provided the first near-linear time algorithm, whose running time is O~(m/poly( )), for computing a weak variant of the ( , 2/poly(log n))-expander decompo-

sition, where, instead of ensuring that each resulting graph G[Vi] has high conductance, the

guarantee is that for each such set Vi there is some larger set Wi of vertices, with Vi  Wi, such that (G[Wi])  2/poly(log n). This caveat was first removed in [NS17], who showed an algorithm for computing an ( , /no(1))-expander decomposition in time O(m1+o(1)) (we note

that [Wul17] provided similar results with somewhat weaker parameters). More recently, [SW19] provided an algorithm for computing ( , /poly(log n))-expander decomposition in time O~(m/ ).

Unfortunately, all algorithms mentioned above are randomized.

Recently, a superset of the authors [CGL+20] obtained the first deterministic algorithm for

computing an ( , /no(1))-expander decomposition in m1+o(1) time, which immediately implied

near-optimal deterministic algorithms for many fundamental optimization problems, from dy-

namic connectivity to (1 + )-approximate max-flow in undirected graphs. While the expander

decomposition algorithm only works for unweighted graphs, the applications can be adapted to

work on weighted graphs via problem-specific reductions to the weighted case. However, since

their initial work, further applications of expander decomposition have been discovered which

require more sophisticated settings for the expander decomposition primitive itself, including

weighted graphs [Li21] and even custom, arbitrary "demands" on the vertices [LP20].

In this note, we provide a fast, general-purpose expander decomposition algorithm that works

for the widest setting known thus far: weighted graphs with custom demands on the vertices.

While our algorithm is deterministic, we remark that even a randomized almost-linear-time

algorithm in this setting was never explicitly shown before in the literature.

1.1 Preliminaries from [CGL+20]

In this section, we introduce notation, definitions, and results from [CGL+20] relevant to this
note.
All graphs considered in this paper are positively weighted and undirected. Given a graph
G = (V, E, w), for every vertex v  V , we denote by degG(v) the sum of weights of edges incident to v in G. For any set S  V of vertices of G, the volume of S is the sum of degrees of all nodes
in S: VolG(S) = vS degG(v). For an edge e  E, we denote by w(e) the weight of edge w, and for a subset F  E of edges, we define w(F ) = eF w(e).
We use standard graph theoretic notation: for two subsets A, B  V of vertices of G, we
denote by EG(A, B) the set of all edges with one endpoint in A and another in B. We sometimes write w(A, B) = w(EG(A, B)). Assume now that we are given a subset S of vertices of G. We denote by G[S] the subgraph of G induced by S. We also denote S = V \ S, and G - S = G[S].
An important subroutine in our algorithm is computing a spectral sparsifier of a graph,
defined below.

Definition 1.1 (Spectral sparsifier). The Laplacian LG of G is a matrix of size n × n whose entries are defined as follows:

 0   LG(u, v) = -wuv

  

w (u,u )E: uu

u=u

u = v, (u, v)  E u = v, (u, v)  E u = v.

We

say

that

a

graph

H

is

an

-approximate

spectral

sparsifier

for

G

iff

for

all

x



Rn,

1 

x

LGx 

x LH x   · x LGx holds.

1

Theorem 1.2 (Corollary 6.4 of [CGL+20]). There is a deterministic algorithm, that we call SpectralSparsify that, given an undirected n-node m-edge graph G = (V, E, w) with edge weights in the range [1, U ], and a parameter 1  r  O(log m), computes a (log m)O(r2)approximate spectral sparsifier H for G, with |E(H)|  O (n log n log U ), in time
O m1+O(1/r) · (log m)O(r2) log U .

2 Weighted BalCutPrune and Expander Decomposition

In our setting, every vertex v  V (G) has a non-negative demand d(v) that is independent of the edge weights. As usual, the demand of a set S  V (G) of vertices is d(S) = vS d(v). Given a subset S  V of vertices, we denote by d|S the vector d of demands restricted to the vertices of S. We start by defining a weighted variant of sparsity and of expander decomposition.
Definition 2.1 (Weighted Sparsity). Given a graph G = (V, E) with non-negative weights w(e)  E on its edges e  E, and non-negative demands d(v)  0 on its vertices v  V , the d-sparsity of a subset S  V of vertices with 0 < d(S) < d(V ) is:

dG(S)

=

w(EG(S, V \ min{d(S), d(V

S)) \ S)}

.

The d-sparsity of graph G is d(G) = minSV :0<d(S)<d(V ) dG(S).
Observe that if w(e) = 1 for all e  E and d(v) = deg(v) for all v  V , then this definition is exactly the conductance of the graph. Here, we use the term sparsity instead of conductance because traditionally, sparsity concerns the number of vertices in the denominator of the ratio, while conductance uses volume which is closely related to the number of edges. However, for lack of an alternative term, we will stick with the term expander to describe a graph of high weighted sparsity. We now define an expander decomposition for the weighted sparsity, which generalizes the standard definition for conductance.

Definition 2.2 (Weighted Expander Decomposition). Given a graph G = (V, E, w, d) with non-negative weights w(e)  0 on its edges e  E, and non-negative demands d(v)  0 on its vertices v  V , a ( , )-expander decomposition of G is a partition P = {V1, . . . , Vk} of the set V of vertices, such that:

1. For all 1  i  k, the graph G[Vi] has d|Vi -sparsity at least , and

2.

k i-1

w(EG(Vi

,

V

\ Vi))



d(V ).

Similarly to [CGL+20], the key subroutine of our expander decomposition algorithm is solving the following WeightedBalCutPrune problem, a generalization of BalCutPrune from [CGL+20]

that allows both weighted edges and "demands" on the vertices.

Definition 2.3 (WeightedBalCutPrune problem). The input to the -approximate WeightedBalCutPrune problem is a graph G = (V, E) with non-negative weights w(e)  0 on edges e  E, a nonzero vector d  RV0 of demands, a sparsity parameter 0 <   1, and an approximation factor . The goal is to compute a partition (A, B) of V (G) (where possibly B = ), with w(E(A, B))   · min{d(A), d(B)},1 such that one of the following hold: either
1. (Cut) d(A), d(B)  d(V )/3; or 2. (Prune) d(A)  d(V )/2, and d|A (G[A])  .

The main technical result of this note is the following algorithm for WeightedBalCutPrune.

1We remark that this guarantee is stronger than what we would obtain if we directly translated BalCutPrune from [CGL+20]. The latter only requires that |E(A, B)|   · Vol(G) in their setting, which would translate to
w(E(A, B))   · d(V ) in our setting.

2

Theorem 2.4. There is a deterministic algorithm that, given an m-edge connected graph G = (V, E) with edge weights 1  w(e)  U for all e  E and demands d(v)  {0}  [1, U ] for all v  V that are not all zero, together with parameters 0 <   1 and r  1, solves the (logO(r4) m)-approximate WeightedBalCutPrune problem in time m · (mU )O(1/r).
We provide the proof of Theorem 2.4 in the following subsections. Before we do so, we obtain the following corollary, whose proof follows similarly to the reduction from expander decomposition to BalCutPrune in [CGL+20]. For completeness, we include the proof in Section 2.2.
Corollary 2.5. There is a deterministic algorithm that, given an m-edge graph G = (V, E) with weights 1  w(e)  U on its edges e  E, and demands d(v)  {0}  [1, U ] for its vertices v  V that are not all zero, together with a parameter  (0, 1] and r  1, computes a ( , )-expander decomposition of G, for  = / logO(r4) m log U , in time m · (mU )O(1/r) log(mU ).
2.1 Weighted Most-Balanced Sparse Cut
We first define the Weighted Most-Balanced Cut problem, and provide a bi-criteria approximation algorithm for it, this time based on recursively applying the j-tree framework of Madry [Mad10b]. In Section 2.2, we then show our algorithm for Weighted Most-Balanced Cut can be used in order to approximately solve the WeightedBalCutPrune problem.
Definition 2.6 ((s, b)-most-balanced -sparse cut). Given a graph G = (V, E) and parameters s, b  1, a set S  V with d(S)  d(V )/2 is a (s, b)-most-balanced -sparse cut if it satisfies:
1. w(S, V \ S)   · d(S).
2. Define  := /s and let S  V be the set with maximum d(S) out of all sets S satisfying w(S , V \ S )   · min{d(S ), d(V \ S )} and d(S )  d(V )/2. Then, d(S)  d(S)/b.
Let us first motivate why we consider a completely different recursive framework based on recursive j-trees [Mad10b] instead of the recursive KKOV cut-matching game framework [KKOV07] as used in [CGL+20]. This is because KKOV recursion scheme does not generalize easily to the weighted setting. The main issue that in a weighted graph, the flows constructed by the matching player cannot be decomposed into a small number of paths; the only bound we can prove is at most m paths by standard flow decomposition arguments. Hence, the graphs constructed by the cut player are not any sparser, preventing us from obtaining an efficient recursive bound. Madry's j-tree framework, on the other hand, generalizes smoothly to weighted instances and can even be adapted to solve the sparsest cut problem with general demands, for which Madry provided efficient randomized algorithms in his original paper [Mad10b].
Below, we give a high-level description of Madry's approach. But first, let us state the definition of j-trees as follows.
Definition 2.7. A graph G is a j-tree if it is a union of:
· a subgraph H of G (called the core), induced by a set VH of at most j vertices; and
· a forest (that we refer to as peripheral forest), where each connected component of the forest contains exactly one vertex of VH . For each core vertex v  VH , we let TG(v) denote the unique tree in the peripheral forest that contains v. When the j-tree G is unambiguous, we may use T (v) instead.
In Madry's approach, the input graph is first decomposed into a small number of j-trees (formally stated in Lemma 2.9), so that it suffices to solve the problem on each j-tree and take the best solution. For a given j-tree, one key property of the generalized sparsest cut problem is that either the optimal solution only cuts edges of the core, or it only cuts edges of the peripheral forest. Therefore, the algorithm can solve two separate problems, one on the core and one on the peripheral forest. The former becomes a recursive call on a graph of j vertices, and the latter simply reduces to solving the problem on a tree.
3

This same strategy almost directly translates over to the Weighted Most-Balanced Cut problem. The main additional difficulty is in ensuring the additional balanced guarantee in our Weighted Most-Balanced Cut problem, which is the biggest technical component of this section. We remark that our algorithm for computing the weighted most-balanced sparse cut is a modification of the algorithm in Section 8 of [GLN+19]. In particular, the algorithms WeightedBalCut and RootedTreeBalCut presented below are direct modifications of Algorithm 4 and Algorithm 5 in Section 8 of [GLN+19], respectively. Still, we assume no familiarity with that paper and make no references to it.
We now state formal definition of graph embedding and Madry's decomposition theorem for j-trees below.

Definition 2.8. Let G, H be two graphs with V (G) = V (H). An embedding of H into G is a collection P = {P (e) | e  E(H)} of paths in G, such that for each edge e  E(H), path P (e) connects the endpoints of e in G. We say that the embedding causes congestion  iff every edge e  E(G) participates in at most  paths in P.

Lemma 2.9 ([Mad10a]). There is a deterministic algorithm that, given an edge-weighted graph

G

=

(V, E, w)

with

|E|

=

m

and

capacity

ratio

U

=

, maxeE we
maxeE we

together

with

a

parameter

t



1,

computes, in time O~(tm), a distribution {i}ti=1 over a collection of t edge-weighted graphs

G1, . . . , Gt, where for each 1  i  t, Gi = (V, Ei, wi), and the following hold:

·

for

al l

1



i



t,

graph

Gi

is

an

(

m

logO(1) t

m

log

U

)-tree,

whose

core

contains

at

most

m

edges;

· for all 1  i  t, G embeds into Gi with congestion 1; and
· the graph that's the average of these graphs over the distribution, G~ = embedded into G with congestion O(log m(log log m)O(1)).

i iGi can be

Moreover, the capacity ratio of each Gi is at most O(mU ).

In particular, Definition 2.8 and Lemma 2.9 imply that, for any cut (S, V \ S), we have that w(EGi (S, V \ S))  w(EG(S, V \ S)) for all i, and there exists i where w(EGi (S, V \ S))   · w(EG(S, V \ S)). This is the fact that we will use later.
Our algorithm WeightedBalCut first invokes Lemma 2.9 to approximately decompose the input graph G into t many j-trees, where j = O(m/t) and t is small (say, m for some constant
> 0). Since the distribution of j-trees approximates G, it suffices to solve the Weighted MostBalanced Cut problem on each j-tree separately and take the best overall. For a given j-tree H, the algorithm computes two types of cuts--one that only cuts edges in the core of H, and one that only cuts edges of the peripheral forest of H--and takes the one with better weighted sparsity. In our analysis (specifically Lemma 2.11), we prove our correctness by showing that for any cut S of the j-tree H = (VH , EH ), there exists a cut S that
1. either only cuts core edges or only cuts peripheral edges, and
2. has weighted sparsity and balance comparable to those of H, i.e., wH (EH (S , VH \ S ))  O(wH (EH (S, VH \ S))) and d(S )  (d(S)).
To compute the best way to cut the core, the algorithm first contracts all edges in the peripheral forest, summing up the demands on the contracted vertices. This leaves a graph of j = O(m/t) vertices, but the number of edges can still be (m). To ensure the number of edges also drops by a large enough factor, the algorithm sparsifies the core using Theorem 1.2, computing a sparse graph with only O~(m/t) edges that -approximates all cuts of the core for some  = (log m)O(r2). Finally, the algorithm recursively solves the problem on the sparsified core. The approximation factor blows up by polylog(m) per recursion level, but the number of edges decreases by roughly t = m , so over the O(1/ ) recursion levels, the overall approximation factor becomes (log m)O(r2/ ), which is no(1) appropriate choices of r and .
The algorithm for cutting the peripheral forest is much simpler and non-recursive. The algorithm first contracts the core of H, obtaining a tree in which to compute an approximate

4

Weighted Most-Balanced Cut. Then, RootedTreeBalCut roots the tree at an appropriately chosen "centroid" vertex and greedily adds subtrees of small enough sparsity into a set S until either d(S) is large enough, or no more sparse cuts exist.
WeightedBalCut(G, , , b) with    and b  1, and G has demands d: 1. Fix an integer r  1 and parameter t = m10/r(log m)O(1) log2 U , where m0 is the number of edges in the original input graph to the recursive algorithm, m  m0 is the number of edges of the input graph G to the current recursive call, and U is the capacity ratio of G. 2. Fix parameters  = (log m)O(r2) as the approximation factor from Theorem 1.2, and  = O(log m(log log m)O(1)) as the congestion factor from Lemma 2.9. 3. Compute O(m/t)-trees G1, . . . , Gt using Lemma 2.9 with G and t as input. For each i, let Ki denote the vertex set in the core of Gi 4. For each i  [t]: (a) Hi  Gi[Ki] with demands dHi on Ki as dHi (v) = uV (TGi (v)) d(u) (so that dHi (Ki) = d(V )). (b) Hi  -approximate spectral sparsifier of Hi (with the same demands) (c) SHi  WeightedBalCut(Hi, /, 3, b/3) (d) SHi  SHi with each vertex v replaced with V (TGi(v)) (see Definition 2.7) (e) Construct a tree Ti = (VTi, ETi, wTi) with demands dTi as follows: Starting with Gi, contract Ki into a single vertex ki with demand d(Ki). All other vertices have demand d(v) (so that dTi(VTi) = d(V )). (f) Root Ti at a vertex ri  VTi such that every subtree rooted at a child of ri has total weight at most dTi(V )/2 = d(V )/2. (g) STi  RootedTreeBalCut(Ti, ri, ) (h) STi  STi with the vertex ki replaced with Ki if ki  STi 5. Of all the cuts S = SHi or S = STi computed satisfying w(S, V \ S)   · min{d(S), d(V \ S)}, consider the set S with maximum min{d(S), d(V \ S)}, and output S if d(S)  d(V \ S) and V \ S otherwise. If no cut S satisfies w(S, V \ S)   · min{d(S), d(V \ S)}, then return .
RootedTreeBalCut(T = (VT , ET , wT ), r, T ):
0. Assumption: T is a weighted tree with demands dT . The tree is rooted at a root r such that every subtree Vu rooted at a vertex u  VT \ {r} has total demand dT (Vu)  dT (VT )/2. Output: a set S  VT satisfying the conditions of Lemma 2.12.
1. Find all vertices u  VT \ {r} such that if Vu is the vertices in the subtree rooted at u, then wT (E[Vu, VT \ Vu])/dT (Vu)  2T . Let this set be X.
2. Let X denote all vertices u  X without an ancestor in X (that is, there is no v  X \ {u} with u  Tv).
3. Starting with S = , iteratively add the vertices Vu for u  X. If dT (S)  dT (VT )/4 at any point, then terminate immediately and output S. Otherwise, output S at the end.
We now analyze our algorithm WeightedBalCut by showing the following:
5

K S
Ur

K SH
S

U

U

U ST

Figure 1: Left: Cases 1a and 1b of Lemma 2.11. The set S is the cyan vertices. Right: Cases 2a and 2b.

Lemma 2.10. Fix parameters b  6,  > 0, and   12 ·  for  as defined in Line 2 of WeightedBalCut algorithm. WeightedBalCut outputs a (/, b)-most-balanced (, d)-sparse cut.
We now state our structural statement on cuts in j-trees: for each j-tree Gi, either the core Hi contains a good balanced cut or the "peripheral" tree Ti (produced by contracting the core) does.
Lemma 2.11. Fix i  [t], and let S  V be any cut with d(S)  d(V )/2. For simplicity, define K = Ki, k = ki, T = Ti, r = ri, and H = Hi. One of the following must hold:
1. There exists a cut ST  VT in T satisfying wT (ET (ST , VT \ ST ))  w(EGi (S, V \ S)) and d(S)/2  dT (ST )  2d(V )/3, and ST is the disjoint union of subtrees of T rooted at r.
2. There exists a cut SH  K in core H satisfying wH (EH (SH , K \SH ))  w(EGi (S, V \S)) and min{dH (SH ), dH (K \ SH )}  d(S)/3.
The statement itself should not be surprising. If S only cuts edges in the peripheral forest of Gi, then the cut survives when we contract the core H to form the tree T , and its dT -sparsity is the same as its original d-sparsity. Likewise, if S only cuts edges in the core H, then the cut survives when we contract the all edges in the peripheral forest to form K, and its dH -sparsity is the same as its original d-sparsity. The difficulty is handling the possibility that S cuts both peripheral forest edges and core edges, which we resolve through some casework below.
Proof. We need a new notation. For a j-tree Gi and a vertex v on peripheral forest F , we define cGi (v) as the unique vertex shared by F and the core H of Gi.
Let S  V the set as described in Definition 2.6 (w(S, V \S)  ·min{d(S), d(V \S)}). Let U be the vertices u  V whose (unique) path to cGi (u) in F contains at least one edge in EGi (S, V \ S). In Figure 1, U is the set of vertices with green circle around. Note that U  K =  and EGi (U, V \ U )  EGi (S, V \ S). Observe further that U is a union of subtrees of T rooted at k (not r). This is because, when we root the tree T at k, for each vertex u  U , its entire subtree is contained in U .
Case 1: r  U . In this case, we will construct a cut in the tree T to fulfill condition (1). Let F be the peripheral forest of Gi (see Definition 2.7) and let T be the tree in F that contains r. Define U = T  U (Figure 1 left). In words, U contains all vertices of U in the tree of F that contains r. Let us re-root T at vertex r, so that the vertices in VT \ U now form a subtree. We now consider a few sub-cases based on the size of U .

6

Case 1a: r  U and dT (U )  3d(V )/4. Define ST  VT as ST := VT \ U . By our

selection of r,

dT (ST )

=

dT (VT

\

U

)



d(V ) 2.

Moreover,

dT (ST )

=

dT (VT

\

U

)



d(V ) 4



d(S) 2

and ET (ST , V \ ST )  EGi (U, V \ U )  EGi (S, V \ S),

fulfilling condition (1).

Case 1b: r  U and dT (U )  3d(V )/4. Define U as all vertices u  U whose (unique) tree path to root (r) contains at least one vertex not in S (possibly u itself). As this set
contains all vertices in U not in S, we have U  U \ S, and in turn

dT

U

 dT (U

\ S) = dT (U )-dT (U



S)



dT

(U

)-d (S)



3d (V 4

) d (V -2

)

=

d(V 4

) .

Moreover, U is a union of subtrees of T rooted at r and satisfies

EGi U , V \ U  EGi (S, V \ S) .

By our choice of r, each subtree T of U satisfies dT (V (T ))  d(V )/2. We perform one further case work based on the largest size of one of these subtrees to show that we can find a tree cut that satisfies condition (1).

· If there exists a subtree T  U with dT (V (T ))  d(V )/4, then set ST := V (T ).

· Otherwise, since dT (U )  d(V )/4, we can greedily select a subset of subtrees of U with total d(·) value in the range [d(V )/4, d(V )/2], and set ST as those vertices.

In both cases we have

d (S) 2



d(V ) 4



dT

(ST )



d (V ) 2

which gives the volume condition on S, and the cut size bound follows from w(ET (ST , V \ST ))  w(EGi (U , V \ U )).

Case 2: r / U . In this case, we will cut either the tree T or the core H depending on a few further sub-cases.

Case 2a: r / U and dT (U )  d(V )/6. Since r / U , every subtree in U has weight at most d(V )/2. Let U be a subset of these subtrees of total d(·) value in the range [d(V )/6, 2d(V )/3]. Define the tree cut ST := U , which satisfies

d(V 2

)



dT (ST )



d(V ) 6



d(S) 3

and ET (ST , V \ ST )  EGi (U, V \ U )  EGi (S, V \ S),
fulfilling condition (1).

7

Case 2b: r / U and dT (U ) < d(V )/6. In this case, let S := S  U , which satisfies
d(S)  d(S)  d(S) + dT (U )  d(S) + d(V )/6  2d(V )/3
and EGi (S, V \ S)  EGi (S, V \ S). Next, partition S into SH and ST according to Figure 1, where SH consists of the vertices of all connected components of Gi[S] that intersect K, and ST := S \ SH is the rest. We have
EGi (SH , V \ SH )  EGi (S, V \ S) and EGi (ST , V \ ST )  EGi (S, V \ S).
Observe that ST is a tree cut, and SH is a core cut since it does not cut any edges of the peripheral forest. We will select either ST or SH based on one further case work.
Since d(ST ) + d(SH ) = d(S), we can case on whether dT (ST )  d(S)/2 or d(SH )  d(S)/2. · If dT (ST )  d(S)/2, then the set ST satisfies condition (1). · Otherwise, d(SH )  d(S)/2. Since EGi (SH , V \ SH ) does not contain any edges in the
peripheral forest F , we can "contract" the peripheral forest to obtain the set SH := {cGi (v) : v  SH }  K such that SH is the vertices in the trees in F intersecting SH . This also means that V \ SH is the vertices in the trees of F intersecting K \ SH . It remains to show that SH fulfills condition (2). We have
wH (EH (SH , K \ SH )) = w(EGi (SH , V \ SH ))  w(EGi (S, V \ S))
and min{dH (SH ), dH (K \ SH )} = min{d(SH ), d(V \ SH )}.
It remains to show that min{d(SH ), d(V \ SH )}  d(S)/3. This is true because d(SH )  d(S)/2  d(S)/2 and d(SH )  d(S)  2d(V )/3 which means that d(V \SH )  d(V )/3  d(S)/3.
If the graph H contains a good balanced cut, then intuitively, the demands dH are set up so that the recursive call on H will find a good cut as well. The lemma below shows that if the tree T contains a good balanced cut, then RootedTreeBalCut will perform similarly well.
Lemma 2.12. RootedTreeBalCut(T = (VT , ET , wT ), dT , r, T ) can be implemented to run in O(|VT |) time. The set S output satisfies TdT (S) = wT (ET (S, VT \ S))/ min{dT (S), dT (VT \ S)}  6T . Moreover, for any set S with wT (ET (S, VT \ S))/dT (S)  T and dT (S)  2dT (VT )/3, and which is composed of vertex-disjoint subtrees rooted at vertices in T , we have min{dT (S), dT (VT \ S)}  dT (S)/3.
Proof. Clearly, every line in the algorithm can be implemented in linear time, so the running time follows. We focus on the other properties.
Every set of vertices Vu added to S satisfies wT (ET (Vu, VT \ Vu))/dT (Vu)  2T . Also, the added sets Vu are vertex-disjoint, so wT (ET (S, VT \ S)) = VuS wT (ET (Vu, VT \ Vu)). This means that RootedTreeBalCut outputs S satisfying wT (ET (S, VT \ S))/dT (S)  2T . Since every set Vu has total weight at most dT (VT )/2, and since the algorithm terminates early if dT (S)  dT (VT )/4, we have dT (S)  3dT (VT )/4. This means that min{dT (S), dT (VT \ S)}  dT (S)/3, so wT (ET (S, VT \ S))/ min{dT (S), dT (VT \ S)}  3wT (ET (S, VT \ S))/dT (S)  6T .
It remains to prove that S is balanced compared to S. There are two cases. First, suppose that the algorithm terminates early. Then, as argued above, min{dT (S), dT (VT \ S)}  dT (VT )/4, which is at least (2dT (VT )/3)/3  dT (S)/3, so min{dT (S), dT (VT \ S)}  dT (S)/3.
Next, suppose that S does not terminate early. From the assumption of S, there are sets S1, . . . , S of vertices in the (vertex-disjoint) subtrees that together compose S, that is,
i Si = S. Note that ET (Si, VT \ Si) is a single edge in ET for each i. Suppose we reorder the sets Si so that S1, . . . , Sq are the sets that satisfy wT (ET (Si, VT \ Si))/dT (Si)  2T . From
8

the assumption on S, we have wT (ET (S, VT \ S))/dT (S)  T , by a Markov's inequalitylike argument, we must have i[q] dT (Si)  (1/2) i[ ] dT (Si) = dT (S)/2. Observe that by construction of X, each of the subsets S1, . . . , Sq is inside Vu for some u  X. Therefore, the set S that RootedTreeBalCut outputs satisfies dT (S)  i[q] dT (Si)  dT (S)/2.
Finally, we prove Lemma 2.10:

Proof (Lemma 2.10). Let S  V be the set for G as described in Definition 2.6 with parameters s = / and b; that is, it is the set with maximum d(S) out of all sets S satisfying dG(S )   and d(S )  d(V )/2. If d(S) = 0, then the output of WeightedBalCut always satisfies the

definition of (s, b)-most-balanced -sparse cut, even if it outputs . So for the rest of the proof,

assume that d(S) > 0, so that By Lemma 2.9, there exists

dG(S i  [t]

) and such

dGi that

(S) are well-defined. w(EGi (S, V \ S)) 



· w(EG(S, V

\ S)),

which means that

dGi (S)   · dG(S)   · .

For the rest of the proof, we focus on this i, and define K = Ki, H = Hi, and T = Ti. We break into two cases, depending on which condition of Lemma 2.11 is true:

1. Suppose condition (1) is true for the cut ST . Then, since wT (ET (ST , VT \ ST ))  w(EGi (S, V \ S)) and dT (ST )  d(S)/2, we have

wT (ET (ST , VT dT (ST )

\

ST ))



w(EGi (S, V \ d(S)/2

S))



2dGi (S)



2

· .

Also, dT (ST )  2d(V )/3 = 2dT (VT )/3. Let ST be the cut in T that RootedTreeBalCut outputs and let ST the corresponding cut in Gi after the uncontraction in Step 4h. Applying Lemma 2.12 with T = 2 · , the cut ST satisfies dTT (ST )  6T = 12 ·  and min{dT (ST ), dT (VT \ ST )}  dT (ST )/3. By construction, d(ST ) = d(ST )  dT (ST )/3  d(S)/6  d(S)/b and dGi (ST ) = dTT (ST )  12 ·   .
2. Suppose condition (2) is true for the cut SH . Since wH (EH (SH , K \ SH ))  w(EGi (S, V \ S)) and min{dH (SH ), dH (K \ SH )}  d(S)/3, we have dHH (SH )  3dGi (S)  3 · . Since H is an -approximate spectral sparsifier of H, we have dHH (SH )  ·3dHH (SH )  3·. By induction on the smaller recursive instance WeightedBalCut(H , dH , /, 3, b/3), the cut SH computed is a (3, b/3)-most-balanced (/, dH )-sparse cut. Since H is an -approximate spectral sparsifier of H, we have dHH (SH )  ·dHH (SH )  ·/ = . Let SH be the cut in Gi corresponding to SH after the uncontraction in Step 4d. By construction, dGi (SH ) = dHH (SH )   and d(SH ) = dH (SH ). Since SH is a cut with HdH (SH )  3, we have

d(SH )

=

dH (SH )



min{dH (SH ), dH (K b/3

\ SH )}



d(S)/3 b/3

=

d(S) .
b

In both cases, the computed cut is a (/, b)-most-balanced -sparse cut.

The lemma below will be useful in bounding the running time of the recursive algorithm.

Lemma 2.13. For any integer t  1 (as defined by the algorithm), the algorithm makes t

recursive calls WeightedBalCut(H , dH , /, 3, b/3) on graphs H

with

O~(

m

log t

U

)

vertices

and

O~(

m

log2 t

U

)

edges,

and

runs

in

O~(tm)

time

outside

these

recursive

cal ls.

Proof. By Lemma 2.9, computing the graphs G1, . . . , Gt takes O~(tm) time. By Lemma 2.12,

RootedTreeBalCut runs in O(m) time for each Gt, for a total of O(tm) time. Since each

graph

Gi

is

a

O~(

m

log t

U

)-tree,

by

construction,

each

graph

Hi

has

at

most

O~(

m

log t

U

)

vertices.

By

Theorem

1.2,

the

sparsified

graphs

Hi

have

at

most

O~(

m

log t

U

)

log

m

log

U



O~(

m

log2 t

U

)

edges.

9

Finally, we plug in our value t = m1/r(log m)O(1) log2 U that balances out the running time O~(tm) outside the recursive calls and the number r of recursion levels.
Theorem 2.14. Fix parameters  > 0 and 1  r  O(log m), and let  = 12 · (32)r · . There is a deterministic algorithm that, given a weighted graph G with m edges and capacity ratio U and demands d, computes a (12 · (32)r, 6 · 3r)-most-balanced -sparse cut in time m1+1/r (log(mU ))O(1). Note that 12 · (32)r = (log m)O(r3).

Proof. Let G be the original graph with m = m0 edges. Let G be the current input graph in
a recursive call of WeightedBalCut, with m edges and capacity ratio U . Set the parameters t = m1/r(log m )O(1) log2 U from the algorithm and  = (log m )O(r2) from Theorem 1.2 and

 = O(log m (log log m )O(1))  (log m )O(1) from Lemma 2.9. By Lemma 2.13, the algorithm

makes t = m1/r(log m )O(1) log2 U

many

recursive

calls

to

graphs

with

at

most

O~( m

log2 U t

)



m /m1/r edges, where U is the capacity ratio of the current graph, so there are r levels of

recursion. By Lemma 2.9, the capacity ratio of the graph increases by an O(m) factor in each

recursive call, so we have U  O(m)rU for all recursive graphs, which means t  m1/r(r log m+ log U )O(1). By Lemma 2.13, the running time O~(tm ) outside the recursive calls for this graph

is m m1/r(r log m + log U )O(1). For recursion level 1  i  r, there are mi/r (r log m + log U )O(i)

many graphs at this recursion level, each with m  m1-i/r, so the total time spent on graphs

at this level, outside their own recursive calls, is at most

mi/r (r log m + log U )O(i) · m1-i/rm1/r(r log m + log U )O(1) = m1+1/r(r log m + log U )O(i).

Summed over all 1  i  r and using r  O(log m), the overall total running time becomes m1+1/r(log(mU ))O(r).
We also need to verify that the conditions   12 ·  and b  6 of Lemma 2.10 are always
satisfied throughout the recursive calls. Since each recursive call decreases the parameter b by a factor of 3, and b = 6 · 3r initially, the value of b is always at least 6. Also, in each recursive call, the ratio / decreases by a factor 32, so for the initial value  = 12 · (32)r ·  in the theorem statement, we always have /  12.

2.2 Completing the Proof of Theorem 2.4 and Corollary 2.5

The proofs in this section follow the template from [NS17] but generalize it to work in weighted graphs and general demand. In order to prove Theorem 2.4, we first present the lemma below. Roughly, it guarantee the following. Given a set V where G[V ] is "close" to being an expander in the sense that any sparse cut (A, B) in V must be unbalanced: min {d(A), d(B)}  z, then the algorithm returns a large subset Y  V such that Y is "closer" to being an expander. That is, any sparse cut (A , B ) in Y must be even more unbalanced: min {d(A ), d(B )}  z z.
Lemma 2.15. Let G = (V, E) be a weighted graph with edge weights in [1, U ], and demands d(v)  {0}  [1, U ] for all v  V that are not all zero. There is a universal constant c1 > 0 and a deterministic algorithm, that, given a vertex subset V  V with d(V )  d(V )/2, and parameters r  1, 0 <  < 1, 0 < z < z, such that for every partition (A, B) of V with w(EG(A, B))   · min {d(A), d(B)}, min {d(A), d(B)}  z holds, computes a partition (X, Y ) of V , where d(X)  d(Y ) (where possibly X = ), w(EG(X, Y ))   · d(X), and one of the following holds:

1. either d(X), d(Y )  d(V )/3 (note that this can only happen if z  d(V )/3); or 2. for every partition (A , B ) of the set Y of vertices with

w(EG(A

,B

))



 (log(mU ))c1r3

· min {d(A

), d(B

)} ,

min {d(A ), d(B )}  z must hold (if z < 1, then graph G[Y ] is guaranteed to have d-sparsity at least /(log(mU ))c1r3 ).

10

The running time of the algorithm is O

z z

· m1+1/r (log(mU ))O(1)

.

Proof. Our algorithm is iterative. At the beginning of iteration i, we are given a subgraph Gi 

G, such that d(V (Gi))  2d(V )/3; at the beginning of the first iteration, we set G1 = G[V ]. At

the end of iteration i, we either terminate the algorithm with the desired solution, or we compute

a subset Si  V (Gi) of vertices, such that d(Si)  d(V (Gi))/2, and w(EGi (Si, V (Gi) \ Si))   · d(Si)/2. We then delete the vertices of Si from Gi, in order to obtain the graph Gi+1, that

serves as the input to the next iteration. The algorithm terminates once the current graph Gi

satisfies d(V (Gi)) < 2d(V )/3 (unless it terminates with the desired output beforehand).

We now describe the execution of the ith iteration. We assume that the sets S1, . . . , Si-1

of vertices are already computed, and that

i-1 i =1

d(Si

)  d(V

)/3.

Recall

that

Gi

is

the

sub-

graph of G[V ] that is obtained by deleting the vertices of S1, . . . , Si-1 from it. Recall also that

we are guaranteed that d(V (Gi))  2d(V )/3  d(V )/3. We apply Theorem 2.14 to graph Gi with parameters  = (/2)/(log(mU ))c1r3 and r, and let X be the returned set, which is a

(, 6 · 3r)-most-balanced ((log(mU ))c1r3 · , d)-sparse cut satisfying d(X)  d(V )/2.

We set parameter z = z /(6·3r). If d(X)  z, then we terminate the algorithm, and return

the partition (X, Y ) of V where X =

i i

=1 Si

,

and

Y

=V

\ X.

This satisfies the second

condition of Lemma 2.15, since by the most-balanced sparse cut definition, every partition

(A , B ) of the set Y

of vertices with w(EG^(A , B )) 

 (log(mU ))c1r3

· min {d(A ), d(B )} must

satisfy min{d(A ), d(B )}  6 · 3r · d(X) < 6 · 3r · z = z .

Otherwise, d(X) > z. In this case, we set Si = X and continue the algorithm. If

i i

=1

d(Si

)



d(V

)/3

continues

to

hold,

then

we

let

Gi+1

=

Gi

\ Si,

and

continue

to

the

next iteration. Otherwise, we terminate the algorithm, and return the partition (X, Y ) of V

where X =

i i

=1

Si

,

and

Y

=V

\ X. Recall that we are guaranteed that d(X)  d(V )/3.

To show that w(EG^ (X, Y ))   · d(X), note that every cut Si satisfies w(EGi (Si, V (Gi) \

Si))  (/2)d(Si), so w(EG(X, Y )) 

i i

=1 w(EGi (Si, V (Gi)

\

Si))



(/2)

i i

=1

d(Si)

=

(/2)d(X), which is at most  min{d(X), d(V \ X)} since d(X)  2d(V )/3.

The bound on the running time of the algorithm proceeds similarly. Observe that we are

guaranteed that for all i, d(Si)  z. Notice however that throughout the algorithm, if we

set A =

i i

=1

Si

and B = V

\ A, then d(A) < d(B) holds, and w(EG(A, B))   · d(A).

Therefore, from the condition of the lemma, d(A)  z must hold. Overall, the number of

iterations in the algorithm is bounded by z/z = 6 · 3r · z/z , and, since every iteration takes

time

m1+1/r (log(mU ))O(1),

total

running

time

of

the

algorithm

is

bounded

by

z z

· m1+1/r

·

(log(mU ))O(1).

We are now ready to complete the proof of Theorem 2.4, which is almost identical to the proof of Theorem 7.5 of [CGL+20]. For completeness, we include the proof below.

Proof (Theorem 2.4). We first show that we can safely assume that d(V )  2 · 4r. Otherwise, consider the following expression in Item 2 of Lemma 2.15 and its upper bound:

 (log(mU ))c1r3

· min {d(A

), d(B

)}



1 (log(mU ))c1r3

· 2 · 4r

<

1,

which holds for large enough c1 > 0. Since G is connected and all edges have weight at least
1, the condition in Item 2 only applies with A =  or B = . Therefore, the algorithm can
trivially return X =  and Y = V and satisfy Item 2. For the rest of the proof, assume that d(v)  2 · 4r. Our algorithm will consist of at most
r iterations and uses the following parameters. First, we set z1 = d(V )/2, and for 1 < i  r, we set zi = zi-1/(d(V )/2)1/r  zi-1/4; in particular, zr = 1 holds. We also define parameters 1, . . . , r, by letting r = , and, for all 1  i < r, setting i = 8 · (log(mU ))c1r3 · i+1, where c1 is the constant from Lemma 2.15. Notice that 1   · (log m)O(r4).
In the first iteration, we apply Lemma 2.15 to the set V = V of vertices, with the parameters
 = 1, z = z1, and z = z2. Clearly, for every partition (A, B) of V with wG(EG(A, B)) 

11

1 · min {d(A), d(B)}, it holds that min {d(A), d(B)}  z1 = d(V )/2. If the outcome of the

algorithm from Lemma 2.15 is a partition (X, Y ) of V satisfying d(X), d(Y )  d(V )/3 and

wG(EG(X, Y ))  1 · min {d(X), d(Y )}   · (log m)O(r4) min {d(X), d(Y )}, then we return

the cut (X, Y ) and terminate the algorithm.

We assume from now on that the algorithm from Lemma 2.15 returned a partition (X, Y ) of

V , where d(X)  d(Y ) (where possibly X = ), d(X)  d(V )/3, wG(EG(X, Y ))  1 · d(X),

and the following guarantee holds: For every partition (A , B ) of the set Y of vertices with

wG(EG(A , B ))  82 · min {d(A ), d(B )}, it holds that min {d(A ), d(B )}  z2. We set

S1 = X, and we let G2 = G \ S1.

The remainder of the algorithm consists of r - 1 iterations i = 2, 3, . . . , r. The input to

iteration i is a subgraph Gi  G with d(V (Gi))  d(V )/2, such that for every cut (A , B )

of Gi with wG(EG(A , B ))  i · min {d(A ), d(B )}, it holds that min {d(A ), d(B )}  zi.

(Observe that, as established above, this condition holds for graph G2). The output is a subset

Si  V (Gi) of vertices, such that d(Si)  d(V (Gi))/2 and wG(EGi (Si, V (Gi) \ Si))  i · d(Si), and, if we set Gi+1 = Gi \ Si, then we are guaranteed that for every cut (A , B ) of Gi+1 with

wG(EG(A , B ))  8i+1 · min {d(A ), d(B )}, it holds that min {d(A ), d(B )}  zi+1. In

particular, if wG(EG(A , B ))  i+1 · min {d(A ), d(B )}, then min {d(A ), d(B )}  zi+1

holds. In order to execute the ith iteration, we simply apply Lemma 2.15 to the set V =

V (Gi) of vertices, with parameters  = i, z = zi and z = zi+1. As we show later, we will

ensure that d(V (Gi))  d(V )/2. Since, for i > 1, zi  d(V )/8 < d(V )/6  d(V (Gi))/3, the

outcome of the lemma must be a partition (X, Y ) of V , where d(X)  d(Y ) (where possibly

X = ), wG(EG(X, Y ))  i · d(X), and we are guaranteed that, for every partition (A , B )

of the set Y of vertices with wG(EG(A , B ))  8i+1 · min {d(A ), d(B )}, it holds that

min {d(A ), d(B )}  zi+1. Therefore, we can simply set Si = X, Gi+1 = Gi \ Si, and continue

to the next iteration, provided that d(V (Gi+1))  d(V )/2 holds.

We next show that this indeed must be the case. Recall that for all 2  i  i, we guarantee

that d(Si )  zi  d(V )/(2·4i -1). Therefore, if we denote by Z =

i i

=2

Si

and Z

= V (G2)\Z,

then d(Z)  d(V )/2 ·

i i

=2

1/4i

-1



d(V

)/6,

so

d(V (Gi+1)) = d(Z ) = d(V (G2)) - d(Z)  2d(V )/3 - d(V )/6 = d(V )/2.

as promised.

We continue the algorithm until we reach the last iteration, where zr = 1 holds. Apply

Lemma 2.15 to the final graph Gr with z = 1/2 to obtain Sr  V (Gr). Since z < 1, the

discussion in Item 2 implies that graph Gr \ Sr has d-sparsity at least  (recall that r = ).

We define our final partition as Y = V (Gr) \ Sr and X = V \ Y =

r i=1

Si.

By the same

reasoning as before, we are guaranteed that d(Y )  d(V )/2  d(X). Finally,

r

r

wG(EG(X, Y ))  wG(EG(Si, V (Gi) \ Si))  i · d(Si)   · (log m)O(r4) · d(X),

i=1

i=1

which concludes the proof of Theorem 2.4.

Finally, we prove Corollary 2.5, which is almost identical to the proof of Corollary 8.5 of [CGL+20].

Proof (Corollary 2.5). We maintain a collection H of disjoint sub-graphs of G that we call clusters, which is partitioned into two subsets, set HA of active clusters, and set HI of inactive clusters. We ensure that each inactive cluster H  HI has d|V (H)-sparsity at least . We also maintain a set E of "deleted" edges, that are not contained in any cluster in H. At the beginning of the algorithm, we let H = HA = {G}, HI = , and E = . The algorithm proceeds as long as HA = , and consists of iterations. For convenience, we denote  = (log m)O(r4) the approximation factor achieved by the algorithm from Theorem 2.4, and we set  = /(c · log(mU )), for some large enough constant c, so that  =  / logO(r4) m log U
holds.

12

In every iteration, we apply the algorithm from Theorem 2.4 to every graph H  HA, with

the same parameters , r, and . Consider the partition (A, B) of V (H) that the algorithm

computes,

with

w(EH (A, B))   · d(V (H)) 

·d(V (H)) c log(mU )

.

We

add the

edges of

EH (A, B)

to

set E . If d(A), d(B)  d(V (H))/3, then we replace H with H[A] and H[B] in H and in HA.

Otherwise, we are guaranteed that d(A)  d(V (H))/2 and d|A (H[A])  . Then we remove

H from H and HA, add H[A] to H and HI , and add H[B] to H and HA.

When the algorithm terminates, HA = , and so every graph H  H has d|V (H)-sparsity at least . Notice that in every iteration, the maximum value of d(V (H)) of a graph H  HA must

decrease by a constant factor. Therefore, the number of iterations is bounded by O(log(mU )).

It is easy to verify that the total weight of edges added to set E in every iteration is at most

c

·d(V ) log(mU

)

.

Therefore,

by

letting

c

be

a

large

enough

constant,

we

can

ensure

that

w(E

)



d(V ).

The output of the algorithm is the partition P = {V (H) | H  H} of V . From the above

discussion, we obtain a valid ( , )-expander decomposition, for  =  / logO(r4) m log U .

It remains to analyze the running time of the algorithm. The running time of a single iteration is bounded by m · (mU )O(1/r). Since the total number of iterations is bounded by O(log(mU )), we get that the total running time of the algorithm is m · (mU )O(1/r) log(mU ).

Acknowledgements
We thank Julia Chuzhoy and Richard Peng for helping improving the presentation of this note and helpful comments.

References

[CGL+20] Julia Chuzhoy, Yu Gao, Jason Li, Danupon Nanongkai, Richard Peng, and Thatchaphol Saranurak. A deterministic algorithm for balanced cut with applications to dynamic connectivity, flows, and beyond. In 61st IEEE Annual Symposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA, November 16-19, 2020, pages 1158­1167, 2020. , 1, 2, 3, 11, 12

[GLN+19] Yu Gao, Jason Li, Danupon Nanongkai, Richard Peng, Thatchaphol Saranurak, and Sorrachai Yingchareonthawornchai. Deterministic graph cuts in subquadratic time: Sparse, balanced, and k-vertex. arXiv preprint arXiv:1910.07950, 2019. 4

[GR99]

Oded Goldreich and Dana Ron. A sublinear bipartiteness tester for bounded degree graphs. Combinatorica, 19(3):335­373, 1999. 1

[KKOV07] Rohit Khandekar, Subhash Khot, Lorenzo Orecchia, and Nisheeth K Vishnoi. On a cut-matching game for the sparsest cut problem. Univ. California, Berkeley, CA, USA, Tech. Rep. UCB/EECS-2007-177, 2007. 3

[KVV04] Ravi Kannan, Santosh Vempala, and Adrian Vetta. On clusterings: Good, bad and spectral. J. ACM, 51(3):497­515, 2004. 1

[Li21]

Jason Li. Deterministic mincut in almost-linear time. STOC, 2021. 1

[LP20]

Jason Li and Debmalya Panigrahi. Deterministic min-cut in poly-logarithmic maxflows. In Symp. Foundations of Computer Science (FOCS), 11 2020. 1

[Mad10a] Aleksander Madry. Fast approximation algorithms for cut-based problems in undirected graphs. In FOCS, pages 245­254. IEEE Computer Society, 2010. 4

[Mad10b]

Aleksander Madry. Faster approximation schemes for fractional multicommodity flow problems via dynamic graph algorithms. In Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 121­130, 2010. 3

13

[NS17]
[ST04] [SW19] [Wul17]

Danupon Nanongkai and Thatchaphol Saranurak. Dynamic spanning forest with worst-case update time: adaptive, Las Vegas, and O(n1/2- )-time. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 1122­1129, 2017. 1, 10
Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. In STOC, pages 81­90. ACM, 2004. 1
Thatchaphol Saranurak and Di Wang. Expander decomposition and pruning: Faster, stronger, and simpler. In SODA, pages 2616­2635. SIAM, 2019. 1
Christian Wulff-Nilsen. Fully-dynamic minimum spanning forest with improved worst-case update time. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 1130­1143, 2017. 1

14

