End-to-End Hierarchical Relation Extraction for Generic Form Understanding
Tuan Anh Nguyen Dang, Duc Thanh Hoang, Quang Bach Tran, Chih-Wei Pan, Thanh Dat Nguyen Cinnamon AI Labs Hanoi, Vietnam
Corresponding author email: kristopher@cinnamon.is

arXiv:2106.00980v1 [cs.AI] 2 Jun 2021

Abstract--Form understanding is a challenging problem which aims to recognize semantic entities from the input document and their hierarchical relations. Previous approaches face significant difficulty dealing with the complexity of the task, thus treat these objectives separately. To this end, we present a novel deep neural network to jointly perform both entity detection and link prediction in an end-to-end fashion. Our model extends the Multistage Attentional U-Net architecture with the Part-Intensity Fields and Part-Association Fields for link prediction, enriching the spatial information flow with the additional supervision from entity linking. We demonstrate the effectiveness of the model on the Form Understanding in Noisy Scanned Documents (FUNSD) dataset, where our method substantially outperforms the original model and state-of-the-art baselines in both Entity Labeling and Entity Linking task. 1
I. INTRODUCTION
Administrative documents are one of the most widely used medium for data storage and communication, which raises the need for an automated solution for form understanding. However, this task remains largely difficult due to the layout variations between forms for different purposes, organizations, etc. Owning to its complexity, classical works like heuristicbased [1], [2], or recent deep learning-based methods [3], [4] often break the problem into several sub-components: text-line detection, entity recognition, relation extraction, etc. The rationale of the division is that we may have better control of the input-output correspondence while cascading information for later steps. Following [5], the problem of Form Understanding can be formulated as 3 main tasks: Word Grouping, Entity Labeling - detection of questions (keys) and answers (values), and Entity Linking - their linking. Within this scope of this paper, we take another look at this prevalent problem by proposing an end-to-end model to solve both Entity Labeling and Entity Linking task at the same time.
Aside from the aforementioned heuristic-based methods for document information extraction (i.e using hand-crafted rules to build up and extract tables and identifying key-value pairs), deep-learning-based methods are also received much attention during the recent years [6], [7], [8]. Although deep-learningbased methods have achieved state-of-the-art accuracy recently in both entity segmentation in documents [9], link prediction on large social network graph [10], and matching between human pose joint [11], there have been few works on endto-end models performing both entity extraction and link
1https://github.com/hoangthanh283/MSAUPAF.git

prediction of a document at the same time. Our hypothesis is that while being advantageous for intermediate supervision, the division of architecture to different stages are not necessary for optimal speed and performance. As they heavily depend on each other as a flow, if the previous steps like entity recognition do not perform well, it would severely affect the performance of the later entity linking stage by error accumulations.
In this paper, we present the extension of Semantic Entity Labeling with relational linking by adapting the PartAssociation-Field mechanism for mutual relations extraction among entities. Our work is based on the previously proposed Multi-Stage Attentional U-Net (MSAU) architecture [9] for its proven effectiveness in entity segmentation from the char-grid embedding [12]. We go beyond its limitation of having no direct supervision from the linking between entities by adding Part-Intensity Fields and Part-Association Fields (PIF-PAF) [13] which results in an additional two-head branch for entity linking and prediction from document images. In addition to original segmentation output, the Part-Intensity Field head produces of confidence score of each entity's key points, while the Part-Association Fields head allows the associations between entities. Apart from the major addition of PIF-PAF module, our work also improved upon MSAU by adding Corner Pooling [14] to solve the issue of long-range distance between entities, enabling the wider-range propagation of information both horizontally and vertically. The Coordinate Convolution (CoordConv) [15] is also included to incorporate translational signals to the model. The effectiveness of these modules will be shown in Section IV.
We demonstrate the capability of our proposed model on the Form Understanding in Noisy Scanned Documents dataset (FUNDS) [5], focusing on the Entity Labeling and Entity Linking task. While the line of work for Entity Labeling is actively developed in [4], [9], [3], [16], for the combined Entity Labeling and Entity Linking task, the methods reported in [5], [17], [18] only achieved limited performance due to the bottleneck in exploiting both semantic and spatial features and the highly imbalanced links between entities. Under those challenging conditions, experiments demonstrate that our approach outperforms various strong baselines (including Multi-layered Perceptron (MLP) with BERT, recently proposed MSAU [9], LayoutLM [17], and Graph-based methods [19]), achieving state-of-the-art performance.

In short, we summarize our contribution list:
· We proposed an end-to-end architecture combining semantic entity extraction and link prediction for form understanding, which incorporates the Part-Association Field and Part-Intensity Field to the baseline MSAU model.
· We improved upon the original architecture of MSAU by adding Corner Pooling and Coordinate Convolution to aid spatial context modeling, which has been reaffirmed by our empirical experiments.
· Ablation studies show that utilizing the relations supervision can improve segmentation results for entity extractions in comparison with other baselines.
The rest of the paper is organized as follows: Section II describes the related works, followed by that is the proposed method in Section III, the experiments in Section IV.
II. RELATED WORK
Our works focus on incorporating both information extraction from document images and Entity Linking into one model. The former consists of several sub-themes: heuristic methods, semantic segmentation on images, graph-based information extraction. The Entity Linking part of our model in particular also incorporates techniques from human pose estimation and keypoints association, which we will describe briefly below and in detail in Section III.
Heuristic methods: Heuristic methods for structured data extraction are either concerned with the layout analysis problem before the information extraction, the first stage (Word Grouping) or the third stage (Entity Linking). Prior to the information extraction stage, [20], [21] tries to build up the components using carefully designed feature engineering (ridges and whitespaces [20], connected component analysis [21]) for text lines segmentation from images. This step is important because its performance determines the accuracy for the rest of the pipeline, less-precise segmentation requires more robustness (for over/under segmentation errors) in later steps. Heuristic methods for table segmentation and extraction [1], [22] can be used in either the first or the third step in form understanding task. While [1] separated table detection tasks into specific stages: Clustering words to text lines, merging text lines into tables by heuristic rules (Same task as Word-Grouping [5]). [22] represents text lines as graph nodes and finds the linking between them by calculating the edged-weights (the cost of two nodes belonging to the same types). Since these algorithms are designed tailoring a certain condition (i.e specific type of forms, layouts, required information), their scalability/generalizability towards large datasets is limited. The MSAU [9] and our model, however, are based on character-level mask, which leads to naturally more robust to these kinds of errors [9], the deep segmentation model also improves generalizability.
Deep learning-based segmentation and Link prediction: Early work in information extraction from document images formalize the problem as a segmentation problem based on either Fully Convolutional Networks [23] or U-Net [24],

[25] applied fully convolutional network to directly segment regions of interest from historical documents. [3] proposed architecture takes the spatial structure into account by using convolutional operations on the concatenated document text and image modalities, the text modality is represented by embedding the extracted text in a spatial grid. Our work built upon [9] is more aligned with recent end-to-end systems incorporating both semantic structure and spatial image features [12], [16]. Similar to [9], char-grid [12] is also employed instead of sub-word, word, or sentence level embedding. The character-level embedding has two advantages: embedding character-level is less memory-consuming and has been proven to be robust against potential accumulated text-line OCR and segmentation errors [9].
Information extraction with Graph Neural Networks: Recently, there are several works on modeling the entity's relationship based on graph convolution networks [4], [26] which have achieved promising results. [4] introduced edge embeddings into the graph convolution network, which models the relationship between vertices directly while both works [26] used the nearest spatial distanced entity as a key to determining the linking for graph building. Graph Neural Networks in general, have also been used for link-prediction problems (which is close to the Entity Linking stage) [10]. However, to the best of our knowledge, there have not been any attempts to incorporate both the tasks of information extraction (formalized as node classification in [4], [26]), and Entity Linking. We believe that this line of direction is very potential due to the capability of incorporating both semantic, spatial information. However, the implementation of these methods, have yet to reach the best possible performance under the condition of erroneous output from earlier steps (text lines recognition and optical character recognition). The char-grid [12] is naturally robust to these types of errors as stated in [9], this is the reason we chose to develop our work of Entity Linking upon the line of segmentation over char-grid.
Human pose estimation: As mentioned earlier, our improvement for the incorporation of Entity Linking is highly relevant to the technique used in Human Pose Estimation, applied in a sense that linking between entities can be similar to linking between joints of human poses. In the context of human pose estimation, there are mainly two approaches: top-down and bottom-up. The top-down approach starts by identifying and localizing individual person instances, followed by single pose estimation [27], [28]. Although this approach makes the human pose estimation is more straightforward, it might not be applicable to our case because unlike human joints which are often close to each other, key-value pairs can be distanced apart. Furthermore, a single key in our case can match multiple values, making the process of pinpointing accurately bounding box of all pairs complicated due to overlapping. Hence we found this is not for capturing the entity's relation in the documents.
In the bottom-up approach, some recent works use greedy decoders in combination with additional tools to generate person instance as in [7] propose associative embedding to

identify key points from the same person, [11] use Part Affinity Fields to infer human poses. Inspired from [11] the PIF-PAF [13] incorporates human body parts with a PartAssociation field, which yields better quality results, and drastically reduces prediction time. The characteristic of this method involving bipartite matching, we believe, is similar to the match between key-value pairs. Inspired from this line of work, we incorporate Part-Association Fields as part of the model to encode of joint association all entities in the document.
In short, we built our work upon the line of deep-learningbased models, in particular, MSAU [9]. Considering the advantageous characteristics of Part-Association Fields in matching human joints, we incorporated the technique into our model for end-to-end key-value detection and matching. On top of that, as we consider key-point is a text line's corner to localize and the text-line size of text-lines can vary greatly or be occluded, also does the translational information (i.e Title is likely on top of the page). Of which, Corner Pooling from the work of Corner Net [14] has been proven to be useful in dealing with text line size variations. For the incorporation of translational information, which is potentially useful for link-prediction, Coordinate Convolution [15] mechanism has also been proven to be effective. We incorporate both of these [15], [14], in our model as described in Section III.
III. METHODOLOGY
An overview of the proposed method, MSAU-PAF, is depicted in Figure 1, it takes the same input as MSAU, namely the char-grid (see Subsection III-A). The model aims to handles Entity Recognition and Entity Linking in an endto-end fashion. The Entity Recognition output is the same as MSAU (Subsection III-B), for which the baseline architecture of MSAU is incorporated and nearly unchanged except the addition of CoordConv (Subsection III-D.
The model is also expected to produce the linking between them correspondingly with class and relation of entities in the dataset. For this task Part-Intensity Fields and Part-Association Fields (PIF-PAF) modules [13] are employed, MSAU-PAF interfaces between MSAU and PIF-PAF by concatenating output from bottleneck layer of MSAU and make use of Corner Pooling (see Subsection III-C).
The head of MSAU will be used to segment entity classes while the other two head networks (PIF-PAF) are used as follows: PIF head is used for predicting confidence, precise location, and size of keypoints, while PAF head predicts associations between keypoints (entities), called Part-Association Field (PAF).
A. Char-grid
We adopt the char-grid representation [12] as the method of embedding each character's information into a feature map region that the character occupies. This embedding method allows the incorporation of both character position and semantic information at the same time. Thus it is proven to be effective in information extraction from document images [12], [9]. We

obtain the char-grid CM  {0, 1, ..., Nchar}H×W similar to [9], where Nchar is the number of chosen characters, H and W are the height and width of the input, respectively.
B. Multi-stage Attentional U-Net
We choose MSAU [9] as baseline due to several advantages that give us even more supervision over the model output and the evaluated model's empirical performance. MSAU [9] improved upon Coupled U-Net [30] with several techniques such as Non-local block [31], and Box Convolution [32]. The idea is to segment entities from char-grid with multiple intermediate outputs (segment key mask at one block and value mask at another) for better supervision. As a result, MSAU has been proven to be effective in entity segmentation tasks [9]. We extend the work of MSAU to integrate Entity Linking explicitly to the model, especially long-ranged semantic relations. our improvement over the base line architecture is shown in MSAU block of Figure 1, where we added the coordinate convolution layers [15] prior to the first U-Net block. The first U-Net block of MSAU is used for output key masks, while the second output full key-value segmentation. The output of attention layers from the bottleneck (output from the yellow block in the diagram) is used to feed into corner-pooling modules, followed by PIF-PAF [13] module to output the keypoints confidence score and Entity Linking maps. The rest of the architecture is left untouched.
C. Part-Intensity Fields and Part-Association Field
Inspire from recent human pose estimation advancement, the Part-Intensity Fields (PIF), and Part-Association Fields (PAF) [13] are all confidence maps with vector fields. Each vector field composes a scalar component for confidence, size of the keypoint and vector components. The outputs from bottlenecks of each U-Net block of MSAU are concatenated and go through corner pooling before being fed to PIF-PAF. Thus, the input of PIF-PAF block is a set of feature maps F. PIF-PAF has multiple stages, each stage is a PIF-PAF block, similar to each U-Net block of MSAU. The PIF will find and localize key points via predicting a composite field, containing confidence score c, a vector (x, y) with spread b and a scale , in each location (i, j) in the output maps which can be written as pij = {picj, pixj, piyj, pibj, pij}. On the other side, the role of PAF would be to find two entities, which not necessarily stay close to each other and could be far away, and determine the confidence of their association. At every output location, PAFs will be represented as aij = {aicj , aixj1, aiyj1, aibj1, aixj2, aiyj2, aibj2}, this help the PAFs have the capacity to estimate multiple entities' associations. By enjoying the fine-grained information from PIFs, the PAFs could utilize this to predict entity's locations and their association which have been shown to be efficient and robust in dense and occluded documents via our experiments.
D. Coordinate Convolution
We take into account the fact that, since MSAU-PAF's inputs (character masks) are built from the output potentially

Fig. 1: The architecture of MSAU-PAF prediction. The MSAU Block is based on MSAU [9] with adding some components which are Attention layers [29] and CoordConv layers [15] to exploit spatial information from the image. After this stage, the prediction is forwarded through a Corner Pooling and taken as input of Part-Association-Field. The Part-Association-Field block is a two-heads net with Part-Intensity Fields (PIF) and Part-Association Fields (PAF) and produce the linking prediction in document image.

erroneous text line recognition steps from scanned or camerabased document images, there might be variations in terms of text-lines scale, along with other distortions such as slight rotation, skewing, and non-aligned text-lines.
In order to improve MSAU-PAF's translational awareness in the entity's localization, Coordinate Convolution (CoordConv) [15] is applied. CoordConv adds two extra channels (in case of images) filled with coordinate information, concatenated at the end existing input feature map, allowing the model to learn varying degrees of translation-based cues. Zero weights for these channels would imply full translational invariance like standard convolution while any other value would lead the network to learn translation variance up to a degree which becomes a training parameter depending on the task. It is applied in both MSAU and the head networks.
E. Corner Pooling
Since text-lines and key-value pairs' distances vary in sizes, representing entities' key points as single pixels in feature maps might make it hard for the PAF heads to propagate information spatially between entities. In detail, intuitively, the receptive fields in normal convolution layers may not be able to effectively capture the local visual evidence in a long range distance. Furthermore, there exist cases where linked entities are far from each other, such as item values with respect to headers in a table. To address this issue, we apply Corner Pooling [14] to better gather all visual features in a longdistance by max-pooling horizontally towards the right and vertically towards the bottom of the feature maps. The corner

pooling encodes explicit prior knowledge about the objects' corners. Given H × W feature maps, the corner pooling layer first perform max-pooling on all feature vectors between (i, j) and (i, H) in ft, resulting in a feature vector tij. Followed by that, all feature vectors between (i, j) and (W, j) in ft are max-pooled to a feature vector lij. Finally, tij and lij are added together.
F. Training details
In this section, we describe the detail of the training stage, including data augmentation, model hyper-parameters, and the loss function. Our model is set up as follows:
· Data augmentation We perform several data augmentation methods in order to increase the capacity of our model against the variety of layouts as follows: random character replacement to mimic OCR errors, random text box shifting, random affine transformation, random background padding. In addition, the text-regions in the input image are down-scaled so that the median text height is 3 pixels in the char-grid.
· Implementation detail With char-grid input, we use the most frequency characters N = 90. We follow the MSAU's implementation with the number of UNet blocks nblocks = 2, the depth of the ResBlock res depth = 2. the number of downsampling blocks in an encoder ndownsampling = 4. Moreover, we also add CoordConv operation [15] to each layer of U-Net's encoder and use corner-net at the end of the final U-Net's

decoder. As in Figure 1, at the end of MSAU, the model is separated into two branches which are PAFs and PIFs. · Loss function The multi-task loss used in the training process is the sum of three smaller losses. We apply Cross-Entropy loss [9] at the output of MSAU block to guide the network to learn the segmentation mask between ground-truth label and predicted label. Moreover, to predict the confidence maps of entities in the first branch PIF and the PAFs in the second branch, we used two Laplace losses [13] at each branch between the estimated predictions and the ground-truth maps. The Laplace losses, which is a modified L1 type loss with injecting a predicted spread b dependence, helps the network handle the diversity of scales of text-lines in the documents. The loss is formulated as below:

TABLE I: The statistics of FUNSD dataset [5]: The table shows number of words and number of entities in total. And between entities, the relation between them are labeled by linking the "id" of entities

Split Training Testing

Forms 149 50

Words 22, 512 8, 973

Entities 7, 411 2, 332

Relations 4, 236 1, 076

Fig. 2: The class distribution of entities in FUNSD dataset [5]. There are four main classes which are Header, Question, Answer and Other.

Ltotal = 1LCE + 2LLaplace-P IF + 3LLaplace-P AF (1)
with 1, 2 and 3, loss weights, are set to 1, 1e-2, 1e-2 respectively.

IV. EXPERIMENTS
A. Dataset
For evaluation, the FUNSD (Form Understanding in Noisy Scanned Document) dataset is used [5]. The dataset consists of 3 different tasks: Text Detection, Text Recognition (Optical Character Recognition), and Form Understanding. Following the description in [5], Form Understanding is further broken down into three sub tasks: Word Grouping, Semantic Entity Labeling and Entity Linking.
Word Grouping Word grouping concerns the clustering of words belong to the same entity. The task takes input as word locations, each as a tuple (x1, y1, x2, y2) denoting the upper left corner's and lower right corner's image coordinates of each word's bounding box. It is required from the task to clusters the words that belongs to the same entity, namely, the system solving the challenge is expected to output the bounding box of each entity and the merged text from accumulated words.
Semantic Entity Labeling The input of this task is the coordinate of text line (received from the output of word grouping tasks). This task requires classifying entities into one of four pre-defined categories: question, answer, header and, other. See Table 2 for the distribution of classes.
Entity Linking The task takes accumulated output information from 2 previous tasks as input: coordinates of text-line regions, text-recognition result, and Entity Labeling. With each text line given an ID, the system is expected to output entities relations under the form of pairs (id1, id2) where id1 is the unique ID of entity labeled as question and id2 is the ID of entity labeled as the answer.
The dataset contains 199 fully annotated forms with a variety of structure and appearance (e.g marketing, science reports, invoice), separated into two subsets which are training set and test set. The statistics of the dataset can be seen in Table I and Figure 2, further detail can be found in the description paper [5].

B. Experiment setup
In this paper, we only focus on the tasks that belong to Form understanding. Due to the characteristic of the model not requiring entities to be grouped (first task output), the evaluation is done on the two sub-tasks: Semantic Entity Labeling and Entity Linking
For Semantic Entity Labeling, F1-score is employed for evaluation. The F1-score is calculated from the semantic segmentation output of the model using the box F1-Score introduced in [9] - a predicted box is marked as a correct detection if its IoU ratio with one of the ground-truth boxes is larger than a certain threshold 0.8.
The Entity Linking task remains the same. Given the output relations between by pair (id1, id2), the system is benchmarked using the F1-score of all possible pairs as in [5]
For the task of Semantic Entity Labeling, these baselines are used:
· MLP - Multi-layered Perceptron (results are referenced directly from [5])
· MSAU - Multi-stage Attentional U-Net [9] · LayoutLM - Pre-training of Text and Layout for Docu-
ment Image Understanding [17]
For Entity Linking, these baselines are used:
· MLP - Multi-layered Perceptron (results are referenced from [5].
· GraphCNN with Link prediction. · Heuristics method.

The baseline methods are tested with the normal input of each task (i.e MSAU only evaluated on one task of Semantic Entity Labeling, GraphCNN with Link Prediction is run given assumed perfect output from Semantic Entity Labeling). Detail of the baseline methods can be referred to in the next section. For MSAU-PAF, it is emphasized that no intermediate input is given between the first input and Entity Linking output. We set up our baseline models (MSAU and Graph) and MSAU-PAF, the training processes are performed on a server with Tesla V100 (with 16GB memory GPU). The model is trained 200 epochs with a mini-batch size of 4. In order to optimize the model, we use the RMSProp method. The learning rate is set to 0.001 at the beginning and decreased every 10 epochs with a polynomial decay of 0.9. The result is shown in Subsection IV-D
C. Baseline methods
MLP For Entity Labeling, the input of the model is features extracted from pre-trained model BERT [33] as a 733dimensional vector and forward through two 500-units hidden layers with ReLU activation and the final output is put through a softmax layer. For the Entity Linking task, the input of the model is the concatenation of every pair of Semantic Entity.
MSAU Following the settings of MSAU in [9], 2 U-Net blocks are used, with ndownsampling = 5 (5 down-sampling layers each block) and res depth = 2 (2 residual convolutions in each down/up-sampling layer) and without Box Convolution. The MSAU-PAF shares the same structure with MSAU except that the output is modified as a binary classification.
GraphCNN with Link prediction the Graph Convolution Operations employed in the experiment is Spatial Graph Filtering, introduced in [19], text lines are encoded from BERT with 733-dimensional vector, combined with normalized coordinate (x1, y1, x2, y2) to produce 737-dimensional vector as input. The spatial graph of text lines is built from nearest aligned texts as an indication of having a spatial edge. Given N × F (F = 737) and A  {0, 1}N×N×L (L = 4: Top, bottom, left, right) as input node features and adjacency matrix. The baseline model has 4 layers, encoding the input to 256, 128, 128 sequentially, the final layer's outputs of the model for each node pairs is then fed to a single hidden layer with 500 units and output whether a link exists between two nodes.
Heuristics method The input of the model is the groundtruth class and coordinate of text-line regions. After that, several Rules-based are set up as follows: with an entity labeled as the answer, the model finds the nearest entity labeled as a question by measuring the Euclid distance between this entity and others "question" entities and the result are pairs (id1, id2) of entities with id1 is the ID of question text-line region and id2 is the ID of answer text-line region.
D. Results
We visualize the final output by images in table II. The first and second row are input and the char-grid of images after pre-processing respectively. The result of model can be visualized in the Output row. As we can see, the Heatmap

row shows the entity locations through bottom-left points, while the output row shows the full results of our model. The header, question, and answer are drawn by yellow, green, and blue segmentation masks respectively. The linking between entities (if have) are visualized through orange lines. These lines start from the bottom-left points of entities labeled as the answer to the bottom-left points of entities called the question. Our model has this ability because from keypoint detection of each entity, PAFs are likely to link these text-regions by using a directed vector.
In order to benchmark the performance of our model, we evaluate it with other methods on FUNSD dataset [5]. All results are measured by F1-score.
Entity Labeling Task Table III shows the comparison between MSAU-PAF and the baselines. MSAU-PAF model achieves the highest F1-score 0.83, which significantly improves the accuracy compared to MLP [5] the baseline model - MSAU [9] by 0.26 and 0.1 respectively. On top of that, our model outperforms the state of the art LayoutLM [17] by a large margin. The improvement in accuracy hints that the supervision from Entity Linking can be one of the factors improving the accuracy of the model in comparison with segmentation-only supervision of MSAU [9]. It is noted that the MLP baseline [5] utilized a much more sophisticated pre-trained model than MSAU and MSAU-PAF (BERT [33] is proven to be state-of-the-arts in many tasks from the aforementioned paper in comparison with one-hot character encoding). The improvement in MSAU and MSAU-PAF accuracy is potential because of the model making better use of spatial information and char-gird embeddings, as mentioned in [9]. Meanwhile, the LayoutLM mainly relies on the image embeddings via utilizing bounding box regions in the prior step, which appears to fail to capture well the spatial and relational information among entities.
Entity Linking Task With this task, MSAU-PAF continues outperforming other methods (see in Table IV). It is observable that the F1-score of GraphCNN and MLP baselines are low, this can be due to the effect of highly dense and imbalanced class distribution in conjunction with Deep-learning based matching. The low F1 scores come mainly from low precision (model predicts most of non-link for most of the pairs). The same hypothesis has been made in the original paper [5]. In contrast, the PIF-PAF heads could particularly handle well in densely and occluded documents via effectively encoding fine-grained information and confidently predicting confidence association between two entities with composite field structure. This leads to 0.71 and 0.62 improvement in F1-score from MLP and GraphCNN with Link prediction respectively. It is noted that other baselines are assumed to already know the ground-truth of the class of entities, while our model only uses the text-line regions and optical character.
Compared to the Heuristic method with Ground-truth entities class, the MSAU-PAF seems to have a little lower F1score. This can be because of the use of the ground-truth label for each entity, while in the MSAU-PAF experiment, we combine both two tasks and the result is the joint probability of

TABLE II: Result of MSAU-PAF: The first and second rows are the input and char-grid after being processed respectively. The third row shows the entity locations through bottom-left points and the fourth row visualizes the results of the model. The header, question, and answer are segmented by yellow, green, and blue color correspondingly. The links between entities (if existed) are visualized through orange lines.
Images

Char-grid

Heatmap

Output

TABLE III: Result of Entity Labeling: The comparison of TABLE IV: Result of Entity Linking: The comparison of

baseline models and MSAU-PAF on Entity Labeling task of baseline models and MSAU-PAF on Entity Linking task of

FUNSD dataset, measured by F1-score.

FUNSD dataset, measured by F1-score.

Methods MLP (BERT embedding) [5] MSAU (one-hot character) [9] LayoutLMBASE [17] MSAU-PAF (one-hot character)

Metric (F1 score) 0.57 0.73 0.79 0.83

Entity Labeling and Entity Linking. The hypothesis is verified when we applied MSAU-PAF middle output as input to the heuristic rules.
E. Ablation study
The MSAU-PAF is also leveraged by two other components: Coordinate Convolution (CoordConv), and Corner pooling. To verify the effectiveness of these modules, the experiment is set up by testing MSAU-PAF as the baseline, following the hyperparameter settings in Subsection III-F. After that, each of the components is added and retested in both tasks. The

Methods MLP (BERT embedding + Positional feat) [5] (Ground-truth entities class) GraphCNN with Link prediction (Ground-truth entities class) Heuristics (with MSAU-PAF middle output) Heuristics (with Ground-truth entities class) MSAU-PAF (Joint classification and link prediction)

Metric (F1 score) 0.04
0.13
0.64 0.80 0.75

results of these modules applied in MSAU-PAF architecture is shown in Table V. It can be observed that these component has improved model performance over vanilla MSAU-PAF.
Ablation study for CoordConv The results of the experiments hint at the effectiveness of adding translational information by using CoordConv [15]. As shown in Table V, when making the convolution access to its own input coordinates through concatenating extra coordinate channels, MSAU-PAF with Coordinate Convolution experienced a total

TABLE V: Ablation study on CoordConv and Corner Pooling

Model
MSAU-PAF MSAU-PAF + CoordConv MSAU-PAF + Corner Pooling MSAU-PAF + CoordConv + Corner Pooling

Entity Labeling (F1-Score) 0.80 0.82 0.81 0.83

Entity Linking (F1-Score) 0.72 0.74 0.73 0.75

gain of 0.02 in F1 score in both Entity Labeling and Entity Linking. It should be emphasized that using two or more CoordConvs does not bring a noticeable improvement in the experiment (less than 0.01 improvement in F1 on both tasks), so it is omitted in Table V. Thus, it is suggested that a single CoordConv layer will ensure both decent performance while avoiding additional computational cost.
Ablation study for Corner Pooling Corner Pooling is another additional component integrated, situating between MSAU and PIF-PAF. It is expected from the aforementioned paper [14] that Corner Pooling will help aggregate information better in long distance for key points detection. This has also been hinted by the experiment result by an increase of 0.01 in F1 score in Entity Labeling task and 0.01 in Entity Linking. It is noted that combining CoordConv and Corner Pooling can even increase the model performance further: The last row shows the results that we test on combining all improvements, which improves the F1 score by 0.03 by the total in both Entity Labeling and Entity Linking.
V. CONCLUSION
We have proposed the MSAU-PAF - an end-to-end deep architecture for form understanding tasks. Our model extends the original MSAU architecture with link-prediction between entities by integrating the PIF-PAF mechanism and other enhancements for exploiting spatial correlation. Experiment results on the FUNSD dataset show significant improvement in comparison with other baselines. In the future, we wish to extend MSAU-PAF to other document structure parsing problems, i.e table structure recognition, document de-warping.
REFERENCES
[1] Ermelinda Oro and Massimo Ruffolo. PDF-TREX: An approach for recognizing and extracting tables from PDF documents. ICDR, 2009.
[2] Luisa Simoes, Goncalo; Galhardas, Helena; Coheur. Information Extraction tasks : a survey. 2005.
[3] Rasmus Berg Palm, Florian Laws, and Ole Winther. Attend, Copy, Parse - End-to-end information extraction from documents. CBDAR, 2019.
[4] Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao. Graph Convolution for Multimodal Information Extraction from Visually Rich Documents. NAACL, 2019.
[5] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents. ICDR, 2019.
[6] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image Generation from Scene Graphs. CVPR, 2018.
[7] Alejandro Newell and Jia Deng. Pixels to Graphs by Associative Embedding. NIPS, 2017.
[8] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. Graph R-CNN for Scene Graph Generation. ECCV, 2018.

[9] Nguyen Dang Tuan Anh and Nguyen Thanh Dat. End-to-End Information Extraction by Character-Level Embedding and Multi-Stage. BMVC, 2019.
[10] Muhan Zhang and Yixin Chen. Link Prediction Based on Graph Neural Networks. NIPS, 2018.
[11] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. CVPR, 2016.
[12] Anoop Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes Ho¨hne, and Jean Baptiste Faddoul. Chargrid: Towards Understanding 2D Documents. EMNLP, 2018.
[13] Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. PifPaf: Composite Fields for Human Pose Estimation. CVPR, 2019.
[14] Hei Law and Jia Deng. CornerNet: Detecting Objects as Paired Keypoints. CVPR, 2018.
[15] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski. An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution. NIPS, 2018.
[16] Rasmus Berg Palm, Ole Winther, and Florian Laws. CloudScan - A Configuration-Free Invoice Analysis System Using Recurrent Neural Networks. 2018.
[17] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. LayoutLM: Pre-training of Text and Layout for Document Image Understanding. 2019.
[18] Subhojeet Pramanik, Shashank Mujumdar, and Hima Patel. Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning. 2020.
[19] Felipe Petroski Such, Shagan Sah, Miguel Alexander Dominguez, Suhas Pillai, Chao Zhang, Andrew Michael, Nathan D. Cahill, and Raymond Ptucha. Robust Spatial Filtering with Graph Convolutional Neural Networks. IEEE JSTSP, 2017.
[20] Syed Saqib Bukhari, Faisal Shafait, and Thomas M Breuel. High Performance Layout Analysis of Arabic and Urdu Document Images. ICDR, 2011.
[21] Byeongyong Ahn, Jewoong Ryu, Hyung Il Koo, and Nam Ik Cho. Textline detection in degraded historical document images. Eurasip Journal on Image and Video Processing, 2017.
[22] Deepayan Chakrabarti, Ravi Kumar, and Kunal Punera. A graphtheoretic approach to webpage segmentation. WWW, 2008.
[23] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. MICCAI, 2015.
[25] Sofia Ares Oliveira, Benoit Seguin, and Frederic Kaplan. dhSegment: A generic deep-learning approach for document segmentation. ICFHR, 2018.
[26] D. Lohani, A. Bela¨id, and Y. Bela¨id. An Invoice Reading System Using a Graph Convolutional Network. 2019.
[27] George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, and Kevin Murphy. Towards Accurate Multi-person Pose Estimation in the Wild. CVPR, 2017.
[28] Bin Xiao, Haiping Wu, and Yichen Wei. Simple Baselines for Human Pose Estimation and Tracking. ECCV, 2018.
[29] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Nonlocal Neural Networks. CVPR, 2018.
[30] Zhiqiang Tang, Xi Peng, Shijie Geng, Yizhe Zhu, and Dimitris N. Metaxas. Cu-Net: Coupled U-nets. BMVC, 2019.
[31] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Nonlocal Neural Networks. Technical report, 2018.
[32] Egor Burkov. Deep Neural Networks using Box Convolutions. NIPS, 2018.
[33] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-HLT, 2018.

