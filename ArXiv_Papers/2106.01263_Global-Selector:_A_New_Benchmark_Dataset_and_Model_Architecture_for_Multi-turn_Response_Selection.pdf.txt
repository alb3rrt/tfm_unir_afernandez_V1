Global-Selector: A New Benchmark Dataset and Model Architecture for Multi-turn Response Selection

arXiv:2106.01263v1 [cs.CL] 2 Jun 2021

Chiyu Song Westlake University songchiyu@westlake.edu.cn

Hongliang He Westlake University hehongliang@westlake.edu.cn

Huachuan Qiu Westlake University qiuhuachuan@westlake.edu.cn

Haofei Yu Zhejiang University haofeiyu@zju.edu.cn

Zhenzhong Lan Westlake University lanzhenzhong@westlake.edu.cn

Abstract
As an essential component of dialogue systems, multi-turn response selection aims to pick out the optimal response among a set of candidates to improve the dialogue fluency. In this paper, we investigate three problems of current response selection approaches, especially for generation-based conversational agents: (i) Existing approaches are often formulated as a sentence scoring problem, which does not consider relationships between responses. (ii) Existing models tend to select undesirable candidates that have large overlaps with the dialogue history. (iii) Negative instances in training are mainly constructed by random sampling from the corpus, whereas generated candidates in practice typically have a closer distribution. To address the above problems, we create a new dataset called ConvAI2+ and propose a new response selector called Global-Selector. Experimental results show that Global-Selector trained on ConvAI2+ have noticeable improvements in both accuracy and inference speed.
1 Introduction
Nowadays, dialogue response generation has gained increasing attention in the NLP community. An approach we call "generate-then-select" has been proven effective in giving natural and fluent responses and is widely used in many advanced chatbots, such as Meena (Adiwardana et al., 2020), DialoGPT (Zhang et al., 2020), PLATO (Bao et al., 2020), and Blenderbot (Roller et al., 2021). This approach works as follows: (i) Generating and decoding multiple candidate responses from a generator. (ii) Sending them to a selector to find the best one. (iii) Returning the optimal response. Thus, for generation-based dialogue systems, a good selector plays a vital role in boosting conversation quality (Adiwardana et al., 2020).
As we all know, the arrival of self-attention mechanism (Vaswani et al., 2017) and pre-trained models (BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), etc.) has led to remarkable progress in various natural language understanding tasks. In this paper, we focus on employing pre-trained models for multi-turn response selection and find three defects to be improved:
Equal contribution. Corresponding author.
Preprint. Under review.

1. This kind of task has always been regarded as a sentence scoring problem where every candidate is ranked separately. Each of them is binary classified as the ground truth or a negative sample during the training period, then the response with the highest confidence is selected at inference time. We would explore a much more powerful approach to deliberate choices where all candidates can thoroughly attend to each other.
2. Responses with higher semantic similarities to the contexts have higher probabilities to be selected. Such responses may have good relatedness to the topic but not coherent. Consequently, the dialogue content gradually tends to be repetitive as the conversation goes on.
3. Negative samples in many existing datasets are usually constructed by random sampling from the corpus. However, models can easily distinguish them from the ground truth because of their notable differences in topics and tones. Hence, in practical scenarios, strong negative candidates could severely degenerate models' performances.
We make the following two major contributions to address these issues: In terms of model architecture, we present Global-Selector, which treats the multi-turn response selection task as a machine reading comprehension problem rather than sentence scoring. Instead of ranking each response individually, it performs global attention among all given candidates and selects the best alignment with contexts in one shot. We show it can improve both the effectiveness and efficiency over other commonly used architectures. In terms of datasets, we apply two more negative sampling strategies on the ConvAI2 dataset (Dinan et al., 2019), together with some data augmentation approaches specially designed for the Global-Selector architecture. Experiments prove this new benchmark dataset that we call ConvAI2+ can mitigate the problem 2 and 3 mentioned above. Furthermore, our proposing methods are purely unsupervised. No additional annotations are needed.
2 Related Work
Generation-based chatbots are notorious for giving responses with uncontrollable quality. The basic greedy decoding picks the next token with the highest probability and frequently creates short and generic responses. Many other decoding strategies have also been studied, like Beam search, Top-K sampling (Fan et al., 2018), and Top-P sampling (Holtzman et al., 2020), aiming to offer more flexible contents. These approaches are practical in generating more diverse responses but still very unstable even after careful hyperparameters tuning. Moreover, external rules like temperature, 3-grams blocking (Paulus et al., 2018), and length penalty (Wu et al., 2016) are usually applied for more constraints. However, generations could remain either uninformative or too novel to be coherent and sensible. Intuitively, we believe the response selector, just like other strategies mentioned above, is used to increase generations' steerability. It balances generations' coherence and diversity in a more data-driven approach.
To the best of our knowledge, a model architecture referred to as Cross-encoder is widely used in many cutting-edge chatbots (Bao et al., 2020) as a response selector. Such an architecture concatenates the contexts and a single response, then feed it into an encoder to get a classification decision. This approach typically yields a good accuracy because tokens in the contexts and tokens in the candidate can directly attend to each other.
Another popular Bi-encoder architecture encodes contexts and the candidate separately, and then outputs their corresponding representations. After that, representations are combined to receive a selection probability through some scoring layers. Reimers and Gurevych (2019) carries out some ablation studies for Bi-encoders' design, while Lowe et al. (2015), Yoshino et al. (2019), and Dinan et al. (2019) conduct related experiments on response selection tasks.
Urbanek et al. (2019) and Humeau et al. (2019) prove that Cross-encoders perform better than Bi-encoders in terms of effectiveness, but Bi-encoders are more efficient because their candidates' representations can be cached and reused once they are created. However, since it is impossible to precompute embeddings for freshly generated responses, Bi-encoders cannot take this advantage during inference time in a generation-based dialogue system. Recently, another work named Polyencoders (Humeau et al., 2019) has also drawn our attention. It is a variant of Bi-encoders that achieve comparable accuracy as Cross-encoders in response selection with only a slight increase in computational cost.
2

Figure 1: Model architectures of Cross- and Bi- encoder.
Figure 2: Input embeddings of Cross-encoder. Only one type of segment embeddings is used in RoBERTa-based models, in order to keep consistent with its pre-training settings.
3 Methods
This section detailedly presents the implementations of Cross-encoder, Bi-encoder, and GlobalSelector on the multi-turn response selection task. 3.1 A Sentence Scoring Formulation Multi-turn response selection is normally regarded as a sentence scoring problem where we pair the dialogue history ctxt = X = {x1, . . . , xn} with each candidate response cand = Y = {y1, . . . , ym} respectively. Then every pair is assigned a score by neural nets, which indicates how appropriate this response is to the context. Eventually, the response with the highest ranking is selected. As shown in Figure 1, we build our Cross-encoder and Bi-encoder models based on pre-trained BERT or RoBERTa according to this formulation. Some of our settings follow these works: Reimers and Gurevych (2019), Humeau et al. (2019), Wolf et al. (2019), and Wang et al. (2020). 3.2 The Cross-Encoder Architecture Cross-encoder is a typical design where contexts and one candidate are concatenated and encoded by a single encoder. It allows rich attention among all tokens in the input.
H = encode(ctxt, cand) In our implementation, as depicted in Figure 2: two special role symbols are used as the separator between input utterances. We also insert [CLS] at the start of the whole sequence and [SEP ] after each segment. Each input token is represented as the sum of their corresponding token embeddings, segment embeddings, and positional embeddings.
3

Figure 3: Model architecture of Global-Selector.
Figure 4: Input embeddings of the candidate encoder in Global-Selector. Only one type of segment embeddings is used in RoBERTa-based models, in order to keep consistent with its pre-training settings.
After that, the encoded representation is fed forward into a sigmoid layer to binary classify the candidate as a positive or negative sample. We minimized a binary cross-entropy loss between its predicated probability and ground-truth label.
score = sigmoid(Wt · H)
3.3 The Bi-Encoder Architecture Unlike the Cross-encoder, contexts and a candidate response are fed into two separate encoders in a Bi-encoder model.
Hctxt = encode1(ctxt) Hcand = encode2(cand) These two encoders share parameters in our implementation. Their output representations are then concatenated and scored. The usage of special tokens, the scoring process, and the optimization objectives are similar to what we described in the Cross-encoder section. score = sigmoid(Hctxt · HcTand)
4

3.4 A Machine Reading Comprehension Formulation
Instead of sentence scoring, we formulate this multi-turn response selection task as a machine reading comprehension problem. In our study, all the candidates {cand1, . . . , candk} are concatenated, serving as a document. The given context ctxt serves as a question. Our Global-Selector model aims to identify the optimal response from the concatenated text to continue the dialogue. Some of our design is inspired by Liu (2019) and Zhang et al. (2020).

3.5 The Global-Selector Architecture

Figure 3 depicts the Global-Selector architecture that also leverages pre-trained BERT or RoBERTa as a basis. It resembles Bi-encoder but has an essential difference. All the candidates to the given context are concatenated and encoded together, which benefits from all-to-all attention across every response. Thus, instead of scoring each response separately, it conducts global comparisons between all candidates to find the best fit.
Hctxt = encode1(ctxt)
Hall_cand = encode2({cand1, . . . , candk})

In our implementation, these two encoders share parameters, and the context is encoded the same as in Bi-encoder. For the candidates' encoder, each candidate is surrounded by [CLS] and [SEP ] tokens, as shown in Figure 4. We also alternately assign two types of segment embeddings to each candidate. Conditioned on their indices in the concatenation, EA is used when it is odd, otherwise EB .

Hctxt and Hall_cand are then fed into a single attention layer where Hall_cand acts as Q and Hctxt acts as K and V . This cross-attention outputs a hidden state Hcross.

QK T

Hcross = Attention(Hall_cand, Hctxt, Hctxt) = softmax

 dk

V

There are two ways to obtain an aggregated embedding Ecandi for each candidate: use the corresponding [CLS] representation or average all token embeddings belonging to it in Hcross. We compare these two options in the Appendix and use the second one in our subsequent experiments. After aggregation, every candidate embedding is projected into a single logit and later merged for a softmax operation.
Ypred = softmax({Ecand1 , . . . , Ecandk })
A ground truth label is one-hot at the index of the only positive sample in concatenated candidates, and the model is optimized by minimizing their cross-entropy loss. We also show in the Appendix that adding a masked language modeling(MLM) task gives better results to Global-Selector and Cross-encoder, so we plus an auxiliary MLM loss to their original training objectives.
ori = cross_entropy(Ypred, Ylabel)
= ori + mlm

4 Datasets
There are multiple prior dialogue datasets with the response selection task:
· Ubuntu Dialogue Corpus(Lowe et al., 2015) contains 1 million conversations about technical support for the Ubuntu system.
· Kakao Corpus (Whang et al., 2020) consists of web-crawled dialogs from Korean social networking services.
· ConvAI2 (Dinan et al., 2019) dataset is obtained by modifying PersonaChat (Zhang et al., 2018), a crowd-sourced dataset with two-speaker talks conditioned on their given persona.
However, they share some common shortcomings because their negative samples are all selected randomly (SR) from the corpus. In this section, we demonstrate what these defects are and possible

5

Table 1: Example of how the model trained on SC utterances eliminates repetitive distractors. The bold value represents a significant confidence drop.

Dialogue Context
A: Hello, how are you doing? B: I love spending time with my family. A: That is great, me too! I'm married and my husband and I've 2 children. B: So then have you ever been to Disneyland? A: No, we recently purchased a new house, so we cannot afford it. Have you? B: Yes I love mickey mouse such a cute little rat. A: I enjoy going to concerts, I see the rolling stones every year.
Ground Truth B: Man, you're lucky. I am a baby delivery nurse and the pay sucks.
Repetitive Distractor B: So you have never been to Disneyland. Do you like mickey mouse?

Confidence w/o SC
0.9981 0.9948

Confidence w/ SC
0.9943 0.2676

Table 2: Example of how the model trained on SS utterances distinguishes strong distractors. The bold value represents a significant confidence drop.

Dialogue Context
A: What do you like to do for a living? B: You hiring? I'm forty, a car salesman and unhappily married. A: I have my own online business. I am 27 and single. B: Wow! My divorce is final tomorrow. Wife is a big spender but doesn't work. A: Wow! That must sucks. I love to make and save money. B: Great! You need a car? I wish I had a business. You happy? A: Yes, I need a new vehicle and yes I'm very happy. B: That's good! What kind of business are you in? You hiring? A: I do marketing and drop shipping, and yes of course I am always hiring!
Ground Truth B: Wow! Is forty old? My wife takes my money. Help!
Strong Distractor B: Please help me after my divorce.

Confidence w/o SS
0.9913 0.9983

Confidence w/ SS
0.8876 0.1147

strategies to alleviate them. Our experiments are conducted on the ConvAI2 dataset, and we simply treat its respondents' persona sentences as additional contexts.
If a dataset builds negative samples purely by random sampling, then models trained on it will prefer responses that significantly overlap with the dialogue history. Such responses have very high semantics similarity to the context but could be very repetitive and not natural. This phenomenon is also observed in Whang et al. (2020). Intuitively, we suppose it is because, under such a setting, the ground truth usually has some semantic relevance with the context, but negative samples do not. Thus, models tend to learn "the more repetition, the better." We tackle this problem by adding a negative sampling strategy called select from the context (SC) which negatively selects samples from its own context. For each response, we replace some SR samples with SC samples as described in the following two paragraphs. Table 1 illustrates a cross-encoder model's performance trained with and without the SC strategy and proves its effectiveness.
Besides, in generation-based dialogue systems, a selector usually needs to choose the best from multiple good candidates. However, the model can barely capture subtle features and distinguish competitive responses if only trained on SR samples. To reduce the discrepancy between training and practical use, we introduce a negative sampling strategy called select by similarity (SS) that selects strong negative samples through some unsupervised methods like TF-IDF (Joachims, 1996) and BM25 (Robertson and Zaragoza, 2009). Our strategy is different from some common ones, such as directly picking out distractors similar to the gold response from the corpus, and we prove in section 6.4 that selecting by response similarity has a potential risk of information leak. Instead, we use context similarity to identify some comparable contexts and then use their succeeding responses as negative samples. Table 2 illustrates how it helps a baseline cross-encoder to identify the optimal response among several strong distractors.
We construct the new benchmark, namely ConvAI2+, based on the ConvAI2 dataset to address the aforementioned problems. First, since ConvAI2 has a hidden test set, we randomly split half of its
6

Table 3: Effect of different multiples of data permutations (validation set of ConvAI2+Easy).

Model

R@1 1X 3X 5X 10X

Global-Selector (RoBERTa) 87.1 88.2 88.1 87.9

Table 4: Efficiency of different model architectures on the test set of ConvAI2+Easy.

Model

Peak GPU Memory Usage Candidates Inference Time

Cross-encoder (RoBERTa) Global-Selector (RoBERTa)

1.85GB

76740

73.0s 14.7s

validation set as the test set. Each original data has one ground-truth response plus 19 SR utterances, and we rename it to ConvAI2+Easy to distinguish it from our version. In the ConvAI2+Hard we build, each data consists of one ground-truth, [0, 2] SC utterances, and the rest are filled up to 20 with SS utterances. Our subsequent experiments are all carried out on this new benchmark, and it is completely open for reproducibility and future research.3 Please refer to the Appendix for more examples.
5 Experiments and Results
5.1 Free Data Augmentation
Global-Selector architecture possesses a very natural but unique data augmentation technique. Since it concatenates all candidates into a long sequence, there are numerous ways to permutate their order. Before comparing it with other baseline models, we first explore the effect of different multiples of permutations. We test on ConvAI2+Easy(same as the original ConvAI2 dataset) and use R@1 as the only metric. Table 3 lists the results of no data augmentation(1X), three(3X), five(5X), and ten(10X) times permutations. We can see 3X data augmentation yields the best outcome, so we keep it a fixed setting for subsequent experiments.
5.2 Comparisons
Global-Selector is then compared against the best baseline architecture Cross-encoder in this section. Our experiments leverage pre-trained BERT-base and RoBERTa-base models provided by HuggingFace.4 Please note that the Cross-encoders' performances in our implementation are comparable or even better than those reported in Humeau et al. (2019). This previous study has also proved that Bi-encoders are constantly inferior to Cross-encoder in effectiveness. Neither can they take advantage of pre-computing embeddings in generation-based dialogue systems as discussed. Therefore, it is unnecessary to consider Bi-encoder or its variant Poly-encoder in our evaluation.
5.2.1 Effectiveness
FULL RESULTS TO BE PRESENTED
5.2.2 Efficiency
Table 4 shows that Global-Selector has a markedly fast inference speed compared to the baseline, evidently because it chooses the best response from concatenated candidates in one shot rather than rank each of them in turn. This feature reduces the number of samples that Global-Selector needs to inference by 20 times on ConvAI2+. Thus, for the sake of fair comparison, we control the peak GPU memory usages of all models to the same value by assigning them different batch sizes. We have also explored a distinct inference trick for our work, which requires multiple inferences on
3Under construction. 4https://huggingface.co/transformers/pretrained_models.html
7

Table 5: Comparison of different model architectures. Results are on the test set of ConvAI2+Easy with variable numbers of distractors.

Model

R@1/2 R@1/5 R@1/12 R@1/17

Global-Selector (RoBERTa) 95.2 95.4 91.6

89.9

candidates varying in permutations. Then the one with the most votes is regarded as the optimal choice. However, we find that our model is fairly robust to the change of permutations. This method is not adopted in our report since the accuracy gain it brings is rather negligible to the trade-off in speed.
Lastly, our source code will be fully public, which contains implementation details and procedures to reproduce.5
6 Analysis and Discussion
In this section, we provide some analyses to understand Global-Selector further. We also discuss some limitations of our design and feasible solutions to address them.
6.1 Variable Numbers of Distractors
Each data in ConvAI2+ has a fixed number of 20 candidates, so Global-Selector always has an input concatenation of size 20 during training. We are interested to know its performance when the number of distractors is variable. On the test set of ConvAI2+Easy, we randomly reduce the candidates' number of each context to 2, 5, 12, and 17. Table 5 lists its accuracy on different concatenation sizes.
6.2 The Global Selection
As shown in the Appendix, examples demonstrate Global-Selector's ability to conduct global comparisons between candidates and always pick out the optimal one. We manually choose some responses for a given context, and there are distinct gaps in quality between them. After applying GlobalSelector, the best response is identified with confidence. Then we remove the best response, and this time the prior second-best candidate gets a very high probability to be selected.
6.3 Too Many Candidates to Fit
There is an obvious drawback of our design: The long concatenated candidates need to be fed into a single encoder, but the number of absolute positional embeddings is usually limited, making it unpractical for some retrieval-based tasks. If such a task has too many candidates to fit into an encoder, we would suggest that: 1.Splitting candidates into K groups with exercisable sizes. 2.Applying GlobalSelector to decide the best from each group. 3. Repeating the procedures hierarchically on former winners if necessary, until the global optimal is found. In addition, giving candidates a preliminary screening is helpful to accelerate the whole process.
6.4 Be Careful to Negative Sampling
As mentioned earlier, we strongly suggest that negative candidates should not be constructed according to the similarity with gold responses. If selecting by response similarity, all negative candidates will have a high degree of similarity with the ground truth, but they are only moderately similar to each other. This task could degenerate into identifying the response with the highest average similarity to other candidates, which is relatively easy for a model like Global-Selector that allows all candidates to attend to each other. After testing, our model can achieve up to 99% accuracy on such datasets but barely learned how to choose appropriate responses. We also exhibit in Appendix that using context similarity as proposed can avoid this problem by balancing the similarity between candidates.
5Under construction.
8

7 Conclusion
This paper indicates three apparent problems when applying pre-trained transformers on the multi-turn response selection task. We address them by proposing a new benchmark dataset and a novel model architecture. ConvAI2+ is created based on the ConvAI2 dataset, and experiments show that models trained on it have some noticeable improvements in practice. We also present the Global-Selector, which can pick out the best response by globally comparing all candidates. It has been proved that our architecture outperforms previous work in both accuracy and inference speed. Moreover, the process of dataset build and model training is entirely unsupervised.
References
Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. 2020. Towards a Human-like Open-Domain Chatbot. arXiv e-prints, page arXiv:2001.09977.
Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. 2020. PLATO: Pre-trained dialogue generation model with discrete latent variable. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 85­96, Online. Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171­4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W Black, Alexander Rudnicky, Jason Williams, Joelle Pineau, Mikhail Burtsev, and Jason Weston. 2019. The Second Conversational Intelligence Challenge (ConvAI2). arXiv e-prints, page arXiv:1902.00098.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889­898, Melbourne, Australia. Association for Computational Linguistics.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019. Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring. arXiv e-prints, page arXiv:1905.01969.
Thorsten Joachims. 1996. A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. Technical report, Carnegie-mellon univ pittsburgh pa dept of computer science.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Yang Liu. 2019. Fine-tune BERT for Extractive Summarization. arXiv e-prints, page arXiv:1903.10318.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv e-prints, page arXiv:1907.11692.
Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 285­294, Prague, Czech Republic. Association for Computational Linguistics.
9

Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive summarization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 3982­3992, Hong Kong, China. Association for Computational Linguistics.
Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc.
Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. 2021. Recipes for building an open-domain chatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 300­325, Online. Association for Computational Linguistics.
Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 673­683, Hong Kong, China. Association for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998­6008.
Yida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong Jiang, Xiaoyan Zhu, and Minlie Huang. 2020. A Large-Scale Chinese Short-Text Conversation Dataset. arXiv e-prints, page arXiv:2008.03946.
Taesun Whang, Dongyub Lee, Dongsuk Oh, Chanhee Lee, Kijong Han, Dong-hun Lee, and Saebyeok Lee. 2020. Do Response Selection Models Really Know What's Next? Utterance Manipulation Strategies for Multi-turn Response Selection. arXiv e-prints, page arXiv:2009.04703.
Thomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. 2019. TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents. arXiv e-prints, page arXiv:1901.08149.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv e-prints, page arXiv:1609.08144.
Koichiro Yoshino, Chiori Hori, Julien Perez, Luis Fernando D'Haro, Lazaros Polymenakos, Chulaka Gunasekara, Walter S. Lasecki, Jonathan K. Kummerfeld, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, Xiang Gao, Huda Alamari, Tim K. Marks, Devi Parikh, and Dhruv Batra. 2019. Dialog System Technology Challenge 7. arXiv e-prints, page arXiv:1901.03461.
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2204­2213, Melbourne, Australia. Association for Computational Linguistics.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270­278, Online. Association for Computational Linguistics.
10

Zhuosheng Zhang, Junjie Yang, and Hai Zhao. 2020. Retrospective Reader for Machine Reading Comprehension. arXiv e-prints, page arXiv:2001.09694.
A Appendix
Under construction.
11

