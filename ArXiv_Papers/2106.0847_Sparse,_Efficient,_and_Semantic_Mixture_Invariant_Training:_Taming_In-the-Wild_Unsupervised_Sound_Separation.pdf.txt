2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics

October 17-20, 2021, New Paltz, NY

arXiv:2106.00847v1 [eess.AS] 1 Jun 2021

SPARSE, EFFICIENT, AND SEMANTIC MIXTURE INVARIANT TRAINING: TAMING IN-THE-WILD UNSUPERVISED SOUND SEPARATION
Scott Wisdom, Aren Jansen, Ron J. Weiss, Hakan Erdogan, John R. Hershey
Google Research
{scottwisdom,arenjansen,ronw,hakanerdogan,johnhershey}@google.com

ABSTRACT
Supervised neural network training has led to significant progress on single-channel sound separation. This approach relies on ground truth isolated sources, which precludes scaling to widely available mixture data and limits progress on open-domain tasks. The recent mixture invariant training (MixIT) method enables training on in-thewild data; however, it suffers from two outstanding problems. First, it produces models which tend to over-separate, producing more output sources than are present in the input. Second, the exponential computational complexity of the MixIT loss limits the number of feasible output sources. These problems interact: increasing the number of output sources exacerbates over-separation. In this paper we address both issues. To combat over-separation we introduce new losses: sparsity losses that favor fewer output sources and a covariance loss that discourages correlated outputs. We also experiment with a semantic classification loss by predicting weak class labels for each mixture. To extend MixIT to larger numbers of sources, we introduce an efficient approximation using a fast least-squares solution, projected onto the MixIT constraint set. Our experiments show that the proposed losses curtail over-separation and improve overall performance. The best performance is achieved using larger numbers of output sources, enabled by our efficient MixIT loss, combined with sparsity losses to prevent over-separation. On the FUSS test set, we achieve over 13 dB in multi-source SI-SNR improvement, while boosting single-source reconstruction SI-SNR by over 17 dB.
Index Terms-- universal sound separation, sparsity, unsupervised learning, sound classification
1. INTRODUCTION
A basic problem in auditory perception is that sounds are superimposed, creating a notoriously difficult inverse problem: in a singlechannel recording there are more unknown variables in the source samples than there are known variables in their mixture. In recent years, great strides have been made in source separation using supervised deep learning, raising the bar in performance on tasks such as separation of speech from non-speech [1, 2], separation of speech from speech [3, 4, 5], and separating arbitrary sounds from each other regardless of their class (universal sound separation) [6, 7].
Supervised separation uses isolated source waveforms as groundtruth output targets, synthetically mixing them to create training inputs. This is necessary because it is not feasible to record isolated sources in the same environment as their natural acoustic mixtures. As a result, supervised training only works well when the synthetic mixtures match the domain in which the system is intended to perform. For universal separation, with an open domain of sound types, it is difficult to match realistic source and mixture characteristics. Databases of isolated sources do not exist for all classes that may

be of interest, and realistic patterns of source activity and acoustic conditions, such as reverberation, are unknown and difficult to infer. However, copious mixture data are available: arguably the vast majority of audio data in existence are unlabeled acoustic mixtures.
Recently, mixture invariant training (MixIT) [8], showed strong performance without relying on isolated ground-truth sources, by training directly on mixtures of unknown source content. MixIT uses mixtures as references instead of isolated sources, and a mixture of these mixtures as its input. The model outputs estimated sources such that they can be recombined to reconstruct the reference mixtures.
One outstanding issue with MixIT is that it can lead to models that over-separate, in the sense of producing a greater number of active source estimates than there are true underlying sources [8, 9]. This is because the MixIT loss is invariant to the number of active estimated sources, as long as they can be remixed to approximate the reference mixtures with the same error. When it is uncertain which components belong to the same mixture, a model may take advantage of MixIT's oracle assignment of sources to mixtures, and separate those components without penalty, avoiding the risk that they belong to different mixtures; this provides an incentive for over-separation.
Over-separation leads to lower separation fidelity with respect to the individual sources. An unmistakable sign is poor reconstruction when a single source is used as input: the source tends to be split across multiple outputs [8]. Including some supervised training data can reduce over-separation [8], but the problem remains in the unsupervised setting, where isolated sources are unavailable.
The over-separation issue worsens as the number of estimated sources increases. We introduce several new losses to address this. A sparsity loss encourages the pattern of activity across sources to be sparse. A covariance loss encourages the model to output decorrelated sources. Finally, we experiment with a classification loss that utilizes weak class labels for each mixture. This encourages the semantic labels for each source to be different from each other, while matching the ensemble labels for the mixture.
To carry out this investigation across a wider range of output sources M , we introduce an efficient version of MixIT that avoids brute force search [8] and enables MixIT to be used with much larger numbers of output sources. Our experiments show that both sparsity and classification losses curtail over-separation, and produce better multi-source separation performance. The best performance is achieved using 16 output sources combined with sparsity losses.
2. METHODS
We train separation networks using the same architecture as previous works [6, 8, 9, 10], which separates sources by masking in a learned transform domain. The network is composed of a learnable encoder/decoder with 2.5 ms window and 1.25 ms hop, com-

2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics

October 17-20, 2021, New Paltz, NY

bined with a time-domain convolutional network (TDCN++). The TDCN++ takes the encoder coefficients as input, and predicts M masks through a sigmoid activation. The masks are elementwise multiplied with the encoder coefficients, and M time-domain sources are reconstructed using the learnable decoder. A mixture consistency projection [11] on these resulting sources ensures that they add up to the input waveform.

2.1. Mixture invariant training (MixIT)
In mixture-invariant training, inputs are formed by summing N reference mixtures xn  RT to a mixture of mixtures (MoM), x¯ =
n xn. The separation model f predicts M sources ^s  RM×T from the MoM: ^s = f(x¯). Given reference mixtures and separated sources, the MixIT loss [8] estimates a mixing matrix A  BN×M :

LMixIT ({xn}, ^s) = min

L (xn, [A^s]n) (1)

ABN×M n

where BN×M is the set of N × M binary matrices where each column sums to 1, i.e., the set of matrices which assign each separated source s^m to one of the reference mixtures xn, and L is a signal-level loss function between reference mixtures and their estimates. We use the negative thresholded SNR as the signal-level loss function:

y2

L(y, y^) = -10 log10 y - y^ 2 +  y 2

(2)

where  = 10-SNRmax/10 acts as a soft threshold that clamps the loss at SNRmax. We find SNRmax = 30 dB to be a good value.

2.2. Efficient MixIT
In the original formulation of MixIT [8], exhaustive O(N M ) search was used to find A in (1). This is tractable when N and M are small, but for larger values, even for N = 2 and M > 8, it quickly becomes infeasible. To address this problem, we propose an efficient version of MixIT. In this efficient version, we simply use least-squares to find the optimal real-valued mixing matrix between reference mixtures x and estimated sources ^s, then project the result to the nearest valid binary mixing matrix:

A^ = PB

argminARN ×M

x - A^s

2 2

,

(3)

where the projection operator PB sets the maximum value in each column to 1, and the rest of the elements to 0. We have found that this surprisingly simple method yields essentially equivalent results to using exhaustive search, at a fraction of the cost.

2.3. Source sparsity loss
To directly address over-separation, we experiment with losses that encourage sparsity. Specifically, we consider sparsity in the distribution of activity among the estimated sources, measured in terms of the root-mean-squared (RMS) level of the time domain signals:

rm = rms(s^m) =def

1 T

|s^m,t|2
t

(4)

The 1-norm has a long history as a sparsity inducing regularizer, used on coefficients in linear regression problems in LASSO [12]. It has also been used as a prior on sources in blind source separation

[13]. The 2-norm on the other hand is well known to favor uniformity over sparsity. Here we consider the 1 norm over sources as
r 1 = m rm, with a loss defined as:

C

1 (r) =def

1 M

r 1,

rms(x¯)

(5)

where the denominator and 1/M factors normalize the scale across different examples and numbers of sources. Note that C 1 (r) encourages all sources to "shrink" towards zero. However the mixture consistency constraint, m s^m = x¯, prevents shrinkage overall, while allowing changes in amplitude among sources. Note that rm is proportional to the 2 norm over time samples, and hence this loss is also related to the group LASSO [14], in which the 2 norm is used within a group, and the 1 norm is used between groups.
A potential drawback of the 1 norm in C 1 (r) is that, because of the way sources combine, it assigns a higher loss when merging
sources that are correlated than when merging independent sources.
This also happens with the 2 norm, and motivates 1 / 2 as a loss:

C 1/ 2 (r) =

1 M

r 1.

r2

(6)

In this loss, the preference for merging independent sources cancels out, leaving only a preference for merging sources. A similar loss has been used [15, 16, 17] to promote scale-invariant sparsity.

2.4. Covariance loss

One over-separation failure mode might be when the same source is evenly divided among multiple outputs. In another failure case, an artifact may be added to one source and subtracted from another, so that they cancel out in the mixture, but disrupt the clean separation of the sources. To combat such possibilities, we introduce a covariancebased loss that encourages separated sources to be uncorrelated. We define such a loss based on the 1 norm of off-diagonal covariances:

Ccov(s^) =

| cov(s^m, s^m )|.

(7)

m=m

2.5. Joint separation and classification

Even when clean source recordings are not available for direct supervision of separation systems, semantic labels may be available for the mixtures used in MixIT training. These labels provide two constraints on the separation output sources. First, assuming exhaustive labeling, the union of the classes that are present across the individual sources should match the semantic labels for the original mixture. Second, when the sources are semantically distinct, the mutually exclusive labels for the original mixture should be distributed across separate sources. These constraints can be used to combat over-separation by feeding the separated outputs into a downstream classifier and using its predictions to define additional losses.
We begin by applying a pretrained K-class sound event classifier, g : RT  [0, 1]K , that maps each of the M separated sources s^m to a corresponding posterior vector pm = g(s^m). Our first loss attempts to align the union of predictions with the ground truth labels provided for the mixture. This is implemented through (i) the aggregation of posterior vectors across separated sources into a single prediction for the mixture, and (ii) the classification loss of that aggregate prediction and the ground truth labels. This cross entropy loss takes the form (assuming multi-labeled data):

Cce(p) = - lk log A(p)[k] + (1 - lk) log(1 - A(p)[k]), (8)
k

2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics

October 17-20, 2021, New Paltz, NY

16

AudioSet train set, FUSS validation set, M=4

16

YFCC100m train set, FUSS validation set, M=4

14

14

MSi (dB)

MSi (dB)

12

12

10

10

85.0 16

7.5

10.0
YFCC100m

12.5
train

set,1FS1U5(d.S0BS)

17.5
validation

set,

20.0
M=8

22.5

25.0 85.0 16

7.5

10.0
YFCC100m

12.5
train

set, 1FS1U5(Sd.0SB)valida1t7i.o5n

set,

20.0
M=16

22.5

25.0

14

14

MSi (dB)

MSi (dB)

12

12

10

10

85.0

7.5

10.0

12.5 1S15(d.0B) 17.5

20.0

22.5

25.0 85.0

7.5

10.0

12.5 1S15(d.0B) 17.5

20.0

22.5

25.0

Baseline 1M Baseline 3.7M C 1/ 2 + 250 · Cce

250 · Cce

·C 1

 · Ccov  · Ccov +  · C 1/ 2

 = 0.25  = 4  = 16

Baseline 1.3M

C 1/ 2 + 250 · Cce + 50 · Ccos 500 · Cce + 50 · Ccos  · C 1/ 2

 · Ccov + 2 · C 1/ 2  = 1

 = 8  = 32

Figure 1: Scatter plots of MSi versus 1S for various losses. Marker size is proportional to the weight . By default all classification losses Cce use OR aggregation, except those denoted with , which use XOR. For Ccov experiment sweeps we used  = 8 M . Arrows denote hyperparameter settings which maximize 3×MSi + 1S, as depicted in Table 1.

where A is an aggregation function mapping p = {pm}M m=1 to a single posterior vector for the mixture, and lk  {0, 1} is the

ground truth label for the k-th class. We consider two aggrega-

tion functions. The first is a soft logical-OR function, Aor(p) =

1-

M m=1

(1

-

pm),

which

allows

any

source

to

constructively

provide evidence for a particular class with no penalty for cross-

source redundancy. The second is a soft logical-XOR1 function,

Axor(p) =

M m=1

pm

M m

=m(1

-

pm

),

which

encourages

only

one source to provide evidence for each class present in the mixture.

Both aggregation functions encourage semantically guided sepa-

ration of the composite sound sources. Axor further encourages each

semantic category to be restricted to a single separation output. How-

ever, XOR aggregation does not explicitly require that each class is

present on a different separation output. To encourage this behavior,

we introduce a second average cosine similarity loss between pairs

of output source posterior vectors, given by

2 Ccos(p) = M (M - 1)
m,m >m

1+

pTmpm pm pm

. (9)

This explicitly encourages posterior vectors to be orthogonal, which encourages both semantic mutual exclusivity of the separated outputs (like XOR) and distribution of semantic content across more sources.

3. EXPERIMENTS
We train unsupervised separation models using MixIT on two large datasets of 10-second clips extracted from in-the-wild audio containing a wide variety of different source classes: YFCC100m [18], containing about 1600 hours of audio, and AudioSet [19], containing about 5800 hours. Each clip is annotated with semantic class labels describing the sound events contained therein. All models are fine-tuned from a baseline checkpoint trained only with the MixIT loss. The Adam optimizer [20] was used with batch size 256 and learning rate 10-3 on 4 Google Cloud TPUs (16 chips). Exhaustive MixIT is used for M  8, and efficient MixIT otherwise2. For YFCC100m, we initialize from a checkpoint trained to 1M steps
1Multiple conventions exist to extend XOR to more than two arguments. We desire one-hot behavior, where 1 is output iff exactly one input is nonzero.
2For M of 4 and 8, we ran experiments with efficient MixIT, and did not observe any significant difference compared to using the exhaustive MixIT.

and fine-tune for an additional 300k steps using various losses. For AudioSet, we use a checkpoint trained to 3.7M steps and fine-tune using early stopping. To apply classification losses, we feed the separation outputs into a pretrained ResNet-18 AudioSet classifier that processes 3-second waveform inputs with a simple learned front end [21]. We found it necessary to jointly train the classifier when using the cosine-similarity loss (9), but that it could be held fixed when using the soft-OR and soft-XOR losses alone.
Separation performance is evaluated using several supervised synthetic datasets containing reference sources. Since we train our separation models on a universal sound separation task, we primarily focus on evaluation with the FUSS dataset [9], which contains 10second mixtures of one to four arbitrary sound sources drawn from Since many practical separation applications focus on speech signals, we additionally evaluate how well unsupervised universal separation models can generalize to two specific task domains: speech separation, using mixtures of two overlapping speakers from Libr2Mix [22], and speech enhancement, using the same dataset as previous work [8] in which speech is drawn from librivox.org, and nonspeech from freesound.org.
We use several separation metrics, as proposed for measuring performance on FUSS [9] and for MixIT-trained models [8].
· MSi: for mixture inputs with 2+ sources, scale-invariant SNR [23] improvement (SI-SNRi) in dB relative to the input for all active reference-estimate pairs.
· 1S: the reconstruction SI-SNR in dB, using a single output source, when passing a single source through the separation model (SISNRi is not useful here because the input has infinite SI-SNR).
· MoMi: the SI-SNRi in dB of reconstructed mixtures found using the optimal binary mixing matrix A using to align the ground truth to the separated sources. This is the same as the loss function in (1), and is the only objective measure we can compute on unsupervised datasets which do not contain isolated sources.
For speech enhancement, we only measure SI-SNR improvement of the target speech source, ignoring the nonspeech source. In Table 1, speech SI-SNRi for noisy speech mixtures is referred to as S+N, and for clean speech input as S-only. All metrics are computed with permutation invariance using the O(M 3) Hungarian algorithm [24], since brute-force O(M !) alignment is infeasible for higher M .
Figure 1 visualizes the trade-off between MSi versus 1S on the FUSS validation set for various M over the set of regularization

2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics

October 17-20, 2021, New Paltz, NY

Table 1: Results on various test sets for models trained with PIT on supervised FUSS, and with MixIT on unsupervised YFCC100m or AudioSet. The best loss type and weight are chosen based on maximizing 3×MSi + 1S on the FUSS validation set. The best test metrics for each train set are bold, and the best FUSS metrics that maximize 3×MSi + 1S are bold and italic.

FUSS

Libri2Mix Enhancement

YFCC100m

Train set Method

Best loss

M MSi 1S MoMi MSi 1S S+N S-only

MoMi

FUSS

Single-mixtures

(supervised)

4 12.4 40.9 4.3

-1.9 15.6

-1.4 11.5

-0.1

8 12.6 36.8 3.5

-2.8 17.9

-3.8 9.3

-0.2

16 13.7 38.5 4.7

-2.6 17.8

-1.3 12.3

-1.9

MoMs

8 13.4 14.8 8.4 -0.6 15.8

0.4 13.3

1.1

16 13.7 11.3 8.9 -0.7 13.2

0.8 13.1

1.0

YFCC100m Baseline 1.3M

4 11.9 13.8 7.4 10.0 24.5

9.3 28.6

8.5

8 12.7 9.9 9.9

8.7 22.9

9.2 23.1

9.1

16 11.0 8.2 10.4

5.8 20.6

9.2 23.3

9.4

Sparsity 1M+0.3M

8 · C 1/ 2 23 · C 1/ 2 64 · C 1/ 2

4 11.6 20.7 6.3

9.3 31.4

8.7 30.6

8.0

8 12.4 23.9 6.4

9.7 34.2

8.8 31.0

8.1

16 13.2 25.5 6.0

8.9 34.9

7.4 32.0

7.4

Sparsity + Cov. 16 · C 1/ 2 + 4 · Ccov

4 10.2 26.3 4.4

9.3 38.0

7.9 35.8

7.6

1M+0.3M

23 · C 1/ 2 + 1 · Ccov

8 12.5 24.6 5.6

9.3 34.7

8.0 31.7

8.0

64 · C 1/ 2 + 0.25 · Ccov

16 13.0 25.8 5.4

8.7 34.3

6.9 32.2

7.5

AudioSet Baseline 3.7M

4 13.4 13.3 9.2 13.4 28.1 10.9 29.1

8.3

Sparsity

16 · C 1/ 2

4 12.5 18.6 7.2 12.8 29.5 10.6 29.5

7.4

Class.

250 · Cce

4 13.2 16.3 8.9 13.3 26.3 10.9 28.4

8.0

Sparsity + Class. 16 · C 1/ 2 + 250 · Cce + 50 · Ccos 4

13.1 19.6 8.3

13.6 29.2

11.1 30.1

7.6

losses, with weight  swept over a wide range. Marker sizes are proportional to . Notice that for the sparsity losses, as  increases, MSi and 1S generally continue to increase up to a critical point. Past this critical point the sparsity losses become too strong, causing the models to under-separate: 1S continues to increase while MSi decreases. Across tasks, we find that using the C 1/ 2 sparsity loss (6) consistently improves 1S compared to C 1 (5). Covariance loss (7) alone does not improve over the baseline; however, combining covariance loss with sparsity leads to small improvements in 1S.
A summary of the best results on the test sets for different M is provided in Table 1, along with supervised baselines using the permutation invariant training (PIT) procedure used in the original FUSS separation model [9] on either single mixtures or MoMs from FUSS. Note that the optimal  increases with M because overseparation in the baseline tends to increase with M . For each loss type, the checkpoint and loss weight that maximizes 3×MSi + 1S on the FUSS validation set is selected. This criterion reflects the proportion of multi-source mixtures to single-source mixtures in FUSS.
Our best results on FUSS, in terms of maximizing MSi and 1S, are achieved by a M = 16 model trained on YFCC100m using the sparsity loss (6). This model achieves a new state-of-the-art for unsupervised models on the FUSS test set: 13.2 dB MSi and 25.5 dB 1S, which is only 0.5 dB MSi behind the fully supervised baseline, and boosts 1S by over 17 dB compared to the unsupervised M = 16 baseline's 8.2 dB 1S. Incorporating covariance loss (7) improves over sparsity alone in terms of 1S, with a slight reduction in MSi.
In general, we find that all unsupervised MixIT models consistently avoid over-separating speech sources, as evidenced by high 1S scores on Libri2Mix and enhancement; however, speech MSi is improved with additional regularization. Supervised FUSS models perform very poorly on speech separation MSi and enhancement S+N, likely due to the limited amount of source data and diversity in FUSS. This highlights an advantage of training on a large amount of real-world data, spanning a variety of source types. Finally, note that incorporating sparsity losses consistently hurts MoMi on all tasks.

As shown in Table 1, the AudioSet baseline trained for 3.7M steps is quite strong, achieving better scores on FUSS compared to the YFCC100m M = 4 model trained for 1.3M steps. Sparsity regularization alone improves 1S at a cost of almost 1 dB MSi. The best classification loss setting improves 1S significantly, with minimal degradation in MSi and scores well on other tasks, indicating its ability to reduce over-separation. Our best results from AudioSet training are produced with a combination of the classifier soft-OR and cosine similarity losses, and sparsity regularization (bottom row).
Finally, note the improved performance on speech tasks using AudioSet models, despite being trained on unmatched data without source-level supervision. In particular, the baseline MSi on Libri2Mix speech separation (13.4 dB) comes surprisingly close to a fully supervised model trained only on matched data (16 dB) [22]. This reflects the fact that about half of AudioSet clips contain speech in some form [19].
4. CONCLUSION
We have presented a number of auxiliary losses that improve unsupervised sound separation models trained using MixIT, especially for models which emit many output sources. These losses include explicit sparsity penalties on the magnitudes of separated sources and their covariances, as well as semantic classification losses. To enable training with a large number of output sources, we proposed an efficient least-squares-based MixIT implementation. Used in combination with sparsity regularization, unsupervised models achieved state-of-the-art performance on universal sound separation, in terms of MSi and 1S on the FUSS test set. These models are also suitable for general purpose use, demonstrating good performance on domain-specific tasks such as speech separation and enhancement. In future work, we plan to scale up model and data size to train even better general-purpose separation systems, as well as continue to explore combinations of sound separation and classification systems.

2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics

October 17-20, 2021, New Paltz, NY

5. ACKNOWLEDGMENTS
Thanks to Kevin Wilson for helpful comments on the manuscript, and to Jiquan Ngiam for the TensorFlow implementation of the Hungarian algorithm that we use.
6. REFERENCES
[1] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis, "Deep learning for monaural speech separation," in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014, pp. 1562­1566.
[2] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Le Roux, J. R. Hershey, and B. Schuller, "Speech enhancement with LSTM recurrent neural networks and its application to noiserobust ASR," in International Conference on Latent Variable Analysis and Signal Separation, 2015, pp. 91­99.
[3] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, "Deep clustering: Discriminative embeddings for segmentation and separation," in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016, pp. 31­35.
[4] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey, "Single-channel multi-speaker separation using deep clustering," in Proc. Interspeech, 2016, pp. 545­549.
[5] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, "Permutation invariant training of deep models for speaker-independent multitalker speech separation," in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017, pp. 241­245.
[6] I. Kavalerov, S. Wisdom, H. Erdogan, B. Patton, K. Wilson, J. Le Roux, and J. R. Hershey, "Universal sound separation," in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2019.
[7] E. Tzinis, S. Wisdom, J. R. Hershey, A. Jansen, and D. P. W. Ellis, "Improving universal sound separation using sound classification," in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020, pp. 96­100.
[8] S. Wisdom, E. Tzinis, H. Erdogan, R. J. Weiss, K. Wilson, and J. R. Hershey, "Unsupervised sound separation using mixture invariant training," in Advances in Neural Information Processing Systems, 2020.
[9] S. Wisdom, H. Erdogan, D. P. W. Ellis, R. Serizel, N. Turpault, E. Fonseca, J. Salamon, P. Seetharaman, and J. R. Hershey, "What's all the FUSS about free universal sound separation data?" in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2021.
[10] Z.-Q. Wang, H. Erdogan, S. Wisdom, K. Wilson, and J. R. Hershey, "Sequential multi-frame neural beamforming for speech separation and enhancement," in Proc. IEEE Spoken Language Technology Workshop (SLT), 2021.
[11] S. Wisdom, J. R. Hershey, K. Wilson, J. Thorpe, M. Chinen, B. Patton, and R. A. Saurous, "Differentiable consistency constraints for improved deep speech enhancement," in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2019, pp. 900­904.
[12] R. Tibshirani, "Regression shrinkage and selection via the lasso," Journal of the Royal Statistical Society: Series B (Methodological), vol. 58, no. 1, pp. 267­288, 1996.

[13] T.-W. Lee, M. S. Lewicki, M. Girolami, and T. J. Sejnowski, "Blind source separation of more sources than mixtures using overcomplete representations," IEEE Signal Processing Letters, vol. 6, no. 4, pp. 87­90, 1999.
[14] M. Yuan and Y. Lin, "Model selection and estimation in regression with grouped variables," Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 68, no. 1, pp. 49­67, 2006.
[15] P. O. Hoyer, "Non-negative matrix factorization with sparseness constraints," Journal of Machine Learning Research, vol. 5, no. 9, 2004.
[16] P. Yin, E. Esser, and J. Xin, "Ratio and difference of 1 and 2 norms and sparse representation with coherent dictionaries," Communications in Information and Systems, vol. 14, no. 2, pp. 87­109, 2014.
[17] D. Krishnan, T. Tay, and R. Fergus, "Blind deconvolution using a normalized sparsity measure," in CVPR 2011. IEEE, 2011, pp. 233­240.
[18] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li, "YFCC100M: The new data in multimedia research," Communications of the ACM, vol. 59, no. 2, pp. 64­73, 2016.
[19] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, "Audio set: An ontology and human-labeled dataset for audio events," in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017, pp. 776­780.
[20] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in Proc. International Conference on Learning Representations (ICLR), 2015.
[21] T. N. Sainath, R. J. Weiss, A. W. Senior, K. W. Wilson, and O. Vinyals, "Learning the speech front-end with raw waveform CLDNNs." in Proc. Interspeech, 2015.
[22] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, "LibriMix: An open-source dataset for generalizable speech separation," arXiv preprint arXiv:2005.11262, 2020.
[23] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, "SDR­ half-baked or well done?" in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2019, pp. 626­630.
[24] H. W. Kuhn, "The hungarian method for the assignment problem," Naval research logistics quarterly, vol. 2, no. 1-2, pp. 83­97, 1955.

