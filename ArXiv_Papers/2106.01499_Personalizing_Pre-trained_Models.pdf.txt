arXiv:2106.01499v1 [cs.CV] 2 Jun 2021

Personalizing Pre-trained Models
Mina Khan1 P Srivatsa* Advait Rane2* Shriram Chenniappa2+ Asadali Hazariwala2+ Pattie Maes
1MIT Media Lab, Cambridge, MA, USA. 2Birla Institute of Technology, Goa, India.
Abstract
Self-supervised or weakly supervised models trained on large-scale datasets have shown sample-efficient transfer to diverse datasets in few-shot settings. We consider how upstream pretrained models can be leveraged for downstream few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP PERsonalized) uses image representations from CLIP, a large-scale image representation learning model trained using weak natural language supervision. We developed a technique, called Multi-label Weight Imprinting (MWI), for multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image representations from CLIP. We evaluated CLIPPER on 10 single-label and 5 multi-label datasets. Our model shows robust and competitive performance, and we set new benchmarks for few-shot, multi-label, and continual learning. Our lightweight technique is also compute-efficient and enables privacy-preserving applications as the data is not sent to the upstream model for fine-tuning. Thus, we enable few-shot, multilabel, and continual learning in compute-efficient and privacy-preserving settings.
1 Introduction
Data-efficiency and generalization are key challenges in deep learning, and representation learning has been at the heart of deep learning [1]. Recently, self-supervised or weakly supervised models have been leveraged to learn from large-scale uncurated datasets and have shown sample-efficient transfer to labeled tasks [5, 52, 22, 20, 11, 54]. However, commonly used transfer techniques, e.g., fine-tuning or distillation, do not currently support few-shot, multilabel, and continual learning. Few-shot learning (FSL) has made great strides in the area of sample-efficient learning [73]. However, FSL models are pretrained on large, domain-specific, and expensive-to-label datasets and have not leveraged pretrained models to avoid training on large and domain-specific labeled datasets. Also, FSL methods do not outperform pretrained models when domain shift is present [6, 36]. We consider the problem of enabling few-shot, multilabel, and continual learning for real-world downstream tasks, and investigate combining representation learning from pretrained self-supervised or weakly supervised models with few-shot, multilabel, and continual learning techniques. Our model CLIPPER (CLIP PERsonalized) uses image representations, i.e., embeddings, from CLIP, a weakly-supervised image representation learning model, for FSL. Inspired by Weight Imprinting [50], an FSL method, we develop an approach called Multilabel Weight Imprinting for few-shot, multilabel, and continual learning. CLIPPER combines image embeddings from CLIP along with Multilabel Weight Imprinting for continual and multilabel few-shot learning. We evaluated CLIPPER on 10 single-label and 5 multi-label datasets. CLIPPER shows robust and competitive performance, e.g., it outperforms the state-of-the-art FSL method on MiniImagenet. We set benchmarks for few-shot, continual, and multilabel learning on several different datasets.
0minakhan01@gmail.com. https://github.com/PAL-ML/CLIPPER *,+Equal Contribution
Preprint. Under review.

We make 3 key contributions.
1. A new methodology combining the flexibility of few-shot learning methods with the sampleefficiency and generalizability of transfer learning methods using self-supervised or weakly supervised pretrained models. Our method eliminates the need for data- and computeintensive pretraining on large, domain-specific, and labeled datasets for FSL.
2. A modified FSL technique, which enables few-shot, continual, and multilabel learning. Combined with our first contribution, our second contribution enables few-shot, continual, and multilabel learning using self-supervised or weakly supervised pretrained models.
3. Evaluations and benchmarks for few-shot, continual, and multilabel learning on 15 multilabel and single-label datasets, showing the robust and competitive performance of our technique.
2 Related Work
2.1 Few-shot Learning (FSL)
There are three types of approaches for FSL: model-, metric-, and optimization-based. Unlike previous work, we use a pretrained weakly supervised model for data- and compute-efficient training.
Our Multilabel Weight Imprinting technique lies in the category of metric-based approaches [35, 63, 65, 70]. More specifically, we use a prototype-based metric-learning approach, as they assign trainable proxies to each category and enable faster convergence via element-category comparison, instead of element-wise comparisons. Our work extends a previous FSL technique, called weight imprinting [50]. We not only use a pretrained base model [50], instead of training a base network from scratch, but we also extend weight imprinting to enable multilabel and continual learning. Other metric-based methods are complementary to our approach and our model can be further extended, e.g., with MatchingNet-like attention [70] or with RelationNet-like relation learning [65].
Model-based methods [61, 48] use especially designed models for rapid parameter updates, and optimization-based techniques [57, 13, 49] adjust the optimization method to meta-learn efficiently. Recent research indicates that learning a good embedding model can be more effective than sophisticated meta-learning algorithms [68] and efficient meta-learning may be predominantly due to the reuse of high-quality features [55]. Nonetheless, these techniques, though relatively training-intensive, are complementary to our work and may be used to improve both upstream and downstream models.
2.2 Self-supervised Representation Learning
Self-supervised and weakly supervised models have been used in natural language processing [9, 53, 11] and computer vision [22, 20, 4, 5, 52] to learn from large-scale unlabeled or weakly labeled datasets. Though pre-training is still imperfect [12, 45], pretrained models trained on largescale datasets have shown robust and sample-efficient transfer to diverse tasks [20, 22, 5, 52].
Transfer learning is closely related to few-shot learning, but FSL does not use a pretrained method. Instead, FSL is trained and evaluated using the same distribution and does not necessarily outperform transfer learning when domain shift is present [6]. Transfer learning, however, could benefit from the specialized FSL techniques [36]. Also, to the best of our knowledge, unlike our work, transfer learning using pretrained models has not been combined with multi-label and continual learning.
Like previous work, we use self-supervised learning via data augmentation to boost FSL [17, 50].
2.3 Multilabel and Continual Learning
Continual learning techniques [46] include regularization-based methods (e.g., Elastic Weight Consolidation), memory-based methods (e.g., Incremental Classifier and Representation Learning [58]), and parameter isolation (like Continual Neural Dirichlet Process Mixture). Previously used continual techniques, however, did not use pretrained models for few-shot, multilabel, and continual learning.
Common multilabel classification techniques include ML-kNN, Multi-label DecisionTree, etc [10]. Multi-Label Image Classification has also been done using knowledge distillation from weakly supervised detection [42]. However, none of the existing methods combine multilabel, continual, and few-shot learning, especially using pre-trained models. Several multilabel and continual learning techniques, nonetheless, are complementary to our work and can be extensions of our work.
2

3 Approach
3.1 Desiderata
We outline 3 desiderata for real-world computer vision applications. First, few-shot learning so that the applications can start well in data-scarce scenarios and can also be customized and personalized for different needs. Second, continual learning to incrementally learn new information and avoid catastrophic forgetting, e.g., replacing of older classes when new ones are added. Third, multilabel learning as the right label may not be just one label but a subset of all the given labels, including 0 to all labels. The multi-label case is important for not only assigning multiple labels to a particular data point but also for assigning zero labels, in case we get data points that we currently do not have labels for, i.e., the continual learning case. Continual learning often considers the addition of data points along with their respective labels. However, we consider the more realistic continual case when a point may be added even before their label is added and thus, the model needs to assign no label.
3.2 Decisions
We made the following three design choices to enable few-shot, multilabel, and continual learning.
Pretrained base model: FSL models are typically pre-trained on large domain-specific training sets, which contain examples not in the support/test set. The models are then trained and tested on support and test sets, which have the same classes. Large-scale, domain-specific, and labeled datasets, however, may not always be available in real-world settings. Also, FSL models trained on domain-specific sets may not generalize well to domain shifts [26]. Large-scale self-supervised or weakly supervised models, on the other hand, learn good representations and can be fine-tuned for data-efficient and diverse downstream tasks [5, 52]. We use pretrained models trained on diverse datasets as base models for FSL, instead of training base models from scratch on domain-specific datasets. As a result, unlike FSL methods, we only train with a support test, which we call train set.
Weight Imprinting (WI): Weight imprinting [50] is a FSL learning method that learns a linear layer on top of the embeddings, where the columns of the linear layer weight matrix are prototype embeddings for each class. Many self-supervised or weakly supervised models have been shown to learn linearly-separable embeddings using linear probes [52] and thus, a linear layer can be added to pretrained embeddings to classify different classes. Compared to transfer learning, which learns a fixed number of classes, weight imprinting adds new classes as new columns of the linear layer weight matrix, making adding classes computationally and conceptually simpler and avoiding catastrophic forgetting. Thus, weight imprinting supports prototype-based few-shot and continual learning.
Sigmoids, not Softmax: The original weight imprinting model uses softmax and thus is compatible with single-label classification. We replace the softmax with sigmoid activations for each class in weight imprinting to enable multi-label learning. Sigmoids also support an output of 0 labels for continual learning, i.e., when the label for a given data point has not yet been added to the label set.
3.3 Details
We created a multilabel version of weight imprinting [50], called Multilabel Weight Imprinting (MWI). Our model has two parts. First, an embeddings extractor,  : RN  RD, maps input image x  RN to a D-dimensional embedding vector (x), followed by an L2 norm. Second, a sigmoid function, f ((x)), maps the embedding using sigmoid activations for each category.
1 fi((x)) = 1 - exp(-wiT (x)) where wi is the i-th column of the weight matrix normalized to unit length (with no bias term).
Each column of the weight imprinting matrix is a template of the corresponding category. The linear layer computes the inner product between the input embeddings (x) and each template embedding wi. The result represents `close-by' templates in the embedding space using a threshold function.
y^ = sgn(wT (x) - )
where sgn is the sign function and  is the threshold.
3

Figure 1: CLIPPER (CLIP-PERsonalized) uses embeddings from CLIP's vision transformer with Multilabel Weight Imprinting technique for few-shot, multilabel, and continual learning.
4 Implementation
We share our implementation details below, model architecture in Fig 1, and algorithm in Appendix. Embeddings Generator: Weight imprinting [50] uses a base classifier trained on "abundant" labeled training samples. We replace the base classifier with CLIP (ViT B/32), a pretrained weakly supervised model [52]. We do not re-train or fine-tune the weights of the pretrained CLIP model. As shown in section 6 (Figure 2), we compared embeddings from different supervised, self-supervised, and weakly supervised models, and chose CLIP because it had the best FSL performance using WI. Image Embeddings: We embed images using CLIP's vision transformer and then use the normalized embeddings for multilabel weight imprinting. Compared to weight imprinting [50], which used 64-dimensional embeddings, we use 512-dimensional embeddings from CLIP. Qi et al. [50] also tried 512-dimensional embeddings and reported no significant effects on the results. Multilabel Weight Imprinting (MWI): The MWI layer is a single dense layer with an input size equal to the embedding size of the embeddings generator and output equal to the number of classes. We initialize the MWI weights as an average of the embeddings for each class corresponding to the weight column. We normalize the weights columns and use sigmoid activations with a threshold. When training the MWI layer, we use the binary cross-entropy loss with an Adam optimizer [34]. MWI+ = MWI + Training (T) + Augmentations (A): When training with non-trivial (nt) augmentations, we use 3 types of augmentations [4]: i. random crop, resize, and random horizontal flip; ii. random color jitter; iii. random Gaussian blur. Trivial (t) augmentations refer to repeating the image. Continual Learning (CL) We use Experience Replay (ER) [39], which involves keeping a memory of old data and rehearsing it. ER has been used for CL [60, 3, 19] and has been shown to outperform many CL approaches with and without a memory buffer [3]. In our multilabel continual learning setting, we retrain the old data with having/not having the new label when new labels are received.
5 Experiment Study
5.1 Datasets
We selected 10 single-label and 5 multi-label datasets based on 5 reasons: i. Few-shot learning: We added commonly used datasets for FSL; ii. Diversity: We included diverse datasets to evaluate performance under distributional and task shifts; iii. Robustness: We also picked an adversarial example dataset to evaluate robustness; iv. Multilabel settings: We chose multilabel datasets, including object detection, fine-grained detection, and overlapping labels; v. AI for good: We included a medical dataset to illustrate the broader impact of our work. Our dataset list is in Table 1.
5.2 Evaluations
We compared FSL in 7 settings: i. using different embeddings generators; ii. using sigmoid (MWI) versus softmax (WI) activations; iii. with and without training (T) and augmentations (A), both trivial (t) and non-trivial (nt) augmentations; iv. in 4 FSL settings like [65]): (5-way 5-shot, 15 test; 20-way
4

Table 1: Details about our chosen datasets, including their abbreviations (Abbr.).

Dataset Name Single Omniglot [38] MiniImagenet [70] Labeled Faces in the Wild [27] UCF101 [64] Imagenet-R [23] Imagenet-Sketch [71] Indoor Scene Recognition [51] CIFAR10 [37] Imagenet-A [24] Colorectal Histology [30] Multi-label CelebA Attributes [43] UTK Faces [75] Yale Faces [15] Common Objects in Context [40] iMaterialist (Fashion) [18]

Abbr.
OM MI LFW UCF IR IS ISR C10 IA CH
CAA UTK YF COCO IM

SOTA
[41] [44] [8] [28] [52] [71] [56] [14] [52] [16]
[31] [29] [32] [59] [18]

Content
Handwritten chars ImageNet subset Faces Action videos ImageNet art Imagenet sketches Indoor location 10 classes Difficult images Medical Images
40 attribute Gender, age, race 11 face labels 90 objects Fashion/apparel

Selection Reason
Few-shot Few-shot Diversity - Faces Diversity - Actions Diversity - Art Diversity - Sketch Diversity - Indoor Diversity Robustness AI for good
Label overlap Label overlap Label overlap Object Detection Fine-grained

5-shot, 5 test; 5 way 1 shot, 19 test; 20-way 1-shot, 10 test); v. in continual learning settings; vi. with CLIP's zero-shot and FSL linear probe; vii. with state-of-the-art (SOTA) results ­ there are no previous few-shot, multi-label, and continual learning evaluations, but we compare with FSL and also full training/test set evaluations. All evaluations are 5-way 5-shot, except for Ch (results in Appendix). We randomly sample classes and data points from each dataset 100 times and average the results.
5.3 Metrics
Commonly used single-label classification metrics, e.g., top-1 accuracy, are not applicable in multilabel settings. Multilabel evaluations have used different metrics, including class and overall precision, recall, and F1, as well as mean average precision (mAP) [72]. We calculated a total of 13 diverse metrics for each of our evaluations and included all the results in the supplementary materials.
We primarily use overall F1-score in this paper since F1-score accounts for class imbalance, which may be present in multilabel datasets, especially in real-world settings. The only downside of F1-score is that compared to mAP, it is threshold-dependent. However, in real-life situations, the threshold is also important, and therefore, we also discuss the optimal cut-off thresholds for our evaluations.
To compare our results with state-of-the-art (SOTA) results, we also report the metrics used by different SOTA results, i.e., top-1 accuracy for single-label datasets and average class accuracy (cAc) for multi-label datasets, except COCO, which uses mAP. We report these metrics along with F1-scores so that the F1-scores can be compared to the different SOTA metrics. SOTA references are in Table 1.
6 Results
We share our main results in this section and ablations in the next. First, CLIP+WI performance is similar to CLIP's linear probe performance, possibly because both are linear layers (Fig 2). Second, without training and augmentations, MWI performs worse than WI for single-label classifications ­ MWI F1-score is worse than WI F1-score (Fig 2), even though the accuracies are comparable (Fig 3). Third, MWI with training (50-80 epochs) and augmentations (10 trivial/non-trivial), i.e., MWI+, does at least as well as WI using CLIP (Fig 2-3), CLIP's linear probes (Fig -2), and state-of-the-art baselines (Fig 3). MWI and MWI+ are also compared with SOTA and CLIP's baselines in Table 2.
Comparing CLIP's embeddings: We compared embeddings from different pretrained supervised (Resnet50 [21], VGG16 [62], Inception V3), self-supervised (SimCLR v2 [5], MoCo v2 Resnet50 [4, 7], PCL Resnet50 [67], SwAV Resnet50 [2]), and weakly supervised (CLIP [52]) models (Figure 2 left). CLIP is trained on 400 million images, while the others are on 14 million Imagenet images. We have three key findings. First, CLIP gives the best results, possibly because CLIP is trained on a
5

Table 2: Comparing CLIPPER Multilabel Weight Imprinting (MWI) with SOTA and CLIP baselines1 C10 Ia Ir Is Isr Lfw Mi Om Ucf Ca Co Im Ut Yf Ch
1 .91 .85 .89 .60 .74 1.0 .92 1.0 .99 .82 .84 .72 .86 .85 .93 2 .91 .75 .82 .91 .95 1.0 .95 .96 .95 3 .89 .75 .79 .89 .94 1.0 .94 .94 .94 .69 .88 .79 .74 .88 4 .70 .54 .60 .72 .83 .94 .78 .66 .83 .62 .75 .48 .60 .66 5 .90 .74 .80 .90 .94 1.0 .94 .95 .94 .69 .73 .56 .78 .79 .69 6 .96 .90 .92 .96 .98 .99 .98 .98 .98 .76 .89 .83 .86 .91 .92 7 .91 .77 .83 .92 .96 1.0 .95 .97 .95 ­ .87 ­ ­ ­ SOTA(1); CLIP Lin. Probe Ac(2); MWI Ac(3),F1(4); MWI+T+A F1(5),CAc(6),Top1/mAP(7)
Figure 2: Comparing embeddings models (left) and Multilabel Weight Imprinting results (right).
Figure 3: Comparing SOTA, WI, and MWI for single-label (left) and multi-label (right) datasets.
bigger dataset than other pretrained models. Second, SimCLR's performance is closest to CLIP, even though it was trained only on a smaller dataset than CLIP. Third, Resnet50-based models performed much worse than the other models, even though all models, except CLIP, were trained on Imagenet. SOTA caveats: There are 3 caveats to our SOTA comparisons (Table 2). First, to the best of our knowledge, there is no prior work on multilabel few-shot learning and hence, we are setting new benchmarks and have no direct prior work to compare with. Second, even though we list the SOTA 5-way 5-shot results for OM and MI, there are two main differences: i. Previous few-shot results were pre-trained on large-scale, domain-specific, and labeled datasets, whereas our model is trained only on the few-shot set. Thus, performance for new domains like OM may not be as good as few-shot models pre-trained on OM; ii. Also, previous few-shot works did not do multilabel few-shot learning. Third, we list SOTA for other datasets, which have previously not been evaluated for few-shot learning, so the SOTA results are for full dataset training and testing and we only list them as a reference. 1
1Both 1. Ia and Ir & 2 in Table 2 use CLIP but 2 uses the ViT B/32 architecture whereas 1. Ia and Ir use the ViT L/14-336px architecture. L/14-336px is a bigger and better performing architecture but is not public [52]
6

Figure 4: Comparing metrics, without training and augmentations, for single- and multi-label datasets.
Figure 5: Comparing MWI with training and augmentations for single- and multi-label datasets .
CLIP baselines: Since we use embeddings from CLIP, we also compare our CLIP + MWI results to CLIP's linear probe, zero-shot, and CLIP + WI performance. We use both F1-score and accuracy, and all comparisons, other than zero-shot, are 5-way 5-shot. MWI+ using CLIP is comparable to CLIP's baselines, but unlike the linear probe, also enables few-shot, multilabel, and continual learning.
7 Ablations
7.1 MWI: Without Training and Augmentations We compare CLIPPER's 5-way 5-shot performance on 9 single-label datasets (Figure 4 left) and 5 multilabel datasets (Figure 4 right). For single-label, we perform two evaluations: i. Weight imprinting with softmax activations (f1-score and accuracy); ii. Multilabel weight imprinting with sigmoids (f1-score, top-1 accuracy, and per-class accuracy). For multilabel datasets with bounding boxes, i.e., COCO and iMaterialist, we compared full-full and patch-patch configurations, where `full' represents the full image and `patch' represents the bounding box of the relevant object. In n-m, n represents the training configuration and m represents the testing configuration. We had three key findings (Fig 4). First, for single-label datasets, WI accuracy is comparable to MWI top-1 accuracy, which means that the sigmoid activation can get us comparable results to the softmax activation. Though, as expected, MWI F1-scores are much lower in value than MWI Top-1 accuracy. Second, multi-label datasets on average have much lower performance than single-label datasets, which is expected as they have more labels than single-label datasets. Third, the patch-patch configuration works best for iMaterialist whereas the full-full configuration works best for COCO, possibly because the background is meaningful in COCO but mostly white in iMaterialist.
7.2 MWI+: With Training and Augmentations We evaluated the performance of Multilabel Weight Imprinting by adding training (T) and augmentations (A) (Figure 5). We had three key findings. First, CLIPPER's performance improved with both training and augmentations ­ after training and augmentations, F1-scores for multi-label
7

Figure 6: Comparing different few-shot settings with MWI+ (L:single-label, R:multi-label datasets)
Figure 7: MWI+ Continual learning results for increasing classes (L:single-, R:multi-label dataset) weight imprinting were comparable to the F1-scores for weight imprinting with softmax. Second, the performance saturates around 50-80 epochs, and trivial (t) augmentations, i.e., image repetitions, are as good or sometimes even better than non-trivial (nt) augmentations. Third, with training, the best threshold values stabilized around 0.5 for most datasets (Figure 8 (left). We also compared 4 few-shot learning settings (Figure 6): 5-way 5-shot, 20-way 5-shot, 5 way 1 shot, 20-way 1-shot. The performance worsens with decreasing shots and with increasing classes.
Figure 8: Optimal thresholds with (left) and without (right) continual learning for all datasets. 7.3 Continual learning We evaluated 5 way 5 shot continual learning. We incrementally added the number of labeled classes and their respective training data and labels, while keeping the test set fixed. We had three key findings. First, CL performance (Figure 7) varies with the number of classes but reaches approximately the same 5-way 5-shot value with continual learning as it does without continual learning (5). Second, the optimal-performance thresholds vary with the number of classes and we share the best accuracies and their respective thresholds for each dataset for different number of classes (Figure 8 right). Third, the thresholds are higher with lower number of classes, possibly because of lesser training data, but converge to approximately the same 5-way 5-shot value with and without continual learning (Fig 8).
8

8 Discussion and Limitations
With advances in representation learning, either using generative or contrastive models, the question arises: how to best use the representations in downstream tasks. Previous work suggests, "combining the strength of zero-shot transfer with the flexibility of few-shot learning is a promising direction" [52] and "obtain better results...by combining few-shot learning methods with fine-tuning" [36].
We outline few-shot, continual, and multilabel learning as the desiderata for downstream tasks and introduce a technique, called Multilabel Weight Imprinting, to meet the desiderata. Our model uses embeddings from a pertained CLIP model and shows promising performance on diverse and challenging tasks. We set few-shot, multilabel, and continual learning benchmarks for many datasets.
Our work has three key findings. First, using pretrained models with an existing FSL technique, i.e., weight imprinting [50], enables sample-efficient learning with two additional benefits: i. Unlike commonly-used transfer learning techniques like fine-tuning and distillation, we have a prototype for each class and can flexibly add/update each class prototype without influencing (e.g., forgetting) the other class prototypes; ii. Unlike commonly-used FSL methods, the base model need not be trained with computationally-intensive techniques involving large, domain-specific, and expensive-to-label datasets. Second, replacing weight imprinting's softmax function with a sigmoid and threshold function enables multilabel weight imprinting, and using training and augmentations helps improve performance. Finally, adding experience reply enables multilabel continual learning.
Our work has three key limitations: i. Multilabel learning has poorer and even threshold-dependent performance compared to single-label learning, but multi-label learning is still more realistic than single-label classification as even single-label datasets have multiple labels [74]; ii. Prototype-based few-shot learning scales the number of prototypes with the number of classes and comparing with every single prototype may not be efficient. Thus, efficient and scalable methods, e.g., hierarchical prototypes, are needed; iii. Experience replay for multilabel continual learning is memory-inefficient and memory-efficient continual learning, e.g., prototype-based contrastive learning, could be leveraged.
We have three key directions for future work: i. Use downstream few-shot learning for correcting/filtering labels from upstream models, without necessarily sending data to the upstream models; ii. Make few-shot, multilabel, and continual learning memory-efficient, robust, and deployable; iii. Deploy and test in real-world settings, e.g., for human-in-the-loop and personalized applications.
9 Broader Impact
We highlight three key areas for positive impact of our work. First, we designed our model for fewshot, multilabel, and continual learning to enable real-world sample-efficient applications, including personalized and AI for good applications (more details in appendix). Second, since we do not train the upstream model, the data does not have to be sent to the upstream model, affording privacypreserving and offline model training. Third, since we only train a linear layer, our model affords easy and lightweight real-world training and deployment, including on mobile, web, and wearable devices, especially if the pretrained base model can be mobile-optimized [25, 66] as in [33].
Like many machine learning techniques, our model is as good as its applications. We have made our model flexible, easy-to-use, and easy-to-train ­ it can be used with any state-of-the-art pre-trained model, trained and run using free Google Colab notebooks, and personalized using only a few examples. Moreover, few-shot and personalized learning may also help mitigate data/labeling bias.
Thus, we hope that our work will enable interdisciplinary stakeholders to ethically design and deploy personalized, privacy-preserving, and meaningful real-world deep learning applications.
10 Conclusion
Data-efficiency and generalization are key challenges for deep learning. Self-supervised or weakly supervised models trained on unlabeled or uncurated datasets have shown promising transfer to few-shot tasks. Few-shot learning methods have also demonstrated sample-efficient learning.
We highlight the need for few-shot, multilabel, and continual learning, and developed a technique, called Multi-label Weight Imprinting (MWI), for few-shot, continual, and multi-label learning.
9

Unlike previous FSL techniques, our model, CLIPPER, uses MWI with pretrained representations from a weakly-supervised model, i.e., CLIP. Thus, CLIPPER combines the sample-efficiency and generalizability of transfer learning with the flexibility and specialization of FSL methods.
CLIPPER shows robust and competitive performance and is the first step in the direction of using pretrained models for few-shot, multilabel, and continual learning. Our model is also lightweight and the data does not have to be sent back to the upstream model, enabling privacy-preserving and on-device downstream training. Thus, our model enables few-shot, multilabel, and continual learning, especially for easy-to-train, light-weight, and privacy-preserving real-world applications.
References
[1] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceedings of ICML workshop on unsupervised and transfer learning, pages 17­36. JMLR Workshop and Conference Proceedings, 2012.
[2] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.
[3] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.
[4] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A Simple Framework for Contrastive Learning of Visual Representations. arXiv:2002.05709 [cs, stat], June 2020. arXiv: 2002.05709.
[5] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. Hinton. Big Self-Supervised Models are Strong Semi-Supervised Learners. arXiv:2006.10029 [cs, stat], Oct. 2020. arXiv: 2006.10029.
[6] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang. A closer look at few-shot classification. arXiv preprint arXiv:1904.04232, 2019.
[7] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.
[8] G. G. Chrysos, S. Moschoglou, G. Bouritsas, Y. Panagakis, J. Deng, and S. Zafeiriou. P-nets: Deep polynomial neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7325­7335, 2020.
[9] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. arXiv preprint arXiv:1511.01432, 2015.
[10] R. Devkar and S. Shiravale. A survey on multi-label classification for images. International Journal of Computer Application, 162(8):39­42, 2017.
[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019. arXiv: 1810.04805.
[12] L. Ericsson, H. Gouk, and T. M. Hospedales. How well do self-supervised models transfer? arXiv preprint arXiv:2011.13377, 2020.
[13] C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. arXiv:1703.03400 [cs], July 2017. arXiv: 1703.03400.
[14] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.
[15] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE transactions on pattern analysis and machine intelligence, 23(6):643­660, 2001.
[16] S. Ghosh, A. Bandyopadhyay, S. Sahay, R. Ghosh, I. Kundu, and K. Santosh. Colorectal histology tumor detection using ensemble deep neural network. Engineering Applications of Artificial Intelligence, 100:104202, 2021.
[17] S. Gidaris, A. Bursuc, N. Komodakis, P. Pérez, and M. Cord. Boosting few-shot visual learning with self-supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8059­8068, 2019.
10

[18] S. Guo, W. Huang, X. Zhang, P. Srikhanta, Y. Cui, Y. Li, H. Adam, M. R. Scott, and S. Belongie. The imaterialist fashion attribute dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0­0, 2019.
[19] T. L. Hayes, N. D. Cahill, and C. Kanan. Memory efficient experience replay for streaming learning. In 2019 International Conference on Robotics and Automation (ICRA), pages 9769­ 9776. IEEE, 2019.
[20] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729­9738, 2020.
[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­ 778, 2016.
[22] O. Henaff. Data-efficient image recognition with contrastive predictive coding. In International Conference on Machine Learning, pages 4182­4192. PMLR, 2020.
[23] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, D. Song, J. Steinhardt, and J. Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.
[24] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. arXiv preprint arXiv:1907.07174, 2019.
[25] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
[26] D. Hu, Q. Lu, L. Hong, H. Hu, Y. Zhang, Z. Li, A. Shen, and J. Feng. How well self-supervised pre-training performs with streaming data? arXiv preprint arXiv:2104.12081, 2021.
[27] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In Workshop on faces in'RealLife'Images: detection, alignment, and recognition, 2008.
[28] M. Kalfaoglu, S. Kalkan, and A. A. Alatan. Late temporal modeling in 3d cnn architectures with bert for action recognition. arXiv preprint arXiv:2008.01232, 2020.
[29] K. Kärkkäinen and J. Joo. Fairface: Face attribute dataset for balanced race, gender, and age. arXiv preprint arXiv:1908.04913, 2019.
[30] J. N. Kather, C.-A. Weis, F. Bianconi, S. M. Melchers, L. R. Schad, T. Gaiser, A. Marx, and F. G. Z"ollner. Multi-class texture analysis in colorectal cancer histology. Scientific reports, 6:27988, 2016.
[31] T. Kehrenberg, M. Bartlett, O. Thomas, and N. Quadrianto. Null-sampling for interpretable and fair representations. In European Conference on Computer Vision, pages 565­580. Springer, 2020.
[32] A. Khalili Mobarakeh, J. A. Cabrera Carrillo, and J. J. Castillo Aguilar. Robust face recognition based on a new supervised kernel subspace learning method. Sensors, 19(7):1643, 2019.
[33] M. Khan and P. Maes. Pal: Intelligence augmentation using egocentric visual context detection. arXiv preprint arXiv:2105.10735 [cs], 2021.
[34] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[35] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML deep learning workshop, volume 2. Lille, 2015.
[36] S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2661­2671, 2019.
[37] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Master's thesis, University of Tront, 2009.
[38] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332­1338, 2015.
11

[39] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293­321, 1992.
[40] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740­755. Springer, 2014.
[41] J. Liu, F. Chao, L. Yang, C.-M. Lin, and Q. Shen. Decoder choice network for meta-learning. arXiv preprint arXiv:1909.11446, 2019.
[42] Y. Liu, L. Sheng, J. Shao, J. Yan, S. Xiang, and C. Pan. Multi-label image classification via knowledge distillation from weakly-supervised detection. In Proceedings of the 26th ACM international conference on Multimedia, pages 700­708, 2018.
[43] Z. Liu, P. Luo, X. Wang, and X. Tang. Large-scale celebfaces attributes (celeba) dataset. Retrieved August, 15(2018):11, 2018.
[44] Y. Luo, Y. Wong, M. Kankanhalli, and Q. Zhao. Direction concentration learning: Enhancing congruency in machine learning. IEEE transactions on pattern analysis and machine intelligence, 2019.
[45] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe, and L. Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages 181­196, 2018.
[46] Z. Mai, R. Li, J. Jeong, D. Quispe, H. Kim, and S. Sanner. Online continual learning in image classification: An empirical survey. arXiv preprint arXiv:2101.10423, 2021.
[47] L. McInnes, J. Healy, and J. Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.
[48] T. Munkhdalai and H. Yu. Meta Networks. arXiv:1703.00837 [cs, stat], June 2017. arXiv: 1703.00837.
[49] A. Nichol, J. Achiam, and J. Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.
[50] H. Qi, M. Brown, and D. G. Lowe. Low-shot learning with imprinted weights. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5822­5830, 2018.
[51] A. Quattoni and A. Torralba. Recognizing indoor scenes. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 413­420. IEEE, 2009.
[52] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.
[53] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. OpenAI, 2018.
[54] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[55] A. Raghu, M. Raghu, S. Bengio, and O. Vinyals. Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.
[56] M. Rahimzadeh, S. Parvin, E. Safi, and M. R. Mohammadi. Wise-srnet: A novel architecture for enhancing image classification by learning spatial resolution of feature maps. arXiv preprint arXiv:2104.12294, 2021.
[57] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. https: // openreview. net/ forum? id= rJY0-Kcll , Nov. 2016.
[58] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001­2010, 2017.
[59] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.
[60] D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, and G. Wayne. Experience replay for continual learning. arXiv preprint arXiv:1811.11682, 2018.
12

[61] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. One-shot learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065, 2016.
[62] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
[63] J. Snell, K. Swersky, and R. S. Zemel. Prototypical Networks for Few-shot Learning. arXiv:1703.05175 [cs, stat], June 2017. arXiv: 1703.05175.
[64] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.
[65] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, and T. M. Hospedales. Learning to compare: Relation network for few-shot learning. CoRR, abs/1711.06025, 2017.
[66] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105­6114. PMLR, 2019.
[67] P. Tang, X. Wang, S. Bai, W. Shen, X. Bai, W. Liu, and A. Yuille. Pcl: Proposal cluster learning for weakly supervised object detection. IEEE transactions on pattern analysis and machine intelligence, 42(1):176­191, 2018.
[68] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola. Rethinking few-shot image classification: a good embedding is all you need? arXiv preprint arXiv:2003.11539, 2020.
[69] UCSD. Computer vision. http://vision.ucsd.edu/content/yale-face-database.
[70] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching Networks for One Shot Learning. arXiv:1606.04080 [cs, stat], Dec. 2017. arXiv: 1606.04080.
[71] H. Wang, S. Ge, E. P. Xing, and Z. C. Lipton. Learning robust global representations by penalizing local predictive power. arXiv preprint arXiv:1905.13549, 2019.
[72] J. Wang, Y. Yang, J. Mao, Z. Huang, C. Huang, and W. Xu. Cnn-rnn: A unified framework for multi-label image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2285­2294, 2016.
[73] Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni. Generalizing from a few examples: A survey on few-shot learning. ACM Computing Surveys (CSUR), 53(3):1­34, 2020.
[74] S. Yun, S. J. Oh, B. Heo, D. Han, J. Choe, and S. Chun. Re-labeling imagenet: from single to multi-labels, from global to localized labels. arXiv preprint arXiv:2101.05022, 2021.
[75] Z. Zhang, Y. Song, and H. Qi. Age progression/regression by conditional adversarial autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.
13

A Method and Algorithm
We describe our problem definition below and the algorithm in Algorithms 1 - 6. We have a train set of N labeled examples: S = {(x1, y1), ..., (xN , yN )} where each xi  RD is the D-dimensional feature vector of an example and yi is a subset of labels L = {1, ..., K}. Sk denotes the set of examples labeled with class k and Sj and Sk are not necessarily disjoint sets for j = k. Moreover, in continual learning setting, the training set S and the labels L can grow over time. St = {(x1, l1), ..., (xM , lM )} and Lt = {1, ..., P } and St = {(x1, l1), ..., (xN , lN )} and Lt = {1, ..., Q} where N > M and Q > P , i.e., the size of labels and the training set are non-decreasing over time.
Algorithm 1 Augmentations Function Input: Original image, x Output: Augmented image, x'
1: function AUGMENT(x) 2: x = RANDOMCROPANDRESIZE(x) 3: x = RANDOMHORIZONTALFLIP(x ) 4: x = RANDOMCOLORJITTER(x ) 5: x = RANDOMGAUSSIANBLUR(x ) 6: return x

Algorithm 2 Embedding Function

Input: Dataset to embed D, Number of augmentations for each image NA Output: Embedded dataset E

1: function CLIPEMBEDDATASET(D, NA)

2: E  {}

3: for (xi, yi)  D do

4:

for j  {1, 2, · · · NA} do

5:

xi  AUGMENT(xi)

6:

i  CLIP(xi)

7:

i  NORMALIZE(i)

8:

APPEND(E, (i, yi))

9: return E

Embedded dataset If augmenting(ablation)

Algorithm 3 Training Function

Input: Embedded dataset E, Weight Imprinting single layer f , Number of training epochs etrain
Output: Trained weight imprinting layer f

1: function TRAIN(E, f, etrain)

2: J  0

3: for (i, yi)  E do

4:

yi = (f (i)

5:

J  J + BINARYCROSSENTROPY(yi, yi)

6: J  MEAN(J )

7: OPTIMIZE(f, J, Adam, etrain)

8: return f

14

Algorithm 4 Predicting Function

Input: Embedding  , Weight Imprinting single layer f , Evaluation threshold threshold Output: Predicted labels labels

1: function PREDICT( , f, threshold)

2: labels  {}

3: yi = (f (i) 4: for j  {1, 2, · · · W } do

5:

if yij  threshold then

6:

APPEND(labels, j)

7: return labels

Algorithm 5 Sampling Function

Input: Number of ways/labels nways, Number of shots per label nshot, Number of test

images per label ntest, Number of episodes nepisodes, Dataset to sample from D

Output: Few shot train dataset Dtrain, Few shot test dataset Dtest

1: function SAMPLE(nway, nshot, ntest, nepisodes, D)

2: Dtrain = {} 3: Dtest = {}
4: L  GETUNIQUELABELS(D)

5: for i  {1, 2, · · · nepisodes} do

6:

dtrain  {}

7:

dtest  {}

8:

Lsampled  RANDOMSAMPLE(L, nway)

Sample nway labels

9:

for l  Lsampled do

10:

Dl  {(xi, yi)  D | l  yi}

11:

d1  RANDOMSAMPLE(Dl \ (dtrain  dtest), nshot)

12:

APPEND(dtrain, d1)

13:

d2  RANDOMSAMPLE(Dl \ (dtrain  dtest  d1), ntest)

14:

APPEND(dtest, d2)

15:

APPEND(Dtest, dtest)

16:

APPEND(Dtrain, dtrain)

17: return Dtrain, Dtest

Algorithm 6 Continual Learning Evaluation

Input: Embedded few shot train dataset Etrain, Embedded few shot test dataset Etest, Evaluation threshold threshold, Number of training epochs etrain

1: function CONTINUALLEARNING(Etrain, Etest, threshold, etrain) 2: L  GETUNIQUELABELS(Etrain) 3: LabelsAdded = {}

4: for l  L do

5:

El  {(i, l) | (i, yi)  Etrain, l  yi}

6:

if ISEMPTY(LabelsAdded) then

7:

f = INITIALIZEWEIGHTIMPRINTING(El)

8:

if !ISEMPTY(LabelsAdded) then

9:

f = ADDCLASSWEIGHTIMPRINTING(f, El)

10:

Etrain  {(i, {lj}) | (i, yi)  Etrain, lj  LabelsAdded}

11:

TRAIN(Etrain, f, etrain)

12:

Etest  {(i, {lj}) | (i, yi)  Etest, lj  LabelsAdded}

13:

for (i, yi)  Etest do

14:

labels  PREDICT(i, f, threshold)

15:

EVALUATEMETRIC(labels, yi)

16:

APPEND(LabelsAdded, l)

15

Figure 9: Design for personalized photo labeling application, showing all labeled images, imagelabeling interface, and cluster labeling interface
Figure 10: Clustering using different embeddings: Kmeans (Left), Aggolomerate Clustering (Right)
B Real-world Applications
We discuss the real-world applications of our work using a personalized photo-labeling application and few-shot results for a medical imaging dataset. Personalized photo labeling: We designed a personalized photo labeling application to enable personalized phot labeling using CLIPPER's. The interface (Figure 9) shows how users can label specific images and also, how clustering can help efficient labeling by clustering similar images. We share the clustering accuracies in Figure 10 ­ image embeddings from each embedding generator were passed to UMAP [47], resulting in 2-dimensional embeddings, which we used for clustering. Real-world Medical Imaging: We tested multilabel few-shot learning on the colorectal cancer histology dataset [30]. Our model, trained with 5 examples per class, shows competitive performance compared to the state-of-the-art model, which was trained using the full dataset [16] (Fig 11).
Figure 11: Comparing CLIPPER's FSL for colorectal cancer histology with state-of-the-art 16

Table 3: Copyright details about our chosen datasets.

Dataset
Single Omniglot [38] MiniImagenet [70] Labeled Faces in the Wild [27] UCF101 [64] Imagenet-R [23] Imagenet-Sketch [71] Indoor Scene Recognition [51] CIFAR10 [37] Imagenet-A [24] Colorectal Histology [30] Multi-label CelebA Attributes [43] UTK Faces [75] Yale Faces [69] Common Objects in Context [40] iMaterialist (Fashion) [18]

Copyright
MIT License Copyright (c) 2015 Brenden Lake MIT License Copyright (c) 2019 Yaoyao Liu Unknown Unknown MIT License Copyright (c) 2020 Dan Hendrycks MIT License Copyright (c) 2021 Haohan Wang Uknown MIT License Copyright (c) 2017 Baptiste Wicht MIT License Copyright (c) 2019 Dan Hendrycks Creative Commons Attribution 4.0 International
"non-commercial research purposes" "non-commercial research purposes" Data files © Original Authors Creative Commons Attribution 4.0 License. MIT License Copyright (c) 2017 Visipedia

Table 4: Details of metrics used for single-label and multi-label classifications.

Metrics
Hamming score Jaccard Subset accuracy Mean Average Accuracy (mAP) Class F1 Overall F1 Class precision Overall precision Class recall Overall recall Top-1 accuracy Top-5 accuracy Class accuracy

Single-label Softmax

Single-label Sigmoid

Multi-label

C Experiments and Results
We share the copyright details for our datasets in Table 3 and our metric details in Table 4.
For UCF and ISR, the images are resized such that the smaller dimension is 224 followed by a center crop of 224x224. For all other datasets, the images are resized to 224x224. Since UCF has videos, we select the middle frame as the target image.
In the rest of the tables, we show the detailed numerical results for all our evaluations, including error bars (Table C) for our few-shot evaluations.

17

Table 5: Comparing FSL (F1-score) using Weight Imprinting for different embeddings models

CLIP SimCLR MoCo PCL SwAV Resnet50 VGG16 Inception V3

C10 0.88 0.76 IA 0.74 0.6 IR 0.77 0.61 IS 0.89 0.83 ISR 0.94 0.82 LFW 0.99 0.66 MI 0.94 0.92 OM 0.93 0.9 UCF 0.94 0.9

0.63 0.63 0.53 0.66 0.54 0.51 0.51 0.55 0.57 0.58 0.52 0.65 0.6 0.44 0.34 0.3 0.44 0.29 0.22 0.19 0.61 0.56 0.64 0.73 0.63 0.44 0.23 0.2 0.91 0.91 0.89 0.91 0.67 0.46 0.29 0.23

0.73 0.45

0.55 0.52

0.61 0.52

0.83 0.72

0.79 0.58

0.6

0.62

0.91 0.81

0.88 0.88

0.84 0.72

Table 6: Comparing Multilabel Weight Imprinting F1-scores for single-label datasets

C10 IA IR IS ISR LFW MI OM UCF

CLIP Zero Shot

0.92 0.78 0.87 0.83 0.95 0.91

CLIP Linear Probe

0.91 0.75 0.82 0.91 0.95 1.0

WI

0.88 0.74 0.77 0.89 0.94 0.99

MWI

0.7 0.54 0.6 0.72 0.83 0.94

+T

0.82 0.65 0.72 0.82 0.91 0.98

+ T + A (Non-trivial 5) 0.85 0.72 0.79 0.89 0.95 1.0

+ T + A (Trivial 5)

0.89 0.74 0.79 0.9 0.94 0.99

+ T + A (Non-trivial 10) 0.87 0.73 0.81 0.91 0.95 1.0

+ T + A (Trivial 10)

0.9 0.74 0.8 0.9 0.95 1.0

0.96 0.2 0.87 0.95 0.96 0.95 0.94 0.93 0.94 0.78 0.66 0.83 0.89 0.82 0.89 0.92 0.88 0.93 0.93 0.93 0.93 0.93 0.92 0.94 0.94 0.95 0.94

Table 7: Comparing Multilabel Weight Imprinting, SOTA, and WI for single-label datasets

C10 IA IR IS ISR LFW MI OM UCF

SOTA (Acc)

0.91 0.85 0.89 0.6 0.74 1.0

WI (Acc)

0.89 0.76 0.78 0.9 0.94 1.0

WI (F1)

0.88 0.74 0.77 0.89 0.94 0.99

MWI (Acc)

0.89 0.75 0.79 0.89 0.94 1.0

MWI (F1)

0.7 0.54 0.6 0.72 0.83 0.94

+T +A (F1)

0.9 0.74 0.8 0.9 0.94 1.0

+T +A (Top-1 Acc) 0.91 0.76 0.84 0.92 0.96 1.0

+T +A (Class Acc) 0.96 0.9 0.92 0.96 0.98 1.0

0.92 1.0 0.99 0.94 0.93 0.95 0.94 0.93 0.94 0.94 0.94 0.94 0.78 0.66 0.83 0.94 0.95 0.94 0.95 0.97 0.95 0.98 0.98 0.98

Table 8: Comparing Multilabel Weight Imprinting, SOTA, and WI for multi-label datasets

CAA COCO_F COCO_P IM_F IM_P UTK YF

SOTA (Acc) 0.82 0.84

0.84

0.72 0.72 0.86 0.85

MWI (Acc) 0.69 0.88

0.77

0.75 0.79 0.74 0.88

MWI (F1) 0.62 0.75

0.44

0.4 0.48 0.6 0.66

+T +A (F1) 0.69 0.73

0.66

0.41 0.56 0.78 0.79

+T +A (Acc) 0.76 0.88

0.88

0.76 0.83 0.86 0.91

18

Table 9: Comparing metrics, without training and augmentations, for single-label datasets

C10 IA IR IS ISR LFW MI OM UCF

SOFT ACC 0.89 0.77 0.78 0.9 0.94 1.0

SOFT F1 0.88 0.74 0.77 0.89 0.94 0.99

SIG F1

0.7 0.54 0.6 0.72 0.83 0.94

SIG TOP1 0.89 0.75 0.79 0.89 0.94 0.99

SIG CACC 0.86 0.83 0.85 0.89 0.93 0.98

0.94 0.93 0.95 0.94 0.93 0.94 0.78 0.66 0.83 0.94 0.93 0.94 0.92 0.8 0.94

Table 10: Comparing metrics, without training and augmentations, for multi-label datasets

CAA COCO_F COCO_P IM_F IM_P UTK YF

SIG F1

0.62 0.75

0.44

0.39 0.48 0.6 0.66

SIG CACC 0.69 0.88

0.77

0.75 0.79 0.74 0.88

Table 11: Comparing MWI with training and augmentations for single-label datasets

C10 IA IR IS ISR LFW MI OM UCF

MWI

0.7 0.54 0.6 0.72 0.83 0.94

+T

0.82 0.65 0.72 0.82 0.91 0.98

+ T + A (Non-trivial 5) 0.85 0.72 0.79 0.89 0.95 1.0

+ T + A (Trivial 5)

0.89 0.74 0.79 0.9 0.94 0.99

+ T + A (Non-trivial 10) 0.87 0.73 0.81 0.91 0.95 1.0

+ T + A (Trivial 10)

0.9 0.74 0.8 0.9 0.95 1.0

0.78 0.66 0.83 0.89 0.82 0.89 0.92 0.88 0.93 0.93 0.93 0.93 0.93 0.92 0.94 0.94 0.95 0.94

Table 12: Comparing MWI with training and augmentations for multi-label datasets

CAA COCO_F COCO_P IM_F IM_P UTK YF

MWI

0.62 0.75

0.44

0.39 0.48 0.6 0.66

+T

0.67 0.74

0.54

0.38 0.55 0.7 0.72

+ T + A (Non-trivial 5) 0.7 0.76

0.56

0.41 0.45 0.78 0.77

+ T + A (Trivial 5)

0.69 0.73

0.61

0.41 0.54 0.78 0.78

+ T + A (Non-trivial 10) 0.71 0.74

0.6

0.41 0.46 0.76 0.77

+ T + A (Trivial 10)

0.69 0.73

0.66

0.41 0.56 0.78 0.79

Table 13: Comparing CLIPPER's FSL for colorectal cancer histology with state-of-the-art

8w5s0a 8w5s5a 8w5s5a_trivial 8w5s10a 8w5s10a_trivial

SOTA
0.93 0.93 0.93 0.93 0.93

Single-label Evaluation Accuracy Overall F1

0.90

0.63

0.85

0.43

0.92

0.68

0.92

0.66

0.92

0.69

Multi-label Evaluation Accuracy Overall F1

0.89

0.43

0.91

0.6

0.92

0.64

0.91

0.64

0.92

0.67

19

Table 14: Best performance (10 trivial augmentations) on all datasets for 5-way 5-shot with training mean metric values with error across 100 episodes.

C10 IA IR IS ISR LFW MI OM UCF CAA COCO_F IM_P UTK YF

MWI+T+A F1
0.9±0.04 0.74±0.09 0.8±0.07 0.9±0.05 0.94±0.04 0.1±0.01 0.94±0.03 0.95±0.03 0.94±0.05 0.69 ± 0.07 0.73±0.08 0.56±0.1 0.78 ± 0.06 0.79±0.14

MWI+T+A CAc
0.96±0.02 0.9±0.03 0.92±0.03 0.96±0.02 0.98±0.01 0.99±0.01 0.98±0.01 0.98±0.01 0.98±0.02 0.76±0.05 0.89±0.1 0.83±0.14 0.86±0.04 0.91±0.05

Table 15: Comparing different few-shot settings with MWI+ for single-label datasets

C10 IA IR IS ISR LFW MI OM UCF

5 way 1 shot 0.72 0.54 0.58 0.72 0.8 0.96 20 way 1 shot 0.6 0.26 0.27 0.46 0.64 0.8 5 way 5 shot 0.9 0.74 0.8 0.9 0.95 1.0 20 way 5 shot 0.83 0.51 0.58 0.78 0.88 0.98

0.8 0.8 0.83 0.55 0.52 0.61 0.94 0.95 0.94 0.82 0.84 0.84

Table 16: Comparing different few-shot settings with MWI+ for multi-label datasets

CAA COCO_F IM_P UTK YF

5 way 1 shot 0.6 0.63 20 way 1 shot 0.6 0.40 5 way 5 shot 0.69 0.73 20 way 5 shot 0.7 0.57

0.51 0.59 0.65 0.22 0.62 0.5 0.56 0.78 0.79 0.34 0.79 0.8

Table 17: MWI+ Continual learning results for increasing classes for single-label datasets

C10 IA IR IS ISR LFW MI OM UCF

1 0.83 0.83 0.85 0.92 0.95 0.96 2 0.75 0.58 0.65 0.76 0.85 0.88 3 0.73 0.66 0.7 0.8 0.88 0.91 4 0.79 0.65 0.73 0.85 0.92 0.95 5 0.84 0.7 0.8 0.9 0.96 0.99

0.86 0.84 0.95 0.74 0.69 0.87 0.82 0.81 0.89 0.88 0.85 0.91 0.91 0.86 0.94

Table 18: MWI+ Continual learning results for increasing classes for multi-label datasets

CAA COCO IM UTK YF

0 0.68 0.89 1 0.66 0.68 2 0.72 0.76 3 0.72 0.81 4 0.73 0.8

0.82 0.74 0.86 0.49 0.67 0.72 0.54 0.73 0.72 0.58 0.76 0.75 0.53 0.8 0.76

20

Table 19: Optimal thresholds without continual learning for single-label datasets

C10 IA IR IS ISR LFW MI OM UCF

0 0.7 0.66 0.67 0.68 0.68 0.69 5 0.68 0.65 0.65 0.66 0.66 0.67 10 0.63 0.62 0.62 0.63 0.62 0.62 15 0.58 0.59 0.59 0.59 0.59 0.58 20 0.54 0.57 0.57 0.56 0.56 0.55 25 0.51 0.55 0.55 0.54 0.54 0.53 30 0.5 0.54 0.54 0.53 0.52 0.51 35 0.49 0.53 0.53 0.52 0.51 0.5 40 0.48 0.52 0.52 0.51 0.5 0.49 45 0.47 0.51 0.51 0.5 0.5 0.49 50 0.47 0.51 0.51 0.5 0.49 0.49 55 0.47 0.51 0.5 0.49 0.49 0.48 60 0.46 0.5 0.5 0.49 0.49 0.48 65 0.46 0.5 0.5 0.49 0.49 0.48 70 0.46 0.5 0.49 0.49 0.49 0.48 75 0.46 0.5 0.49 0.49 0.48 0.48 80 0.46 0.49 0.49 0.48 0.48 0.48

0.69 0.72 0.69 0.67 0.7 0.66 0.62 0.63 0.62 0.58 0.57 0.58 0.55 0.53 0.55 0.53 0.5 0.53 0.51 0.49 0.51 0.5 0.48 0.5 0.49 0.47 0.49 0.49 0.46 0.49 0.48 0.46 0.48 0.48 0.45 0.48 0.48 0.45 0.48 0.48 0.45 0.48 0.48 0.45 0.48 0.47 0.45 0.47 0.47 0.45 0.47

Table 20: Optimal thresholds without continual learning for multi-label datasets

CAA COCO_F COCO_P IM_F IM_P UTK YF

0 0.67 0.7

0.7

0.68 0.68 0.68 0.71

5 0.65 0.68

0.68

0.66 0.66 0.67 0.68

10 0.62 0.63

0.63

0.63 0.63 0.63 0.63

15 0.59 0.58

0.58

0.59 0.59 0.59 0.57

20 0.56 0.54

0.54

0.56 0.56 0.55 0.54

25 0.53 0.51

0.51

0.54 0.54 0.53 0.51

30 0.52 0.5

0.5

0.53 0.53 0.51 0.5

35 0.51 0.49

0.49

0.52 0.52 0.5 0.48

40 0.5 0.48

0.48

0.51 0.51 0.49 0.48

45 0.49 0.47

0.47

0.5 0.5 0.49 0.47

50 0.49 0.47

0.47

0.5 0.5 0.48 0.47

55 0.49 0.47

0.47

0.49 0.49 0.48 0.47

60 0.48 0.46

0.46

0.49 0.49 0.48 0.47

65 0.48 0.46

0.46

0.49 0.49 0.48 0.47

70 0.48 0.46

0.46

0.49 0.49 0.48 0.46

75 0.48 0.46

0.46

0.49 0.49 0.48 0.47

80 0.48 0.46

0.46

0.48 0.48 0.47 0.46

Table 21: Optimal thresholds with continual learning for single- and multi-label datasets
ca c10 co ia ir is im isr lfw mi om ucf utk yf 1 .60 .67 .60 .64 .66 .66 .62 .67 .65 .66 .64 .66 .61 .64 2 .51 .56 .52 .56 .56 .57 .51 .57 .59 .57 .52 .58 .52 .53 3 .50 .54 .51 .55 .54 .55 .5 .55 .57 .55 .50 .55 .50 .52 4 .49 .53 .50 .54 .53 .54 .49 .54 .56 .54 .49 .54 .50 .51 5 .49 .53 .49 .54 .53 .53 .48 .53 .54 .53 .49 .53 .49 .50

21

