arXiv:2106.00286v1 [math.OC] 1 Jun 2021

On Riemannian Optimization over Positive Definite Matrices with the Bures-Wasserstein Geometry
Andi Han Bamdev Mishra Pratik Jawanpuria Junbin Gao
Abstract
In this paper, we comparatively analyze the Bures-Wasserstein (BW) geometry with the popular Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. Our study begins with an observation that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. We build on this to show that the BW metric is a more suitable and robust choice for several Riemannian optimization problems over ill-conditioned SPD matrices. We show that the BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Finally, we verify that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesic convex under the BW geometry. Extensive experiments on various applications support our findings.
1 Introduction
Learning on symmetric positive definite (SPD) matrices is a fundamental problem in various machine learning applications, including metric and kernel learning [TRW05, GVS09, SGH21], medical imaging [PFA06, Pen20], computer vision [HSH14, HWL+17], domain adaptation [MMG19], modeling time-varying data [BSB+19], and object detection [TPM08], among others. Recent studies have also explored SPD matrix learning as a building block in deep neural networks [HG17, GWJH20].
The set of SPD matrices of size n × n, defined as Sn++ := {X : X  Rn×n, X = X, and X 0}, has a smooth manifold structure with a richer geometry than the Euclidean space. When endowed with a metric (inner product structure), the set of SPD matrices becomes a Riemannian manifold [Bha09]. Hence, numerous existing works [PFA06, AFPA07, JHS+13, HWS+15, HG17, Lin19, GWJH20, Pen20] have studied and employed the Riemannian optimization framework for learning over the space of SPD matrices [AMS09, Bou20].
Several Riemannian metrics on Sn++ have been proposed such as the Affine-Invariant [PFA06, Bha09], the Log-Euclidean [AFPA07, QSBM14], the Log-Det [Sra12, CM12], the Log-Cholesky [Lin19], to name a few. One can additionally obtain different families of Riemannian metrics on Sn++ by appropriate parameterizations based on the principles of invariance and symmetry [DKZ09a, CM12, TP19, Pen20]. However, to the best of our knowledge, a systematic study comparing the different metrics for optimizing generic cost functions defined on Sn++ is missing. Practically, the Affine-Invariant (AI) metric seems to be the most widely used metric in Riemannian first and second order algorithms (e.g., steepest descent, conjugate gradients, trust regions) as it is the only Riemannian SPD metric
Discipline of Business Analytics, University of Sydney. Email: {andi.han, junbin.gao}@sydney.edu.au. Microsoft India. Email: {bamdevm, pratik.jawanpuria}@microsoft.com.
1

available in several manifold optimization toolboxes, such as Manopt [BMAS14], Manopt.jl [Ber19], Pymanopt [TKW16], ROPTLIB [HAGH16], and McTorch [MJK+18]. Moreover, many interesting problems in machine learning are found to be geodesic convex (generalization of Euclidean convexity) under the AI metric, which allows fast convergence of optimization algorithms [ZRS16, HS20].
Recent works have studied the Bures-Wasserstein (BW) distance on SPD matrices [MMP18, BJL19, vO20]. It is a well-known result that the Wasserstein distance between two multivariate Gaussian densities is a function of the BW distance between their covariance matrices. Indeed, the BW metric is a Riemannian metric. Under this metric, the necessary tools for Riemannian optimization, including the Riemannian gradient and Hessian expressions, can be efficiently computed [MMP18]. Hence, it is a promising candidate for Riemannian optimization on Sn++. In this work, we theoretically and empirically analyze the quality of optimization with the BW geometry and show that it is a viable alternative to the default choice of AI geometry. Our analysis discusses the classes of cost functions (e.g., polynomial) for which the BW metric has better convergence rates than the AI metric. We also discuss cases (e.g., log-det) where the reverse is true. In particular, our contributions are as follows.
· We observe that the BW metric has a linear dependence on SPD matrices while the AI metric has a quadratic dependence. We show this impacts the condition number of the Riemannian Hessian and makes the BW metric more suited to learning ill-conditioned SPD matrices than the AI metric.
· In contrast to the non-positively curved AI geometry, the BW geometry is shown to be nonnegatively curved, which leads to a tighter trigonometry distance bound and faster convergence rates for optimization algorithms.
· For both metrics, we analyze the convergence rates of Riemannian steepest descent and trust region methods and highlight the impacts arising from the differences in the curvature and condition number of the Riemannian Hessian.
· We verify that common problems that are geodesic convex under the AI metric are also geodesic convex under the BW metric.
· We support our analysis with extensive experiments on applications such as weighted least squares, trace regression, metric learning, and Gaussian mixture model.
2 Preliminaries
The fundamental ingredients for Riemannian optimization are Riemannian metric, exponential map, Riemannian gradient, and Riemannian Hessian. We refer readers to [AMS09, Bou20] for a general treatment on Riemannian optimization.
A Riemannian metric is a smooth, bilinear, and symmetric positive definite function on the tangent space TxM for any x  M. That is, g : TxM × TxM - R, which is often written as an inner product ·, · x. The induced norm of a tangent vector u  TxM is given by u x = u, u x. A geodesic on a manifold  : [0, 1] - M is defined as a locally shortest curve with zero acceleration. For any x  M, u  TxM, the exponential map, Expx : TxM - M is defined such that there exists a geodesic curve  with (0) = x, (1) = Expx(u) and  (0) = u.
2

First-order geometry and Riemannian steepest descent. Riemannian gradient of a differentiable function f : M - R at x, denoted as gradf (x), is a tangent vector that satisfies for any u  TxM, gradf (x), u x = Duf (x), where Duf (x) is the directional derivative of f (x) along u. The Riemannian steepest descent method [Udr13] generalizes the standard gradient descent in the Euclidean space to Riemannian manifolds by ensuring that the updates are along the geodesic and stay on the manifolds. That is, xt+1 = Expxt(-t gradf (xt)) for some step size t.

Second-order geometry and Riemannian trust region. Second-order methods such as trust region and cubic regularized Newton methods are generalized to Riemannian manifolds [ABG07, ABBC20]. They make use of the Riemannian Hessian, Hessf (x) : TxM - TxM, which is a linear operator that is defined as the covariant derivative of the Riemannian gradient. Both the trust region and cubic regularized Newton methods are Hessian-free in the sense that only evaluation of the Hessian acting on a tangent vector, i.e., Hessf (x)[u] is required. Similar to the Euclidean counterpart, the Riemannian trust region method approximates the Newton step by solving a subproblem, i.e.,

1

min

mxt(u) = f (xt) +

uTxt M: u xt 

gradf (xt), u xt + 2

Hxt [u], u xt ,

where Hxt : TxtM - TxtM is a symmetric and linear operator that approximates the Riemannian Hessian.  is the radius of trust region, which may be increased or decreased depending on how
model value mxt(u) changes. The subproblem is solved iteratively using a truncated conjugate gradient algorithm. The next iterate is given by xt+1 = Expxt(u) with the optimized u.
Next, the eigenvalues and the condition number of the Riemannian Hessian are defined as follows,
which we use for analysis in Section 3.

Definition 1. The minimum and maximum eigenvalues of Hessf (x) are defined as min = min u 2x=1

Hessf (x)[u], u x

and

max

=

max

u

2 x

=1

Hessf (x)[u], u x.

The condition number of Hessf (x) is

defined as (Hessf (x)) := max/min.

Function classes on Riemannian manifolds. For analyzing algorithm convergence, we require the definitions for several important function classes on Riemannian manifolds, including geodesic convexity and smoothness. Similarly, we require the definition for geodesic convex sets that generalize (Euclidean) convex sets to manifolds [SH15, Vis18].

Definition 2 (Geodesic convex set [SH15, Vis18]). A set X  M is geodesic convex if for any x, y  X , the distance minimizing geodesic  joining the two points lies entirely in X .

Indeed, this notion is well-defined for any manifold because a sufficiently small geodesic ball is always geodesic convex.

Definition 3 (Geodesic convexity [SH15, Vis18]). Consider a geodesic convex set X  M. A function f : X - R is called geodesic convex if for any x, y  X , the distance minimizing geodesic  joining x and y satisfies  t  [0, 1], f ((t))  (1 - t)f (x) + tf (y). Function f is strictly geodesic convex if the equality holds only when t = 0, 1.

Definition 4 (Geodesic strong convexity and smoothness [SH15, HGA15]). Under the same settings

in Definition 3. A twice-continuously differentiable function f : X - R is called geodesic µ-strongly

convex if for any distance minimizing geodesic  in X with

 (0)

=

1,

it

satisfies

d2f ((t)) dt2



µ,

for

some

µ

>

0.

Function

f

is

called

geodesic

L-smooth

if

d2f ((t)) dt2



L,

for

some

L

>

0.

3

Table 1: Riemannian optimization ingredients for AI and BW geometries.

R.Metric R.Exp R.Gradient R.Hessian

Affine-Invariant
gai(U, V) = tr(X-1UX-1V) Expai,X(U) = X1/2 exp(X-1U)X1/2 gradaif (X) = Xf (X)X Hessaif (X)[U] = X2f (X)[U]X + {Uf (X)X}S

Bures-Wasserstein

gbw(U, V)

=

1 2

tr(LX[U]V)

Expbw,X(U) = X + U + LX[U]XLX[U]

gradbwf (X) = 4{f (X)X}S

Hessbwf (X)[U] = 4{2f (X)[U]X}S + 2{f (X)U}S + 4{X{LX[U]f (X)}S}S - {LX[U]gradbwf (X)}S

3 Comparing BW with AI for Riemannian optimization
This section starts with an observation of a linear-versus-quadratic dependency between the two metrics. From this observation, we analyze the condition number of the Riemannian Hessian. Then, we further compare the sectional curvature of the two geometries. Together with the differences in the condition number, this allows us to compare the convergence rates of optimization algorithms on the two geometries. We conclude this section by showing geodesic convexity of several generic cost functions under the BW geometry.

AI and BW geometries on SPD matrices. When endowed with a Riemannian metric g, the set of SPD matrices of size n becomes a Riemannian manifold M = (Sn++, g). The tangent space at X is TXM := {U : U  Rn×n and U = U}. Under the AI and BW metrics, the Riemannian exponential map, Riemannian gradient, and Hessian are compared in Table 1, where we denote
{A}S := (A + A )/2 and exp(A) as the matrix exponential of A. LX[U] is the solution to the matrix linear system LX[U]X + XLX[U] = U and is known as the Lyapunov operator. We use f (X) and 2f (X) to represent the first-order and second-order derivatives, i.e., the Euclidean
gradient and Hessian, respectively. The derivations in Table 1 can be found in [Pen20, BJL19]. In
the rest of the paper, we use Mai and Mbw to denote the SPD manifolds under the two metrics. From Table 1, the computational costs for evaluating the AI and BW ingredients are dominated by
the matrix exponential/inversion operations and the Lyapunov operator L computation, respectively. Both at most cost O(n3), which implies a comparable per-iteration cost of optimization algorithms
between the two metric choices. This claim is validated in Section 4.

A key observation. From Table 1, the Affine-Invariant metric on the SPD manifold can be rewritten as for any U, V  TXM,

U, V ai = tr(X-1UX-1V) = vec(U) (X  X)-1vec(V),

(1)

where vec(U) and vec(V) are the vectorizations of U and V, respectively. Note that we omit the subscript X for inner product ·, · to simplify the notation. The specific tangent space where the inner product is computed should be clear from contexts.
The Bures-Wasserstein metric is rewritten as, for any U, V  TXM,

1

1

U, V

bw

=

2 tr(LX[U]V)

=

vec(U) 2

(X  X)-1vec(V),

(2)

4

where X  X = X  I + I  X is the Kronecker sum.
Remark 1. Comparing Eq. (1) and (2) reveals that the BW metric has a linear dependence on X while the AI metric has a quadratic dependence. This suggests that optimization algorithms under the AI metric should be more sensitive to the condition number of X compared to the BW metric.
The above observation serves as a key motivation for the further analysis.

3.1 Condition number of Riemannian Hessian at optimality

Throughout the rest of the paper, we make the following assumptions.

Assumption 1. (a). f is at least twice continuously differentiable with a non-degenerate local minimizer X. (b). The subset X  M (usually as a neighbourhood of a center point) we consider
throughout this paper is totally normal, i.e., the exponential map is a diffeomorphism.

Assumption 1 is easy to satisfy. Particularly, Assumption 1(b) is guaranteed for the SPD manifold

under the AI metric because its geodesic is unique. Under the BW metric, for a center point X, we

can choose the neighbourhood such that X = {ExpX(U) : I + LX[U]  Sn++} as in [MMP18]. In other words, X is assumed to be unique-geodesic under both the metrics.

We now formalize the impact of the linear-versus-quadratic dependency, highlighted in Remark

1. At a local minimizer X where the Riemannian gradient vanishes, we first simplify the expression

for the Riemannian Hessian in Table 1.

On Mai, Hessaif (X)[U], U ai = tr(2f (X)[U]U) = vec(U) H(X)vec(U), with H(X) 

Rn2×n2 is the matrix representation of the Euclidean Hessian 2f (X) and U  TXMai. The

maximum

eigenvalue

of

Hessaif (X)

is

then

given

by

max

=

max

U

2 ai

=1

vec(U)

H(X)vec(U),

where

U

2 ai

= vec(U)

(X  X)-1vec(U).

This is a generalized eigenvalue problem with the

solution to be the maximum eigenvalue of (X  X)H(X). Similarly, min corresponds to the

minimum eigenvalue of (X  X)H(X).

On Mbw,

Hessbwf (X)[U], U bw = vec(U)

H(X)vec(U) and the norm is

U

2 bw

=

vec(U)

(X

X)-1vec(U). Hence, the minimum/maximum eigenvalue of Hessbwf (X) equals the minimum/maximum

eigenvalue of (X  X)H(X).

Let ai := (Hessaif (X)) = ((X  X)H(X)) and bw := (Hessbwf (X)) = ((X  X)H(X)). The following lemma bounds these two condition numbers.

Lemma 1. For a local minimizer X of f (X), the condition number of Hessf (X) satisfies

(X)2/(H(X)) ai  (X)2(H(X)) (X)/(H(X)) bw  (X)(H(X)).

It is clear that bw  ai when (H(X))  (X). This is true for linear, quadratic, higherorder polynomial functions and in general holds for several machine learning optimization problems
on the SPD matrices (discussed in Section 4).

Case 1 (Condition number for linear and quadratic optimization). For a linear function f (X) =
tr(XA), its Euclidean Hessian matrix is H(X) = 0. For a quadratic function f (X) = tr(XAXB) with A, B  Sn++, H(X) = A  B + B  A. Therefore, (H(X)) is a constant and for ill-conditioned X, we have (H(X))  (X), which leads to ai  bw.

5

Case 2 (Condition number for higher-order polynomial optimization). For an integer   3,

consider a function f (X) = tr(X)  l=-11((X)l  (X)-l) and bw

with derived H(X) =  = (X  X)( l=-02(Xl

-2 l=0

(Xl



X-l-2).

 X-l-2)). It is

We get ai = apparent that

ai = O((X)) while bw = O((X)-1). Hence, for ill-conditioned X, ai  bw.

One counter-example where bw  ai is the log-det function.

Case 3 (Condition number for log-det optimization). For the log-det function f (X) = - log det(X), its Euclidean Hessian is 2f (X)[U] = X-1UX-1 and H(X) = X-1  X-1. At a local minimizer X, Hessaif (X)[U] = U with ai = 1. While on Mbw, we have bw = ((X X)((X)-1 (X)-1)) = ((X)-1  (X)-1) = (X). Therefore, ai  bw.

3.2 Sectional curvature and trigonometry distance bound

To study the curvature of Mbw, we first show in Lemma 2, the existence of a matching geodesic between the Wasserstein geometry of zero-centered non-degenerate Gaussian measures and the BW geometry of SPD matrices. Denote the manifold of such Gaussian measures under the L2-Wasserstein distance as (N0(), W2) with   Sn++.
Lemma 2. For any X, Y  Sn++, a geodesic between N0(X) and N0(Y) on (N0(), W2) is given by N0((t)), where (t) is the geodesic between X and Y on Mbw.
The following lemma builds on a result from the Wasserstein geometry [AGS08] and uses Lemma 2 to analyze the sectional curvature of Mbw.
Lemma 3. Mbw is an Alexandrov space with non-negative sectional curvature.
It is well-known that Mai is a non-positively curved space [CBG20, Pen20] while, in Lemma 3, we show that Mbw is non-negatively curved. The difference affects the curvature constant in the trigonometry distance bound of Alexandrov space [ZS16]. This bound is crucial in analyzing convergence for optimization algorithms on Riemannian manifolds [ZS16, ZRS16]. In Section 3.3, only local convergence to a minimizer X is analyzed. Therefore, it suffices to consider a neighbourhood  around X. In such a compact set, the sectional curvature is known to be bounded and we denote the lower bound as K-.
The following lemma compares the trigonometry distance bounds under the AI and BW geometries. This bound was originally introduced for Alexandrov space with lower bounded sectional curvature [ZS16]. The result for non-negatively curved spaces has been applied in many work [ZRS16, SKM19, HG20] though without a formal proof. We show the proof in the supplementary material, where it follows from the Toponogov comparison theorem [Mey89] on the unit hypersphere and Assumption 1.

Lemma 4. Let X, Y, Z  , which forms a geodesic triangle on M. Denote x = d(Y, Z), y = d(X, Z), z = d(X, Y) as the geodesic side lengths and let  be the angle between sides y and z such that cos() = Exp-X1(Y), Exp-X1(Z) /(yz). Then, we have
x2  y2 + z2 - 2yz cos(),

where



is

a

curvature

constant.

Under

the

AI

metric,



=

ai

=

|Ka-i |D tanh( |Ka-i |D)

with

D

as

the

diameter

bound of , i.e. maxX1,X2 d(X, Y)  D. Under the BW metric,  = bw = 1.

It is clear that ai > bw = 1, which leads to a tighter bound under the BW metric.

6

3.3 Convergence analysis

We now analyze the local convergence properties of the Riemannian steepest descent and trust region methods under the two Riemannian geometries. Convergence is established in terms of the Riemannian distance induced from the geodesics. We begin by presenting a lemma that shows in a neighbourhood of X, the second-order derivatives of f  ExpX are both lower and upper bounded.

Lemma 5. In a totally normal neighbourhood  around a non-degenerate local minimizer X, for

any

X



,

it

satisfies

that

min/



d2 dt2

f

(ExpX

(tU))



max,

for

some





1

and

U

= 1.

max > min > 0 are the largest and smallest eigenvalues of Hessf (X).

For simplicity of the analysis, we assume such an  is universal under both the Riemannian
geometries. We, therefore, can work with a neighbourhood  with diameter uniformly bounded by
D, where we can choose D := min{Dai, Dbw} such that  is universal. One can readily check that under Lemma 5 the function f is both µ-geodesic strongly convex and
L-geodesic smooth in  where µ = min/ and L = max. We now present the local convergence analysis of the two algorithms, which are based on results in [ZS16, ABG07].

Theorem 1 (Local convergence of Riemannian steepest descent). Under Assumption 1 and consider a non-degenerate local minimizer X. For a neighbourhood  X with diameter bounded by D on

two Riemannian geometries Mai, Mbw, running Riemannian steepest descent from X0   with a

fixed

step

size

=

1 max

yields

for

t  2,

d2(Xt,

X)



2D2

(1

-

min{

1 

,

1 2 

})t-2

.

Theorem 2 (Local convergence of Riemannian trust region). Under the same settings as in Theorem

1, assume further in , it holds that (1) HXt - Hessf (Xt)  gradf (Xt) and (2) 2(f 

ExpXt)(U) - 2(f  ExpXt)(0) Riemannian trust region from X0

 

U for some , yields, d(Xt, X)

u(n2iver+sal

on )(

Mai, Mbw. Then )2d2(Xt-1, X).

running

Theorems 1 and 2 show that Mbw has a clear advantage compared to Mai for learning illconditioned SPD matrices where bw  ai. For first-order algorithms, Mbw has an additional benefit due to its non-negative sectional curvature. As ai > bw = 1, the convergence rate degrades

on Mai. Although the convergence is presented in Riemannian distance, it can be readily converted

to

function

value

gap

by

noticing

µ 2

d2(Xt

,

X)



f (Xt) - f (X)



L 2

d2(Xt,

X).

Additionally,

we

note that these local convergence results hold regardless of whether the function is geodesic convex

or not, and similar comparisons also exist for other Riemannian optimization methods.

3.4 Geodesic convexity under BW metric for cost functions of interest
Finally we show geodesic convexity of common optimization problems on Mbw. Particularly, we verify that linear, quadratic, log-det optimization, and also certain geometric optimization problems, that are geodesic convex under the AI metric, are also geodesic convex under the BW metric.
Proposition 1. For any A  Sn+, where Sn+ := {Z : Z  Rn×n, Z = Z, and Z 0}, the functions f1(X) = tr(XA), f2(X) = tr(XAX), and f3(X) = - log det(X) are geodesic convex on Mbw.
Based on the result in Proposition 1, we also prove geodesic convexity of a reparameterized version of the Gaussian density estimation and mixture model on Mbw (discussed in Section 4). Similar claims on Mai can be found in [HS20].
We further show that monotonic functions on sorted eigenvalues are geodesic convex on Mbw. This is an analogue of [SH15, Theorem 2.3] on Mai.

7

Proposition 2. Let  : Sn++ - Rn+ be the decreasingly sorted eigenvalue map and h : R+ - R be

an increasing and convex function. Then f (X) =

k j=1

h(j (X))

for

1



k



n

is

geodesic

convex

on Mbw. Examples of such functions include f1(X) = tr(exp(X)) and f2(X) = tr(X),   1.

4 Experiments

In this section, we compare the empirical performance of optimization algorithms under different Riemannian geometries for various problems. In addition to AI and BW, we also include the Log-Euclidean (LE) geometry [AFPA07] in our experiments.
The LE geometry explores the the linear space of symmetric matrices where the matrix exponential acts as a global diffeomorphism from the space to Sn++. The LE metric is defined as

U, V le = tr(DU log(X)DV log(X))

(3)

for any U, V  TXM, where DU log(X) is the directional derivative of matrix logarithm at X along U. Following [TRW05, MBS11, QSBM14], for deriving various Riemannian optimization ingredients under the LE metric (3), we consider the parameterization X = exp(S), where S  Sn, i.e., the space of n × n symmetric matrices. Equivalently, optimization on the SPD manifold with the LE metric is identified with optimization on Sn and the function of interest becomes f (exp(S)) for S  Sn. While the Riemannian gradient can be computed efficiently by exploiting the directional derivative of the matrix exponential [AMH09], deriving the Riemannian Hessian is tricky and we rely on finite-difference Hessian approximations [Bou15].
We present convergence mainly in terms of the distance to the solution X whenever applicable. The distance is measured in the Frobenius norm, i.e., Xt - X F. When X is not known, convergence is shown in the modified Euclidean gradient norm Xtf (Xt) F. This is comparable across different metrics as the optimality condition Xf (X) = 0 arises from problem structure itself [JBAS10]. We initialize the algorithms with the identity matrix for the AI and BW metrics and zero matrix for the LE metric (i.e., the matrix logarithm of the identity).
We mainly present the results on the Riemannian trust region (RTR) method, which is the method of choice for Riemannian optimization. Note for RTR, the results are shown against the cumulative sum of inner iterations (which are required to solve the trust region subproblem at every iteration). We also include the Riemannian steepest descent (RSD) and Riemannian stochastic gradient (RSGD) [Bon13] methods for some examples. The experiments are conducted in Matlab using the Manopt toolbox [BMAS14] on a i5-10500 3.1GHz CPU processor.
In the supplementary material, we include additional experiments comparing convergence in objective function values for the three geometries. We also present results for the Riemannian conjugate gradient method, and results with different initializations (other than the identity and zero matrices) to further support our claims.
The code can be found at https://github.com/andyjm3/AI-vs-BW.

Weighted least squares. We first consider the weighted least squares problem with the symmetric

positive definite constraint.

The

optimization

problem

is

minXSn++

f (X)

=

1 2

A

X-B

2 F

,

which

is encountered in for example, SPD matrix completion [Smi08] where A is a sparse matrix. The

Euclidean gradient and Hessian are f (X) = (A X - B) A and 2f (X)[U] = A U A,

respectively. Hence, at optimal X, the Euclidean Hessian in matrix representation is H(X) =

8

Distance to solution Distance to solution Distance to solution Distance to solution Distance to solution

105 BW

100

AI

LE

100

2000 1500

2000 1500

2000 1500

10-10 0

500

1000

Inner iterations (cumsum)

10-5

10-10 0

5000

10000

Inner iterations (cumsum)

1000

0

2000

4000

Inner iterations (cumsum)

1000 0

100 Iterations

1000

200

0

0.2

0.4

0.6

Time (s)

(a) Dense, LowCN (b) Dense, HighCN (c) Sparse, HighCN (d) Sparse, HighCN (e) Sparse, HighCN

(RTR:iter)

(RTR:iter)

(RTR:iter)

(RSD:iter)

(RSD:time)

Figure 1: Weighted least squares problem.

diag(vec(A A)). We experiment with two choices of A, i.e. A = 1n1n (Dense) and A as a random sparse matrix (Sparse). The former choice for A leads to well-conditioned H(X) while the latter choice leads to an ill-conditioned H(X). Also note that when A = 1n1n , ai = (X)2 and bw = (X).
We generate X as a SPD matrix with size n = 50 and exponentially decaying eigenvalues. We consider two cases with condition numbers (X) = 10 (LowCN) and 103 (HighCN). The matrix B is generated as B = A X. Figure 1 compares both RSD and RTR for different metrics. When A is either dense or sparse, convergence is significantly faster on Mbw than both Mai and Mle. The advantage of using Mbw becomes more prominent in the setting when condition number of X is high. Figure 1(e) shows that Mbw is also superior in terms of runtime.
Lyapunov equations. Continuous Lyapunov matrix equation, AX + XA = C with X  Sn++, are commonly employed in analyzing optimal control systems and differential equations [RJ70, LS15]. When A is stable, i.e., i(A) > 0 and C  Sn++, the solution X 0 and is unique [Lan70]. When C  Sn+ and is low rank, X  Sn+ is also low rank. We optimize the following problem for solving the Lyapunov equation [VV10]: minXSn++ f (X) = tr(XAX) - tr(XC). The Euclidean gradient and Hessian are respectively f (X) = AX + XA - C and 2f (X)[U] = AU + UA with H(X) = A  A. At optimal X, the condition number (H(X)) = (A).
We experiment with two settings for the matrix A, i.e. A as the Laplace operator on the unit square where we generate 7 interior points so that n = 49 (Ex1), and A is a particular Toeplitz matrix with n = 50 (Ex2). The generated A matrices are ill-conditioned. The above settings correspond to Examples 7.1 and 7.3 in [LS15]. Under each setting, X is set to be either full or low rank. The matrix C is generated as C = AX + XA. The full rank X is generated from the full-rank Wishart distribution while the low rank X is a diagonal matrix with r = 10 ones and n - r zeros in the diagonal. We label the four cases as Ex1Full, Ex1Low, Ex2Full, and Ex2Low. The results are shown in Figures 2(a)-(d), where we observe that in all four cases, the BW geometry outperforms both AI and LE geometries.

Trace regression. We consider the regularization-free trace regression model [SLH15] for estimat-

ing covariance and kernel matrices [SS02, CZ15]. The optimization problem is minXSd++ f (X) =

1 2m

m i=1

(yi

-

Thus, we have

tr(Ai X))2, f (X) =

where Ai = aiai , i mi=1(ai Xai - yi)Ai

= 1, ..., m are some and 2f (X)[U] =

rank-one measurement mi=1(ai Uai)Ai.

matrices.

We create X as a rank-r Wishart matrix and {Ai} as rank-one Wishart matrices and generate

yi = tr(AiX) +  i with i  N (0, 1),  = 0.1. We consider two choices, (m, d, r) = (1000, 50, 50)

9

Distance to solution

Distance to solution

Distance to solution

Distance to solution

100

BW

100

AI

LE

100 100

Distance to solution

10-10

0

20,000

40,000

Inner iterations (cumsum)

(a) LYA: Ex1Full

600 400

200

0

200

400

600

Inner iterations (cumsum)

(e) TR: SynFull

Distance to solution

10-5

0

5,000

10,000

Inner iterations (cumsum)

(b) LYA: Ex1Low

102

100

0

2000

4000

Inner iterations (cumsum)

(f) TR: SynLow

Euclidean grad norm

10-10 0 20,000 40,000 60,000 Inner iterations (cumsum)
(c) LYA: Ex2Full
100

10-5 0

200

400

600

Inner iterations (cumsum)

(g) DML: glass

Euclidean grad norm

10-5

0

5,000

10,000

Inner iterations (cumsum)

(d) LYA: Ex2Low

105

100

10-5 0

20

40

Inner iterations (cumsum)

(h) DML: phoneme

Figure 2: Lyapunov equation (a, b, c, d), trace regression (e, f), and metric learning (g, h) problems.

and (1000, 50, 10), which are respectively labelled as SynFull and SynLow. From Figures 2(e)&(f), we also observe that convergence to the optimal solution is faster for the BW geometry.
Metric learning. Distance metric learning (DML) aims to learn a distance function from samples and a popular family of such distances is the Mahalanobis distance, i.e. dM(x, y) =
(x - y) M(x - y) for any x, y  Rd. The distance is parameterized by a symmetric positive semi-definite matrix M. We refer readers to this survey [SGH21] for more discussions on this topic. We particularly consider a logistic discriminant learning formulation [GVS09]. Given a training sample {xi, yi}Ni=1, denote the link tij = 1 if yi = yj and tij = 0 otherwise. The objective is given by minMSd++ f (M) = - i,j (tij log pij + (1 - tij) log(1 - pij)) , with pij = (1 + exp(dM(xi, xj)))-1. We can derive the matrix Hessian as H(M) = i,j pij(1-pij)(xi -xj)(xi -xj) (xi -xj)(xi -xj) . Notice (H(M)) depends on M only through the constants pij. Thus, the condition number will not be much affected by (M).
We consider two real datasets, glass and phoneme, from the Keel database [AFSG+09]. The number of classes is denoted as c. The statistics of these two datasets are (N, d, c) = (241, 9, 7) for glass (5404, 5, 2) for phoneme. In Figures 2(g)&(h), we similarly see the advantage of using the BW metric compared to the other two metrics that behave similarly.
Log-det maximization. As discussed in Section 3.1, log-det optimization is one instance where bw  ai. We first consider minimizing negative log-determinant along with a linear function as studied in [WST10]. That is, for some C  Sn++, the objective is minXSn++ f (X) = tr(XC) - log det(X). The Euclidean gradient and Hessian are given by f (X) = C - X-1 and 2f (X)[U] = X-1UX-1. This problem is geodesic convex under both AI and BW metrics. We generate X the same way as in the example of weighted least square with n = 50 and set C = (X)-1. We consider two cases with condition number cn = 10 (LowCN) and 103 (HighCN). As expeted, we observe faster convergence of AI and LE metrics over the BW metric in Figures 3(a)&(b). This is even more evident when the condition number increases.

10

Distance to solution Distance to solution Euclidean grad norm
Loss Euclidean grad norm

104 BW AI LE
102

100

0

10

20

Inner iterations (cumsum)

105

100

100 10-5

10-5 0

100

200

Inner iterations (cumsum)

0

100

200

300

Inner iterations (cumsum)

(a) LD: LowCN (RTR) (b) LD: HighCN (RTR) (c) GMM: (RTR)

3.54 3.52
3.5 3.48 3.46
0

BW: 0.001 AI: 0.1 LE: 0.005

1000 2000 Iterations

3000

(d) GMM: (RSGD)

100 10-1

BW: 0.001 AI: 0.1 LE: 0.005

10-2 0

1000 2000 Iterations

3000

(e) GMM: (RSGD)

Figure 3: Log-det maximization (a, b) and Gaussian mixture model (c, d, e) problems.

Gaussian mixture model. Another notable example of log-det optimization is the Gaussian den-

sity estimation and mixture model problem. Following [HS20], we consider a reformulated problem

on augmented samples yi = [xi ; 1], i = 1, ..., N where xi  Rd are the original samples. The density

is parameterized by the augmented covariance matrix   Rd+1. Notice that the log-likelihood of

Gaussian is geodesic convex on Mai, but not on Mbw. We, therefore, define S = -1 and the repa-

rameterized log-likelihood is pN (Y; S) =

N i=1

log

(2)1-d/2

exp(1/2)

det(S)1/2

exp(-

1 2

yi

Syi)

,

which is now geodesic convex on Mbw due to Proposition 1. Hence, we can solve the problem of

Gaussian mixture model similar as in [HS20]. More details are given in supplementary material.

Here, we test on a dataset included in the MixEst package [HM15]. The dataset has 1580 samples

in R2 with 3 Gaussian components. In Figure 3(c), we observe a similar pattern with RTR as in

the log-det example. We also include performance of RSGD, which is often preferred for large scale

problems. We set the batch size to be 50 and consider a decaying step size, with the best initialized

step size shown in Figures 3(d)&(e). Following [AV06], the algorithms are initialized with kmeans++.

We find that the AI geometry still maintains its advantage under the stochastic setting.

5 Conclusion and discussion
In this paper, we show that the less explored Bures-Wasserstein geometry for SPD matrices is often a better choice than the Affine-Invariant geometry for optimization, particularly for learning ill-conditioned matrices. Also, a systematic analysis shows that the BW metric preserves geodesic convexity of some popular cost functions and leads to better rates for certain function classes.
Our comparisons are based on optimization over generic cost functions. For specific problems, however, there may exist other alternative metrics that potentially work better. This is an interesting research direction to pursue. We also remark that optimization is not the only area where the AI and BW geometries can be compared. It would be also useful to compare the two metrics for other learning problems on SPD matrices, such as barycenter learning. In addition, kernel methods have been studied on the SPD manifold [JHS+13, HSHL12, ZWZL15] that embed SPD matrices to a high dimensional feature space, known as the Reproducing Kernel Hilbert Space (RKHS). Such representations are used for subsequent learning tasks such as clustering or classification. But only a positive definite kernel provides a valid RKHS. We show (in the supplementary material) that the induced Gaussian kernel based on the BW distance is a positive definite kernel unlike the case for the AI metric. This difference highlights a potential advantage of the BW metric for representation learning on SPD matrices.

11

References

[ABBC20] Naman Agarwal, Nicolas Boumal, Brian Bullins, and Coralia Cartis, Adaptive regularization with cubics on manifolds, Mathematical Programming (2020), 1­50.

[ABG07] P-A Absil, Christopher G Baker, and Kyle A Gallivan, Trust-region methods on Riemannian manifolds, Foundations of Computational Mathematics 7 (2007), no. 3, 303­330.

[AFPA07] Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache, Geometric means in a novel vector space structure on symmetric positive-definite matrices, SIAM journal on matrix analysis and applications 29 (2007), no. 1, 328­347.

[AFSG+09] Jesús Alcalá-Fdez, Luciano Sánchez, Salvador Garcia, Maria Jose del Jesus, Sebastian Ventura, Josep Maria Garrell, José Otero, Cristóbal Romero, Jaume Bacardit, Victor M Rivas, et al., Keel: a software tool to assess evolutionary algorithms for data mining problems, Soft Computing 13 (2009), no. 3, 307­318.

[AGS08] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré, Gradient flows: in metric spaces and in the space of probability measures, Springer Science & Business Media, 2008.

[AMH09]

Awad H Al-Mohy and Nicholas J Higham, Computing the Fréchet derivative of the matrix exponential, with an application to condition number estimation, SIAM Journal on Matrix Analysis and Applications 30 (2009), no. 4, 1639­1657.

[AMS09] P-A Absil, Robert Mahony, and Rodolphe Sepulchre, Optimization algorithms on matrix manifolds, Princeton University Press, 2009.

[AV06]

David Arthur and Sergei Vassilvitskii, k-means++: The advantages of careful seeding, Tech. report, Stanford, 2006.

[BCR84] Christian Berg, Jens Peter Reus Christensen, and Paul Ressel, Harmonic analysis on semigroups: theory of positive definite and related functions, vol. 100, Springer, 1984.

[Ber19]

Ronny Bergmann, Optimisation on Manifolds in Julia, 2019, https://github.com/ kellertuer/Manopt.jl.

[Bha09] Rajendra Bhatia, Positive definite matrices, Princeton university press, 2009.

[BJL19]

Rajendra Bhatia, Tanvi Jain, and Yongdo Lim, On the Bures-Wasserstein distance between positive definite matrices, Expositiones Mathematicae 37 (2019), no. 2, 165­191.

[BMAS14] Nicolas Boumal, Bamdev Mishra, P-A Absil, and Rodolphe Sepulchre, Manopt, a Matlab toolbox for optimization on manifolds, The Journal of Machine Learning Research 15 (2014), no. 1, 1455­1459.

[Bon13]

Silvère Bonnabel, Stochastic gradient descent on Riemannian manifolds, IEEE Transactions on Automatic Control 58 (2013), no. 9, 2217­2229.

[Bou15]

Nicolas Boumal, Riemannian trust regions with finite-difference Hessian approximations are globally convergent, International Conference on Geometric Science of Information, Springer, 2015, pp. 467­475.

12

[Bou20]

, An introduction to optimization on smooth manifolds, Available online, May (2020).

[BSB+19]

Daniel A Brooks, Olivier Schwander, Frédéric Barbaresco, Jean-Yves Schneider, and Matthieu Cord, Exploring complex time-series representations for Riemannian machine learning of radar data, IEEE International Conference on Acoustics, Speech and Signal Processing, IEEE, 2019, pp. 3672­3676.

[CBG20] Calin Cruceru, Gary Bécigneul, and Octavian-Eugen Ganea, Computationally tractable Riemannian manifolds for graph embeddings, arXiv:2002.08665 (2020).

[CM12]

Zeineb Chebbi and Maher Moakher, Means of Hermitian positive-definite matrices based on the log-determinant -divergence function, Linear Algebra and its Applications 436 (2012), no. 7, 1872­1889.

[CZ15]

T Tony Cai and Anru Zhang, ROP: Matrix recovery via rank-one projections, Annals of Statistics 43 (2015), no. 1, 102­138.

[DKZ09a] Ian L. Dryden, Alexey Koloydenko, and Diwei Zhou, Non-Euclidean statistics for covariance matrices, with applications to diffusion tensor imaging, The Annals of Applied Statistics 3 (2009), no. 3, 1102­1123.

[DKZ+09b] Ian L Dryden, Alexey Koloydenko, Diwei Zhou, et al., Non-euclidean statistics for covariance matrices, with applications to diffusion tensor imaging, The Annals of Applied Statistics 3 (2009), no. 3, 1102­1123.

[DLR77]

Arthur P Dempster, Nan M Laird, and Donald B Rubin, Maximum likelihood from incomplete data via the EM algorithm, Journal of the Royal Statistical Society: Series B 39 (1977), no. 1, 1­22.

[DPFS20] Henri De Plaen, Michaël Fanuel, and Johan AK Suykens, Wasserstein exponential kernels, International Joint Conference on Neural Networks, IEEE, 2020, pp. 1­6.

[GVS09]

Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid, Is that you? Metric learning approaches for face identification, International Conference on Computer Vision, IEEE, 2009, pp. 498­505.

[GWJH20] Zhi Gao, Yuwei Wu, Yunde Jia, and Mehrtash Harandi, Learning to Optimize on SPD Manifolds, Conference on Computer Vision and Pattern Recognition, 2020, pp. 7700­ 7709.

[HAGH16] Wen Huang, Pierre-Antoine Absil, Kyle A. Gallivan, and Paul Hand, Roptlib: an object-oriented C++ library for optimization on Riemannian manifolds, Tech. Report FSU16-14.v2, Florida State University, 2016.

[HG17]

Zhiwu Huang and Luc Van Gool, A Riemannian network for SPD matrix learning, AAAI, 2017.

[HG20]

Andi Han and Junbin Gao, Variance reduction for Riemannian non-convex optimization with batch size adaptation, arXiv:2007.01494 (2020).

13

[HGA15]

Wen Huang, Kyle A Gallivan, and P-A Absil, A Broyden class of quasi-Newton methods for Riemannian optimization, SIAM Journal on Optimization 25 (2015), no. 3, 1660­ 1685.

[HM15]

Reshad Hosseini and Mohamadreza Mash'al, MixEst: An estimation toolbox for mixture models, arXiv:1507.06065 (2015).

[HS19]

Yorick Hardy and Willi-Hans Steeb, Matrix calculus, kronecker product and tensor product: A practical approach to linear algebra, multilinear algebra and tensor calculus with software implementations, World Scientific, 2019.

[HS20]

Reshad Hosseini and Suvrit Sra, An alternative to EM for Gaussian mixture models: batch and stochastic Riemannian optimization, Mathematical Programming 181 (2020), no. 1, 187­223.

[HSH14]

Mehrtash T Harandi, Mathieu Salzmann, and Richard Hartley, From manifold to manifold: Geometry-aware dimensionality reduction for SPD matrices, European conference on computer vision, Springer, 2014, pp. 17­32.

[HSHL12] Mehrtash T Harandi, Conrad Sanderson, Richard Hartley, and Brian C Lovell, Sparse coding and dictionary learning for symmetric positive definite matrices: A kernel approach, European Conference on Computer Vision, Springer, 2012, pp. 216­229.

[HWL+17] Zhiwu Huang, Ruiping Wang, Xianqiu Li, Wenxian Liu, Shiguang Shan, Luc Van Gool, and Xilin Chen, Geometry-aware similarity learning on SPD manifolds for visual recognition, IEEE Transactions on Circuits and Systems for Video Technology 28 (2017), no. 10, 2513­2523.

[HWS+15] Zhiwu Huang, Ruiping Wang, Shiguang Shan, Xianqiu Li, and Xilin Chen, Log-Euclidean metric learning on symmetric positive definite manifold with application to image set classification, International Conference on Machine Learning, PMLR, 2015, pp. 720­729.

[JBAS10] Michel Journée, Francis Bach, P-A Absil, and Rodolphe Sepulchre, Low-rank optimization on the cone of positive semidefinite matrices, SIAM Journal on Optimization 20 (2010), no. 5, 2327­2351.

[JHS+13]

Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, and Mehrtash Harandi, Kernel methods on the Riemannian manifold of symmetric positive definite matrices, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2013, pp. 73­80.

[Lan70]

Peter Lancaster, Explicit solutions of linear matrix equations, SIAM review 12 (1970), no. 4, 544­566.

[Lin19]

Zhenhua Lin, Riemannian geometry of symmetric positive definite matrices via cholesky decomposition, SIAM Journal on Matrix Analysis and Applications 40 (2019), no. 4, 1353­1370.

[LS15]

Yiding Lin and Valeria Simoncini, A new subspace iteration method for the algebraic riccati equation, Numerical Linear Algebra with Applications 22 (2015), no. 1, 26­47.

14

[MBS11]

Gilles Meyer, Silvère Bonnabel, and Rodolphe Sepulchre, Regression on fixed-rank positive semidefinite matrices: a Riemannian approach, The Journal of Machine Learning Research 12 (2011), 593­625.

[Mey89] Wolfgang Meyer, Toponogov's theorem and applications, Lecture Notes, Trieste (1989).

[MJK+18] Mayank Meghwanshi, Pratik Jawanpuria, Anoop Kunchukuttan, Hiroyuki Kasai, and Bamdev Mishra, Mctorch, a manifold optimization library for deep learning, Tech. report, arXiv:1810.01811, 2018.

[MK04]

Jorma K Merikoski and Ravinder Kumar, Inequalities for spreads of matrix sums and products, Applied Mathematics E-Notes 4 (2004), 150­159.

[MMG19] Sridhar Mahadevan, Bamdev Mishra, and Shalini Ghosh, A unified framework for domain adaptation using metric learning on manifolds, ECML-PKDD, 2019.

[MMP18] Luigi Malagò, Luigi Montrucchio, and Giovanni Pistone, Wasserstein Riemannian geometry of Gaussian densities, Information Geometry 1 (2018), no. 2, 137­179.

[OPI+19] Jung Hun Oh, Maryam Pouryahya, Aditi Iyer, Aditya P Apte, Allen Tannenbaum, and Joseph O Deasy, Kernel Wasserstein distance, arXiv:1905.09314 (2019).

[Pen20]

Xavier Pennec, Manifold-valued image processing with SPD matrices, Riemannian Geometric Statistics in Medical Image Analysis, Elsevier, 2020, pp. 75­134.

[PFA06] Xavier Pennec, Pierre Fillard, and Nicholas Ayache, A Riemannian framework for tensor computing, International Journal of computer vision 66 (2006), no. 1, 41­66.

[QSBM14] Minh Ha Quang, Marco San Biagio, and Vittorio Murino, Log-hilbert-schmidt metric between positive definite operators on hilbert spaces, Advances in neural information processing systems, 2014, pp. 388­396.

[RJ70]

D Rothschild and A Jameson, Comparison of four numerical algorithms for solving the Liapunov matrix equation, International Journal of Control 11 (1970), no. 2, 181­198.

[SGH21]

Juan Luis Suárez, Salvador García, and Francisco Herrera, A tutorial on distance metric learning: Mathematical foundations, algorithms, experimental analysis, prospects and challenges, Neurocomputing 425 (2021), 300­322.

[SH15]

Suvrit Sra and Reshad Hosseini, Conic geometric optimization on the manifold of positive definite matrices, SIAM Journal on Optimization 25 (2015), no. 1, 713­739.

[SKM19]

Hiroyuki Sato, Hiroyuki Kasai, and Bamdev Mishra, Riemannian stochastic variance reduced gradient algorithm with retraction and vector transport, SIAM Journal on Optimization 29 (2019), no. 2, 1444­1472.

[SLH15]

Martin Slawski, Ping Li, and Matthias Hein, Regularization-free estimation in trace regression with symmetric positive semidefinite matrices, Advances in neural information processing systems, 2015.

15

[Smi08]

Ronald L. Smith, The positive definite completion problem revisited, Linear Algebra and its Applications 429 (2008), no. 7, 1442­1452.

[Sra12]

Suvrit Sra, A new metric on the manifold of kernel matrices with application to matrix geometric means, Advances in Neural Information Processing Systems 25 (2012), 144­ 152.

[SS02]

Bernhard Schölkopf and Alexander J Smola, Learning with kernels: support vector machines, regularization, optimization, and beyond, MIT press, 2002.

[Tak08]

Asuka Takatsu, On Wasserstein geometry of the space of Gaussian measures, arXiv:0801.2250 (2008).

[TKW16]

James Townsend, Niklas Koep, and Sebastian Weichwald, Pymanopt: A Python toolbox for optimization on manifolds using automatic differentiation, The Journal of Machine Learning Research 17 (2016), no. 1, 4755­4759.

[TP19]

Yann Thanwerdas and Xavier Pennec, Is affine-invariance well defined on SPD matrices? a principled continuum of metrics, International Conference on Geometric Science of Information, Springer, 2019, pp. 502­510.

[TPM08]

Oncel Tuzel, Fatih Porikli, and Peter Meer, Pedestrian detection via classification on Riemannian manifolds, IEEE Transactions on Pattern Analysis and Machine Intelligence 30 (2008), no. 10, 1713­1727.

[TRW05]

Koji Tsuda, Gunnar Rätsch, and Manfred K. Warmuth, Matrix exponentiated gradient updates for on-line learning and Bregman projection, Journal of Machine Learning Research 6 (2005), no. 34, 995­1018.

[Udr13]

Constantin Udriste, Convex functions and optimization methods on Riemannian manifolds, vol. 297, Springer Science & Business Media, 2013.

[Vis18]

Nisheeth K Vishnoi, Geodesic convex optimization: Differentiation on manifolds, geodesics, and convexity, arXiv:1806.06373 (2018).

[vO20]

Jesse van Oostrum, Bures-Wasserstein geometry, arXiv:2001.08056 (2020).

[VV10]

Bart Vandereycken and Stefan Vandewalle, A Riemannian optimization approach for computing low-rank solutions of Lyapunov equations, SIAM Journal on Matrix Analysis and Applications 31 (2010), no. 5, 2553­2579.

[WST10]

Chengjing Wang, Defeng Sun, and Kim-Chuan Toh, Solving log-determinant optimization problems by a Newton-CG primal proximal point algorithm, SIAM Journal on Optimization 20 (2010), no. 6, 2994­3013.

[ZRS16] Hongyi Zhang, Sashank J Reddi, and Suvrit Sra, Riemannian SVRG: Fast stochastic optimization on Riemannian manifolds, arXiv:1605.07147 (2016).

[ZS16]

Hongyi Zhang and Suvrit Sra, First-order methods for geodesically convex optimization, Conference on Learning Theory, PMLR, 2016, pp. 1617­1638.

16

[ZWZL15] Jianjia Zhang, Lei Wang, Luping Zhou, and Wanqing Li, Learning discriminative stein kernel for SPD matrices and its applications, IEEE Transactions on Neural Networks and Learning Systems 27 (2015), no. 5, 1020­1033.
17

Supplementary

A Bures-Wasserstein geometry of SPD matrices

Here, we include a complete summary of the Bures-Wasserstein geometry. We refer readers to
[BJL19, vO20, MMP18] for a more detailed discussion. The Bures-Wasserstein distance on Sn++ is given by:

dbw(X, Y) =

tr(X) + tr(Y) - 2tr(X1/2YX1/2)1/2

1/2
,

(4)

which corresponds to the L2-Wasserstein distance between zero-centered non-degenerate Gaussian measures. The distance is realized by solving the Procrustes problem, i.e. dbw = minPO(n) X1/2 - Y1/2P F, where O(n) denotes the orthogonal group. The minimum is attained when P is the unitary polar factor of Y1/2X1/2. The distance defined in (4) is indeed a Riemannian distance on Sn++ induced from a Riemannian submersion. That is, the space of SPD matrices can be identified as a quotient space on the general linear group GL(n) with the action of orthogonal group O(n).
The quotient map  : GL(n) - GL(n)/O(n) thus defines a Riemannian submersion. By endowing
a Euclidean metric on GL(n), we can induce the BW metric on SPD manifold, shown in Table 1.
Similarly the induced geodesic is given by the following proposition [BJL19, vO20].

Proposition 3 (Geodesics of Mbw [BJL19, vO20]). For any X, Y  Mbw, a geodesic  connecting X, Y is given by

(t) = (1 - t)X1/2 + tY1/2P (1 - t)X1/2 + tY1/2P ,

where P  O(n) is the unitary polar factor of Y1/2X1/2.
Followed by this proposition, one can derive the Riemannian exponential map as in Table 1. The inverse exponential map, also known as the logarithm map only exists in a open set around a center point X. This is because the BW geometry is not unique-geodesic due to the non-negative curvature. Such open neighbourhood around X is given by  = {ExpX(U) : I + LX[U]  Sn++}. In this set, the exponential map is a local diffeomorphism from the manifold to the tangent space and the logarithm map is provided by LogX(Y) = (XY)1/2 + (YX)1/2 - 2X, for any X, Y  . It is noted that Mbw is geodesic incomplete while Mai and Mle are geodesic complete. One can follow [Tak08] to complete the space by extending the metric to positive semi-definite matrices.

Relationship between the BW metric and the Procrustes metric. Here we highlight that the BW metric is a special form of the more general Procrustes metric, which is studied in [DKZ+09b].
Definition 5 (Procrustes metric). For any X, Y  Sn++, the Procrustes distance is defined as dpc(X, Y) = minPO(n) LX - LYP F, where X = LXLX, Y = LYLY for some decomposition factors LX, LY.
Thus it is easy to see that under the BW metric, LX = X1/2, LY = Y1/2. Another choice of LX, LY can be the Cholesky factor, which is a lower triangular matrix with positive diagonals. The optimal P = UV is obtained from the singular value decomposition of LYLX = UV . Under Procrustes metric, one can similarly derive a geodesic as c(t) = ((1 - t)LX + tLYP) ((1 - t)LX + tLYP) , which corresponds to (t) in Proposition 3. This space is also incomplete with non-negative curvature.

18

B Log-Euclidean geometry and its Riemannian gradient computation

This section presents a summary on the Log-Euclidean (LE) geometry [AFPA07, QSBM14] and derives its Riemannian gradient for Riemannian optimization, which should be of independent interest.
The Log-Euclidean metric is a bi-invariant metric on the Lie group structure of SPD matrices with the group operation X Y := exp(log(X) + log(Y)) for any X, Y  Sn++. This metric is induced from the Euclidean metric on the space of symmetric matrices, Sn through the matrix exponential. Hence the LE metric is given by U, V le = tr(DU log(X)DV log(X)), for U, V  Sn and the LE distance is dle(X, Y) = log(X) - log(Y) F. One can also derive the exponential map associated with the metric as ExpX(U) = exp(log(X) + DU log(X)).
Because of the derivative of matrix logarithm in the LE metric, it appears challenging to derive a simple form of Riemannian gradient based on the definition given in the main text. Hence, we follow the work [TRW05, MBS11, QSBM14] to consider the parameterization of SPD matrices by the symmetric matrices through the matrix exponential. Therefore, the optimization of f (X), X  Sn++ becomes optimization of g(S) := f (exp(S)), S  Sn, which is a linear space with the Euclidean metric. Then, the Riemannian gradient of g(S) is derived as

gradg(S) = {Df(exp(S)) exp(S)}S.
To compute the Riemannian gradient, we need to evaluate the directional derivative of matrix exponential along f (exp(S)). This can be efficiently computed via the function over a block triangular matrix [AMH09]. That is, for any V  Sn, the directional derivative of exp(S) along V is given by the upper block triangular of the following matrix:

exp

SV 0S

=

exp(S) 0

DV exp(S) exp(S)

.

This provides an efficient way to compute the Riemannian gradient of g(S) over Sn. However, computing the Riemannian Hessian of g(S), requires further evaluating the directional derivative of gradg(S), which to the best of our knowledge, is difficult. Thus in experiments, we approach the Hessian with finite difference of the gradient. This is sufficient to ensure global convergence of the Riemannian trust region method [Bou15].

Remark 2 (Practical considerations). For Riemannian optimization algorithms, every iteration requires to evaluate the matrix exponential for a matrix of size 2n × 2n, which can be costly. Also, the matrix exponential may result in unstable gradients and updates, particularly when g(S) involves matrix inversions. This is the case for the log-det optimization problem where f (exp(S)) = - log det(exp(S)). Hence, f (exp(S)) = (exp(S))-1. Nevertheless, for log-det optimization, we can simplify the function to f (exp(S)) = -tr(S), with f (exp(S)) = -I.

C Positive definite kernel on BW geometry
In this section, we show the existence of a positive definite Gaussian kernel on Mbw. [OPI+19, DPFS20] have studied the Wasserstein distance kernel. First, we present the definition of a positive (resp. negative) definite function as in [BCR84].
19

Definition 6. Consider X be a nonempty set. A function f : X × X - R is called positive definite

if and only if k is symmetric and for all integers m  2, {x1, ..., xm}  X and {c1, ..., cm}  R, it

satisfies

m i,j=1

cicj

f

(xi,

xj

)

same conditions, it satisfies

 0. A function f

m i,j=1

ci

cj

f

(xi

,

xj

)

is 

called 0 with

negative definite

m i=1

ci

=

0.

if

and

only

if

under

the

The following Theorem shows the Gaussian kernel induced from BW distance is positive definite on SPD manifold.

Theorem 3. The induced Gaussian kernel k(·, ·) := exp(-d2bw(·, ·)/(22)) is positive definite.

Proof of negative

Theorem definite.

3. From Theorem 4.3 in [JHS+13], it suffices to prove the Indeed for any m  2, {X1, ..., Xm}  Mbw, {c1, ..., cm} 

BW distance

R with

m i=1

d2bw ci =

is 0,

we have

m
cicj d2bw(Xi, Xj )
i,j=1

mm

mm

m

= cj citr(Xi) + ci cjtr(Xj) - 2

cicj tr(Xi1/2Xj X1i /2)1/2

j=1 i=1

i=1 j=1

i,j=1

m

=-2

cicj tr(X1i /2Xj X1i /2)1/2  0.

i,j=1

This shows d2bw is negative definite and thus the exponentiated Gaussian kernel is positive definite.

D Proof for Section 3.1: Condition number of Riemannian Hessian

Proof of Lemma 1. Under AI metric, first note that for any X  Sn++,

(X  X) = = =

X  X 2 (X  X)-1 2

X  X 2 X-1  X-1 2

X

2 2

X-1

2 2

=

(X)2,

where we apply the norm properties for Kronecker product. Next denote the i-th largest eigenvalue as i(A) for 1  i  d where A  Rd×d. Then,

ai

=

((X



X)H(X))

=

1((X  X)H(X)) n2((X  X)H(X))



1((X  X))n2(H(X)) n2((X  X))1(H(X))

= ((X  X))/(H(X))

= (X)2/(H(X)),

where the first inequality uses the eigenvalue bound for matrix product, i.e. i(A)d(B)  i(AB)  i(A)1(B) for A, B  Rd×d [MK04]. The upper bound on ai is easily obtained by noting k(AB)  (A)(B).
Similarly for the BW metric, we first note that because X  Sn++, X  X  Sn+2+ by spectrum
property of Kronecker sum [HS19]. Then we have

(X  X) = 1(X  X) = 21(X) = (X), n2(X  X) 2n(X)

20

where the second equality is again due to the spectrum property. Then the lower and upper bounds of the condition number on ((X  X)H(X)) are derived similarly.

E Proofs for Section 3.2: Sectional Curvature and trigonometry distance bound derivation

Proof of Lemma 2. The proof follows by noticing that the push-forward interpolation between two
non-degenerate Gaussians is a Gaussian with covariance given by interpolation of the covariances. From Lemma 2.3 in [Tak08], for any X, Y  Sn++, the geodesic between N0(X) and N0(Y) under
L2-Wasserstein metric is N0((t)), where

(t) = ((1 - t)I + tT) X ((1 - t)I + tT) ,

(5)

with T = Y1/2(Y1/2XY1/2)-1/2Y1/2 as the pushforward map from N0(X) to N0(Y). It is clear that the interpolation of two non-degenerate Gaussian measures is also a non-degenerate Gaussian. To show (t) = (t), We only need to show Y1/2PX-1/2 = T, where P is the unitary polar factor of Y1/2X1/2. By noting that P = Y1/2(XY)-1/2X1/2 from eq. (35) in [BJL19], we have Y1/2PX-1/2 = Y(XY)-1/2. On the other hand, T = YY-1/2(Y1/2XY1/2)-1/2Y1/2 = Y(XY)-1/2, where the second equality can be seen as follows. Denote C := (Y1/2XY1/2)-1/2, then
I = CY1/2XY1/2C = Y-1/2CY1/2XY1/2CY1/2
= Y-1/2CY1/2XYY-1/2CY1/2.

From this result, we have Y-1/2CY1/2 = (XY)-1/2. This completes the proof.

Proof of Lemma 3. Let µ0, µ1,   N0 with covariance matrix X, Y, Z  Sn++ and denote µt := ((1 - t)id + tTµ0- µ1)#µ0, which is the interpolated Gaussian measure between µ0, µ1. From the matching geodesics in Lemma 2, we have µt  N0((t)). Then based on standard Theorem on Wasserstein distance (e.g. Theorem 7.3.2 in [AGS08]), we have W22(µt, )  (1 - t)W22(µ0, ) + tW22(µ1, ) - t(1 - t)W22(µ0, µ1). Given the accordance between L2-Wasserstein distance between zero-mean Gaussians and geodesic distance between their corresponding covariance matrices on
Mbw, we have

d2bw((t), Z)  (1 - t)d2bw(X, Z) + td2bw(Y, Z) - t(1 - t)d2bw(X, Y)
holds for any X, Y, Z  Sn++. This suggests Mbw is a non-negatively curved Alexandrov space with non-negative sectional curvature.

Proof of Lemma 4. Given Mai is a non-positively curved space, the proof under AI metric can be

found in [ZS16], which reduces to proving the claim for hyperbolic space with constant curvature

-1. Similarly, for non-negatively curved space, it becomes studying the hypersphere with constant

curvature 1. Let x~y~z~ be the comparison triangle on TXMbw such that y~ = y, z~ = z and 

is the angle between side y~ and z~. Because  is a uniquely geodesic subset as per Assumption

1, we have d(X, Y) = Exp-X1(Y) bw for any X, Y  . Thus, we can immediately see x~2 =

Exp-X1(Y) - Exp-X1(Z)

2 bw

= y2 + z2 - 2yz cos().

Then

from

the

Toponogov

Theorem

(Theorem

21

2.2 in [Mey89]) and the assumption of unique geodesic, we have x  x~, which shows for unit

hypersphere:

x2  y2 + z2 - 2yz cos().

(6)

Next, we see that for the space of constant curvature 0, it satisfies x2 = y2 + z2 - 2yz cos(). Thus

we can focus on where the curvature is positive, i.e. K > 0. For such space, we have the following

generalized law of cosines [Mey89]:











cos( Kx) = cos( Ky) cos( Kz) + sin( Ky) sin( Kz) cos(),

 

which can be viewed as a geodesic triangle on unit hypersphere with side lengths Kx, Ky, Kz.

Thus, substituting these side lengths in (6) proves the desired result for positively curved space.

F Proofs for Section 3.3: Convergence analysis

Proof

of

Lemma

5.

The

proof

follows

mainly

from

the

continuity

of

d2 dt2

(f

 Exp)

in

both

t, X, U.

First note at optimality, we have for U  TX with U = 1, min  Hessf (X)[U], U  max.

Because exponential map is a second-order retraction, by standard theory (e.g. Proposition 5.5.5

in [AMS09]), Hessf (X) = 2(f  ExpX)(0) and

Hessf (X)[U], U

=

d2 dt2

f

(ExpX

(tU))|t=0

for

any

X  M, U  TXM. Thus at optimality, we have

min



d2 dt2 f (ExpX(tU))|X=X,t=0



max.

By

the

continuity

of

d2 dt2

(f

 Exp),

we

can

always

find

a

constant





1

such

that

min/



d2 dt2

f

(ExpX

(tU))



max

holds

for

all

X



,

U

= 1 and t such that ExpX(tU)  . In general,

 scales with the size of .

Proof of Theorem 1. From Theorem 14 in [ZS16], we have for either metric,

f (Xt)

-

f (X)



1 (1
2

-

1 min{ ,


µ })t-2D2L, L

where L, µ are the constants for geodesic smoothness and strongly convex. As discussed in the

main text, L = max and µ = min/, where min and max are eigenvalues under either metric.

Based

on

standard

result

on

µ-geodesic

strongly

convexity,

we

have

f (Xt)

-

f (X)



µ 2

d2(Xt,

X

).

Combining this result and Lemma 4 and 1 gives the result.

Proof of Theorem c d2(Xt-1, X) for

2. From Theorem 4.13

some

c



(
min

+

min

+

in)([A)B2G07(]2,we+ha)v(e f)o2r.

either

metric,

d(Xt, X)



G Proofs for Section 3.4: Geodesic convexity

G.1 Preliminaries

In addition to the definition of geodesic convexity in the main text, we also use a second-order characterization of geodesic convexity, which is equivalent to the definition [Vis18].

Lemma 6 (Second-order characterization of geodesic convexity). Under the same settings as

in Definition 3, a twice-continuously differentiable function f is called geodesic convex if  t 

[0, 1],

d2f ((t)) dt2



0.

22

G.2 Proof

Proof of Proposition 1. The main idea is to apply the second-order characterization of geodesic
convexity. Let f1(X) = tr(XA) and f2(X) = tr(XAX). For claim of linear function, given any A  Sn+, it can be factorized as A = L L for some L  Rm×n. Thus f1(X) = tr(LXL ). Denote (t) := (1 - t)X1/2 + tY1/2P and thus the geodesic (t) = (t)(t) . By standard calculus, we can
write the first-order and second-order derivatives as

df1((t)) = 2tr L(Y1/2P - X1/2)(t) L , dt

d2f1((t)) dt2

=

2tr

L(Y1/2P - X1/2)(Y1/2P - X1/2)

L

 0.

For claim on quadratic function f2(X), let X~ := X1/2, Y~ := Y1/2P and the first-order derivative can be similarly derived as

df2((t)) =2tr(Y~ (t) A(t)(t) ) - 2tr(X~ (t) A(t)(t) ) - 2tr(X~ (t) (t)(t) A) dt + 2tr(Y~ (t) (t)(t) A).

The second-order derivative is derived and simplified as

d2f2((t)) dt2

=2

Y~ (t)

L

- X~ (t)

L

2 F

+2

LY~ (t)

- LX~ (t)

2 F

(7)

+ 4tr (X~ - Y~ )(X~ - Y~ ) {A(t)(t) }S

(8)

+ 4tr L Y~ (t) - X~ (t)

2
L

 0,

(9)

where · F is the Frobenius norm. Terms (7) and (9) are clearly non-negative. Term (8) is also non-negative by noting {A(t)(t) }S  Sn+.
To prove the claim on geodesic convexity of f3(X) = - log det(X), we use the definition of geodesic convexity and applies the fact that det(AA ) = (det(A))2 and det(A + B)  det(A) + det(B) for
A, B 0. That is, for any X, Y  Sn++ and t  [0, 1], the geodesic (t) with respect to metric gbw joining
X, Y is given in Proposition 3. Thus,

log det((t)) = 2 log det((t))

(10)

= 2 log det((1 - t)I + tY1/2PX-1/2)X1/2)

= 2 log det((1 - t)I + tY1/2PX-1/2) + 2 log det(X1/2)

 2 log((1 - t) det(I) + t det(Y1/2PX-1/2)) + 2 log det(X1/2)

(11)

 2(1 - t) log det(I) + 2t log det(Y1/2PX-1/2) + 2 log det(X1/2)

(12)

= 2t log det(Y1/2P) - 2t log det(X1/2) + 2 log det(X1/2)

= t log det(Y) + (1 - t) log det(X)

(13)

where (10) uses the fact that det(AA ) = (det(A))2 and inequality (11) uses the fact that det(A + B)  det(A) + det(B) for A, B  Sn++ and from Lemma 1 in [vO20], we have Y1/2PX-1/2  Sn++

23

with P as the orthogonal polar factor of X1/2Y1/2. Inequality (12) follows from the concavity of logarithm. Equality (13) uses the fact that det(P)2 = 1 for P  O(n). This shows log det is geodesically concave. And because logarithm is strictly concave, inequality (12) reduces to equality only when t = 0, 1. Thus strict geodesic concavity is proved. Now the proof is complete.

Proposition 4. The log-likelihood of reparameterized Gaussian f (S) = pN (Y; S) is geodesic concave on Mbw.

Proof of Proposition 4. To prove f (S) is geodesic convex, it suffices to show that f (S) = log(det(S)1/2

exp(-

1 2

yi

Syi))

is

geodesic

concave.

That

is,

for

a

geodesic

(t)

connecting

X, Y,

we

have

f ((t)) = log

det((t))1/2

1 exp(- 2 yi

(t)yi)

1

1

= 2 log det((t)) - 2 yi (t)yi

1-t

t

1-t

t



2

log det(X) + log det(Y) - 2

2 yi Xyi - 2 yi Yyi

(14)

= (1 - t)f (X) + tf (Y).

where inequality (14) follows from Proposition 1. We further notice that as log det is strictly geodesically concave, so is f (S).

Remark 3 (Gaussian mixture model). Under the BW metric, consider the reformulated GMM model with K components:



n

K

max

L = log 

{Sj Sd+++1}K j=1,{j }K j=-11

i=1

j=1



exp(j )

K j=1

exp(j

)

pN

(yi;

S)

,

(15)

where

K

=

0,

pN (yi; S)

:=

(2)1-d/2

det(S)1/2

exp(

1 2

-

1 2

yi

Syi).

It

is

easy

to

see

that

problem

(15)

is geodesically convex for each component. Also, the optimal solution for problem (15) is unchanged

given the inverse transformation on SPD matrices is one-to-one. That is, if Sj maximizes problem

(15), (Sj )-1 maximizes the problem in [HS20]. Based on Theorem 1 in [HS20], our local maximizer

can be written as parameters of the original GMM problem, i.e. {µj, j}:

(Sj )-1 =

j + µj µj T µj

µj , 1

for j = 1, 2, ..., K.

Proof of Proposition 2. The proof is based on [BJL19, Theorem 6], where we can show the geometric mean under BW geometry also satisfies the convexity with respect to the Loewner ordering. That is, (t) (1 - t)X + tY. Then the proof then follows as in Theorem 2.3 in [SH15].

24

H Additional experimental results
Here, we include additional experiment results to further consolidate the findings in the main text. For each problem instance, we compare the convergence of Riemannian steepest descent (RSD), Riemannian conjugate gradient (RCG), and Riemannian trust region (RTR) methods wherever applicable. In addition to the distance to solution in the main text, we also include convergence in loss values and compare the results against runtime. Robustness to randomness in initialization is examined as well.
H.1 Weighted least squares
Figures 4, 5, and 6 present results on weighted least squares problem for RSD, RCG, and RTR methods respectively. On all the problem instances, we observe the consistent advantage of using the BW geometry over other alternatives, particularly for learning ill-conditioned matrices. For the case where A is sparse, algorithms on AI and LE geometry may converge to some other local minimizers that are far from the optimal solution. Figure 7 shows five independent runs with different initializations. We see that the AI and LE geometries can be sensitive to the initialization as they may converge to different solutions depending on the initializations.
H.2 Lyapunov matrix equation
From Figures 8, 9, and 10, we observe that the BW geometry still maintains its superiority for learning full rank and approximating low rank matrices. Figure 11 presents the sensitivity to randomness where we find that BW geometry is more stable.
H.3 Trace regression
From Figures 12, 13, 14, and 15, similar observations can be made regarding the superiority and robustness of the BW geometry.
H.4 Metric learning
For the example of distance metric learning, apart from the two datasets considered in the main text, we also include experiments on four other datasets, Iris, Balance, Newthyroid and Popfailure from the Keel database [AFSG+09].
The results are presented in Figures 16, 17, and 18 for the RCG and RTR methods where BW geometry also shows its advantage on real datasets.
H.5 Log-det maximization and Gaussian mixture model
Finally, the results for log-det maximization and Gaussian mixture model (GMM) problems are shown in Figures 19, 20, 21, and 22. For these two problems, we see a comparative advantage of using the AI geometry over the BW geometry, consistent with the discussion in Section 3.1.
Although LE geometry performs similarly as AI for the example of log-det maximization, its per-iteration cost is much higher. For the example of GMM, we include the convergence plot of the Expectation-Maximization baseline algorithm [DLR77].
25

We see the consistent superiority of AI geometry over the other two alternatives. We also observe a comparable performance between the LE and BW geometries while LE geometry appears to be less stable for first-order methods.

Loss

Distance to solution Distance to solution
Loss

1010

105

105

100

100

100

10-10

10-5

10-5

10-20 0

100

200

Iterations

(a) Dense, LowCN (Loss)

10-10 0

100

200

Iterations

(b) Dense, LowCN (Disttosol)

10-10 0

0.2

0.4

Time (s)

(c) Dense, LowCN (Time)

105 102

100 0

100

200

Iterations

(d) Dense, HighCN (Loss)

100 0

100

200

Iterations

(e) Dense, HighCN (Disttosol)

Distance to solution

Distance to solution

Loss Distance to solution Distance to solution

3000

3000

102

100

2500

2500

100 0

0.2

0.4

0.6

Time (s)

(f) Dense, HighCN (Time)

0

100

200

Iterations

(g) Sparse, LowCN (Loss)

2000

2000 0

100

200

Iterations

(h) Sparse, LowCN (Disttosol)

2000

2000 0

0.2

0.4

Time (s)

(i) Sparse, LowCN (Time)

Loss

105

100

0

100

200

Iterations

(j) Sparse, HighCN (Loss)

1500

1500

Distance to solution Distance to solution

1000

1000

0

100

200

Iterations

(k) Sparse, HighCN (Disttosol)

0

0.2

0.4

0.6

Time (s)

(l) Sparse, HighCN (Time)

Figure 4: Riemannian steepest descent on weighted least square problem (loss, distance to solution, runtime).

26

Loss

Distance to solution

Distance to solution

Distance to solution Distance to solution
Loss

1010

105

105

100

100

100

100

100

10-10

10-5

10-5

10-20 0

20 40 60 80 Iterations

(a) Dense, LowCN (Loss)

100

10-10 0

20 40 60 80 Iterations

(b) Dense, LowCN (Disttosol)

105

10-10 0

0.1

0.2

Time (s)

(c) Dense, LowCN (Time)

3000 2500

0

100

200

Iterations

(d) Dense, HighCN (Loss)

3000 2500

Loss

0

100

200

Iterations

(e) Dense, HighCN (Disttosol)

106

104

Loss Distance to solution Distance to solution

0 0.2 0.4 0.6 Time (s)
(f) Dense, HighCN (Time)

0

100

200

Iterations

(g) Sparse, LowCN (Loss)

2000

2000 0

100

200

Iterations

(h) Sparse, LowCN (Disttosol)

2000

2000 0

0.2

0.4

Time (s)

(i) Sparse, LowCN (Time)

102 0

100

200

Iterations

(j) Sparse, HighCN (Loss)

1500

1500

Distance to solution Distance to solution

1000

1000

0

100

200

Iterations

(k) Sparse, HighCN (Disttosol)

0

0.2

0.4

Time (s)

(l) Sparse, HighCN (Time)

Figure 5: Riemannian conjugate gradient on weighted least square problem (loss, distance to solution, runtime).

27

Loss

Distance to solution

Distance to solution

Distance to solution Distance to solution
Loss

1010

105

BW

100

100

AI LE

100

100

100

10-10

10-5

10-20

0

500

1000

Inner iterations (cumsum)

10-10 0

500

1000

Inner iterations (cumsum)

10-10 0

0.5

1

Time (s)

10-20

1.5

0

5000

10-10

10000

0

5000

10000

Inner iterations (cumsum)

Inner iterations (cumsum)

(a) Dense, LowCN (b) Dense, LowCN (c) Dense, LowCN (d) Dense, HighCN (e) Dense, HighCN

(Loss)

(Disttosol)

(Time)

(Loss)

(Disttosol)

105 100 10-5 10-10
0

Loss Distance to solution Distance to solution

100

5

10

Time (s)

10-20

15

0

2000

4000

Inner iterations (cumsum)

3000 2500

2000

0

2000

4000

Inner iterations (cumsum)

3000
2500
2000 0

2 Time (s)

100

Loss

10-20

4

0

2000

4000

Inner iterations (cumsum)

(f) Dense, HighCN (g) Sparse, LowCN (h) Sparse, LowCN (i) Sparse, LowCN (j) Sparse, HighCN

(Time)

(Loss)

(Disttosol)

(Time)

(Loss)

2000 1500

2000 1500

Bures-Wasserstein Affine-Invariant Log-Euclidean

Distance to solution Distance to solution

1000

1000

0

2000

4000

0

2

4

Inner iterations (cumsum)

Time (s)

(k) Sparse, HighCN (l) Sparse, HighCN

(Disttosol)

(Time)

Figure 6: Riemannian trust region on weighted least square problem (loss, distance to solution, runtime).

28

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

105

100

100

100

100

100

10-5

10-10

0

200

400

Inner iterations (cumsum)

10-10 0

200

400

Inner iterations (cumsum)

10-10

0

200

400

Inner iterations (cumsum)

10-10

0

200

400

Inner iterations (cumsum)

10-10

0

100 200 300

Inner iterations (cumsum)

(a) Dense, LowCN (Run1)
105

(b) (Run2)
105

(c) (Run3)
105

(d) (Run4)
105

(e) (Run5)
105

Distance to solution

Distance to solution

Distance to solution

Distance to solution

100

100

10-5

10-5

10-10 0

500

1,000

Inner iterations (cumsum)

10-10 0

500

1,000

Inner iterations (cumsum)

(f) Dense, HighCN (Run1)

(g) (Run2)

3500

3500

3000

3000

100

10-5

0

200 400 600

Inner iterations (cumsum)

(h) (Run3)

3500 3000

2500

2500

2500

Distance to solution

Distance to solution

2000

0

2,000

4,000

Inner iterations (cumsum)

2000 0 1,000 2,000 3,000 Inner iterations (cumsum)

(k) Sparse, LowCN (Run1)

(l) (Run2)

2000

2000

2000 0 1,000 2,000 3,000 Inner iterations (cumsum)
(m) (Run3)
2000

Distance to solution

100

100

10-5

0

500

1,000

Inner iterations (cumsum)

10-5

10-10 0

500

1,000

Inner iterations (cumsum)

(i) (Run4)

(j) (Run5)

3400 3200 3000 2800 2600 2400 2200
0 1,000 2,000 3,000 Inner iterations (cumsum)
(n) (Run4)

Distance to solution

3000 2500

2000

0

1,000

2,000

Inner iterations (cumsum)

(o) (Run5)

2000

2000

1500

1500

1500

1500

1500

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

1000

1000

0 1,000 2,000 3,000 Inner iterations (cumsum)
(p) Sparse, HighCN (Run1)

0 1,000 2,000 3,000 Inner iterations (cumsum)
(q) (Run2)

1000

1000

1000

0 1,000 2,000 3,000 Inner iterations (cumsum)
(r) (Run3)

0

2,000

4,000

Inner iterations (cumsum)

(s) (Run4)

0

1,000

2,000

Inner iterations (cumsum)

(t) (Run5)

Figure 7: Sensitivity to randomness on weighted least square problem with Riemannian trust region.

29

Loss

Distance to solution

104

103

103

102

101

Distance to solution

Distance to solution Distance to solution
Loss

103

102

101 0

50

100

Iterations

(a) Ex1Full (Loss)

103

102

102

101 0

50 Iterations

101

100

0

0.1

0.2

Time (s)

(b) Ex1Full (Disttosol)

(c) Ex1Full (Time)

104

103

101

100

10-1 0

50

100

Iterations

(d) Ex1Low (Loss)

103

100

10-1 0

50

100

Iterations

(e) Ex1Low (Disttosol)

102

Loss Distance to solution Distance to solution

102

101 0

0.1

0.2

Time (s)

(f) Ex1Low (Time)

102

102

102

101

101

0

50

100

Iterations

(g) Ex2Full (Loss)

100

100 0

50

100

Iterations

(h) Ex2Full (Disttosol)

101

100 0

0.1

0.2

0.3

Time (s)

(i) Ex2Full (Time)

100

Distance to solution Distance to solution

10-2

0

50

100

Iterations

(k) Ex2Low (Disttosol)

10-1 0

0.1

0.2

0.3

Time (s)

(l) Ex2Low (Time)

Loss

101

100

10-1 0

50

100

Iterations

(j) Ex2Low (Loss)

Figure 8: Riemannian steepest descent on Lyapunov equation problem (loss, distance to solution, runtime).

30

Loss

Distance to solution

104

103

103

102

Distance to solution Distance to solution
Loss

102

102

100

102

101

101

0

50

100

Iterations

(a) Ex1Full (Loss)

103 102 101

100 0

50 Iterations

100

100

0

0.2

0.4

Time (s)

(b) Ex1Full (Disttosol)

(c) Ex1Full (Time)

104

102

102

10-2 0

50

100

Iterations

(d) Ex1Low (Loss)

102

Loss Distance to solution Distance to solution

100 0

0.2 Time (s)

100

0.4

0

50

100

Iterations

(f) Ex1Low (Time) (g) Ex2Full (Loss)

100 0

50

100

Iterations

(h) Ex2Full (Disttosol)

100 0

0.2

0.4

Time (s)

(i) Ex2Full (Time)

Loss

Distance to solution

100

10-2

0

50

100

Iterations

(e) Ex1Low (Disttosol)

102

100

10-2 0

50

100

Iterations

(j) Ex2Low (Loss)

100

100

Distance to solution Distance to solution

10-2

10-2

0

50

100

Iterations

(k) Ex2Low (Disttosol)

0

0.2

0.4

Time (s)

(l) Ex2Low (Time)

Figure 9: Riemannian conjugate gradient on Lyapunov equation problem (loss, distance to solution, runtime).

31

Loss

Distance to solution

Distance to solution

Distance to solution Distance to solution
Loss

100

BW

100

AI

100

100

LE

100

10-10

10-10

10-10

0

20,000

40,000

Inner iterations (cumsum)

0

20,000

40,000

Inner iterations (cumsum)

0

20

40

60

Time (s)

(a) Ex1Full (Loss)

(b) Ex1Full (Disttosol)

(c) Ex1Full (Time)

10-5

10-5

0

5,000

10,000

Inner iterations (cumsum)

(d) Ex1Low (Loss)

0

5,000

10,000

Inner iterations (cumsum)

(e) Ex1Low (Disttosol)

100

100

100

100

100

Loss

Loss Distance to solution Distance to solution

10-10

0

20

40

60

Time (s)

(f) Ex1Low (Time)

10-10 0 20,000 40,000 60,000 Inner iterations (cumsum)

10-10 0 20,000 40,000 60,000 Inner iterations (cumsum)

10-10 0

20

40

Time (s)

(g) Ex2Full (Loss)

(h) Ex2Full (Disttosol)

(i) Ex2Full (Time)

10-5

0

5,000

10,000

Inner iterations (cumsum)

(j) Ex2Low (Loss)

100

100

Distance to solution Distance to solution

10-5

10-5

0

5,000

10,000

Inner iterations (cumsum)

0

2

4

6

8

Time (s)

(k) Ex2Low (Disttosol)

(l) Ex2Low (Time)

Figure 10: Riemannian trust region on Lyapunov equation problem (loss, distance to solution, runtime).

32

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

105

105

100

100

100

100

100

10-5

10-5

10-10
0 10,000 20,000 30,000 Inner iterations (cumsum)

10-10
0 20,000 40,000 60,000 Inner iterations (cumsum)

10-10 0 20,000 40,000 60,000 Inner iterations (cumsum)

10-10 0 20,000 40,000 60,000 Inner iterations (cumsum)

(a) Ex1Full (Run1)

(b) (Run2)

(c) (Run3)

(d) (Run4)

10-10 0 20,000 40,000 60,000 Inner iterations (cumsum)
(e) (Run5)

100

100

100

100

100

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

10-5

10-5

10-5

10-5

10-5

0

5,000

10,000

Inner iterations (cumsum)

(f) Ex1Low (Run1)

105

100

0

5,000

10,000

Inner iterations (cumsum)

(g) (Run2)

0

5,000

10,000

Inner iterations (cumsum)

(h) (Run3)

0

5,000

10,000

Inner iterations (cumsum)

(i) (Run4)

0

5,000

10,000

Inner iterations (cumsum)

(j) (Run5)

100

100

100

100

Distance to solution

Distance to solution

Distance to solution

Distance to solution

10-5
10-10 0 20,000 40,000 60,000 Inner iterations (cumsum)

10-10
0 20,000 40,000 60,000 Inner iterations (cumsum)

(k) Ex2Full (Run1)

(l) (Run2)

10-10
0 20,00040,00060,00080,000 Inner iterations (cumsum)

10-10 0 20,000 40,000 60,000 Inner iterations (cumsum)

10-10
0 20,000 40,000 60,000 Inner iterations (cumsum)

(m) (Run3)

(n) (Run4)

(o) (Run5)

100

100

100

100

100

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

10-5

10-5

10-5

10-5

10-5

0

5,000

10,000

Inner iterations (cumsum)

(p) Ex2Low (Run1)

0

5,000

10,000

Inner iterations (cumsum)

(q) (Run2)

0

5,000

10,000

Inner iterations (cumsum)

(r) (Run3)

0

5,000

10,000

Inner iterations (cumsum)

(s) (Run4)

0

5,000

10,000

Inner iterations (cumsum)

(t) (Run5)

Figure 11: Sensitivity to randomness on Lyapunov equation problem with Riemannian trust region.

33

Loss

Distance to solution

Distance to solution

106

500

500

400

400

104

0

20

40

Iterations

(a) SynFull (Loss)

300

300

0

20

40

Iterations

(b) SynFull (Disttosol)

0

0.2

0.4

Time (s)

(c) SynFull (Time)

150

150

104

100

100

Distance to solution

Distance to solution

Loss

102 0

20

40

Iterations

(d) SynLow (Loss)

50

0

20

40

Iterations

(e) SynLow (Disttosol)

50

0

0.2

0.4

Time (s)

(f) SynLow (Time)

Figure 12: Riemannian steepest descent on trace regression problem (loss, distance to solution, runtime).

Loss

Distance to solution

Distance to solution

106

600

600

500

500

400

400

104

300

300

102 0

20

40

Iterations

(a) SynFull (Loss)

106

104

200

0

20

40

Iterations

(b) SynFull (Disttosol)

200

0

0.2

0.4

Time (s)

(c) SynFull (Time)

150

150

100

100

50

50

Distance to solution

Distance to solution

102

0

20

40

Iterations

(d) SynLow (Loss)

0

20

40

Iterations

(e) SynLow (Disttosol)

0

0.2

0.4

Time (s)

(f) SynLow (Time)

Loss

Figure 13: Riemannian conjugate gradient on trace regression problem (loss, distance to solution, runtime).

34

Loss

Distance to solution

Distance to solution

600

600

100

400

400

10-20

0

200 400 600

Inner iterations (cumsum)

(a) SynFull (Loss)

105

200

200

0

200

400

600

0

1

2

3

Inner iterations (cumsum)

Time (s)

(b) SynFull (Disttosol)

(c) SynFull (Time)

102

102

Distance to solution

Distance to solution

Loss

100

100

100

0

2000

4000

Inner iterations (cumsum)

(d) SynLow (Loss)

0

2000

4000

Inner iterations (cumsum)

(e) SynLow (Disttosol)

0

10

20

Time (s)

(f) SynLow (Time)

Figure 14: Riemannian trust region on trace regression problem (loss, distance to solution, runtime).

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

800

600

600

600

600

600

500

500

500

400

400

400

400

400

300

300

300

200

200

200

200

200

0

200

400

600

Inner iterations (cumsum)

(a) SynFull (Run1)

0 200 400 600 800 Inner iterations (cumsum)
(b) (Run2)

0 200 400 600 800 Inner iterations (cumsum)
(c) (Run3)

0

200

400

600

Inner iterations (cumsum)

(d) (Run4)

0 200 400 600 800 Inner iterations (cumsum)
(e) (Run5)

102

102

102

102

102

Distance to solution

Distance to solution

Distance to solution

Distance to solution

100

100

100

100

100

0

1000 2000 3000

Inner iterations (cumsum)

(f) SynLow (Run1)

0

2000 4000 6000

Inner iterations (cumsum)

(g) (Run2)

0

2000

4000

Inner iterations (cumsum)

(h) (Run3)

0 2000 4000 6000 8000 Inner iterations (cumsum)
(i) (Run4)

0 2000 4000 6000 8000 Inner iterations (cumsum)
(j) (Run5)

Figure 15: Sensitivity to randomness on trace regression problem with Riemannian trust region.

35

Loss

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Loss

Euclidean grad norm

Euclidean grad norm

120

102

102

105

1500

100

80

100

100

1000

100

60

10-2

10-2

0

10

20

30

0

10

20

30

0

Iterations

Iterations

0.1

0.2

Time (s)

(a) Iris (Loss) (b) Iris (Egradnorm) (c) Iris (Time)

105

100

10-5

0

1

2

Time (s)

(f) Balance (Time)

105

Loss

600 500 400 300
200

0

10

20

30

Iterations

(g) Glass (Loss)

105

Euclidean grad norm

102

100

0

10

20

30

Iterations

(h) Glass (Egradnorm)

300 250 200

Euclidean grad norm

500

0

10

20

30

Iterations

(d) Balance (Loss)

102

100

0

0.1

0.2

0.3

Time (s)

(i) Glass (Time)

100

Loss

10-5

0

10

20

30

Iterations

(e) Balance (Egradnorm)
104 2.5
2 1.5
1

0.5

0

10

20

30

Iterations

(j) Phoneme (Loss)

100

Loss

100

100

150

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

0

10

20

30

Iterations

(k) Phoneme (Egradnorm)

0

50

100

150

Time (s)

(l) Phoneme (Time)

104

0

10

20

30

Iterations

(m) Newthyroid (Loss)

105

100

0

10

20

30

Iterations

(n) Newthyroid (Egradnorm)

105

100

Euclidean grad norm

Euclidean grad norm

Loss

103

0

10

20

30

Iterations

(p) Popfailure (Loss)

10-5

0

10

20

30

Iterations

(q) Popfailure (Egradnorm)

10-5

0

1

2

Time (s)

(r) Popfailure (Time)

0

0.1

0.2

0.3

Time (s)

(o) Newthyroid (Time)

Figure 16: Riemannian conjugate gradient on metric learning problem (loss, modified Euclidean gradient, runtime).

36

Loss

Euclidean grad norm

Euclidean grad norm

Loss

Euclidean grad norm

Euclidean grad norm

120 100

100

100

1500

100

1000

80

60

10-10

10-10

500

0

50

100

Inner iterations (cumsum)

0

50

100

Inner iterations (cumsum)

0

0.2

0.4

Time (s)

0

10

20

30

Inner iterations (cumsum)

(a) Iris (Loss) (b) Iris (Egradnorm) (c) Iris (Time) (d) Balance (Loss)

100

10-10 0

1

2

Time (s)

(f) Balance (Time)

105

100

10-5 0

20

40

Inner iterations (cumsum)

(k) Phoneme (Egradnorm)

Euclidean grad norm

Loss

600 500 400 300
200

0

200

400

600

Inner iterations (cumsum)

(g) Glass (Loss)

105

100

10-5 0

100

200

Time (s)

(l) Phoneme (Time)

Loss

Euclidean grad norm

100

10-5 0

200

400

600

Inner iterations (cumsum)

(h) Glass (Egradnorm)

300 250 200

150

0 20 40 60 80 Inner iterations (cumsum)
(m) Newthyroid (Loss)

Euclidean grad norm

Euclidean grad norm

100

10-5

0

2

4

Time (s)

(i) Glass (Time)

100

10-10
0 20 40 60 80 Inner iterations (cumsum)
(n) Newthyroid (Egradnorm)

Euclidean grad norm

Loss

10-10

0

10

20

30

Inner iterations (cumsum)

(e) Balance (Egradnorm)
104 2.5
2 1.5
1

0.5

0

20

40

Inner iterations (cumsum)

(j) Phoneme (Loss)

100

10-10

0

0.2

0.4

Time (s)

(o) Newthyroid (Time)

104

100

100

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Loss

103

0

10

20

Inner iterations (cumsum)

(p) Popfailure (Loss)

10-10

0

10

20

Inner iterations (cumsum)

(q) Popfailure (Egradnorm)

10-10

0

1

2

Time (s)

(r) Popfailure (Time)

Figure 17: Riemannian trust region on metric learning problem (loss, modified Euclidean gradient, runtime).

37

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

100

100

100

100

100

10-10

0

50

100 150

Inner iterations (cumsum)

(a) Balance (Run1)

100

10-10

0

50

100 150

Inner iterations (cumsum)

(b) (Run2)

100

10-10

0

50

100 150

Inner iterations (cumsum)

(c) (Run2)

100

10-10

0

50

100 150

Inner iterations (cumsum)

(d) (Run2)

105

100

10-10

0

50

100 150

Inner iterations (cumsum)

(e) (Run2)

100

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

10-10

0

20

40

60

Inner iterations (cumsum)

(f) Popfailure (Run1)

10-10

0

20

40

60

Inner iterations (cumsum)

(g) (Run2)

105

10-10

0

20

40

60

Inner iterations (cumsum)

(h) (Run2)

105

10-5

10-10 0

20

40

60

Inner iterations (cumsum)

(i) (Run2)

10-10

0

20

40

60

Inner iterations (cumsum)

(j) (Run2)

105

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

100

100

10-5

0

500

1000

Inner iterations (cumsum)

(k) Glass (Run1)

10-5

10-10 0

200 400 600 800

Inner iterations (cumsum)

(l) (Run2)

100

10-5

10-10 0

500

1000

Inner iterations (cumsum)

(m) (Run2)

100

0

500

1000

Inner iterations (cumsum)

(n) (Run2)

100

10-5

0

500

1000

Inner iterations (cumsum)

(o) (Run2)

Figure 18: Sensitivity to randomness on metric learning problem with Riemannian trust region.

Euclidean grad norm

Loss

0 -100 -200 -300 -400

0

2

4

6

8

Iterations

(a) LowCN (Loss)

0 -100 -200

Distance to solution

4000 3000
2000

1000

0

2

4

6

8

Iterations

(b) LowCN (Disttosol)

2500 2000 1500
1000

Distance to solution

4000 3000 2000
1000
0 0.02 0.04 0.06 0.08 Time (s)
(c) LowCN (Time)
2500 2000 1500 1000

Distance to solution

Distance to solution

Loss

-300

0

2

4

6

8

Iterations

(d) HighCN (Loss)

0

2

4

6

8

Iterations

(e) HighCN (Disttosol)

0 0.02 0.04 0.06 0.08 Time (s)
(f) HighCN (Time)

Figure 19: Riemannian conjugate gradient on log-det maximization problem (loss, distance to solution, runtime).

38

Loss

0 -100 -200 -300 -400

0

10

20

Inner iterations (cumsum)

(a) LowCN (Loss)

0

Distance to solution

104 BW AI LE
102

100

0

10

20

Inner iterations (cumsum)

(b) LowCN (Disttosol)

105

Distance to solution

104

102

100

0

0.05

0.1

0.15

Time (s)

(c) LowCN (Time)

105

Distance to solution

Distance to solution

Loss

-100

-200

-300 0

100

200

Inner iterations (cumsum)

(d) HighCN (Loss)

100

10-5 0

100

200

Inner iterations (cumsum)

(e) HighCN (Disttosol)

100

10-5 0

0.5

1

Time (s)

(f) HighCN (Time)

Figure 20: Riemannian trust region on log-det maximization problem (loss, distance to solution, runtime).

Loss

Euclidean grad norm

Euclidean grad norm Euclidean grad norm
Loss

3.54 3.52
3.5 3.48 3.46
0

20

40

Iterations

100

10-2

10-4

10-6

60

0

20

40

Iterations

100

10-2

10-4

10-6

60

0

1

2

Time (s)

Euclidean grad norm

3.54 3.52
3.5

BW: 0.001 AI: 0.1 LE: 0.005

100 10-1

BW: 0.001 AI: 0.1 LE: 0.005

3.48

3.46

10-2

3

0

1000 2000 3000

0

1000 2000 3000

Iterations

Iterations

(a) RGD (Loss) (b) RGD (Egradnorm) (c) RGD (Time)

(d) RSGD (Loss) (e) RSGD (Egradnorm)

100
10-1
10-2 0

1

2

Time (s)

Loss Euclidean grad norm Euclidean grad norm

3.54 3.52
3.5 3.48 3.46
0

20

40

Iterations

100

10-2

10-4

10-6

60

0

20

40

Iterations

100

10-2

10-4

10-6

60

0

1

2

Time (s)

3.54

3.52

Loss

3.5

3.48

3.46

3

0

BW AI LE EM

5

10

Iterations

(f) RSGD (Time)

(g) RCG (Loss) (h) RCG (Egradnorm) (i) RCG (Time)

(j) RTR (Loss)

100

100

Euclidean grad norm Euclidean grad norm

10-5

10-5

0

100

200

300

Inner iterations (cumsum)

(k) RTR (Egradnorm)

0

0.5

1

Time (s)

(l) RTR (Time)

Figure 21: On Gaussian mixture model (loss, modified Euclidean gradient norm, runtime).

39

105

105

105

105

105

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

100

100

100

100

100

0

10

20

30

Inner iterations (cumsum)

(a) LD: LowCN (Run1)

105

0

10

20

30

Inner iterations (cumsum)

(b) (Run2)

105

0

10

20

30

Inner iterations (cumsum)

(c) (Run3)

105

0

10

20

30

Inner iterations (cumsum)

(d) (Run4)

105

0

10

20

30

Inner iterations (cumsum)

(e) (Run5)

105

Distance to solution

Distance to solution

Distance to solution

Distance to solution

Distance to solution

100

100

100

100

100

10-5

0

100

200

300

Inner iterations (cumsum)

10-5

0

100

200

300

Inner iterations (cumsum)

(f) LD: HighCN (Run1) (g) (Run2)

100

100

10-5 0

100

200

300

Inner iterations (cumsum)

(h) (Run3)

100

10-5

0

100

200

300

Inner iterations (cumsum)

(i) (Run4)

100

10-5 0

100

200

300

Inner iterations (cumsum)

(j) (Run5)

100

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

Euclidean grad norm

10-5

10-5

10-10 0

100 200 300

Inner iterations (cumsum)

(k) GMM: (Run1)

0

100

200

Inner iterations (cumsum)

(l) (Run2)

10-5

10-5

0

100

200

300

Inner iterations (cumsum)

(m) (Run3)

0

50

100 150

Inner iterations (cumsum)

(n) (Run4)

10-5

10-10 0

100

200

Inner iterations (cumsum)

(o) (Run5)

Figure 22: Sensitivity to randomness on log-det maximization and Gaussian mixture model with Riemannian trust region.

40

