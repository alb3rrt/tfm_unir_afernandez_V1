arXiv:2106.01722v1 [cs.CV] 3 Jun 2021

GMAIR : Unsupervised Object Detection Based on Spatial Attention and Gaussian Mixture
Weijin Zhu, Yao Shen, Linfeng Yu, Lizeth Patricia Aguirre Sanchez Department of Computer Science Shanghai Jiao Tong University
{weijinzhu,yshen,ylf2017,lizethaguirre}@sjtu.edu.cn
Abstract
Recent studies on unsupervised object detection based on spatial attention have achieved promising results. Models, such as AIR and SPAIR, output "what" and "where" latent variables that represent the attributes and locations of objects in a scene, respectively. Most of the previous studies concentrate on the "where" localization performance; however, we claim that acquiring "what" object attributes is also essential for representation learning. This paper presents a framework, GMAIR, for unsupervised object detection. It incorporates spatial attention and a Gaussian mixture in a unified deep generative model. GMAIR can locate objects in a scene and simultaneously cluster them without supervision. Furthermore, we analyze the "what" latent variables and clustering process. Finally, we evaluate our model on MultiMNIST and Fruit2D datasets and show that GMAIR achieves competitive results on localization and clustering compared to state-of-the-art methods.
1 Introduction
The perception of human vision is naturally hierarchical. We can recognize objects in a scene at a glance and classify them according their appearances, functions, and other attributes. It is expected that an intelligent agent can also decompose scenes to meaningful object abstraction, which is known as an object detection task in machine learning. In the last decade, there have been significant developments in supervised object detection tasks. However, its unsupervised counterpart continues to be challenging.
Recently, there has been some progress in unsupervised object detection. Attend, infer, repeat (AIR, Eslami et al. (2016)), which is a variational autoencoder (VAE, Kingma and Welling (2013)) based method, achieved encouraging results. Spatially invariant AIR (SPAIR, Crawford and Pineau (2019)) replaced the recurrent network in AIR by a convolutional network that attained better scalability and lower computational cost. SPACE (Lin et al. (2020)), which combines spatial-attention and scene-mixture approaches, performed better in background prediction.
Despite the recent progress in unsupervised object detection, results of previous studies remain unsatisfactory. One of the reasons for this could be that previous studies on unsupervised object detection were mainly concentrated on object localization and lacked analysis and evaluation on the "what" latent variables, which represent the attributes of objects. These variables are essential for many tasks such as clustering, image generation, and style transfer. Another important concern is that they do not directly reason about the category of objects in the scene, which is beneficial to know in many cases, unlike most of the studies on corresponding supervised tasks.
This paper presents a framework for unsupervised object detection that can directly reason about the category and localization of objects in the scenes and provide an intuitive way to analyze the
Preprint. Under review.

Figure 1: Architecture of GMAIR. This is a VAE-based model that consists of a probabilistic encoder, q(z|x), and a probabilistic decoder, p(x|z). In encoder q(z|x), feature maps with dimension H × W × D are extracted from data x going through a backbone network representing feature of H × W divided regions. They are then fetched into three separated modules: pres-head, depthhead, and where-head, which produce the posterior of zpres, zdepth, and zwhere, respectively. A cat-encoder module generates The posterior of zcat with H × W input glimpses transformed by a spatial transformer network (STN) as input, and the posterior of zwhat is generated by a what-encoder module with H × W input glimpses and zcat as input. In decoder p(x|z), each H × W latent zwhat is fetched into a glimpse decoder to generate decoded glimpses rendered by the renderer to recover to the final generated image. Finally, the priors of zpres, zdepth, zwhere, and zcat are fixed, whereas the prior of zwhat is generated by a "what priors" module using zcat as input.
"what" latent variables by simply incorporating a Gaussian mixture prior assumption. In Sec. 2, we introduce the architecture of our framework, GMAIR. We introduce related works in Sec. 3. We analyze the "what" latent variables in Sec. 4.1. We describe our model for image generation in Sec. 4.2. Finally, we present quantitative evaluation results of both clustering and localization in Sec. 4.3.
Our main contributions are:
· We combine spatial attention and a Gaussian mixture in a unified deep generative model, enabling our model to cluster discovered objects.
· We analyze the "what" latent variables, which are essential because they represent the attributes of the objects.
· Our method achieves competitive results on both clustering and localization compared to state-of-the-art methods.
2 Gaussian Mixture Attend, Infer, Repeat
In this section, we introduce our framework, GMAIR, for unsupervised object detection. GMAIR is a spatial-attention model with a Gaussian mixture prior assumption for the "what" latent variables, and this enables the model to cluster discovered objects. An overview of GMAIR is presented in Fig. 1.
2.1 Structured Object-semantic Latent Representation
We follow SPAIR to attain object abstraction latent variables (Crawford and Pineau (2019)); the image is divided into H × W regions. Latent variables z = (z1, z2, ..., zHW ) is a concatenation of HW latent variables where zi is the latent variable for the i-th region representing the semantic
2

Table 1: Priors of latent variables

Latent Variables Priors

zpres zwhat zcat zwhere
zdepth

(p)
N (µ(zcat), (zciat)2) Cat()
N (µwprhioerre, pwrhioerre2) N (µdpreipotrh, pdreipotrh2)

feature of the object centered in the i-th region. Furthermore, for each region we divide zi into five seperate latent variables, zi = (zpires, zwi hat, zciat, zwi here, zdi epth), where zpires  {0, 1}, zwi hat  RA, zciat  {0, 1}C , zwi here  R4, zdi epth  R, A is the dimension of "what" latent variables and C is the number of clusters. The meaning of zpres, zwhat, zwhere, zdepth are the same as in Crawford and Pineau (2019), while zcat are one-hot vectors for categories.
GMAIR imposes a prior on those latent variables as follow:

HW

p(z) =

p(zpi res )

p(zciat)p(zwi hat|zciat)p(zwi here)p(zdi epth)

zpires
.

(1)

i=1

Gaussian Mixture Prior Assumption Latent variables zcat are one-hot vectors that act as classification indicators. They obey the categorical distribution, Cat(), where   [0, 1]C. For simplicity, we assume that k = 1/C for all 1  k  C.
We assume that zwi hat conditional on zciat obeys a Gaussian distribution. In that case, zwi hat obeys a Gaussian mixture model, that is,

C

p(zwi hat) = p(zci,akt = 1)p(zwi hat|zci,akt = 1)

(2)

k=1

C
= p(zci,akt = 1)f (x; µk, k2)

k=1

where f (x; µ, 2) = 1 exp
 2

(x-µ)2 22

is the probability density function of Gaussian distribution,

µk, k (k = 1..C) are the mean and standard derivation of the k-th Gaussian distribution. We

let µk and k be learnable parameters that are jointly trained with other parameters. During the implementation, µk = µ(zciat) and k = (zciat) if zci,akt = 1 where µ and  can be modeled as linear

layers. They are called "what priors" module in Figure 1.

For other latent variables, zpres are modeled using a Bernoulli distribution, (p), where p is the present probability. zwhere and zdepth are modeled using normal distributions, N (µwprhioerre, pwrhioerre2) and N (µdpreipotrh, pdreipotrh2), respectively. All priors of latent variables are listed in Table 1.

2.2 Inference and Generation Model
Inference Model q(z|x) In the inference model, latent variables conditional on data x are modeled by Eqn. 3.

HW
q(z|x) = q(zpires|x)

q(zwi here|x)q(zdi epth|x)q(zciat|x, zwi here)q(zwi hat|x, zwi here, zciat)

zpires
.

(3)

i=1

3

During implementation, feature maps with dimension H × W × D are extracted from a backbone network using data x as input, where D is the number of channels of feature maps. Further, the posteriors of zpres, zwhere, and zdepth are reasoned by pres-head, where-head, and depth-head, respectively. Input images are cropped into H × W glimpses by a spatial transformer network, and each of these is transferred to the cat-encoder module to generate posteriors of zcat. Subsequently, we use the concatenation of the i-th glimpse and zciat (1  i  HW ) as the input of the what-encoder to generate posteriors of zwhat.
Generation Model p(x|z) In the generation model, each zwi hat (1  i  HW ) is changed back into a glimpse by using a glimpse decoder. Then, a renderer combines HW glimpses to generate x^. We use the same render algorithm as in previous studies (Eslami et al. (2016), Crawford and Pineau (2019)).
2.3 The Loss Functions
Evidence Lower Bound In general, we learn parameters of VAE jointly by maximizing the evidence lower bound (ELBO), which can be formulated as:

p(x, z)

ELBO = Eq(z|x) log q(z|x)

(4)

q(z|x) = Eq(z|x) [log (p(x|z))] - Eq(z|x) log p(z)

where, the first term is called the reconstruction term denoted by -Lrecon and the second term, the regularization term. The regularization term can be further decomposed into five terms by substituting Eqn. 1 and Eqn. 3 into Eqn. 4, and each of the five terms corresponding to the Kullback­Leibler divergence (or its expectation) between a type of latent variables and its prior:

q(z|x)

Eq(z|x) log p(z)

= Lpres + Lwhere + Ldepth + Lcat + Lwhat.

(5)

The terms in Eqn. 5 are:

HW

Lpres = KL (q(zpires|x)||p(zpires))

(6)

i=1

HW

Lwhere =

q(zpires = 1|x)KL q(zwi here|x)||p(zwi here)

(7)

i=1

HW

Ldepth =

q(zpires = 1|x)KL q(zdi epth|x)||p(zdi epth)

(8)

i=1

HW

Lcat =

q(zpires = 1|x)Eq(zwi here|x) KL q(zciat|x, zwi here)||p(zciat)

(9)

i=1

HW

Lwhat =

q(zpi res = 1|x)Eq(zwi here,zciat|x) KL q(zwi hat|x, zwi here, zciat)||p(zwi hat|zciat) . (10)

i=1

A complete derivation is given in Appendix A.

Overlap Loss During actual implementation, we find that penalizing on overlaps of objects some-
times helps. Therefore, we introduce an auxiliary loss called overlap loss. First, we calculate HW images with size 3×Himg ×W img, where Himg and W img are respectively the height and width of the
input image, transformed by HW decoded glimpses by a spatial transformer network. The overlap loss is then calculated as the average of the sum subtract by the maximum for each Himg × W img
pixels.

4

This loss, inspired by the boundary loss in SPACE (Lin et al. (2020)), is utilized to penalize if the model tries to split a large object into multiple smaller ones. However, we achieve this by using a different calculation method that incurs a lower computational cost.

Total Loss The total loss is:

L = xLx

(11)

xS

where, S = {recon, overlap, pres, where, depth, cat, what}, and : are the coefficients of the corresponding loss terms.

3 Related Works
Several studies on unsupervised object detection have been conducted, including spatial-attention methods such as AIR (Eslami et al. (2016)), SPAIR (Crawford and Pineau (2019)), and SPACE (Lin et al. (2020)), and scene-mixture methods such as MONet (Burgess et al. (2019)), IODINE (Greff et al. (2019)), and GENESIS (Engelcke et al. (2019)). Most of them including our work are based on a VAE (Kingma and Welling (2013)).
The AIR (Eslami et al. (2016)) framework uses a VAE-based hierarchical probabilistic model marking a milestone in unsupervised scene understanding. In AIR, latent variables are structured into groups of latent variables z1:N , for N discovered objects, each of which consists of "what," "where," and "presence" variables. A recurrent neural network is used in the inference model to produce z1:N , and there is a decoder network for decoding the "what" variables of each object in the generation model. A spatial transformer network (Jaderberg et al. (2015)) is used for rendering.
Because AIR attends one object at a time, it does not scale well to scenes that contain many objects. SPAIR (Crawford and Pineau (2019)) attempted to address this issue by replacing the recurrent network with a convolutional network that follows a spatially invariant assumption. Similar to YOLO (Redmon et al. (2016)), in SPAIR, the locations of objects are specified relative to local grid cells.
Scene-mixture models such as MONet (Burgess et al. (2019)), IODINE (Greff et al. (2019)), and GENESIS (Engelcke et al. (2019)) perform segmentation instead of explicitly finding the zwhere location of objects. SPACE (Lin et al. (2020)) employs a combination of both methods. It consists of a spatial-attention model for the foreground and a scene-mixture model for the background.
In the area of deep unsupervised clustering, recent methods include AAE (Makhzani et al. (2015)), GMVAE (Dilokthanakul et al. (2016)), IIC (Ji et al. (2019)). AAE combines the ideas of generative adversarial networks and variational inference. GMVAE uses a Gaussian mixture model as a prior distribution. In IIC, objects are clustered by maximizing mutual information of pairs of images. All of them show promising results on unsupervised clustering.
GMAIR incorporates a Gaussian mixture model for clustering, similar to the GMVAE framework1. It worth noting that our attempt may simply be a choice amongst many given options. Unless previous research, our main contribution is to show the feasibility of performing clustering and localization simultaneously. Moreover, our method provides a simple and intuitive way to analyze the mechanics of the detection process.

4 Models and Experiments
The experiments were divided into three parts: a) the analysis of "what" representation and clustering along with the iterations, b) image generation, and c) quantitative evaluation of the models.
We evaluate the models on two datasets :
· MultiMNIST: A dataset generated by placing 1­10 small images randomly chosen from MNIST (a standard handwritten digits dataset, (LeCun (1998))) to random positions on 128 × 128 empty images.
· Fruit2D: A dataset collected from a real-world game. In the scenes, there are 9 types of fruits of various sizes. There is a large difference between both the number and the size of
1We also refer to a blog post (http://ruishu.io/2016/12/25/gmvae/) published by Rui Shu.

5

small objects and large objects. The ratio of the size of the largest type of objects to that of the smallest type of objects is ~6, and there are ~31 times objects in the smallest size than in the largest size. These settings make it difficult to perform localization and clustering.
In the experiments, we compared GMAIR to two models, SPAIR and SPACE, both of which achieve state-of-the-art in unsupervised object detection in localization performance. Separated Gaussian mixture models are applied to the "what" latent variables generated by the compared models to obtain the clustering results. We set the number of clusters C = 10 and Monte Carlo samples M = 1 except as otherwise defined for all experiments. We present the details of models in Appendix B.
It is worth mentioning that the model sometimes successfully locates an object and encloses it with a large box. In that case, IoU between the ground truth and the predicted one will be small, and therefore, will not count to be a correct bounding box when calculating AP. We fix this issue by removing the empty area in generated glimpses to obtain the real size of predicted boxes.

4.1 "What" Representation and Cluster Analysis
We conducted the experiments using the MultiMNIST dataset. We ran GMAIR for 440k iterations and observed the change in the values of the average precision (AP) of bounding boxes, accuracy (ACC), and normalized mutual information (NMI) of clustering until 100k iterations. We also visualized the "what" latent variables in the latent space during the process, as shown in Fig. 2. Although all values continued to increase even after 100k iterations, the visualization results were similar to those at the 100k iteration. For integrity, we reserved the results from 100k to 440k iterations in Appendix D. Details of calculating the AP, ACC, and NMI are discussed in Appendix C.
The results showed that at an early stage (~10k iterations) of training, models can already locate objects well with AP > 0.9 (Fig. 2a). At the same time, zwhat, representations of objects were still evolving, and the results of clustering (in Fig. 2b) was not desirable ((ACC, NMI) was (0.24, 0.15)); the digits were a blur in Fig. 2f. After 50k and 100k iterations of training, the clustering effect of zwhat was increasingly apparent, and the digits were clearer (Fig. 2g, 2h). The clustering results ((ACC, NMI) was (0.55, 0.43) at 50k, and (0.65, 0.55) at 100k iterations) were improved (Fig. 2c, 2d).
It should be noted that even if the clustering effect of zwhat is sufficiently enough, the model may fail to locate the centers of clusters (for example, the large cluster in light red in Fig. 2d), leading to poor clustering results. In the worst case, the model may learn to converge all µk, k(1  k  C) to the same values, µ, , and the Gaussian mixture model may degenerate to a single Gaussian distribution, N (µ, 2), resulting in a miserable clustering result. In general, we found that this phenomenon usually occurs at the early stage of training and can be avoided by adjusting the learning rate of relative modules and the coefficients of the loss functions.

4.2 Image Generation

It is expected that µk(1  k  C) represents the average feature of the k-th type of objects, and zwi hat latent variable can be decomposed into:

zwi hat = zaivg + zliocal.

(12)

zaivg = µk(1  k  C) if the i-th object is in the k-th category and zlocal represents the local feature of the object. By altering zavg or zlocal, we should obtain new objects that belong to other categories or the same category with different styles, respectively. In the experiment, we altered zavg and zlocal and observed the generated images for each object, as shown in Fig. 3. In Fig. 3a, objects in each cluster correspond to a type of digit, which is exactly what we expected (except for digit 8 in column 3). In Fig. 3b, categories with a large number of objects are grouped into multiple clusters, while categories with a small number are grouped into one cluster. This is due to the significant difference in number between various types. However, objects in a cluster come from a category in general.

The structure of GMAIR ensures its ability to control object categories, object styles, and the positions of each object of the generated images by altering zavg, zlocal, and zwhere. Examples are shown in Fig.
4.

6

(a) AP(IoU=0.5), ACC and (b) "What" latent space, at (c) "What" latent space, at (d) "What" latent space, at

NMI during training 10k iterations

50k iterations

100k iterations

(e) Original image

(f) Generated image, at 10k (g) Generated image, at (h) Generated image, at

iterations

50k iterations

100k iterations

Figure 2: "What" representation and cluster analysis. (a) Average precision (AP), accuracy (ACC),
normalized mutual information (NMI) during training. (b-d) Visualized "what" latent space by t-SNE
(Van der Maaten and Hinton (2008)) at 10k, 50k, and 100k iterations, respectively. Each small dot represents a sample of zwhat, and different colors represent the ground-truth categories of the
corresponding objects. The large dots are µk(1  k  C) described in Sec. 2.1, and each of these can be seen as the center of a cluster. The closures represent results of clustering, which are closures of the closest n zwhat points to µk that are assigned to the k-th cluster (where 1  k  C and we choose n = 200). The color of µk(1  k  C) and closures are decided by a matching algorithm such that a maximum number of zwhat are correctly classified to the ground-truth label. (e) Sample of
original image. (f-h) Samples of generated image at 10k, 50k, and 100k iterations, respectively.

(a) MultiMNIST

(b) Fruit2D

Figure 3: Generated objects by varying zavg and zlocal. The horizontal axis represents varying zavg, and the vertical axis represents varying zlocal, on both (a) and (b).

7

(a) MultiMNIST
(b) Fruit2D Figure 4: Generated images by varying attributes and locations of objects. Columns 1 to 5 are numbered from left to right. Column 1 shows original images. Column 2 shows the generated images without varying zavg, zlocal, and zwhere. Column 3 presents images generated by setting all zavg to the same random µk(1  k  C). Column 4 depicts images generated by varying zlocal. Column 5 shows images generated by applying a random shuffle to zi.
8

Table 2: Quantitative Results on Localization (AP) and Clustering (Accuracy and NMI)

Model

Dataset

AP (%, IoU=0.5) ACC (%)

NMI (%)

IIC

MNIST

--

AAE (C=16)

MNIST

--

AAE (C=30)

MNIST

--

GMVAE (M=1) MNIST

--

GMVAE (M=10) MNIST

--

98.4 ± 0.652 -- 90.45 ± 2.05 -- 95.90 ± 1.13 -- 77.78 ± 5.75 -- 82.31 ± 3.75 --

GMAIR SPAIR + GMM SPACE + GMM

MultiMNIST 97.3 ± 0.10 MultiMNIST 90.3 MultiMNIST 96.7

80.4 ± 0.48 59.4 ± 1.50 68.8 ± 3.43

75.5 ± 0.66 56.3 ± 1.41 65.8 ± 2.85

GMAIR SPAIR + GMM SPACE + GMM

Fruit2D Fruit2D Fruit2D

84.9 ± 1.56 83.3 93.8

90.9 ± 0.32 88.1 ± 0.70 95.0 ± 1.99

85.7 ± 1.25 78.4 ± 0.51 87.0 ± 2.20

This could provide a new approach for tasks such as style transfer, image generation, and data augmentation. Note that previous methods such as AIR, SPAIR, and its variants can also obtain similar results, but we achieve them in finer granularity.
4.3 Quantitative Evaluations
We quantitatively evaluate the models in terms of the AP of bounding boxes, ACC and NMI of the clusters, and the results are listed in Table 2. In the first part, we summarize some results of the state-of-the-art models for unsupervised clustering on MNIST dataset for comparison. In the second and the third part, we compare GMAIR to the state-of-the-art models for unsupervised object detection on MultiMNIST and Fruit2D dataset, respectively. The clustering results of SPAIR and SPACE are obtained by Gaussian mixture models (GMMs). Results show that GMAIR achieves competitive results on both clustering and localization.
5 Conclusion
We introduce GMAIR, which combines spatial attention and a Gaussian mixture, such that it can locate and cluster unseen objects simultaneously. We analyze the "what" latent variables and clustering process, provide examples of GMAIR application to the task of image generation, and evaluate GMAIR quantitatively compared with SPAIR and SPACE.
Acknowledgments and Disclosure of Funding
This work was partially supported by the Research and Development Projects of Applied Technology of Inner Mongolia Autonomous Region, China under Grant No. 201802005, the Key Program of the National Natural Science Foundation of China under Grant No. 61932014, and Pudong New Area Science & Technology Development Fundation under Grant No. PKX2019-R02. Yao Shen is the corresponding author.

9

References
C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.
R. Charakorn, Y. Thawornwattana, S. Itthipuripat, N. Pawlowski, P. Manoonpong, and N. Dilokthanakul. An explicit local and global representation disentanglement framework with applications in deep clustering and unsupervised object detection. arXiv preprint arXiv:2001.08957, 2020.
E. Crawford and J. Pineau. Spatially invariant unsupervised object detection with convolutional neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3412­3420, 2019.
E. Crawford and J. Pineau. Exploiting spatial invariance for scalable unsupervised object tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3684­3692, 2020.
N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and M. Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.
K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian. Centernet: Keypoint triplets for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6569­6578, 2019.
M. Engelcke, A. R. Kosiorek, O. P. Jones, and I. Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052, 2019.
S. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari, K. Kavukcuoglu, and G. E. Hinton. Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint arXiv:1603.08575, 2016.
M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303­338, 2010.
K. Greff, S. Van Steenkiste, and J. Schmidhuber. Neural expectation maximization. arXiv preprint arXiv:1708.03498, 2017.
K. Greff, R. L. Kaufman, R. Kabra, N. Watters, C. Burgess, D. Zoran, L. Matthey, M. Botvinick, and A. Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning, pages 2424­2433. PMLR, 2019.
C. Gu, H. Xie, X. Lu, and C. Zhang. Cgmvae: Coupling gmm prior and gmm estimator for unsupervised clustering and disentanglement. IEEE Access, 2021.
K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630­645. Springer, 2016.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448­456. PMLR, 2015.
M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial transformer networks. arXiv preprint arXiv:1506.02025, 2015.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
X. Ji, J. F. Henriques, and A. Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9865­9874, 2019.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
A. R. Kosiorek, H. Kim, I. Posner, and Y. W. Teh. Sequential attend, infer, repeat: Generative modelling of moving objects. arXiv preprint arXiv:1806.01794, 2018.
Y. LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
10

Z. Lin, Y.-F. Wu, S. V. Peri, W. Sun, G. Singh, F. Deng, J. Jiang, and S. Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and decomposition. arXiv preprint arXiv:2001.02407, 2020.
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21­37. Springer, 2016.
M. M. Loper and M. J. Black. Opendr: An approximate differentiable renderer. In European Conference on Computer Vision, pages 154­169. Springer, 2014.
A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779­788, 2016.
J. T. Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
K. Stelzner, R. Peharz, and K. Kersting. Faster attend-infer-repeat with tractable probabilistic models. In International Conference on Machine Learning, pages 5966­5975. PMLR, 2019.
S.-H. Sun. Multi-digit mnist for few-shot learning, 2019. URL https://github.com/shaohua0116/ MultiDigitMNIST.
L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.
D. Wang, M. Jamnik, and P. Lio. Unsupervised and interpretable scene discovery with discrete-attend-infer-repeat. arXiv preprint arXiv:1903.06581, 2019.
Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3­19, 2018.
Q. Zhu, J. Su, W. Bi, X. Liu, X. Ma, X. Li, and D. Wu. A batch normalized inference network keeps the kl vanishing away. arXiv preprint arXiv:2004.12585, 2020.
11

A Derivation of The KL Terms

In this section, we derive the KL terms in Eqn. 5. By assumption of q(z|x) and p(z) (Eqn. 3 and Eqn. 1), we have:

q(z|x)

HW

Eq(z|x)

log(

)

p(z)

=

Eq(zi|x) log

i=1

q(zi|x) p(zi)

.

(13)

The term Eq(zi|x) log

q (zi |x) p(zi )

can further be expanded as follow:

Eq(zi|x) log

q(zi|x) p(zi)

=q(zpires = 0|x) log

q(zpires = 0|x) p(zpires = 0)

+ q(zpires = 1|x)

log

q(zpires = 1|x) p(zpires = 1)

+E log q(zwi here,zdiepth,zciat,zwi hat|x) =KL (q(zpires|x)||p(zpires))

q(zwi here, zdi epth, zciat, zwi hat|x) p(zwi here, zdi epth, zciat, zwi hat)

+ q(zpi res = 1|x)Eq(zwi here,zdiepth,zciat,zwi hat|x) log

q(zwi here, zdi epth, zciat, zwi hat|x) p(zwi here, zdi epth, zciat, zwi hat)

(14) .

Continue to expand Eqn. 14:

E log q(zwi here,zdiepth,zciat,zwi hat|x)

q(zwi here, zdi epth, zciat, zwi hat|x) p(zwi here, zdi epth, zciat, zwi hat)

=Eq(zwi here|x) log

q(zwi here|x) p(zwi here)

+Eq(zdiepth|x) log

q(zdi epth|x) p(zdi epth)

+Eq(zciat,zwi here|x) log

q(zciat|x, zwhere) p(zciat)

+E log q(zwi hat,zwi here,zciat|x)

q(zwi hat|x, zwhere, zciat) p(zwi hat|zciat)

.

(15)

By the definition of Kullback­Leibler divergence, the four terms in the RHS of Eqn. 15 are indeed
KL q(zwi here|x)||p(zwi here) , KL q(zdi epth|x)||p(zdi epth) ,
Eq(zwi here|x) KL q(zciat|x, zwi here)||p(zciat) , and Eq(zwi here,zciat|x) KL q(zwi hat|x, zwi here, zciat)||p(zwi hat|zciat) ,
respectively. Therefore, we complete the proof of Eqn. 5. During the implementation, we model discrete variables zpres and zcat using the Gumbel-Softmax approximation (Jang et al. (2016)). Therefore, all variables are differentiable using the reparameterization trick.

B Implementation Details
Our code is available at https://github.com/EmoFuncs/GMAIR-pytorch.

12

Layer
resnet deconv layer 1 deconv layer 2

Table 3: Architecture of Backbone

Type

Size Act./Norm.

ResNet18 (w/o fc)

Deconv

128 ReLU/BN

Deconv

64 ReLU/BN

Output Size
512 × 4 × 4 128 × 8 × 8 64 × 16 × 16

B.1 Models
Here, we describe the architecture of each module of GMAIR, as shown in Fig. 1. The backbone is a ResNet18 (He et al. (2016)) network with two deconvolution layers replacing the fully connected layer, as shown in Table 3. Pres-head, depth-head, and where-head are convolutional networks that are only different from the number of output channels, as shown in Table 4. What-encoder and cat-encoder are multiple layer networks, as shown in Table 5. Finally, the glimpse decoder is a deconvolutional network, as shown in Table 6.
For other models, we make use of code from https://github.com/yonkshi/SPAIR_pytorch for SPAIR, and https://github.com/zhixuan-lin/SPACE for SPACE. We utilize most of the default configuration for both models, and only change A (the dimension of zwi hat) to 256 for comparison, the size of the base bounding box to 72 × 72 for large objects.
B.2 Training and Hyperparameters
The base set of hyperparameters for GMAIR is given in Table 7. The value p (the prior on zpres) drops gradually from 1 to the final value 6e-6, and the value overlap drops from 2 to 0 in the early stage of training for stability. The learning rate is in the range of [5e-5, 1e-4].
B.3 Testing
During testing phase, in order to obtain deterministic results, we use the value with the largest probability (density) for latent variables z, instead of sampling them from the distributions. To be specific, we use , µdepth, p, µwhat, and µwhere for zcat, zdepth, zpres, zwhat and zwhere, respectively.

C Calculation of AP, ACC and NMI
The value of AP is calculated at threshold IoU = 0.5 by using the calculation method from the VOC (Everingham et al. (2010)). Before calculating the ACC and NMI of clusters, we filter the incorrect bounding boxes. A predicted box P B is correct iff there is a ground-truth box GB such that IoU(P B, GB) > 0.5, and the class of a correct predicted box P B is assigned to the class of the ground-truth box GB such that IoU(P B, GB) is maximized. After filtering, all correct predicted boxes are used for the calculation of ACC and NMI. Note that we still have many ways to assign each predicted category to a real category when calculating the value of ACC. In all of the ways, we select the one such that ACC is maximized, following Dilokthanakul et al. (2016). Formulas are shown in Eqn. 16 and Eqn. 17 for the calculation of ACC and NMI:

ACC =

C k=1

max1jC

| {i|Gi

= j, Pi = k} |

(16)

|P |

2I(G, P )

NMI =

(17)

H(G) + H(P )

where G and P are respectively the ground-truth categories and predicted categories for all correct boxes, C and C are the number of clusters and real classes, H(·) and I(·, ·) are the entropy and mutual information function, respectively.

13

Table 4: Architectures of Pres/Depth/Where-Head

Layer Type Size

Act./Norm. Output Size

Input Hidden Conv [3 × 3, 128] × 3 ReLU Output Conv 1 × 1, 1/1/4

64 × 16 × 16 128 × 16 × 16 1/1/4 × 16 × 16

Layer
Input Layer 1 Layer 2 Layer 3 Output

Table 5: Architectures of What/Cat-Encoder

Type Size

Act./Norm. Output Size

Flatten Linear Linear Linear Linear

3072 × 128 128 × 256 256 × 512 512 × A/C

ReLU ReLU ReLU

(3 × 32 × 32 =)3072 128 256 512 A/C

Layer
Input Layer 1 Layer 2 Layer 3 Layer 4 Conv Layer 5 Output

Table 6: Architecture of Glimpse-Decoder

Type Size

Act./Norm. Output Size

Linear Deconv Deconv Deconv Deconv Conv DeConv Conv

A × 256 128 128 64 32 3 × 3, 32 16 3 × 3, 3

ReLU ReLU/GN(8) ReLU/GN(8) ReLU/GN(8) ReLU/GN(8) ReLU/GN(8) ReLU/GN(4)

256 × 1 × 1 128 × 2 × 2 128 × 4 × 4 64 × 8 × 8 32 × 16 × 16 32 × 16 × 16 16 × 32 × 32 3 × 32 × 32

Table 7: Base Hyperparameters

Description

Variable

Value

Base bbox size
Batch size Dim. of zwi hat Dim. of zciat Glimpse size
Learning rate
Loss Coef. of Lcat Loss Coef. of Loverlap Loss Coef. of Ldepth Loss Coef. of Lpres Loss Coef. of Lrecon Loss Coef. of Lwhat Loss Coef. of Lwhere Prior on zcat
Prior on zdepth Prior on zpres Prior on zwhere
Prior on zwhat

(ah, aw)
A C (Hobj , Wobj )
cat overlap depth pres recon recon recon  (µdpreipotrh, pdreipotrh) p (µwprhioerre, pwrhioerre) (µwprhioart, pwrhioart)

(72, 72) 16 256 10 (32, 32) [5e-5, 1e-4] 1 20 1 1 8,16 1 1 (1/C, ..., 1/C)
(0, 1)
1  6e-6 (0, 1)
(0, 1)

14

(a) AP(IoU=0.5), ACC and (b) "What" latent space, at (c) "What" latent space, at (d) "What" latent space, at

NMI during training

220k iterations

330k iterations

440k iterations

(e) Original image

(f) Generated image, at (g) Generated image, at (h) Generated image, at

220k iterations

330k iterations

440k iterations

Figure 5: "What" representation and cluster analysis after 100k iterations. (a) Average precision (AP), accuracy (ACC), normalized mutual information (NMI) during training. (b-d) Visualized "what" latent space by t-SNE (Van der Maaten and Hinton (2008)) at 220k, 330k, and 440k iterations, respectively. (e) Sample of original image. (f-h) Samples of generated image at 220k, 330k, and 440k iterations, respectively.

D Additional Experiment Results
The graphs of "what" representation after 100k iterations are shown in Fig. 5.

15

