Atomistic Line Graph Neural Network for Improved Materials Property Predictions
Brian DeCost1, Kamal Choudhary1,2 1 Materials Measurement Laboratory, National Institute of Standards and Technology, Gaithersburg, MD, 20899, USA. 2 Theiss Research, La Jolla, California 92037, USA.
Abstract: Graph neural networks (GNN) have been shown to provide much improved performance for representing and modeling atomistic materials compared with descriptor-based machine-learning models. While most existing GNN models for atomistic predictions are based on atomic distance information, they do not explicitly incorporate bond angles, which are critical for distinguishing many atomic structures. Furthermore, many material properties are known to be sensitive to slight changes in bond angles. We develop Atomistic Line Graph Neural Network (ALIGNN) using a GNN architecture that performs message passing on both the interatomic bond graph and its line graph corresponding to bond angles. We demonstrate that angle information can be explicitly and efficiently included for materials to provide much improved performance. We train 44 models for predicting several solid-state material properties available in the JARVIS-DFT and materialsproject databases. ALIGNN can outperform some of the previously known GNN models by up to 43.8 %.
Corresponding author: Kamal Choudhary (kamal.choudhary@nist.gov)
1

Introduction Graphs are a powerful non-Euclidean data structure method for establishing relationships between features (nodes) and their relationships (edges) 1,2. Graph neural networks (GNN)3,4 have immense potential for modeling complex phenomena. Common applications of GNNs include community detection and link prediction in social networks5,6, functional time series on brain structures7, gene DNA on regulatory networks8, information flow through telecommunications networks9, and property prediction for molecular and solid materials10. From a quantum chemistry point of view, GNNs provide a unique opportunity to predict properties of solids, molecules, and proteins in a much faster way rather than by solving computationally expensive Schrodinger equation11-14.
There has been rapid progress in the development of GNN architectures for predicting material properties such as SchNet10, Crystal Graph Convolutional Neural Networks (CGCNN)15, MatErials Graph Network (MEGNet)16, improved Crystal Graph Convolutional Neural Networks (iCGCNN)17 and similar variants18-31. This family of models represents a molecule or crystalline material as a graph with one node for each constituent atom and edges corresponding to interatomic bonds. A common theme is the use of elemental properties as node features and interatomic distances and/or bond valences as edge features. Through multiple layers of graph convolution, updating node features based on their local chemical environment, these models can implicitly represent many-body interactions. However, many important material properties (especially electronic properties such as band gaps) are highly sensitive to structural features such as bond angles. It is possible that these models are not able to efficiently learn the importance of such many-body interactions. Explicit inclusion of angle-based information has already been shown to improve models with hand-crafted features such as classical force-field inspired descriptors
2

(CFID)32. Recently, there has been growing interest in the explicit incorporation of bond angles and other many-body features17,20. In this work, we use line graph neural networks inspired by those proposed in Ref6 to develop an alternative way to include angular information to provide high accuracy models. Briefly, the line graph L(g) is a graph derived from another graph g that describes the connectivity of the edges in g. While the nodes of an atomistic graph correspond to atoms and its edges correspond to bonds, the nodes of an atomistic line graph correspond to bonds and its edges correspond to bond angles. Our model alternates between graph convolution on these two graphs, propagating bond angle information through interatomic bond representations to the atom-wise representations and vice versa. We use both the bond distances and angles in the line graph to incorporate finer details of atomic structure which leads to higher model performance accuracies. Our Atomistic Line Graph Neural Network (ALIGNN) models are implemented using the deep graph library (DGL) 33 which allows efficient construction and neural message passing for different types of graphs. ALIGNN is a part of the Joint Automated Repository for Various Integrated Simulations (JARVIS) infrastructure34. We train several crystalline material properties from JARVIS-density functional theory (DFT) 34-44 and materials project45 datasets.
Results and discussion ALIGNN performs Edge-gated graph convolution4,46 message passing updates on both the atomistic bond graph (atoms are nodes, bonds are edges) and its line graph (bonds are nodes, bond pairs with one common atom are edges). The Edge-gated graph convolution variant has the distinct advantage of updating both node and edge features. Because each edge in the bond graph directly corresponds to a node in the line graph, ALIGNN can aggregate features from bond pairs to
3

efficiently update node representations by alternating between message passing updates on the bond graph and its line graph.
A. Atomistic graph representation For crystals, we use a periodic 12-nearest-neighbor graph construction. We expand this nearestneighbor graph to include edges to all atoms in the neighbor shell of the 12th-nearest neighbor. The atomistic information is then used to build DGL graphs using 9 node features: electronegativity, group number, covalent radius, valence electrons, first ionization energy, electron affinity, block and atomic volume. This feature set is inspired by the CGCNN15 model. The initial edge features are interatomic bond distances. We use a radial basis function (RBF) expansion with support between 0 and 8 Å. This undirected graph then can be represented as G = (, ) where  are nodes and  are edges i.e., a collection of (i, j) linking vertices from i to j. G has an associated node feature set H={h1,...,hN) , where hi is the feature vector associated with node i.
B. Atomistic line graph representation The atomistic line graph is derived from the atomistic graph. Each node in the line graph corresponds to an edge in the original graph; both entities represent interatomic bonds, and in our work, they share latent representations. Edges in the line graph correspond to triplets of atoms or pairs of interatomic bonds; the initial line graph edge features are an RBF expansion of the bond angle cosines:  =  (||.||), where rij and rjk are atomic displacement vectors between atoms i, j, and k. A schematic of an atomistic graph and corresponding atomistic line graph is shown in Fig. 1.
4

Fig. 1 Schematic showing undirected crystal graph representation and corresponding line graph construction for a SiO4 polyhedron. For simplicity, only Si-O bonds are illustrated. The ALIGNN convolution layer alternates between message passing on the bond graph (left) and its line graph (or bond adjacency graph, right). C. Edge gated graph convolution ALIGNN uses Edge-gated graph convolution4,46 convolution for updating both node and edge
features. This convolution is similar to the CGCNN update, except that edge features are only
incorporated into normalized edge gates. Furthermore, edge gated graph convolution uses the pre-
aggregated edge messages to update the edge representations.
Edge gated graph convolution updates node representations hl from layer l according to the
formula:

+1

=



(

{

}


)

(1)

5

+1 =  +  (( +   ))

(2)

 

=

()  ()+

(3)

 = -1 +  ((-1 + -1 + -1))

(4)

The edge messages in this equation (4) are equivalent to the gating term in the CGCNN update15,

which coalesces the weight matrices A, B, and C into Wgate, and the augmented edge representation

zij = hi  hj  eij

(5)

 = -1 +  ((-1))

(6)

D. ALIGNN update One ALIGNN layer composes an edge-gated graph convolution on the bond graph (g) with an edge-gated graph convolution on the line graph (L(g)). To avoid ambiguity between the node and edge features of the atomistic graph and its line graph, we write atom, bond, and triplet representations as h, e, and t.

,  = ((), -1, -1)

(7)

,  = (, -1, )

(8)

The line graph convolution produces bond messages ml that are propagated to the atomistic graph, which further updates the bond features in combination with atom features h.

6

Fig. 2 A schematic showing the ALIGNN model architecture. E. Overall model architecture and training We use N layers of ALIGNN updates followed by M layers of edge-gated graph convolution (GCN) updates on the bond graph. We use Swish/Sigmoid Linear Unit (SiLU) activations instead of rectified linear unit (ReLU) or Softplus because it is twice differentiable like Softplus but can result in better empirical performance like ReLU on many tasks. We finally use fully connected layers to predict the properties. A schematic showing the ALIGNN model architecture is shown in Fig. 2.
7

F. Model performance
Model performance could be highly susceptible to the selection of a dataset. To evaluate the performance of ALIGNN, we currently use two different datasets: materials project (MP) and JARVIS-DFT. Note that both of these datasets have been continuously being updated, hence, we select a snapshot preferably the one used by previous works to report our performance. It is likely that as these dataset sizes increase in the future the performance of the model can be further improved. We select the 2018.6.1 version of materials project which consists of 69239 materials with properties such as Perdew Burke-Ernzerhof functional (PBE)47 bandgaps and formation energies. Similarly, we use 2021.5.16 version of JARVIS-DFT dataset, which consists of 48525 materials with several properties such as van der Waals correction with optimized Becke88 functional (OptB88vdW)48 bandgaps, formation energies, dielectric constants, Tran-Blaha modified Becke Johnson potential (MBJ)49 bandgaps and dielectric constants, bulk, shear modulus, magnetic moment, density functional perturbation theory (DFPT) based maximum piezoelectric coefficients, Boltztrap50 based Seebeck coefficient, power factor, maximum absolute value of electric field gradient. All of these properties are critical for functional materials design. For the MP dataset we use a train-validation-test split of 60000-5000-4239 as used by SchNet10, and MEGNet16. For the JARVIS-DFT dataset and its properties, we use 80 %:10 %: 10 % splits.
Performance of ALIGNN on MP dataset is shown in Table.1 which shows the regression model performance in terms of mean absolute error metric (MAE). The MAEs for formation energy (Ef) and band gap (Eg) with ALIGNN are 0.0221 eV/atom and 0.218 eV respectively. In terms of Ef, ALIGNN outperforms CGCNN, MEGNet and SchNet models by 43.3 %, 21.07 % and 36.86 % respectively. For Eg, ALIGNN outperforms CGCNN and MEGNet by 43.81 % and 33.93 % respectively. Good performance on well-known and well-characterized datasets ensures high
8

prediction accuracy of ALIGNN models. Because each property has different units and in general a different variance, we also report the mean absolute deviation (MAD) for each property to facilitate an unbiased comparison of the model performance between different properties. The MAD values represent the performance of a random guessing model with average value prediction for each data point. We also report the CFID based predictions for comparison. Clearly, all the neural networks, especially ALIGNN, perform much better than the corresponding MAD of the dataset as well as CFID performance. Analyzing the MAD: MAE (ALIGNN) ratio, we observe that the ratio could be as high as 42.08 model. Generally, a model with high MAD:MAE ratio (such as 5 and above) is considered a good predictive model51.

Table 1 Performance on the materials project dataset.

Prop Unit

MAD CFID CGCNN MEGNet SchNet ALIGNN MAD:MAE

Ef eV/atom 0.93 0.104 0.039 0.028 0.035 0.0221 42.08

Eg eV

1.35 0.434 0.388 0.33

-

0.218 6.19

Similarly, we train ALIGNN models on the JARVIS-DFT34-44 dataset which consists of data for 48525 materials. In addition to properties such as formation energies, and bandgaps it also consists several unique quantities such as solar-cell efficiency (spectroscopic limited maximum efficiency, SLME), topological spin-orbit spillage, dielectric constant with (x (DFPT)) and without ionic contributions (x (OPT, MBJ)), exfoliation energies for two-dimensional (2D), electric field gradients (EFG), Voigt bulk (Kv) and shear modulus (Gv), energy above convex hull (ehull), maximum piezoelectric stress (eij) and strain (dij) tensors, n-type and p-type Seebeck coefficient and power factors (PF), crystallographic averages of electron (me) and hole (mh) effective masses. As we converge plane wave-cutoff (ENCUT) and k-points used in Brillouin zone integration
9

(Kpoint-length), we attempt to make machine learning predictions on these quantities as well. Such a large variety of properties allow a thorough testing of our ALIGNN model. More details for individual properties, its precision with respect to experimental measurements, applicability and limitations can be found in respective works. However, it is important to mention that many important issues such as tackling underestimation of bandgap problems, the inclusion of van der Waals bonding, the inclusion of spin-orbit coupling interactions, which are critically important for materials-design perspective, have been key areas of improvements for JARVIS-DFT dataset. For instance, meta-GGA(generalized gradient approximation) based Tran-Blaha modified Becke Johnson potential (TBmBJ) band gaps are more reliable and comparable to experimental data than Perdew Burke-Ernzerhof functional (PBE) or van der Waals correction with optimized Becke88 functional (OptB88vdW) bandgaps, but their calculations are computationally expensive and hence they are relatively smaller. In addition to the ALIGNN performance, we also include handcrafted Classical force-field inspired descriptors (CFID) descriptor performance for these properties using identical data-splits.
In Table. 2 we show the performance on regression models for different properties in the JARVISDFT database. We observe that ALIGNN models outperform CFID descriptors by upto 4 times suggesting GNNs can be a very powerful method for multiple material property predictions. Comparing Table. 1 and Table. 2 for identical material properties such as formation energies and band gaps (though different dataset size, DFT method and settings), we observe that the MAE for formation energy on MP dataset is lower by 42.08 %, which can be attributed to the larger dataset size. The MAE for band gaps is much lower (by 40.82 %) for the JARVIS-DFT dataset, which is interesting as the JARVIS-DFT dataset is still smaller than MP. More detailed works need to be done on the effect of the selection of datasets on machine learning performance, which is beyond
10

the scope of the present work. A few possible explanations for better bandgap on JARVIS-DFT could be different DFT methods used as shown in Ref. 37 or limited spread of materials in the JARVIS-DFT database but it requires more detailed investigation. Nevertheless, application of ALIGNN models on different datasets shows improvements for materials-property predictions. Both CFID and ALIGNN models' MAEs are lower than the corresponding MADs. The MAD:MAE ratios can vary for energy related quantities from a high value of 50.0 (total energy), and 27.389 (formation energy model) to low values such as for DFPT based piezoelectric strain coefficients (1.285) and dielectric constant with ionic contributions (1.517). The results indicate that there is still much room for improvement for the GNN models especially for electronic properties.
11

Table 2: Regression model performances on JARVIS-DFT dataset for 32 properties using ALIGNN and CFID models.

Property Units

MAD CFID

ALIGNN

MAD: MAE

Total energy

eV(atom)-1 1.79 0.22

0.0358

50.000

Formation energy eV(atom)-1 0.86 0.118

0.0314

27.389

Ehull

eV

1.14 0.22

0.102

11.176

Bandgap (OPT)

eV

1.02 0.261

0.129

7.907

Bandgap (MBJ)

eV

1.80 0.474

0.316

5.696

Kv

GPa

53.50 13.98

10.69

5.005

Gv

GPa

27.20 11.48

9.32

2.918

Mag. mom

µB

1.24 0.44

0.249

4.980

x (OPT)

No unit

58.64 25.46

21.21

2.765

y (OPT)

No unit

58.81 25.08

20.74

2.836

z (OPT)

No unit

57.46 24.94

20.69

2.777

x (MBJ)

No unit

66.19 29.90

24.65

2.685

y (MBJ)

No unit

66.32 29.61

23.72

2.796

z (MBJ)

No unit

62.62 28.13

23.57

2.657

 (DFPT:elec+ionic) No unit

45.80 40.22

30.19

1.517

Max. piezoelectric CN-1 strain coeff (dij)

24.57 31.82

19.12

1.285

Max. piezo. stress Cm-2 coeff (eij)

0.26 0.20

0.123

2.114

SLME (%)

No unit

10.88 5.70

4.21

2.584

Spillage

No unit

0.52 0.38

0.367

1.417

Exfo. energy

meV(atom)-1 62.63 63.13

51.48

1.217

Kpoint-length

Å

18.68 9.53

9.48

1.970

12

Plane-wave cutoff Max. EFG avg. me
avg. mh
n-Seebeck n-PF p-Seebeck p-PF

eV

260.19 114.16

1021 Vm-2

43.90 23.74

electron mass 0.22 0.13 unit

electron mass 0.41 0.19 unit

µVK-1

113.0 56.50

µW(mK2)-1 697.80 512.86

µVK-1

166.33 62.60

µW(mK2)-1 691.67 483.10

111.1 19.16 0.085
0.122
41.42 450.46 42.32 430.81

2.342 2.291 2.588
3.361
2.728 1.549 3.93 1.606

As we notice above, the regression tasks for some of the electronic properties do not show very high MAD: MAE, we train classification models for some of them. Classification tasks predict labels such as high value/low value (based on a selected threshold) as 1 and 0 instead of predicting actual data in regression tasks. Such models can be useful for fast screening purposes38 for computationally expensive methods. A random guessing model has a classification task receiver operating characteristic curve area under the curve (ROC AUC) as 0.5, while a perfect model would be a ROC AUC of 1.0. We notice most of our classification models have ROC AUCs that are high and up to a maximum value of 0.95 (for convex hull stability) showing their usefulness for material classification-based applications. All results are based on the performance of 10 % test data which is never used during the training procedure.

13

Table 3: Classification task ROC AUC performance on JARVIS-DFT dataset for ALIGNN models.

Model

Threshold

ROC AUC

Metal/non-metal classifier (OPT)

0.01 eV

0.94

Metal/non-metal classifier (MBJ)

0.01 eV

0.92

Magnetic/non-Magnetic classifier

0.05 µB

0.92

High/low SLME High/low spillage Stable/unstable (ehull) High/low-n-Seebeck High/low-p-Seebeck High/low-n-powerfactor High/low-p-powerfactor

10 %

0.81

0.1

0.80

0.1 eV

0.95

-100 µVK-1

0.88

100 µVK-1

0.92

1000 µW(mK2)-1

0.74

1000µW(mK2)-1

0.73

In summary, we have developed an ALIGNN model which uses the line graph neural network that improves the performance of GNN predictions for solids. As most of the pre-existing GNN models are based on atomic distances only and do not take into account explicit bond-angle information, our model allows us to explicitly include this information. We have demonstrated that the inclusion of angle-based networks in GNNs can significantly improve the performance of the model. A key advancement of this work is the inclusion and development of both the undirected and its line graph counterpart for solid-state materials. We develop regression and classification ALIGNN
14

models for some of the well-known pre-existing databases and it can be easily applied for other datasets as well. Our model significantly improved accuracies over prior GNN models. We believe the ALIGNN model will rapidly improve the machine-learning prediction for several material properties and classes.
Methods
The JARVIS-DFT34-44 dataset is developed using Vienna Ab-initio simulation package (VASP)52,53 software54. Most of the properties are calculated using the OptB88vdW functional48. For a subset of the data we use TBmBJ49 for getting better band gaps. We use density functional perturbation theory (DFPT)55 for predicting piezoelectric and dielectric constants with both electronic and ionic contributions. The linear response theory-based56 frequency based dielectric function was calculated using both OptB88vdW and TBmBJ and the zero-energy value was trained for the machine learning model. Note that the linear response based dielectric constants lack ionic contributions. The TBmBJ frequency dependent dielectric functions are used to calculate the spectroscopic limited maximum efficiency (SLME)38. The magnetic moments are calculated using spin-polarized calculations considering only ferromagnetic initial configurations and neglecting any density functional theory (DFT)+U effects. The thermoelectric coefficients such as Seebeck coefficients and power factors are calculated using BoltzTrap50 software using constant relaxation time approximation. Exfoliation energy for the van der Waals bonded two-dimensional materials are calculated as the energy per atom differences between the bulk and corresponding monolayer counterparts. The spin-orbit spillage40 is calculated using the difference in wavefunctions of a material with and without inclusion of spin orbit coupling effects. All the JARVIS-DFT data and Classical force-field inspired descriptors (CFID)32 are generated using the JARVIS-Tools package. The CFID descriptors are trained using the LightGBM57 package54 using the models developed in
15

Ref.32. All the neural network models are trained using PyTorch58 and Torch-ignite59 libraries54. All the undirected and line-graphs are constructed using deep graph library (DGL)33. For regression targets we minimize the mean squared error (MSE) loss, and for classification targets we minimize the standard negative log likelihood loss. We train all models for 300 epochs using the AdamW60 optimizer with normalized weight decay of 10-5 and a batch size of 64. The learning rate is scheduled according to the one-cycle policy61 with a maximum learning rate of 0.001. We use the same model configuration for each regression and classification target. We use the initial atom representations from the CGCNN paper, 80 initial bond radial basis function (RBF) features, and 40 initial bond angle RBF features. The atom, bond, and bond angle feature embedding layers produce 64-dimensional inputs to the graph convolution layers. The main body of the network consists of 4 ALIGNN and 4 graph convolution (GCN) layers, each with hidden dimension 256. The final atom representations are reduced by atom-wise average pooling and mapped to regression or classification outputs by a single linear layer. The ALIGNN models are trained on the Nisaba cluster with Tesla V100 SXM2 32 gigabyte Graphics processing unit (GPUs) at National Institute of Standards and Technology (NIST)54. For the MP dataset we use a trainvalidation-test split of 60000-5000-4239. For the JARVIS-DFT dataset, we use 80 %:10 %: 10 % splits. The 10 % test data is never used during training procedures.
Acknowledgements: B. D. and K.C. thank the National Institute of Standards and Technology for funding, computational, and data management resources.
Data-availability: All data used in this work is available at Figshare link https://figshare.com/collections/ALIGNN_data/5429274 . During the training these datasets are accessed using JARVIS-Tools's figshare module.
16

Code-availability: The code and full model and training configurations used in this work are
available on GitHub at https://github.com/usnistgov/alignn (currently private), along with general
tooling at https://github.com/usnistgov/jarvis (public).
Contributions: Both B.D. and K.C. equally contributed in developing the model and writing the
manuscript.
References:
1 LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436-444 (2015). 2 Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M. & Monfardini, G. The graph neural
network model. IEEE transactions on neural networks 20, 61-80 (2008). 3 Wu, Z. et al. A comprehensive survey on graph neural networks. IEEE transactions on
neural networks and learning systems (2020). 4 Dwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y. & Bresson, X. Benchmarking graph
neural networks. arXiv preprint arXiv:2003.00982 (2020). 5 Fan, W. et al. in The World Wide Web Conference. 417-426. 6 Chen, Z., Li, X. & Bruna, J. Supervised community detection with line graph neural
networks. arXiv preprint arXiv:1705.08415 (2017). 7 Li, X. & Duncan, J. Braingnn: Interpretable brain graph neural network for fmri analysis.
bioRxiv (2020). 8 Baumbach, J. CoryneRegNet 4.0­A reference database for corynebacterial gene regulatory
networks. BMC bioinformatics 8, 1-11 (2007). 9 Vinayakumar, R., Soman, K. & Poornachandran, P. in 2017 International Conference on
Advances in Computing, Communications and Informatics (ICACCI). 1222-1228 (IEEE). 10 Schütt, K. T. et al. Schnet: A continuous-filter convolutional neural network for modeling
quantum interactions. arXiv preprint arXiv:1706.08566 (2017). 11 Duvenaud, D. et al. Convolutional networks on graphs for learning molecular fingerprints.
arXiv preprint arXiv:1509.09292 (2015). 12 Kearnes, S., McCloskey, K., Berndl, M., Pande, V. & Riley, P. Molecular graph
convolutions: moving beyond fingerprints. Journal of computer-aided molecular design 30, 595-608 (2016). 13 Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O. & Dahl, G. E. in International Conference on Machine Learning. 1263-1272 (PMLR). 14 Faber, F. A. et al. Prediction errors of molecular machine learning models lower than hybrid DFT error. Journal of chemical theory and computation 13, 5255-5264 (2017). 15 Xie, T. & Grossman, J. C. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Physical review letters 120, 145301 (2018). 16 Chen, C., Ye, W., Zuo, Y., Zheng, C. & Ong, S. P. Graph networks as a universal machine learning framework for molecules and crystals. Chemistry of Materials 31, 3564-3572 (2019).
17

17 Park, C. W. & Wolverton, C. Developing an improved Crystal Graph Convolutional Neural Network framework for accelerated materials discovery. Physical Review Materials 4, 063801 (2020).
18 Shui, Z. & Karypis, G. Heterogeneous Molecular Graph Neural Networks for Predicting Molecule Properties. arXiv preprint arXiv:2009.12710 (2020).
19 Sanyal, S. et al. MT-CGCNN: Integrating crystal graph convolutional neural network with multitask learning for material property prediction. arXiv preprint arXiv:1811.05660 (2018).
20 Klicpera, J., Groß, J. & Günnemann, S. Directional message passing for molecular graphs. arXiv preprint arXiv:2003.03123 (2020).
21 Unke, O. T. & Meuwly, M. PhysNet: A neural network for predicting energies, forces, dipole moments, and partial charges. Journal of chemical theory and computation 15, 3678-3693 (2019).
22 Noh, J., Gu, G. H., Kim, S. & Jung, Y. Uncertainty-Quantified Hybrid Machine Learning/Density Functional Theory High Throughput Screening Method for Crystals. Journal of chemical information and modeling 60, 1996-2003 (2020).
23 Schütt, K. T., Arbabzadah, F., Chmiela, S., Müller, K. R. & Tkatchenko, A. Quantumchemical insights from deep tensor neural networks. Nature communications 8, 1-8 (2017).
24 Anderson, B., Hy, T.-S. & Kondor, R. Cormorant: Covariant molecular neural networks. arXiv preprint arXiv:1906.04015 (2019).
25 Zhang, S., Liu, Y. & Xie, L. Molecular Mechanics-Driven Graph Neural Network with Multiplex Graph for Molecular Structures. arXiv preprint arXiv:2011.07457 (2020).
26 Lubbers, N., Smith, J. S. & Barros, K. Hierarchical modeling of molecular energies using a deep neural network. The Journal of chemical physics 148, 241715 (2018).
27 Schutt, K. et al. SchNetPack: A deep learning toolbox for atomistic systems. Journal of chemical theory and computation 15, 448-455 (2018).
28 Jha, D. et al. Elemnet: Deep learning the chemistry of materials from only elemental composition. Scientific reports 8, 1-13 (2018).
29 Westermayr, J., Gastegger, M. & Marquetand, P. Combining SchNet and SHARC: The SchNarc machine learning approach for excited-state dynamics. The journal of physical chemistry letters 11, 3828-3834 (2020).
30 Wen, M., Blau, S. M., Spotte-Smith, E. W. C., Dwaraknath, S. & Persson, K. A. BonDNet: a graph neural network for the prediction of bond dissociation energies for charged molecules. Chemical Science (2020).
31 Isayev, O. et al. Universal fragment descriptors for predicting properties of inorganic crystals. Nature communications 8, 1-12 (2017).
32 Choudhary, K., DeCost, B. & Tavazza, F. Machine learning with force-field-inspired descriptors for materials: Fast screening and mapping energy landscape. Physical Review Materials 2, 083801 (2018).
33 Wang, M. et al. Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv preprint arXiv:1909.01315 (2019).
34 Choudhary, K. et al. The joint automated repository for various integrated simulations (JARVIS) for data-driven materials design. npj Computational Materials 6, 1-13 (2020).
35 Choudhary, K., Cheon, G., Reed, E. & Tavazza, F. Elastic properties of bulk and lowdimensional materials using van der Waals density functional. Physical Review B 98, 014107 (2018).
18

36 Choudhary, K., Kalish, I., Beams, R. & Tavazza, F. High-throughput identification and characterization of two-dimensional materials using density functional theory. Scientific Reports 7, 1-16 (2017).
37 Choudhary, K. et al. Computational screening of high-performance optoelectronic materials using OptB88vdW and TB-mBJ formalisms. Scientific data 5, 1-12 (2018).
38 Choudhary, K. et al. Accelerated discovery of efficient solar cell materials using quantum and machine-learning methods. Chemistry of Materials 31, 5900-5908 (2019).
39 Choudhary, K., Garrity, K. F. & Tavazza, F. High-throughput discovery of topologically non-trivial materials using spin-orbit spillage. Scientific reports 9, 1-8 (2019).
40 Choudhary, K., Garrity, K. F., Ghimire, N. J., Anand, N. & Tavazza, F. High-throughput search for magnetic topological materials using spin-orbit spillage, machine learning, and experiments. Physical Review B 103, 155131 (2021).
41 Choudhary, K., Ansari, J. N., Mazin, I. I. & Sauer, K. L. Density functional theory-based electric field gradient database. Scientific Data 7, 1-10 (2020).
42 Choudhary, K., Garrity, K. F. & Tavazza, F. Data-driven discovery of 3D and 2D thermoelectric materials. Journal of Physics: Condensed Matter 32, 475501 (2020).
43 Choudhary, K. et al. High-throughput density functional perturbation theory and machine learning predictions of infrared, piezoelectric, and dielectric responses. npj Computational Materials 6, 1-13 (2020).
44 Choudhary, K. & Tavazza, F. Convergence and machine learning predictions of Monkhorst-Pack k-points and plane-wave cut-off in high-throughput DFT calculations. Computational materials science 161, 300-308 (2019).
45 Jain, A. et al. Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. APL materials 1, 011002 (2013).
46 Bresson, X. & Laurent, T. Residual gated graph convnets. arXiv preprint arXiv:1711.07553 (2017).
47 Perdew, J. P., Burke, K. & Ernzerhof, M. Generalized gradient approximation made simple. Physical review letters 77, 3865 (1996).
48 Klimes, J., Bowler, D. R. & Michaelides, A. Chemical accuracy for the van der Waals density functional. Journal of Physics: Condensed Matter 22, 022201 (2009).
49 Tran, F. & Blaha, P. Accurate band gaps of semiconductors and insulators with a semilocal exchange-correlation potential. Physical review letters 102, 226401 (2009).
50 Madsen, G. K. & Singh, D. J. BoltzTraP. A code for calculating band-structure dependent quantities. Computer Physics Communications 175, 67-71 (2006).
51 Ward, L., Agrawal, A., Choudhary, A. & Wolverton, C. A general-purpose machine learning framework for predicting properties of inorganic materials. npj Computational Materials 2, 1-7 (2016).
52 Kresse, G. & Furthmüller, J. J. P. r. B. Efficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set. 54, 11169 (1996).
53 Kresse, G. & Furthmüller, J. J. C. m. s. Efficiency of ab-initio total energy calculations for metals and semiconductors using a plane-wave basis set. 6, 15-50 (1996).
54 Please note commercial software is identified to specify procedures. Such identification does not imply recommendation by National Institute of Standards and Technology (NIST).
55 Baroni, S. & Resta, R. Ab initio calculation of the macroscopic dielectric constant in silicon. Physical Review B 33, 7017 (1986).
19

56 Gajdos, M., Hummer, K., Kresse, G., Furthmüller, J. & Bechstedt, F. Linear optical properties in the projector-augmented wave methodology. Physical Review B 73, 045112 (2006).
57 Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems 30, 3146-3154 (2017).
58 Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703 (2019).
59 Fomin, V. et al. (2020). 60 Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017). 61 Smith, L. N. & Topin, N. in Artificial Intelligence and Machine Learning for Multi-Domain
Operations Applications. 1100612 (International Society for Optics and Photonics).
20

