arXiv:2106.01603v1 [cs.CV] 3 Jun 2021

Published as a conference paper at ICLR 2021
CT-NET: CHANNEL TENSORIZATION NETWORK FOR VIDEO CLASSIFICATION
Kunchang Li12, Xianhang Li14, Yali Wang13, Jun Wang 4& Yu Qiao13
1Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China 2University of Chinese Academy of Sciences 3SIAT Branch, Shenzhen Institute of Artificial Intelligence and Robotics for Society 4University of Central Florida {kc.li, yl.wang, yu.qiao}@siat.ac.cn xianhangli@knights.ucf.edu, Jun.Wang@ucf.edu
ABSTRACT
3D convolution is powerful for video classification but often computationally expensive, recent studies mainly focus on decomposing it on spatial-temporal and/or channel dimensions. Unfortunately, most approaches fail to achieve a preferable balance between convolutional efficiency and feature-interaction sufficiency. For this reason, we propose a concise and novel Channel Tensorization Network (CTNet), by treating the channel dimension of input feature as a multiplication of K sub-dimensions. On one hand, it naturally factorizes convolution in a multiple dimension way, leading to a light computation burden. On the other hand, it can effectively enhance feature interaction from different channels, and progressively enlarge the 3D receptive field of such interaction to boost classification accuracy. Furthermore, we equip our CT-Module with a Tensor Excitation (TE) mechanism. It can learn to exploit spatial, temporal and channel attention in a high-dimensional manner, to improve the cooperative power of all the feature dimensions in our CT-Module. Finally, we flexibly adapt ResNet as our CT-Net. Extensive experiments are conducted on several challenging video benchmarks, e.g., Kinetics-400, Something-Something V1 and V2. Our CT-Net outperforms a number of recent SOTA approaches, in terms of accuracy and/or efficiency. The codes and models will be available on https://github.com/Andy1621/CT-Net.
1 INTRODUCTION
3D convolution has been widely used to learn spatial-temporal representation for video classification (Tran et al., 2015; Carreira & Zisserman, 2017). However, over parameterization often makes it computationally expensive and hard to train. To alleviate such difficulty, recent studies mainly focus on decomposing 3D convolution (Tran et al., 2018; 2019). One popular approach is spatial-temporal factorization (Qiu et al., 2017; Tran et al., 2018; Xie et al., 2018), which can reduce overfitting by replacing 3D convolution with 2D spatial convolution and 1D temporal convolution. But it still introduces unnecessary computation burden, since both spatial convolution and temporal convolution are performed over all the feature channels. To further decrease such computation cost, channel separation has been recently developed via operating 3D convolution in the depth-wise manner (Tran et al., 2019). However, it inevitably loses accuracy due to the lack of feature interaction between different channels. For compensation, it has to introduce point-wise convolution to preserve interaction with extra computation. So there is a natural question: How to construct effective 3D convolution to achieve a preferable trade-off between efficiency and accuracy for video classification?
*Equally-contributed first authors ({kc.li, yl.wang}@siat.ac.cn, xianhangli@knights.ucf.edu)
Corresponding author (yu.qiao@siat.ac.cn)
1

Published as a conference paper at ICLR 2021

Figure 1: Simple illustration of channel tensorization (K = 2). We tensorize the channel dimension of input feature as a multiplication of K sub-dimensions. Via performing spatial/temporal tensor separable convolution along each sub-dimension, we can achieve a preferable balance between convolutional efficiency and feature-interaction sufficiency. Introduction shows more explanations.

Method

3D Convolution (t × h × w)

Convolutional Efficiency Spatial-temporal Channel

C3D (Tran et al., 2015)

F ull : 3 × 3 × 3

R(2+1)D (Tran et al., 2018)

F ull : 1 × 3 × 3 F ull : 3 × 1 × 1

CSN (Tran et al., 2019)

F ull : 1 × 1 × 1 DW : 3 × 3 × 3

Our CT-Net

C1 : C1 × · · · × 1 × (1 × 3 × 3 + 3 × 1 × 1)

(C = C1 × · · · × CK ) CK : 1 × · · · × CK × (1 × 3 × 3 + 3 × 1 × 1)

1 Interact Field means the receptive field for feature interaction.

Feature-Interaction Sufficiency Interact Manner Interact Field1

ST C

33

SC TC C ST ST C1 ST CK

33 33 (2K + 1)3

Table 1: Two design principles to build effective video representation and efficient convolution.

This paper attempts to address this question by investigating two design principles. (1) Convolutional Efficiency. As shown in Table 1, current designs of spatial-temporal convolution mainly focus on decomposition from either spatial-temporal (Tran et al., 2018) or channel dimension (Tran et al., 2019). To enhance convolutional efficiency, we consider decomposing convolution in a higher dimension with a novel representation of feature tensor. (2) Feature-Interaction Sufficiency. Table 1 clearly shows that, for current decomposition approaches (Tran et al., 2018; 2019), feature interaction only contains one or two of spatial, temporal and channel dimensions at each sub-operation. Such a partial interaction manner would reduce classification accuracy. On one hand, it decreases the discriminative power of video representation, due to the lack of joint learning on all the dimensions. On the other hand, it restricts feature interaction in a limited receptive field, which ignores rich context from a larger 3D region. Hence, to boost classification accuracy, each sub-operation should achieve feature interaction on all the dimensions, and the receptive field of such interaction should be progressively enlarged as the number of sub-operations increases.
Based on these desirable principles, we design a novel and concise Channel Tensorization Module (CT-Module). Specifically, we propose to tensorize the channel dimension of input feature as a multiplication of K sub-dimensions, i.e., C = C1 × C2 × · · · × CK . Via performing spatial/temporal separable convolution along each sub-dimension, we can effectively achieve convolutional efficiency and feature-interaction sufficiency. For better understanding, we use the case of K = 2 as a simple illustration in Figure 1. First, we tensorize the input channel into C = C1 × C2. Naturally, we separate the convolution into distinct ones along each sub-dimension, e.g., for the 1st sub-dimension, we apply our spatial-temporal tensor separable convolution with the size C1×1×t×h×w, which allows us to achieve convolutional efficiency on all the spatial, temporal and channel dimensions. After that, we sequentially perform the tensor separable convolution sub-dimension by sub-dimension. As a result, we can progressively achieve feature interaction on all the channels, and enlarge the spatialtemporal receptive field. For example, after operating 1st tensor separable convolution on the 1st sub-dimension, C1 channels interact, and 3D receptive field of such interaction is 3 × 3 × 3. Via further operating 2nd tensor separable convolution on the 2nd sub-dimension, all C1 × C2 = C

2

Published as a conference paper at ICLR 2021

channels have feature interaction, and 3D receptive field of such interaction becomes 5 × 5 × 5. This clearly satisfies our principle of feature-interaction sufficiency.
We summarize our contributions in the following. First, we design a novel Channel Tensorized Module (CT-Module), which can achieve convolutional efficiency and feature-interaction sufficiency, via progressively performing spatial/temporal tensor separable convolution along each sub-dimension of the tensorized channel. Second, we equip CT-Module with a distinct Tensor Excitation (TE) mechanism, which can further activate the video features of each sub-operation by spatial, temporal and channel attention in a tensor-wise manner. Subsequently, we apply this full module in a residual block, and flexibly adopt 2D ResNet as our Channel Tensorized Network (CT-Net). In this case, we can gradually enhance feature interaction from a broader 3D receptive field, and learn the key spatial-temporal representation with light computation. Finally, we conduct extensive experiments on a number of popular and challenging benchmarks, e.g., Kinetics (Carreira & Zisserman, 2017), Something-Something V1 and V2 (Goyal et al., 2017b). Our CT-Net outperforms the state-of-the-art methods in terms of classification accuracy and/or computation cost.

2 RELATED WORKS
2D CNN for video classification. 2D CNN is a straightforward but useful method for video classification (Karpathy et al., 2014; Simonyan & Zisserman, 2014; Wang et al., 2016; Liu et al., 2020; Jiang et al., 2019). For example, Two-stream methods (Simonyan & Zisserman, 2014) learn video representations by fusing the features from RGB and optical flow respectively. Instead of sampling a single RGB frame, TSN (Wang et al., 2016) proposes a sparse temporal sampling strategy to learn video representations. To further improve accuracy, TSM (Lin et al., 2019) proposes a zeroparameter temporal shift module to exchange information with adjacent frames. However, these methods may lack the capacity of learning spatial-temporal interaction comprehensively, which often reduces their discriminative power to recognize complex human actions.
3D CNN for video classification. 3D CNN has been widely used to learn a rich spatial-temporal context better (Tran et al., 2015; Carreira & Zisserman, 2017; Feichtenhofer et al., 2019; Sudhakaran et al., 2020; Feichtenhofer, 2020). However, it introduces a lot of parameters, which leads to a difficult optimization problem and large computational load. To resolve this issue, I3D (Carreira & Zisserman, 2017) inflates all the 2D convolution kernels pre-trained on ImageNet, which is helpful for optimizing. Other works also try to factorize 3D convolution kernel to reduce complexity, such as P3D (Qiu et al., 2017) and R(2+1)D (Tran et al., 2018). Recently, CSN (Tran et al., 2019) operates 3D convolution in the depth-wise manner. Nevertheless, all these methods still do not achieve a good trade-off between accuracy and efficiency. To tackle this challenge, we propose CT-Net which learns on spatial-temporal and channel dimensions jointly with lower computation than previous methods.

3 METHODS

In this section, we describe our Channel Tensorization Network (CT-Net) in detail. First, we formally introduce our CT-Module in a generic manner. Second, we design a Tensor Excitation (TE) mechanism to enhance CT-Module. Finally, we flexibly adapt ResNet as our CT-Net to achieve a preferable trade-off between accuracy and efficiency for video classification.

3.1 CHANNEL TENSORIZATION MODULE

As discussed in the introduction, the previous approaches have problems in convolutional efficiency

or feature-interaction sufficiency. To tackle such a problem, we introduce a generic Channel Ten-

sorization Module (CT-Module), by treating the channel dimension of input feature as a multipli-

cation of K sub-dimensions, i.e., C = C1 × C2 × · · · × CK . Naturally, this tensor representation allows to tensorize the kernel size of convolution TConv() as a multiplication of K sub-dimensions,

too. To simplify the notation, the channel dimension of the output is omitted by default. The output

Xout can be calculated as follows:

Xout = TConv Xin, W C1×C2×···×CK ×t×h×w

(1)

where Xin and W denote the tensorized input and kernel respectively. However, such an operation requires large computation, so we introduce the tensor separable convolution to alleviate the issue.

3

Published as a conference paper at ICLR 2021

Tensor Separable Convolution. We propose to factorize TConv() along K channel sub-dimensions. Specifically, we decompose TConv() as K tensor separable convolutions TSConv(), and apply TSConv() sub-dimension by sub-dimension as follows:

Xk = TSConv Xk-1, W 1×···×Ck×···×1×t×h×w

(2)

where X0 = Xin and Xout = XK . On one hand, the kernel size of the kth TSConv() is (1 × · · · × Ck × · · · × 1 × t × h × w). It illustrates that only Ck channels interact in the kth suboperation, which leads to convolution efficiency. On the other hand, as we stack the TSConv(), each
convolution performs on the output features of the previous convolution. Therefore, the spatialtemporal receptive field is enlarged. Besides, interactions first occur in C1 channels, second in C1 × C2 channels and so on. Finally, C1 × C2 × · · · × CK = C channels can progressively interact. This clearly satisfies our principle of feature-interaction sufficiency.

Spatial-Temporal Tensor Separable Convolution. To further improve convolution efficiency, we
factorize the 3D TSConv() into 2D spatial TSConv() and 1D temporal TSConv(). Thus, we can obtain the output features XSk and XTk as follows:

XSk = S-TSConv Xk-1, W 1×···×Ck×···×1×1×h×w

(3)

XTk = T-TSConv Xk-1, W 1×···×Ck×···×1×t×1×1

(4)

where S-TSConv() and T-TSConv() represent spatial and temporal tensor separable convolution re-
spectively. Finally, we attempt to aggregate spatial and temporal convolution. There are various
connection types of spatial and temporal tensor separable convolution, e.g., parallel and serial types.
According to the results of the experiments in Section 4, we utilize the parallel method, which illustrates that we sum the spatial feature XSk and temporal feature XTk :

Xk = XSk + XTk

(5)

3.2 TENSOR EXCITATION

Our CT-Module separates feature along spatial, temporal and channel dimensions. To make full use
of their cooperative power to learn distinct video features, we design a concise Tensor Excitation
(TE) mechanism for each dimension. First of all, we attempt to utilize the TE mechanism to enhance spatial and temporal features respectively. For the spatial feature XSk obtained by Equation 5, our corresponding spatial TE mechanism can be formulated as:

Uk = XSk  Sigmod(S-TSConv(T-Pool(XSk )))

(6)

where T-Pool() represents global temporal pooling, i.e., T × 1 × 1 average pooling. By performing it on XSk , we obtain the feature with the size (C1 × C2 × · · · × CK × 1 × H × W ), which gathers spatial contexts along temporal dimension. Subsequently, the spatial tensor separable convolution S-
TSConv() and the activate function Sigmod() are performed to generate the spatial attention heatmap.
Finally, the element-wise multiplication  broadcasts the spatial attention along the temporal dimension. Similarly, we perform the temporal TE mechanism for the temporal feature XTk :

Vk = XTk  Sigmod(T-TSConv(S-Pool(XTk )))

(7)

where S-Pool() and T-TSConv() are global spatial pooling and temporal tensor separable convolution correspondingly. At last, after aggregating the spatial and temporal features by addition, i.e., Rk = Uk + Vk, we perform a channel-wise TE mechanism as follows:

Xk = Rk  Sigmod(PW-TSConv(S-Pool(Rk)))

(8)

We adopt point-wise tensor separable convolution PW-TSConv() to learn the weights for aggregating distinctive channels. The rest follows the previous design. Note that all tensor separable convolutions are performed on the same sub-dimension as the previous convolution, which is essentially different from the SE mechanism (Hu et al., 2020). Through the cooperation of the TE mechanism along three different dimensions, the spatial-temporal features can be significantly enhanced.

4

Published as a conference paper at ICLR 2021
Figure 2: The pipelines of CT-Blocks and the overall architecture of CT-Net. We replace one of every two ResBlocks in ResNet with our CT-Block and the extra point-wise convolution in the last sub-dimension (k = K) is ignored. More details can be found in Section3.3.
3.3 CHANNEL TENSORIZATIONN NETWORK We regard ResNet as an exemplar and build up our CT-Net from ResNet50 (or ResNet101). First, we design a simple CT-Block in Figure 2(a), which adapts the 3 × 3 convolutional layer in Residual Block (ResBlock) into our CT-Module. It can achieve both convolutional efficiency and featureinteraction sufficiency. Second, we equip our simple CT-Block with the TE mechanism in Figure 2(b), forming a full CT-Block that can improve the cooperative power of all the feature dimensions. Besides, extra point-wise convolutions are added between different sub-operations, which are beneficial for more sufficient feature interaction. At last, we build up a novel CT-Net with CT-Block. As shown in Figure 2(c), we replace one of every two ResBlocks with our CT-Block in every stage. Such a method guarantees a better balance between efficiency and accuracy in our experiments. Discussion. In fact, the popular methods in video classification like C3D, R(2+1)D and CSN (Tran et al., 2015; 2018; 2019) can be viewed as special cases of our CT-Net. We can generate different forms by adjusting three hyper-parameters: the number of sub-dimensions (K), the corresponding dimension size (Ck) and the spatial-temporal kernel size (Kernelk). To degenerate into C3D, we can set K = 1 and Kernel1 = 3 × 3 × 3. When K = 2, Kernel1 = 1 × 3 × 3, Kernel2 = 3 × 1 × 1 and C1 = C2 = C without channel tensorization, it becomes R(2+1)D. Unfortunately, because of lacking the decomposition of channels, C3D and R(2+1)D still have a large computational load. When K = 2, C = C1 × C2 = C × 1, Kernel1 = 1 × 1 × 1, Kernel2 = 3 × 3 × 3, obviously it is equivalent to CSN. However, CSN has a limited receptive field of spatial-temporal interaction. In our CT-Net, we utilize channel tensorization and perform tensor separable convolution along each sub-dimension in turn. Such design can not only preserve interaction among spatial, temporal and channel dimensions but also enlarge the receptive field of feature interaction progressively.
4 EXPERIMENTS AND RESULTS
Datasets and implementation details. We conduct experiments on three large video benchmarks: Kinetics-400 (Carreira & Zisserman, 2017), Something-Something V1 and V2 (Goyal et al., 2017b). We choose ResNet50 and ResNet101 (He et al., 2016) pre-trained on ImageNet as the backbone and the parameters of CT-Module are randomly initialized. For training, we utilize the dense sampling strategy (Wang et al., 2018) for Kinetics and sparse sampling strategy (Wang et al., 2016) for Something-Something. Random scaling and cropping are applied for data argumentation. Finally, we resize the cropped regions to 256 × 256. For testing, we sample multiple clips per video (4 for Kinetics, 2 for others) for pursuing high accuracy, and average all scores for the final prediction.
5

Published as a conference paper at ICLR 2021

Method C3D-Module (Tran et al., 2015) R(2+1)D-Module (Tran et al., 2018) CSN-Module (Tran et al., 2019)
Our CT-Module

3D Convolution (t × h × w) F ull : 3 × 3 × 3
F ull : 1 × 3 × 3 + F ull : 3 × 1 × 1 F ull : 1 × 1 × 1 + DW : 3 × 3 × 3 C1, · · · , CK : 1 × 3 × 3 + 3 × 1 × 1

GFLOPs 59.9 45.8 35.6 36.3

Top-1 46.1 47.0 46.8 47.3

Top-5 75.0 76.1 75.7 76.2

(a) Effectiveness of CT-Module. CT-Module outperforms the recent modules for video modeling.

Number GFLOPs Top-1 Top-5

Type Top-1 Top-5

C2 GFLOPs Top-1 Top-5

1D

45.8 46.5 75.6

coupling 47.1 76.0

1

45.9 47.0 76.1

2D

36.3 47.3 76.2

serial 47.2 76.1

4

37.6 46.8 76.0

3D

35.7 47.1 75.8

parallel 47.3 76.2

16

36.4 47.2 76.0

4D

35.6 46.5 75.3

C 36.3 47.3 76.2

(c) Connection type of (b) Number of sub-dimensions. The spatiotemporal convolution. (d) Dimension size. C = C1 × C2

higher the dimension, the smaller the The parallel connection and the best trade-off is achieved

GFLOPs. 2D channel tensorization between spatial and temporal whenadopting the rounded middle

achieves the best trade-off.

convolution is the best choice. size C .

Number

GFLOPs Top-1 Top-5

Kernel size

GFLOPs Top-1 Top-5

+0 (TSN) +1 stage5

43.0 41.9

16.9 42.0 42.3 71.1

C1 1 × 1 × 1 || 1 × 1 × 1 C2 1 × 3 × 3 || 3 × 1 × 1

35.5

46.1 75.1

+4 stage4-5 38.9 +6 stage3-5 37.1

46.9 75.7 47.2 76.1

C1 1 × 1 × 1 || 1 × 1 × 1 C2 1 × 5 × 5 || 5 × 1 × 1

36.6

47.2 76.2

+7 stage2-5 36.3 +12 stage2-5 31.5

47.3 76.2 45.9 76.1

C1 1 × 3 × 3 || 3 × 1 × 1 C2 1 × 3 × 3 || 3 × 1 × 1

36.3

47.3 76.2

C1 1 × 3 × 3 || 3 × 1 × 1 (e) Number and location of CT-Blocks. Simply C2 1 × 5 × 5 || 5 × 1 × 1

37.4

replacing 1 block in stage5 can bring significant C1 1 × 5 × 5 || 5 × 1 × 1 performance improvement. As we replace more C2 1 × 5 × 5 || 5 × 1 × 1

38.9

blocks from the bottom up, the GFLOPs contin-

47.5 76.4 47.6 76.5

ues to decrease. Replacing 7 blocks achieves the (f) Kernel sizes along different dimensions. The larger ker-

best trade-off between accuracy and GFLOPs. nel size brings improvement with more calculation.

Model Baseline (TSN) +CT-Module +CT-Module+PWConv +CT-Module+PWConv+SE +CT-Module+PWConv+TE

GFLOPs 43.0 36.3 37.2 37.2 37.3

Top-1 16.9 47.3 48.0 48.8 50.1

Top-5 42.0 76.2 76.7 77.4 78.8

(g) Impact of different modules. CT-Module is essential for temporal modeling and TE mechanism is also beneficial.

Input Train: 224 × 224 Test: 224 × 224 Train: 224 × 224 Test: 256 × 256 Train: 256 × 256 Test: 256 × 256

GFLOPs 28.6 37.3 37.3

Top-1 49.1 49.7 50.1

Top-5 77.4 77.7 78.8

(h) Impact of different spatial resolution.

Table 2: Ablation studies on Something-Something V1. All models use ResNet50 as the backbone

We follow the same strategy in Non-local (Wang et al., 2018) to pre-process the frames and take 3 crops of 256 × 256 as input. Because some multi-clip models in Table 3 and Table 4 sample crops of 256× 256, we simply multiply the GFLOPs reported in the corresponding papers by (256/224)2
for a fair comparison. When considering efficiency, we use just 1 clip per video and the final crop is scaled to 256 × 256 to ensure comparable GFLOPs.

4.1 ABLATION STUDIES
Table 2 shows our ablation studies on Something-Something V1, which is a challenging dataset that requires video architecture to have a robust spatial-temporal representation ability and is suitable to verify the effectiveness of our method. All models use ResNet50 as the backbone.
Effectiveness of CT-Module. In Table 2a, we replace the 3×3 convolutional layer in ResNet50 with different modules in recent methods (Tran et al., 2015; 2018; 2019). Compared with CSN-Module, our module achieves a better result with similar computation, which reflects the importance of sufficient feature interaction. Besides, it is slightly better than R(2+1)D-Module with much lower calculation, showing the necessity of efficient convolution. Such results demonstrate the effectiveness of our CT-Module. It illustrates our two design principles give preferable guidance for designing an efficient module for temporal modeling.

6

Published as a conference paper at ICLR 2021

Method
ECOEN Lite (Zolfaghari et al., 2018) NL I3D + GCN (Wang & Gupta, 2018) ir-CSN (Tran et al., 2019) ir-CSN (Tran et al., 2019) CorrNet (Wang et al., 2020)
TSN (Wang et al., 2016) TSM (Lin et al., 2019) bLVNet-TAM (Fan et al., 2019) TEINet (Liu et al., 2020) TEA (Li et al., 2020b) PEM+TDLoss (Weng et al., 2020) PEM+TDLoss (Weng et al., 2020) Our CT-Net Our CT-Net
TSN (Wang et al., 2016) TSM (Lin et al., 2019) bLVNet-TAM (Fan et al., 2019) TEINet (Liu et al., 2020) TEA (Li et al., 2020b) PEM+TDLoss (Weng et al., 2020) PEM+TDLoss (Weng et al., 2020) Our CT-Net Our CT-Net Our CT-NetEN

Backbone
Incep+3D R18 3D R50 3D R101 3D R152 3D R50
2D R50 2D R50 bLR50 2D R50 2D Res2Net50 2D R50+TIM 2D R50+TIM 2D R50 2D R50
2D R50 2D R50 bLR50 2D R50 2D Res2Net50 2D R50+TIM 2D R50+TIM 2D R50 2D R50 2D (R50)×4

#Frame
92 32×3×2 32×1×10 32×1×10 32×1×10
8 8 8×2 8 8 8 8×3×2 8 8×3×2
16 16 16×2 16 16 16 16×3×2 16 16×3×2 8+12+16+24

GFLOPs
267 1818 738 967 1150
33 33 24 33 35 33 259 37 224
66 66 48 66 70 66 517 75 447 280

SomethingV1 Top-1 Top-5
46.4 46.1 76.8 48.4 49.3 49.3 -
19.7 46.6 45.6 74.2 46.4 76.6 47.4 48.9 78.1 49.8 50.4 50.1 78.8 51.7 80.1
19.9 47.3 47.2 77.1 48.4 78.8 49.9 51.9 80.3 50.9 52.0 52.5 80.9 53.4 81.7 56.6 83.9

SomethingV2 Top-1 Top-5

-

-

-

-

-

-

-

-

-

-

27.8 57.6

59.1 85.6

59.1 86.0

61.3 -

-

-

62.6 -

63.5 -

62.5 87.7

63.9 88.8

30.0 60.5

63.4 88.5

61.7 88.1

62.1 -

-

-

63.8 -

65.0 -

64.5 89.3

65.9 90.1

67.8 91.1

Table 3: Comparison with the state-of-the-art on Something-Something V1&V2. Our CTNet16f outperforms all the single-clip models in Something-Something and even better than most of the multi-clip models. And our CT-NetEN outperforms all methods with very lower computation.

Number of sub-dimensions. Increasing the number of sub-dimensions saves a lot of computation, but the corresponding accuracy first increases and then decreases as shown in Table 2b. Compared with the 1D method, the 4D method significantly reduces GFLOPs, achieving comparable accuracy. As for the decrease of accuracy when K is too large, we argue that the number of channel in the shallow layer is small (64/128), thus there are too few channels in a single dimension, leading to insufficient feature-interaction. Since the 2D method obtains the best trade-off, we set K = 2 in all the following experiments.
Connection type of spatiotemporal convolution. The coupling 3 × 3 × 3 convolution can be decomposed into serial or parallel spatial/temporal convolution. Table 2c reveals that factorizing the 3D kernel can boost results as expected. Besides, the parallel connection is better, thus we adopt parallel connection as the default.
Dimension size. As we set K = 2, it is essential to explore the impact of changingthe dimension size C2. We can demonstrate that the computation is the lowest when C1= C2 = C. Since C is not always a perfect square number, we adopt the rounded middle size C . Table 2d shows that when C2 = C , the model not only requires the fewest computation cost but also achieves the best performance. Hence, we set C2 = C naturally.
Number and location of CT-Blocks. Table 2e illustrates that simply replacing 1 block in stage5 can bring significant performance improvement (16.9% vs. 42.3%). As we replace more blocks from the bottom up, the GFLOPs continues to decrease. Moreover, the bottom blocks seem to be more beneficial to temporal modeling, since replacing the extra 3 blocks in stage2 and stage3 only improve the accuracy by 0.4% (46.9% vs. 47.3%). Since replacing 7 blocks achieves the highest accuracy, we replace 7 blocks by default.
Kernel sizes along different dimensions. In Table 2f, we can observe that two concatenated 33 convolution kernels are slightly better than those with the same receptive field (13+53). Furthermore, the larger kernel size can bring performance improvement but more calculation. It reveals that our CT-Module avoids the limited receptive field of feature interaction, and it can progressively enlarge the receptive field of such interaction on all the dimensions. Considering a better trade-off between accuracy and computation, we choose two concatenated 33 convolution kernels.
7

Published as a conference paper at ICLR 2021

Method
R(2+1)D (Tran et al., 2018) TSN (Wang et al., 2016) I3D (Carreira & Zisserman, 2017)
TSM (Lin et al., 2019) TEINet (Liu et al., 2020) bLVNet-TAM (Fan et al., 2019) TEA (Li et al., 2020b) PEM+TDLoss (Weng et al., 2020) CorrNet (Wang et al., 2020) SlowFast (Feichtenhofer et al., 2019) SlowFast (Feichtenhofer et al., 2019) Our CT-Net
X3D-XL (Feichtenhofer, 2020) SmallBigNet (Li et al., 2020a) ip-CSN (Tran et al., 2019) ip-CSN (Tran et al., 2019) CorrNet (Wang et al., 2020) SlowFast (Feichtenhofer et al., 2019) SlowFast (Feichtenhofer et al., 2019) NL I3D (Wang & Gupta, 2018) Our CT-Net Our CT-NetEN

Backbone
2D R34 Inception Inception
2D R50 2D R50 bLR50 2D Res2Net50 2D R50+TIM 3D R50 3D R50+R50 3D R50+R50 2D R50
2D R101 3D R101 3D R152 3D R101 3D R101+R101 3D R101+R101 3D R101 2D R101 2D R50+R101

#Frame
32×1×10 25×10×1 64×N/A×N/A
16×3×10 16×3×10 (16×2)×3×3 16×3×10 16×3×10 32×1×10 36=(4+32)×3×10 40=(8+32)×3×10 16×3×4
16×3×10 32×3×4 32×3×10 32×3×10 32×3×10 40=(8+32)×3×10 80=(16+64)×3×10 128×3×10 16×3×4 (16+16)×3×4

GFLOPs
1520=152×10 800=80×10 108×N/A
2580=86×30 2580=86×30 561=62.3×9 2730=91×30 2580=86×30 1150=115×10 1083=36.1×30 1971=65.7×30 895=74.6×12
1452=48.4×30 6552=546×12 2490=83.0×30 3264=108.8×30 6720=224×30 3180=106×30 6390=213×30 10770=359×30 1746=145.5×12 2641=220.1×12

Top-1
72.0 72.5 71.1
74.7 76.2 72.0 76.1 76.9 77.2 75.2 76.4 77.3
79.1 77.4 76.8 77.8 79.2 77.9 78.9 77.7 78.8 79.8

Top-5
91.4 90.2 89.3
92.5 90.6 92.5 93.0
91.5 92.2 92.7
93.9 93.3 92.5 92.8
93.2 93.5 93.3 93.7 94.2

Table 4: Comparison with the state-of-the-art on Kinetics-400. It shows that CT-Net-R5016f can surpass all existing lightweight models and even SlowFast-R5040f . When fusing different models, our model is 2.4× faster than SlowFast-R10180f and shows an 0.9% performance gain.

Impact of different modules and different spatial resolution. In Table 2g, our CT-Module can significantly boost its baseline (16.9% vs. 47.3%) and the TE mechanism can further improve the accuracy by 2.1% (48.0% vs. 50.1%). The extra point-wise convolution also boosts performance, which demonstrates that it is beneficial for sufficient feature interaction. Compared with the SE mechanisms, our TE mechanism focuses more on features in different sub-dimensions individually, thus effectively enhancing spatial-temporal features. In our experiments, to ensure GFLOPs is comparable with other methods, we crop the input to 256 × 256 during testing. Table 2h shows both training and testing with a larger spatial resolution of input bring clear performance improvement.

4.2 COMPARISONS WITH THE STATE-OF-THE-ARTS
Something-Something V1&V2. We make a comprehensive comparison in Table 3. Compared with NL I3D+GCN32f , our CT-Net8f gains 4.0% top-1 accuracy improvement with 49.1× fewer GFLOPs in Something-Something V1. Besides, our CT-Net8f (51.7%) is better than the ir-CSN32f (49.3%) which adopts ResNet-152 as the backbone. Moreover, our CT-Net16f outperforms all the single-clip models in Something-Something V1&V2 and even better than most of the multiclip models. It illustrates that our CT-Net is preferable to capture temporal contextual information efficiently. Surprisingly, with only 280 GFLOPs, our ensemble model CT-NetEN achieves 56.6%(67.8%) top-1 accuracy in Something-Something V1(V2), which outperforms all methods.
Kinetics-400. Kinetics-400 is a large-scale sence-related dataset, and the lightweight 2D models are usually inferior to the 3D models on it. Table 4 shows our CT-Net-R5016f can surpass all existing lightweight models based on 2D backbone. Even compared with SlowFast-R5040f , our CT-NetR5016f also achieves higher accuracy (77.3% vs. 76.4%). Note that our reproduced SlowFastR50 performs worse than that in the paper (Feichtenhofer et al., 2019), which may result from the missing videos in Kinetics-400. As for the deeper model, compared with SlowFast-R10180f , our CT-Net-R10116f requires 3.7× fewer GFLOPs but gains comparable results (78.8% vs. 78.9%). Besides, it achieves comparable top-1 accuracy with X3D-XL (78.8% vs. 79.1%) under a similar GFLOPs. However, X3D requires extensive model searching with an expensive GPU setting, while our CT-Net can be trained traditionally with feasible computation. We perform score fusion over CT-Net-R5016f and CT-Net-R10116f , which mimics two-steam fusion with two temporal rates. In this setting, our model is 2.4× faster than SlowFast-R10180f and shows an 0.9% performance gain (79.8% vs. 78.9%) but only uses 32 frames.
8

Published as a conference paper at ICLR 2021
Figure 3: Comparison of visualization. Videos are sampled from Something-Something V1. Compared with R(2+1)D and CSN, our CT-Net can localize the action and object better both in space and time thanks to the larger spatial-temporal receptive field.
4.3 VISUALIZATION We use Saliency Tubes (Stergiou et al., 2019) to generate the visualization, for it can show the most discriminative features that the network locates. In Figure 3, we sample two videos from SomethingSomething V1 which requires complex temporal modeling. In the left example, our CT-Net focuses on a larger area around the towel, especially in the fourth and fifth frames, thus predicting that someone is twisting it. In contrast, R(2+1)D only concentrates on one side of the towel and gives the wrong judgment. The same situation can be seen in the right example. We argue that CT-Net can localize the action and object accurately thanks to the larger spatial-temporal receptive field. As for CSN, the regions of interest seem to be scattered, because it lacks sufficient spatial-temporal interaction, thus ignoring the rich context both in space and time.
5 CONCLUSIONS
In this paper, we construct an efficient tensor separable convolution to learn the discriminative video representation. We view the channel dimension of the input feature as a multiplication of K subdimensions and stack spatial/temporal tensor separable convolution along each of K sub-dimensions. Moreover, CT-Module is cooperated with the Tensor Excitation mechanism to further improve performance. All experiments demonstrate that our concise and novel CT-Net obtains a preferable balance between accuracy and efficiency on large-scale video datasets. Our proposed principles are preferable guidance for designing an efficient module for temporal modeling.
6 ACKNOWLEDGEMENT
This work is partially supported by National Natural Science Foundation of China (6187617, U1713208), the National Key Research and Development Program of China (No. 2020YFC2004800), Science and Technology Service Network Initiative of Chinese Academy of Sciences (KFJ-STS-QYZX-092), Shenzhen Institute of Artificial Intelligence and Robotics for Society.
REFERENCES
Joa~o Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4724­ 4733, 2017.
Quanfu Fan, Chun-Fu Chen, Hilde Kuehne, Marco Pistoia, and D. Cox. More is less: Learning efficient video representations by temporal aggregation module. In NeurIPS 2019, 2019.
Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. 2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 200­210, 2020.
9

Published as a conference paper at ICLR 2021
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 6201­ 6210, 2019.
Priya Goyal, P. Dolla´r, Ross B. Girshick, P. Noordhuis, L. Wesolowski, Aapo Kyrola, Andrew Tulloch, Y. Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. ArXiv, abs/1706.02677, 2017a.
Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fru¨nd, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The "something something" video database for learning and evaluating visual common sense. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5843­5851, 2017b.
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770­778, 2016.
Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-excitation networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 42:2011­2023, 2020.
Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion encoding for action recognition. 2019 IEEE International Conference on Computer Vision (ICCV), pp. 2000­2009, 2019.
A. Karpathy, G. Toderici, Sanketh Shetty, T. Leung, R. Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1725­1732, 2014.
Hilde Kuehne, Hueihan Jhuang, E. Garrote, T. Poggio, and Thomas Serre. Hmdb: A large video database for human motion recognition. 2011 International Conference on Computer Vision, pp. 2556­2563, 2011.
X. Li, Yali Wang, Zhipeng Zhou, and Yu Qiao. Smallbignet: Integrating core and contextual views for video classification. 2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1089­1098, 2020a.
Yinong Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. ArXiv, abs/2004.01398, 2020b.
Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. 2019 IEEE International Conference on Computer Vision (ICCV), pp. 7082­7092, 2019.
Zhaoyang Liu, D. Luo, Yabiao Wang, L. Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. Teinet: Towards an efficient architecture for video recognition. ArXiv, abs/1911.09435, 2020.
I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017.
Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5534­ 5542, 2017.
K. Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In NIPS, 2014.
K. Soomro, A. Zamir, and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. ArXiv, abs/1212.0402, 2012.
Alexandros Stergiou, G. Kapidis, Grigorios Kalliatakis, C. Chrysoulas, R. Veltkamp, and R. Poppe. Saliency tubes: Visual explanations for spatio-temporal convolutions. 2019 IEEE International Conference on Image Processing (ICIP), pp. 1830­1834, 2019.
Swathikiran Sudhakaran, S. Escalera, and O. Lanz. Gate-shift networks for video action recognition. 2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1099­1108, 2020.
10

Published as a conference paper at ICLR 2021
Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. 2015 IEEE International Conference on Computer Vision (ICCV), pp. 4489­4497, 2015.
Du Tran, Hong xiu Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6450­6459, 2018.
Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification with channelseparated convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5551­5560, 2019.
Heng Wang, Du Tran, L. Torresani, and Matt Feiszli. Video modeling with correlation networks. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 349­358, 2020.
L. Wang, Yuanjun Xiong, Zhe Wang, Y. Qiao, D. Lin, X. Tang, and L. Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016.
X. Wang and A. Gupta. Videos as space-time region graphs. In ECCV, 2018.
Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7794­7803, 2018.
Junwu Weng, D. Luo, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Xudong Jiang, and J. Yuan. Temporal distinct representation learning for action recognition. ArXiv, abs/2007.07626, 2020.
Saining Xie, C. Sun, J. Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In ECCV, 2018.
Mohammadreza Zolfaghari, K. Singh, and T. Brox. Eco: Efficient convolutional network for online video understanding. ArXiv, abs/1804.09066, 2018.
A APPENDIX
A.1 MORE TRAINING DETAILS
We use SGD with momentum 0.9 and cosine learning rate schedule (Loshchilov & Hutter, 2017) to train the entire network. The first 10 epochs are used for warm-up (Goyal et al., 2017a) to overcome early optimization difficulty. For kinetics, the batch, total epochs, initial learning rate, dropout and weight decay are set to 64, 110, 0.01, 0.5 and 1e-4 respectively. All these hyper-parameters are set to 64, 45, 0.02, 0.3 and 5e-4 respectively for Something-Something.
A.2 TENSOR EXCITATION MECHANISM
The implementation of our Tensor Excitation is shown in Figure 4. Different from the SE module, we use tensor separable convolution in the TE mechanism. Moreover, when obtaining the spatial attention, we squeeze the temporal dimension and perform spatial tensor separable convolution, because temporal information is insignificant for spatial attention and vice versa. We add the Batch Normalization (BN) layer for better optimization.
A.3 RESULTS ON UCF101 AND HMDB51
To verify the generation ability of our CT-Net on smaller datasets, we conduct transfer learning experiments from Kinetics400 to UCF101 (Soomro et al., 2012) and HMDB-51 (Kuehne et al., 2011). We test CT-Net with 16 input frames and evaluate it over three splits and report the averaged results. As shown in Table 5, our CT-Net16f achieves competitive performance when compared with the recent methods, which demonstrates the generation ability of our CT-Net.
11

Published as a conference paper at ICLR 2021

Figure 4: The implementation of our Tensor Excitation (TE) mechanism.

Method
C3D(Tran et al., 2015) I3D(Carreira & Zisserman, 2017) ECO(Zolfaghari et al., 2018) TSN(Wang et al., 2016) TSM(Lin et al., 2019) STM(Jiang et al., 2019) Our CT-Net

Backbone
3D VGG-11 3D Inception Inception+3D R18 Inception 2D R50 2D R50 2D R50

Pretrain
Sports-1M ImageNet+Kinetics ImageNet+Kinetics ImageNet+Kinetics ImageNet+Kinetics ImageNet+Kinetics ImageNet+Kinetics

UCF101
82.3 95.1 94.8 91.1 94.5 96.2 96.2

HMDB51
51.6 74.3 72.4
70.7 72.2 73.2

Table 5: Comparison results on UCF101 and HMDB51.

A.4 MORE RESULTS ON SOMETHING-SOMETHING V1&V2
Table 6 shows more results on Something-Something V1&V2. We train CT-Net with a different number of input frames and then test these models with different sampling strategies. We average the prediction scores obtained from the previous models to evaluate the ensemble models. With more input frames, the corresponding accuracy becomes higher. As for the reason that CT-Net24f is worse than CT-Net16f , we argue that is because the model is hard to optimize with too many input frames. Sampling more clips or more crops also boosts performance. Moreover, our ensemble models gain the state-of-the-art top-1 accuracy of 56.6%(68.3%) on Something-Something V1(V2).

A.5 MORE ABLATION STUDIES ON MINI-KINETICS AND SOMETHING-SOMETHING V2
To verify the effectiveness of our module comprehensively, we also conduct experiments in MiniKinetics and Something-Something V2 and report the multi-clip accuracy and single-clip accuracy respectively. Mini-Kinetics covers 200 action classes and is a subset of Kinetics-400, while Something-Something V2 covers the same action classes as Something-Something V1 but contains more videos. As shown in Table 7, the performance trend of different modules is similar to that shown in Table 2a. Since Mini-Kinetics does not highly depend on temporal modeling, the gap becomes smaller but still demonstrates the effectiveness of our CT-Module.

A.6 ADAPTING DIFFERENT PRE-TRAINED IMAGENET ARCHITECTURES AS CT-NET
In fact, by directly replacing the 3×3 convolution with our CT-Module, we can easily adapt different pre-trained ImageNet architectures as CT-Net. Table 8 shows that it is also sensible to use InceptionV3 as the backbone. We believe that through more elaborate design, our CT-Net based on different backbones can achieve comparable performance.

A.7 VALIDATION PLOT
In Figure 5, we plot the accuracy vs per-clip GFLOPs on Kinetics-400. It reveals that our CT-Net achieves a better trade-off than most of the existing methods on Kinetics-400.

12

Published as a conference paper at ICLR 2021

Method Our CT-Net Our CT-NetEN

#Frame
8 12 16 24 8×1×2 12×1×2 16×1×2 24×1×2 8×3×2 12×3×2 16×3×2 24×3×2 8 + 16 (8 + 12 + 16 + 24) × 1 × 2 (8 + 12 + 16 + 24) × 1 × 2 (8 + 12 + 16 + 24) × 3 × 2

GFLOPs
37 56 75 112 75 112 151 224 224 336 447 672 112 280 560 1679

#Param 21.0M 21.0M 21.0M 83.8M

SomethingV1 Top-1 Top-5
50.1 78.8 52.1 80.0 52.5 80.9 52.5 80.9 51.6 79.7 52.8 80.6 53.2 81.3 52.9 81.3 51.7 80.1 53.0 81.1 53.4 81.7 53.6 81.6 54.4 82.0 56.6 83.9 56.6 84.0 56.6 83.9

SomethingV2 Top-1 Top-5
62.5 87.7 63.9 88.7 64.5 89.3 64.6 89.1 63.5 88.5 64.6 89.3 65.2 89.7 65.0 89.3 63.9 88.8 65.3 89.6 65.9 90.1 65.5 89.8 66.2 90.4 67.8 91.1 67.8 91.3 68.3 91.3

Table 6: More results on Something-Something V1&V2.

Method
C3D-Module (Tran et al., 2015) R(2+1)D-Module (Tran et al., 2018) CSN-Module (Tran et al., 2019) Our CT-Module

Backbone
2D R50 2D R50 2D R50 2D R50

GFLOPs
59.9 45.8 35.6 36.3

Mini-Kinetics Top-1 Top-5
77.5 93.0 77.8 93.2 77.6 93.2 78.0 93.6

SomethingV2 Top-1 Top-5
59.1 85.5 60.0 86.0 59.5 86.0 60.3 86.4

Table 7: More ablation studies on Mini-Kinetics and Something-Something V2.

Method
Baseline (TSN) Our CT-Net Baseline (TSN) Our CT-Net

Backbone
2D ResNet-50 2D ResNet-50 InceptionV3 InceptionV3

GFLOPs
43.0 37.3 45.8 43.9

#Param.(M)
23.9 21.0 22.1 20.9

Top-1
16.9 50.1 18.3 47.2

Top-5
42.0 78.8 43.9 76.1

Table 8: Adapting different pre-trained ImageNet architectures as CT-Net.

Figure 5: Accuracy vs per-clip GFLOPs on Kinetics-400. 13

