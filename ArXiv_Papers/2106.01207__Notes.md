
# Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning

[arXiv](https://arxiv.org/abs/2106.01207), [PDF](https://arxiv.org/pdf/2106.01207.pdf)

## Authors

- Forrest Davis
- Marten van Schijndel

## Abstract

A growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models. Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations. We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge. We tested this claim by exploring a single phenomenon in four languages: English, Chinese, Spanish, and Italian. While human behavior has been found to be similar across languages, we find cross-linguistic variation in model behavior. We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints, uncovering otherwise dormant linguistic knowledge in models. Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking, with mismatches in either producing non-human-like behavior.

## Comments

Proceedings of 59th Annual Meeting of the Association for Computational Linguistics (ACL 2021)

## Source Code

Official Code

- [https://github.com/forrestdavis/ImplicitCausality](https://github.com/forrestdavis/ImplicitCausality)

Community Code

- [https://paperswithcode.com/paper/uncovering-constraint-based-behavior-in](https://paperswithcode.com/paper/uncovering-constraint-based-behavior-in)

## Bibtex

```tex
@misc{davis2021uncovering,
      title={Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning}, 
      author={Forrest Davis and Marten van Schijndel},
      year={2021},
      eprint={2106.01207},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

